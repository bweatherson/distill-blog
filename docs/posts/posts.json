[
  {
    "path": "posts/2021-11-16-permissivism-and-symmetric-games/",
    "title": "Epistemic Permissiveness and Symmetric Games",
    "description": "Permissivism in epistemology is a family of theses, each of which says that rationality is compatible with a number of distinct attitudes. This paper ar- gues that thinking about symmetric games gives us new reason to believe in permissivism. In some finite games, if permissivism is false then we have to think that a player is more likely to take one option rather than another, even though each have the same expected return given that player’s credences. And in some infinite games, if permissivism is false there is no rational way to play the game, although intuitively the games could be rationally played. The latter set of arguments rely on the recent discovery that there are symmetric games with only asymmetric equilibria. It was long known that there are symmetric games with no pure strategy symmetric equilibria; the surprising new discovery is that there are symmetric games with asymmetric equilibria, but no symmetric equilibria involving either mixed or pure strategies.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2021-11-16",
    "categories": [
      "games and decisions",
      "epistemic permissiveness",
      "epistemic uniqueness",
      "unpublished"
    ],
    "contents": "\n\nContents\nIntroduction\nChicken\nElections\nObjections\nConclusion\n\nIntroduction\nPermissivism in epistemology is a family of theses, each of which says that rationality is compatible with a number of distinct attitudes. This paper argues that thinking about symmetric games gives us new reason to believe in permissivism. In some finite games, if permissivism is false then we have to think that a player is more likely to take one option rather than another, even though each have the same expected return given that player’s credences. And in some infinite games, if permissivism is false there is no rational way to play the game, although intuitively the games could be rationally played. The latter set of arguments rely on the recent discovery that there are symmetric games with only asymmetric equilibria. It was long known that there are symmetric games with no pure strategy symmetric equilibria; the surprising new discovery is that there are symmetric games with asymmetric equilibria, but no symmetric equilibria involving either mixed or pure strategies.\n\nImage by United Nations via Creative Commons.\nThe permissivist theses that have been the focus on recent philosophical attention vary along two dimensions.1\nThe first dimension concerns what we hold fixed when we say that multiple attitudes are rationally permissible. The weakest possible theory just says that two people with distinct attitudes may both be rational. No one really denies this. The strongest theory says that holding every fact about a situation constant, there are two possible rational attitudes. In between we have a number of interesting theses. For instance, we can ask whether multiple attitudes are rationally compatible holding constant the evidence the believer has. And we can ask whether multiple attitudes are rationally compatible holding constant both the evidence and the believer’s prior doxastic states. A classic form of subjective Bayesianism answers yes to the first question, and no to the second. The focus here will be on a thesis very close to the strongest one - whether two people who are alike in all qualitative respects can rationally have different attitudes.\nThe second dimension concerns whether the folks holding these distinct attitudes can acknowledge that rival attitudes are rational. Some permissivists hold that distinct attitudes can be rationally compatible with holding fixed evidence or priors or whatever, but the people holding these attitudes cannot acknowledge that attitudes other than theirs are rational. The argument I’m going to offer draws the stronger conclusion that multiple responses are rational, and rational thinkers can acknowledge that alternative responses to theirs are rational.\nThe negation of a permissive thesis is a Uniqueness thesis. The name suggests that there is precisely one rational attitude to take in a specified situation, but we’ll interpret it as the view that there is at most one rational attitude to take so as to ensure each Uniqueness thesis is the negation of a permissive thesis. As with Permissivism, Uniqueness comes in weaker and stronger varieties. The strongest version is the literally incredible view that there is only one doxastic attitude that is rationally permissible. (Presumably it is the view that is certain of all and only truths.) The weakest version, which is still interesting, is that once a situation is described in full detail, there is precisely one doxastic attitude that is rationally permissible. Everyone holds that, since the normative supervenes on the descriptive, that describing a situation in full detail fixes which doxastic states are rationally permissible. The Uniqueness theorist adds the claim that there are 0 or 1 such states.2\nAs I said, I’m interested in defending a strong, but not maximally strong, version of Permissivism. Equivalently, I’m interested in attacking a weak, but not quite maximally weak, version of Uniqueness. Here is the version of Uniqueness that I want to reject.\nFor all kinds \\(K\\), and evidence \\(E\\), there is at most one credal distribution that is rational for an agent of kind \\(K\\) with evidence \\(E\\).\nI mean to be fairly liberal over what counts as a `kind’, so if any such theory is false, we have proven that a lot of Uniqueness theories are false. So a kind could be a prior probability function, a set of privileged predicates that one uses for induction, an attitude to inductive risk, and so on. The only assumption I’ll make is that kinds are shareable; so there is no such thing as the kind Being John Malkovich. In principle we could say that what evidence one has is part of one’s kind, but the discussion below will be clearer if we separate out evidence and kinds.\nThe next two sections set out two symmetric games where Uniqueness leads to surprising results. I think the results are surprising, indeed implausible, enough that we should reject Uniqueness. But even if one doesn’t accept that, it’s still interesting to see what Uniqueness entails. In all cases we’ll assume that the following things are common knowledge among players of the game.\nEach player is rational, so they form rational credences, and maximise expected utility.\nEach player is of kind \\(K\\).\nEach player knows the payout structure of the game.\nEach player is self-aware; they know their own credences.\nIf Uniqueness is true, then each player knows that Uniqueness is true.\nEach player has no other relevant evidence about the game or the players.\nLet evidence \\(E\\), unless otherwise stated, be the evidence specified by those six bullet points. We’ll be continually thinking about propositions of the form:\nA rational agent of kind \\(K\\) with evidence \\(E\\) will perform action \\(\\varphi\\) in this game.\nSince the games are symmetric, we don’t have to ask about which player will make this move; we can think abstractly about what any rational player would do.\nChicken\nSome finite symmetric games have no symmetric pure-strategy equilibria. One notable example is Chicken. Here’s a version of Chicken that will do.3\n\nStay\nSwerve\nStay\n-100, -100\n1, -1\nSwerve\n-1, 1\n0, 0\nThe symmetric pure-strategy pairs \\(\\langle\\) Stay, Stay \\(\\rangle\\) and \\(\\langle\\) Swerve, Swerve \\(\\rangle\\) are not equilibria; in each case both parties have an incentive to defect. But the game does have a symmetric equilibria. It is that both players play the mixed strategy of Stay with probability 0.01, and Swerve with probability 0.99.\nLet Swerve be the proposition that a rational player of kind \\(K\\) with evidence \\(E\\) will Swerve. And call the players Row and Column. Given our assumptions so far, plus Uniqueness, we can prove that Row’s credence in Swerve is 0.99. Here’s the proof.\nLet \\(x\\) be Row’s credence in Swerve.\nBy self-awareness, Row knows that \\(x\\) is her credence in Swerve.\nSince she knows she is rational, Row can infer that \\(x\\) is a rational credence in Swerve.\nSince she knows Uniqueness is true, Row can infer that \\(x\\) is the only rational credence in Swerve.\nSince she knows Column is rational, she can infer that \\(x\\) is Column’s credence in Swerve.\nSince all the assumptions so far are common knowledge, she can infer that Column knows that \\(x\\) is her credence in Swerve.\nIf \\(x = 1\\), then Row can infer that it is rational for Column to Swerve, while knowing that Row will also Swerve. But this is impossible, since if Column knows Row will Swerve, it is best to Stay. So \\(x \\neq 1\\).\nIf \\(x = 0\\), then Row can infer that it is rational for Column to Stay, while knowing that Row will also Stay. But this is impossible, since if Column knows Row will Stay, it is best to Swerve. So \\(x \\neq 0\\).\nSo \\(0 < x < 1\\).\nSince Row knows Column’s credence that Row will Swerve (whatever it is), and Row knows Column is rational, but Row does not know what Column will do, it must be that Column is indifferent between Stay and Swerve given her credences about what Row will do.\nColumn is indifferent between Stay and Swerve only if her credence that Row will Swerve is 0.99. (This is a reasonably simple bit of algebra to prove.)\nSo from 10 and 11, Column’s credence that Row will Swerve is 0.99.\nBy (known) Uniqueness, it follows that the only rational credence in Swerve is 0.99.\nSo since Row is rational, it follows that \\(x = 0.99\\).\nNow there is nothing inconsistent in this reasoning. In a sense, it is purely textbook reasoning. But the conclusion is deeply puzzling. We’ve proven that Column is indifferent between her two options. And we’ve proven that Row knows this. But we’ve also proven that Row thinks it is 99 times more likely that Column will choose one of the options over the other. Why is that? It isn’t because there is more reason to do one than the other; given Column’s attitudes, the options are equally balanced. It is purely because Uniqueness pushes us to a symmetric equilibrium, and this is the only symmetric equilibrium.\nI don’t think this result will convince many devotees of Uniqueness to give up their view. It’s not a particularly novel claim that rational players will end up at the unique Nash equilibrium of a game. And to be sure, if this game was being played repeatedly, much weaker assumptions entail that each player should Stay 1% of the time, with those Stays being randomly distributed across the plays of the game. But it is still odd, at least to me, to see the same conclusion drawn in the single shot game, where each player is known to be indifferent between their choices.\nThe next case is I think much worse for Uniqueness.\nElections\nThe cases that really inspired this paper come from some recent work on this rather old question,\n\nIf a symmetric game has an equilibrium, does it have a symmetric equilibrium?\n\nOver the years, a positive answer was given to various restricted forms of that question. But recently it has been proven that the answer to the general question is no. Mark Fey (2012) showed that there are symmetric positive-sum two-player games that have only asymmetric equilibria.4 And Dimitrios Xefteris (2015) showed that there is a symmetric three-player zero-sum that has only asymmetric equilibria. In fact, he showed that a very familiar game, a version of a Hotelling–Downs model of elections, has this property. Here’s how he describes the game.\n\nConsider a unit mass of voters. Each voter is characterized by her ideal policy. We assume that the ideal policies of the voters are uniformly distributed in \\([0, 1]\\). We moreover assume that three candidates \\(A\\), \\(B\\) and \\(C\\) compete for a single office. Each candidate \\(J \\in \\{A, B, C\\}\\) announces a policy \\(s_J \\in [0, 1]\\) and each voter votes for the candidate who announced the policy platform which is nearest to her ideal policy. If a voter is indifferent between two or among all three candidates she evenly splits her vote betweenamong them. A candidate \\(J \\in \\{A, B, C\\}\\) gets a payoff equal to one if she receives a vote-share strictly larger than the vote-share of each of the two other candidates. If two candidates tie in the first place each gets a payoff equal to one half. If all three candidates receive the same vote-shares then each gets a payoff equal to one third. In all other cases a candidate gets a payoff equal to zero. (Xefteris 2015, 124)\n\nIt is clear that there is no symmetric pure-strategy equilibrium here. If all candidates announced the same policy, everyone would get a payoff of \\(\\frac{1}{3}\\). But no matter what that strategy is, if \\(B\\) and \\(C\\) announce the same policy, then \\(A\\) has a winning move available.\nWhat’s more surprising, and what Xefteris proves, is that there is no symmetric mixed strategy equilibria either. Again, in such an equilibrium, any player would have a payoff of \\(\\frac{1}{3}\\). Very roughly, the proof that no such equilibrium exists is that random deviations from the equilibrium are as likely to lead to winning as losing, so they have a payoff of roughly \\(\\frac{1}{2}\\). So there is no incentive to stay in equilibrium. So no symmetric equilibrium exists.\nBut if Uniqueness is true, and if it is possible to play the game under circumstances of common knowledge of rationality and kind, then there must be a symmetric equilibrium. The reason is that a version of the proof of the previous section still goes through. Whatever credal distribution \\(A\\) has over \\(B\\)’s possible policies, \\(A\\) must also have over \\(C\\)’s policies (since they both adopt the uniquely rational strategy), and she must know that \\(B\\) and \\(C\\) each have over each other’s policies and over hers, and these distributions must be consistent with each player having these credal distributions while thinking that the other players have the same distributions and are maximising expected utility. In other words, the assumptions we’ve made about the game imply that \\(A\\) has a credal distribution \\(F\\) over \\(B\\)’s possible policies only if the mixed strategy triple where each player adopts \\(F\\) as their mixed strategy is itself an equilibrium. And that would be a symmetric equilibrium. But no symmetric equilibrium exists.\nBut if we drop Uniqueness, it is possible to keep all the other assumptions. As Xefteris points out, the game has asymmetric equilibria. Here is one possible model for the game.\n\\(A\\) plays 0.6 (and wins), \\(B\\) and \\(C\\) each play 0.4 (and lose).\nEach player has a correct belief about what the other players will play.\nBut both \\(B\\) and \\(C\\) know they cannot win given the other player’s moves, so they pick a move completely arbitrarily.\nFurther, each player has a correct belief about why each player makes the move they make.\nThis is the coherent equilibria that Xefteris describes, but note that it is rather implausible that we’d end up there in a real-life version of the game. It requires two of the players to know that one of the other players will be indifferent between their options, but from this draw a correct inference about what they will do. That’s not particularly plausible. So let’s note that there is a somewhat more plausible way to get all three players to make those moves.\n\\(A\\) plays 0.6 (and wins), \\(B\\) and \\(C\\) each play 0.4 (and lose).\nThe only two rational plays are 0.4 and 0.6, but each of them is permissible.\nIn any world that a player believes to be actual, or a player believes another player believes to be actual, or a player believes another player believes another player believes to be actual, etc., the following two conditions hold.\nIf a player plays 0.6, they believe the other two players will play 0.4, and hence playing 0.6 is a winning move.\nIf a player plays 0.4, they believe the other two players will play 0.6, and hence playing 0.4 is a winning move.\nThe main difference between this model and Xefteris’s is that it allows that players have false beliefs. But why shouldn’t they have false beliefs? All they know is that the other players are rational, and rationality (we’re assuming) does not settle a unique verdict for what players will do.5 So I think this strategy set, where the players have rational (but false) beliefs about the other players, is more useful to think about.\nObjections\nThe reductio arguments here have all assumed not just that Uniqueness is true, but that the players know that it is true. What happens if we drop that assumption, and consider the possibility that Uniqueness is true but unknowable?\nThis possibility is a little uncomfortable for philosophical defenders of Uniqueness. If the players in these games do not know that Uniqueness is true, then neither do the authors writing about Uniqueness. And now we have to worry about whether it is permissible to assert in print that Uniqueness is true. I wouldn’t make too much of this though. It is unlikely that a knowledge norm governs assertion in philosophical journals.\nThe bigger worry here is that one key argument for Uniqueness seems to require that Uniqueness is knowable. A number of recent authors have argued that Uniqueness best explains our practice of deferring to rational people. For instance, Greco and Hedden use this principle in their argument for Uniqueness.\n\nIf agent \\(S_1\\) judges that \\(S_2\\)’s belief that \\(P\\) is rational and that \\(S_1\\) does not have relevant evidence that \\(S_2\\) lacks, then \\(S_1\\) defers to \\(S_2\\)’s belief that \\(P\\). (Greco and Hedden 2016, 373)\n\nSimilar kinds of arguments are made by Dogramaci (2012) and Horowitz (2014). But the principle looks rather dubious in the case of these games. Imagine that \\(A\\) forms a belief (we’ll come back to how) that \\(B\\) believes that a rational thing to do in the Xefteris game is to play 0.6, and so she will play 0.6. She should judge that belief to be rational; as we saw it is fully defensible. But although she does not believe that she has evidence that \\(B\\) lacks, she should not defer to it. At least, she should not act as if she defers to it; believing that \\(B\\) will play 0.6 is a reason to play something other than 0.6.\nAnd that’s the general case for these symmetric games with only asymmetric equilibria. Believing that someone else is at an equilibrium point is a reason to not copy them. If it were not a reason to not copy them, then the strategy profile where each player plays the same thing would be a symmetric equilibrium. So thinking about these games doesn’t just give us a rebutting defeater for Uniqueness, as described in the previous two sections, but an undercutting defeater, since they also tell against a premise that has been central to recent defences of Uniqueness.\nI think there is a somewhat better move available to the Uniqueness theorist. They could simply deny that the Xefteris game, as I’ve described it, is even possible.6 This perhaps isn’t as surprising as it might seem.\nNote two things about the Xefteris game. First, it is an infinite game in the sense that each player has infinitely many choices. It turns out this matters to the proof that there is no symmetric equilibrium to the game. Second, we are assuming it is common knowledge, and hence true, that the players are perfectly rational. Third, we are assuming that perfect rationality entails that people will not choose one option when there is a better option available. When you put those three things together, some things that do not look obviously inconsistent turn out to be impossible. Here’s one example of that.\n\n\\(A\\) and \\(B\\) are playing a game. Each picks a real number in the open interval (0, 1). They each receive a payoff equal to the average of the two numbers picked.\n\nFor any number that either player picks, there is a better option available. It is always better to pick \\(\\frac{x+1}{2}\\) than \\(x\\), for example. So it is impossible that each player knows the other is rational, and that rationality means never picking one option when a better option is available.\nSo the Uniqueness theorist could say that the same thing is going on in the Xefteris game. Some infinitely games cannot be played by rational actors (understood as people who never choose sub-optimal options); this is one of them. But if this is all the Uniqueness theorist says, it is not a well motivated response. We can say why it is impossible to rationally play games like the open interval game; the options get better without end. But that isn’t true in the Xefteris game. The only thing that makes the game seem impossible is the Uniqueness assumption. People who reject Uniqueness can easily describe how the Xefteris game can be played by rational players. Simply saying that it is impossible, without any motivation or explanation for this other than Uniqueness itself, feels like an implausible move.\nConclusion\nIf Uniqueness is true, then the following thing happens in games between people who know each other to be the same kind, and to be rational. When someone forms a belief about what the other person will do, they can infer that this is a rational way to play the game given knowledge that everyone else will do the same thing. But sometimes this is a very unintuitive inference. In Chicken, it implies that we should have asymmetric attitudes to someone who is facing a choice between two options with equal expected value. In the election game Xefteris describes, a game that feels consistent turns out to be impossible.\nI think the conclusion to draw from these cases of symmetric interactions this is that Uniqueness is false, and hence permissivism is true. Sometimes in such an interaction one simply has to form a belief about the other player, knowing they may well form a different belief about you. Indeed, sometimes only coherent way to form a belief about the other player is to believe that they will form a different belief about you. And that means giving up on Uniqueness.\n\n\n\nBernheim, B. Douglas. 1984. “Rationalizable Strategic Behavior.” Econometrica 52 (4): 1007–28. https://doi.org/10.2307/1911196.\n\n\nDogramaci, Sinan. 2012. “Reverse Engineering Epistemic Evaluations.” Philosophy and Phenomenological Research 84 (3): 513–30. https://doi.org/10.1111/j.1933-1592.2011.00566.x.\n\n\nFey, Mark. 2012. “Symmetric Games with Only Asymmetric Equilibria.” Games and Economic Behavior 75 (1): 424–27. https://doi.org/10.1016/j.geb.2011.09.008.\n\n\nGreco, Daniel, and Brian Hedden. 2016. “Uniqueness and Metaepistemology.” The Journal of Philosophy 113 (8): 365–95. https://doi.org/10.1111/phc3.12318.\n\n\nHorowitz, Sophie. 2014. “Immoderately Rational.” Philosohical Studies 167 (1): 41–56. https://doi.org/10.1007/s11098-013-0231-6.\n\n\nKopec, Matthew, and Michael G. Titelbaum. 2016. “The Uniqueness Thesis.” Philosophy Compass 11 (4): 189–200. https://doi.org/10.1111/phc3.12318.\n\n\nPearce, David G. 1984. “Rationalizable Strategic Behavior and the Problem of Perfection.” Econometrica 52 (4): 1029–50. https://doi.org/10.2307/1911197.\n\n\nXefteris, Dimitrios. 2015. “Symmetric Zero-Sum Games with Only Asymmetric Equilibria.” Games and Economic Behavior 89 (1): 122–25. https://doi.org/10.1016/j.geb.2014.12.001.\n\n\nFor a much more thorough introduction to the debate, and especially into the varieties of permissivist theses, see Kopec and Titelbaum (2016). Much of the setup here, including for example the use of the subjective Bayesian as an illustrative example, is from that paper.↩︎\nTwo small caveats here. Uniqueness theorists may say that it is permissible to not have any attitude towards a proposition. So it is consistent with Uniqueness, as I understand it, to say that it could be both rational to have credence 0.6 in \\(p\\), and rational to not have any attitude towards \\(p\\). What the Uniqueness theorist denies is that there are distinct credences towards \\(p\\) one could adopt, each of which would be rational. And of course the Uniqueness theorist thinks it could be rational to have credence 0.6 in \\(p\\) and 0.7 in \\(q\\). When I say Uniqueness implies that just one state is rational, I mean to quantify over complete credal states, not attitudes towards single propositions.↩︎\nWhen presenting games in this format, I’ll write Row’s payout first, then Column’s payout. So the top right cell here, for example, says that if Row plays Stay and Column plays Swerve, then Row gets a payout of 1, and Column gets a payout of -1. All payouts are in utils unless otherwise stated. For people unfamiliar with it, the backstory of Chicken is that the players are in cars driving towards each other on a one-lane road. They can stay on the road, possibly winning points for courage and possibly dying, or swerve off.↩︎\nFey also includes a nice chronology of some of the proofs of positive answers to restricted forms of the question.↩︎\nTo use the game-theory jargon, Xefteris describes a Nash equilibrium of the game, but what I’ve described is a a rationalizable strategy triple (Bernheim 1984; Pearce 1984). If Uniqueness is true, then strictly speaking any rationalizable strategy pair for a symmetric game is a Nash equilibrium.↩︎\nThis is really just a response to the argument based on that game; I think they just have to say that in Chicken a rational player will rationally think the other player is more likely to make one of the two choices with equal expected payoffs.↩︎\n",
    "preview": "posts/2021-11-16-permissivism-and-symmetric-games/voting.jpg",
    "last_modified": "2021-11-17T09:30:53-05:00",
    "input_file": "permissivism-and-symmetric-games.knit.md"
  },
  {
    "path": "posts/2021-11-15-a-dynamic-war-on-edt/",
    "title": "A Dynamic WAR on EDT",
    "description": "A common argument for favoring Evidential Decision Theory (EDT) over Causal Decision Theory (CDT) is that EDT has predictably higher expected returns in Newcomb Problems. But this doesn't show much. For almost any pair of theories you can come up with cases where one does, on average, better than the other. Here I describe three cases involving dynamic choice where EDT predictably does worse than CDT.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2021-11-15",
    "categories": [
      "games and decisions",
      "unpublished"
    ],
    "contents": "\n\nContents\nExample One - Split Newcomb\nExample Two - Coins and Signals\nExample Three - Coins and Newcomb\nWhy The Examples Matter\n\nHere is a common form of argument against causal decision theory, as described by Ahmed and Price (2012). (I’ve slightly changed some of the wording, but otherwise this argument is taken directly from page 16 of their paper.)\nIn Newcomb problems, the average returns to one-boxing exceed that to two-boxing.\nEveryone can see that (1) is true.\nTherefore one-boxing foreseeably does better than two-boxing. (by 1, 2)\nTherefore Causal Decision Theory (CDT) is committed to the foreseeably worse option for anyone facing Newcomb’s problem.\nHere’s what they, and many other proponents of Evidential Decision Theory (EDT) say follows from 4.\n\nThe point of the argument is that if everyone knows that the CDT-irrational strategy will in fact do better on average than the CDT-rational strategy, then it’s rational to play the CDT-irrational strategy. (Ahmed and Price 2012, 17)\n\nThis is what Lewis (1981) called the “Why Ain’cha Rich” argument, and what following Bales (2018) I’ll call the WAR argument. I’m going to argue this last step of the WAR argument doesn’t follow. Or, at the very least, that proponents of EDT cannot coherently say that it follows. For there are several cases where EDT foreseeably does worse than CDT. This paper will go over three of them.\nExample One - Split Newcomb\nThis game takes place over three rounds. I’m calling the examples games from now on because, following Harper (1986), I take it is best to think of the original Newcomb Problem as a game. The demon is an actor who considers what the other player is trying to do, and has incentives that explain their decision given their beliefs about the other player. In this case the incentive is to correctly predict what the other player does. That sounds like the demon is a player in a traditional game-theoretic setup. So I’ll make it explicit that that’s what the demon is. With that said, here is the three stage game.\nAt stage one, the human player chooses In or Out. If they choose Out, player gets 5 and demon gets 1. If they choose In, we move onto stage two.\nAt stage two, demon chooses Left or Right, and this choice is announced.\nAt stage three demon and the player simultaneously choose either Up or Down. Demon is very good at predicting what player’s choices will be, and indeed at stage two they were already very good at making such a prediction. And demon wants to use these predictive powers to get as high a payoff as possible, and this is common knowledge.\nIf demon chose Left at stage two, here is the payoff table at stage three. Player chooses the row, demon chooses the column, so between them they determine a cell, and in that cell I’ll list player’s payoff followed by demon’s payoff.\n\nUp\nDown\nUp\n(2, 1)\n(4, 0)\nDown\n(1, 0)\n(3, 3)\nIf demon chose Right at stage two, here is the payoff table at stage three.\n\nUp\nDown\nUp\n(12, 4)\n(14, 0)\nDown\n(11, 0)\n(13, 2)\nIn both games, the CDTer will choose Up, and the EDTer will choose Down. These are both fairly straightforward Newcomb Problems from players perspective, after all. In both games Up causally dominates Down, but Down will get a higher return if you assume, as we did assume, that demon mostly makes correct predictions.\nSo at stage two, demon will know that if the person facing them is an EDTer, they will get a return of 3 from Left and 2 from Right. (They’ll end up in the Down-Down cell either way.) So they will rationally choose Left. On the other hand, if the person facing them is a CDTer, they will get a return of 1 from Left and 4 from Right. (They’ll end up in the Up-Up cell either way.) So they will rationally choose Right. And everything in this paragraph can be deduced by a rational player at stage 1.\nSo at stage one, a CDTer will know that if they play In, they expect to get 12 (the game will go Right then Up-Up), and if they play Out, they know they’ll get 5. So they’ll play In. But an EDTer will know that if they play In, they expect to get 4 (the game will go Left then Down-Down), and if they play Out, they know they’ll get 5. So they’ll play Out.\nThe result of all this is that the CDTer will get 12, and the EDTer will get 5. So the CDTer will predictably do better than the EDTer. Indeed, the EDTer will voluntarily choose at stage one to take a lower payout than the CDTer ends up with. This seems bad for EDT, at least if we think that predictably ending up with a lower outcome is bad.\nNow you might object that this is because at stage two the demon chooses to treat the EDTer differently to how they treat the CDTer. I don’t really agree for two reasons, though I’m not sure either of these reasons work. (Hence the second and third examples that are about to come.) One is that the demon isn’t trying to harm the EDTer; they are just trying to maximise their return. It so happens that EDT is such an impermissive theory that it doesn’t allow for any flexibility, and the demon, knowing this, is forced to take choices that are bad for EDT (and it turns out for the demon). But this isn’t the demon’s fault; it’s the fault of EDT being so impermissive. The other reason is that the demon does not in fact make any choices that hurt the EDTer. The EDTer should expect that the demon will in fact make such choices, in response to their theory, but that’s not quite the same thing. The only player who moves at all in the EDT version of the game is the player. So it’s a little hard to say this is just a case where the EDTer is harmed by the demon’s malicious choices.\nI think those responses work, but I’m not completely sure that they do. So let’s look at a second example, one where the demon doesn’t have these variable payouts.\nExample Two - Coins and Signals\nThis example is a version of a signaling game of the kind introduced by Lewis (1969). And in particular it’s a version of the broadly adversarial kinds of signaling games that are central to the plot of Cho and Kreps (1987). Again, it will involve three stages.\nAt the first stage a fair coin is flipped, and the result shown to the human player, but not the demon.\nAt the second stage, the human will choose Up or Down, and the choice will be publicly announced.\nAt the third stage, the demon will try to guess what the coin showed. The demon knows the payoff table I’m about to show you, and is arbitrarily good at predicting the human’s choice dispositions. That is, the demon can make accurate predictions of the form “If Heads, the human will make this choice, and if Tails, they will make that choice.”\nThe payoffs to each player are a function of what happens at each of the three steps, and are given by the following table.\nCoin\nHuman\nDemon\nHuman Payoff\nDemon Payoff\nH\nU\nH\n40\n1\nH\nU\nT\n400\n0\nH\nD\nH\n0\n1\nH\nD\nT\n0\n0\nT\nU\nH\n40\n0\nT\nU\nT\n28\n1\nT\nD\nH\n28\n0\nT\nD\nT\n36\n1\nOr as a picture,\n\nThe lower case letters are for the players, ‘h’ for human and ‘d’ for demon. The upper case letters are for moves. The dotted lines mean that the demon doesn’t know which of these nodes is actual when a choice is needed.\nThe demon’s payoffs are just as you’d expect - they get rewarded iff they figure out how the coin landed. The human’s payoffs are more complicated, but the big thing to note is they get the biggest rewards if they manage to play Up while the demon makes an incorrect prediction.\nOne last thing about the demon before we analyse the game. If the demon predicts the human will do one thing if Heads and another if Tails, they will use the information from the human’s choice to make their guess about how the coin landed. But if they predict the human will say the same thing whether the coin landed Heads or Tails, they won’t know how the coin landed, and will flip their own coin to make a guess. So in that case it will be 50/50 whether the demon says Heads or Tails.\nOnto the analysis. It should be fairly clear that if the coin lands Heads, the human should say Up. The worst possible return from Up is 40, the best possible return from Down is 0. This argument can be made a bit more rigorous, but I’ll leave that as an exercise for the reader, and just assume that that’s what both a CDTer and an EDTer would do, and hence what the demon would predict that they will do.\nSo what happens if the coin lands Tails? Given the demon will predict Up if Heads, we can work out the value of Up and Down if Tails to the EDTer. If they play Up, the demon will predict that, and hence the demon will flip a coin to choose Heads or Tails. So they have a 50/50 shot at getting either 40 or 28, and so their expected return is 34. If they play Down, the demon will predict that, and hence the demon will say Tails, and they will get a return of 36. Since 36 > 34, they will play Down if Tails.\nThat’s the unique solution to the game for the EDTer. They play Up if Heads, Down if Tails. The demon can figure out that they’ll do this, so will correctly guess what the coin showed. And they will get 40 if the coin landed Heads, and 36 if it landed Tails, for an expected return of 38.\nWhat should the CDTer do? Well it turns out that there are multiple solutions that are consistent with the general spirit of CDT. It’s consistent with CDT to do exactly what the EDTer does. But it’s also consistent to say Up no matter what. Let’s go over why this is consistent. The question is whether the CDTer can endorse their decision to play Up no matter what given each way the coin could land. They can clearly endorse it if the coin lands Heads; in that case Up strictly dominates Down, and the CDTer likes strictly dominant choices. What if the coin lands Tails? Well they think they’ll play Up. So they think the demon will flip a coin to guess in this situation. So they think the expected return of Up is 34 (like the EDTer thinks), and the expected return of Down is 32. The key difference here is that when working out the expected return of a non-chosen option, the CDTer does not change the expected behavior of the demon, while the EDTer does. (That’s what is needed to validate dominance reasoning.) So this CDTer will think that even if the coin lands Tails, they would do worse on average if they switched to playing Down if Tails. So it follows that they can consistently play Up either way.\nAnd if they do play this, the rewards are handsome. The demon won’t have any information about the coin, so the demon will flip their own coin. So lines 1, 2, 5 and 6 of the table are all equally likely to appear. So this CDTer is equally likely to get a return of 40, 400, 40 or 28, for an overall expected return of 127. And this is much higher than the 38 the EDTer is expected to receive. By changing the payout on line 2, we can make the gap in expected returns be arbitrarily large.\nNow you might object that while the CDTer can do better, it doesn’t follow from CDT alone that they will (on average) do better. After all, it is also consistent with CDT that the CDTer makes exactly the same plays as the EDTer. I don’t think that matters much. The point of the WAR is to refute a theory, and if the EDTer does foreseeably worse than one kind of CDTer, that should be enough to refute them. But just in case you think this objection is stronger, we’ll include one last example.\nExample Three - Coins and Newcomb\nThis is just like Example Two, with one twist. If the game goes Tails, Down, Tails, then we don’t immediately end the game and make payouts to the players. Instead we play another game, with a familiar structure. As always, demon is really good at predicting human’s play, and the human payouts are listed first in every cell.\n\nUp\nDown\nUp\n(20, 1)\n(40, 0)\nDown\n(16, 0)\n(36, 1)\nThe EDTer will think they’ll get 36 from this game, so the example will be just like Example Two. And the EDTer will play Up if Heads, Down if Tails, for an expected return of 38.\nBut the CDTer will think that if the game gets to this stage, they’ll get 20. So now they think that in the original game, Up dominates Down no matter whether the coin lands Heads or Tails. So every CDTer will play Up no matter what, and get an expected return of 127.\nWhy The Examples Matter\nI’m putting forward these examples not as part of a direct assault on EDT, but as a war on WAR. The point of the first example is that any theory whatsoever is subject to a WAR argument. That’s because for any theory whatsoever, you can construct pairs of choices like Left and Right, where the theory says to take choices that lead the demon to preferring to go Left. So for any theory whatsoever, or at least any theory that is consequentialist in the sense popularised by Hammond (1988), there is an example where the theory leads to worse returns. So any consequentialist theory is subject to an objection by WAR. It’s the paradigm of an over-generating objection.\nThere is perhaps something a bit interesting about the second example, though it isn’t a problem especially for EDT. What makes the second example work is that the human player is in a situation that rewards unpredictability, but EDT is decisive, and hence predictable. By ‘decisive’ here, I mean it gives a clear verdict in cases that don’t involve ties. Of course, lots of theories are decisive, and hence the behavior of a chooser following such a theory is predictable. Sometimes it is thought to be a virtue of a decision theory that it is decisive, and gives clear advice about what to do in any situation. I’m not so sure. After all, for any such theory there will be a version of example two that can be constructed so as to raise a ‘problem’ for any such theory. Is this a problem for decisive theories, or a problem for the kind of WAR argument that we see in example two? Given how bad the other WAR arguments are, and given how popular decisive theories are, we should probably say this is a problem for WAR.\nWhile this isn’t a direct assault on EDT, it is meant to raise doubts about it. If you think WAR is the intuitively strongest argument for EDT, and it turns out to be a bad argument, that’s not good news for EDT. But whether there is a way to rescue the motivations for EDT is a question for a different, and longer, paper. What I mean to have shown here is that WAR arguments are bad. They are arguments that every theory is bad, and arguments against every theory are successful arguments against no theory.\n\n\n\nAhmed, Arif, and Huw Price. 2012. “Arntzenius on ‘Why Ain’cha Rich?’.” Erkenntnis 77 (1): 15–30. https://doi.org/10.1007/s10670-011-9355-2.\n\n\nBales, Adam. 2018. “Richness and Rationality: Causal Decision Theory and the WAR Argument.” Synthese 195 (1): 259–67. https://doi.org/10.1007/s11229-016-1214-x.\n\n\nCho, In-Koo, and David M. Kreps. 1987. “Signalling Games and Stable Equilibria.” The Quarterly Journal of Economics 102 (2): 179–221. https://doi.org/10.2307/1885060.\n\n\nHammond, Peter J. 1988. “Consequentialist Foundations for Expected Utility.” Theory and Decision 25: 25–78. https://doi.org/10.1007/BF00129168.\n\n\nHarper, William. 1986. “Mixed Strategies and Ratifiability in Causal Decision Theory.” Erkenntnis 24 (1): 25–36. https://doi.org/10.1007/BF00183199.\n\n\nLewis, David. 1969. Convention: A Philosophical Study. Cambridge: Harvard University Press.\n\n\n———. 1981. “Why Ain’cha Rich?” Noûs 15 (3): 377–80. https://doi.org/10.2307/2215439.\n\n\n\n\n",
    "preview": "posts/2021-11-15-a-dynamic-war-on-edt/the-game.png",
    "last_modified": "2021-11-17T09:29:55-05:00",
    "input_file": "a-dynamic-war-on-edt.knit.md",
    "preview_width": 988,
    "preview_height": 717
  },
  {
    "path": "posts/2021-03-14-indecisive-decision-theory/",
    "title": "Indecisive Decision Theory",
    "description": "A decisive decision theory says that in any given decision problem, either one choice is best, or all the choices are equally good. I argue against this, and in favor of indecisive decision theories. The main example that is used is a game with a demon (who is good at predicting others' moves) that has multiple equilibria. It is argued that all the plausible decisive theories violate a principle of dynamic consistency that we should accept.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2021-10-14",
    "categories": [
      "games and decisions",
      "unpublished"
    ],
    "contents": "\n\nContents\nDecisiveness\nDefending The Core Premise\nEarly and Late Choices\nEvidential Decision Theory\nStag Hunt\nWhere To Next?\n\nDecisiveness\nSay a decision theory is decisive iff for any decision problem, it says either:\nThere is a uniquely best choice, and rationality requires choosing it.; or\nThere is a non-singleton set of choices each of which is tied for being best, and each of which can be permissibly chosen.\nA decision theory is decisive over binary choices iff it satisfies this condition for all decision problems where there are just two choices. Most decision theories in the literature are decisive, and of those that are not, most of them are at least decisive over binary choices. I’m going to argue that the correct decision theory, whatever it is, is indecisive. It is not, I’ll argue, even decisive over binary choices.\nThe argument turns on a pair of very similar decision problems. Each problem has the following structure. There is a human Player, and a predictor, who I’ll call Doctor. Doctor is very good, as good as the demon in Newcomb’s problem, at predicting Player’s behavior. Doctor will make two decisions. First, they will opt-in (which I’ll write as I for In), or opt-out (which I’ll write as O for out). If they opt-out, Doctor gets $1, and Player gets $100. (Assume both Doctor and Player prefer more money to less, and indeed that over these small sums there is more or less no declining marginal utility of money.) If they opt-in, this will be publicly announced, an another game will be played. Each of Doctor and Player will (independently) pick a letter: A or B. Doctor will aim to predict Player’s choice, and will be rewarded iff that prediction is correct. Here is the payout table for the possible outcomes of this game.\nHuman Pick\nDoctor Pick\nHuman Reward\nDoctor Reward\nA\nA\n$6\n$4\nA\nB\n$0\n$0\nB\nA\n$3\n$0\nB\nB\n$4\n$1\nIf you prefer this in the way we standardly present games in normal form, it looks like this, with the human as Row, doctor as Column, and in each cell the human’s payout is listed first. (All payouts are in dollars)\n\n\n\n\nA\n\n\nB\n\n\nA\n\n\n6,4\n\n\n0,0\n\n\nB\n\n\n3,0\n\n\n4,1\n\n\nDoctor is good at predictions, and prefers more money to less. So if Doctor predicts Player will choose A, they will opt-in, and also play A, getting $4. But what if they predict Player will choose B. They will get $1 either by opting-out, or by opting-in and choosing B. Since they are indifferent in this case, let’s say they will flip a coin to decide which way to go. And Player knows that this is how Doctor will decide, should Doctor predict Player will choose B. Doctor will also flip a coin to decide what prediction to make if they think Player is completely indifferent between the choices, and Player also knows this.\nNow I said there were going to be two problems. Here’s how they are created. two Players, Amsterdam and Brussels, will play the game. They have identical utility functions over money, and identical prior probability distributions about what Doctor will do conditional on each of their choices. That’s to say, they both think the probability that Doctor will be wrong is vanishingly small. But Brussels is busy and has to run to the bank, so they have to write their decision in an envelope that will be revealed, after Doctor chooses A or B, iff Doctor opts-in in the game with Brussels. Amsterdam, on the other hand, gets to see Doctor’s decision about whether to opt-in or opt-out, and then (if Doctor opts-in) has to write their decision in an envelope that will be revealed after Doctor chooses A or B. For ease of reference, call Brussels’s decision the early decision and Amsterdam’s decision the late decision.\nHere is the core philosophical premise in the argument to follow.\nThe Core Premise\nIf there is is precisely one permissible choice for Amsterdam, and one permissible choice for Brussels, then it must be the same choice. That is, if each of them is obliged to choose a particular letter, it must be the same letter. It can’t be that one is obliged to choose A, and the other obliged to choose B.\n\nI’m calling The Core Premise a premise, though in the next section I’ll offer a few arguments for it, in case you don’t think it is obviously correct. (I sort of think it is obviously correct, but I’m really not going to rely on you sharing that intuition.) And I’ll argue that any decisive theory (that meets minimal coherence standards) has to violate this constraint. Some decisive theories say Amsterdam should choose A and Brussels should choose B, but a few say the reverse. And a few (otherwise implausible) decisive theories say that Amsterdam and Brussels should do the same thing in this game, but different things in games with the same structure but slightly different payouts. In every case, a decisive theory will make some incoherent pair of recommendations, and so is mistaken.\nThe Core Premise is a conditional, but any decisive theory that denies that the choices are tied will meet the condition. So from now on in arguing against decisive theories I’ll mostly just interpret The Core Premise as saying Amsterdam and Brussels must make the same choice. The main thing to check for is that a theory doesn’t say the choices are tied. None of the theories I’ll look at will say that, but it’s an important thing to check.\nBefore we start there are four pieces of important housekeeping.\nFirst, the definition of decisiveness referred to options being tied. For the definition to be interesting, it can’t just be that options are tied if each is rationally permissible. Then a decisive theory would just be one that either says one option is mandatory or many options are permissible. To solve this problem, I’ll borrow a technique from Ruth Chang (2002). Some options are tied iff either is permissible, but this permissibility is sensitive to sweetening. That is, if options \\(X\\) and \\(Y\\) are tied, then for any positive \\(\\varepsilon\\), the agent prefers \\(X + \\varepsilon\\) to \\(Y\\). If either choice is permissible even if \\(X\\) is ‘sweetened,’ i.e.., replaced in the list of choices by \\(X + \\varepsilon\\), we’ll say they aren’t tied. My thesis then is that the correct decision theory says that sometimes there are multiple permissible options, and each of them would still be permissible if one of them was sweetened.\nSecond, there is an important term in the definition of decisiveness that I haven’t clarified: decision problem. Informally, the argument assumes that in setting out the proble facing Amsterdam and Brussels is indeed a decision problem. More formally, I’m assuming it suffices to specify a decision problem to describe the following four values.\nWhat choices Player has;\nWhat possible states of the world there are (where it is understood that the choices of Player make no causal impact on which state is actual)1;\nWhat the probability is of being in any state conditional on making each choice; and\nWhat return Player gets for each choice-state pair.\nMost recent papers on decision theory do not precisely specify what they count as a decision problem, but they seem to implicitly share this assumption, since they will often describe a vignette that settles nothing beyond these four things as a decision problem. And that’s what I did as well! You should understand this as being part of the definition of decisiveness. This implies that there are two ways to reject decisiveness.\nFirst, a theory could say that these four conditions underspecify a real decision problem. In any real situation, decision theory has a decisive verdict, but it rests on information, typically information about Player, not settled by these four values. I’ll say a theory that goes this route is intrapersonally decisive, but not interpersonally decisive.\nSecond, a theory could say that no matter how much one adds to the specification, there will be cases where the correct decision theory does not issue a verdict. Such a theory is not intrapersonally decisive. There is nothing you could add about the person to the specification of a decision problem which decides what they should do, or even which options are tied. I want to ultimately defend such a view, and this paper is a part of the defence. But it’s a proper part. Nothing I say here rules out mere interpersonal indecisiveness. That’s an argument for another day. Today, we have enough to be getting on with.\nThird, I have set up this problem quite explicitly as a game, with another player - Doctor. I don’t think this is particularly big deal, though I gather not everyone agrees. In this respect (among others) I’m following William Harper (1986), who recommended treating Newcomb’s Problem as a game. It’s not clear why it wouldn’t be a game. The Newcomb demon makes a choice, and if you assume the demon gets utility 1 from correct predictions and utility 0 from incorrect predictions, they make the choice that maximises their return given their beliefs about what the other (human) player will do. So game theoretic techniques can and should apply. I think any problem with a predictive demon is best thought of as a game where the demonic player gets utility 1 or 0 depending on whether their prediction is right. Here the predictive player, Doctor, has a utility function with slightly more structure. But if we think decision theory should apply in cases where there is a predictor around, it should still apply when that predictor has preferences with slightly more structure.\nFourth, this project was inspired by reading David Pearce’s argument against ‘Single Solution Concepts’ (Pearce 1983). My initial plan was to simply translate his argument into decision theoretic terms and use it as an argument for indecisiveness. I ended up with a somewhat different argument to his, one that draws heavily on Brian Skyrms’s work on Stag Hunts (Skyrms 1990, 2001, 2004). But the project started out from an idea of Pearce’s. And as you’ll see in the conclusion, it will end with a related idea of is.\nDefending The Core Premise\nIn this section I’ll offer three arguments in defence of The Core Premise. The first will be to argue that Amsterdam and Brussels have in a key sense the same choice, the second will argue that violations of The Core Premise will violate the Sure Thing Principle, and the third is that violations of The Core Premise lead to people being willing to pay to avoid information. The arguments will make frequent use of the following equivalence. A player must choose a letter iff the player prefers choosing that letter. So I’ll move freely from saying that a theory says Amsterdam should choose X to saying Amsterdam should prefer X. I don’t think this should be controversial, but it’s worth noting. Onto the three arguments.\nThink about what Brussels is doing when writing in the envelope. They know that the envelope will only be opened if Doctor opts-in. If Doctor does, then what they play will determine their payout. So they should imagine that Doctor has opted-in, and act accordingly. But in that imaginative situation, they will do the same thing as if they knew that Doctor had opted-in. That is, they’ll do the same thing Amsterdam will do. There isn’t any difference, for purposes of choice, between supposing that Doctor has opted-in, and learning that Doctor has opted-in. And Brussels should suppose that Doctor has opted-in. After all, they are being asked what to contribute to the game if, and only if, Doctor opts-in. So the two choices are effectively the same, and they should get the same verdict. That’s the first argument.\nAssume that a theory says Amsterdam should do X, but Brussels should do Y, where X and Y are distinct. Now ask the theory, what should Brussels prefer conditional on Doctor opting-in, and conditional on Doctor opting-out. Since Amsterdam should choose X, conditional on Doctor opting-in, Brussels thinks X is better than Y. And conditional on Doctor opting-out, Brussels is indifferent between X and Y, so thinks X is as at least as good as Y. The sure thing principle (or at least the version that matters here) says that if Brussels knows that precisely one of a set of outcomes obtains, and X is at least as good as Y conditional on each member of the set, then X is at least as good as Y overall. But that contradicts the assumption that Brussels should choose Y. So that’s the second argument.\nAssume again that a theory says Amsterdam should do X, but Brussels should do Y, where X and Y are distinct. Now imagine a third player, Cardiff. Cardiff isn’t busy, like Brussels. But Cardiff hasn’t yet found out whether Doctor has opted-in. They are offered the chance to buy ear plugs, so they won’t hear the announcement of whether Doctor opts-in or opts-out. They should like to get those, since right now they prefer Y to X, but there’s a chance that they’ll hear Doctor has opted-in, which will leave them in the same situation as Amsterdam, and hence they’ll choose X. And there is nothing they can gain from hearing the announcement. But this is absurd - a player should not pay to avoid relevant information about the game. So that’s the third argument.\nNow this last argument has one caveat. I didn’t calculate how much Player should pay for the ear plugs. That turns out to vary a little depending on just which decisive theory we are looking at. A theory may say that Cardiff should not pay anything for the ear plugs, since they are certain Doctor will opt-out. This third argument isn’t particularly effective, I think, against those theories. But it works well, and I feel is the strongest argument, against some other theories. But we’ll have to look case by case at just how much a theory would recommend Cardiff pay for the ear plugs.\nSo that’s the defence of The Core Premise. What I’ll now show is that a wide range of decisive theories violate it, and so we can conclude they are false.\nEarly and Late Choices\nTo see why theories might violate The Core Premise, it’s helpful to set out explicitly the choices that Amsterdam and Brussels face. And we’ll treat Doctor largely as a non-player character, just as the demon is typically treated in Newcomb’s problem. So from now on the columns will not be Doctor’s choices, but what Doctor predicts the human player chooses. And we’ll assume Doctor maximises their financial return given a correct prediction. It’s easy to set out the choice Amsterdam faces; it’s just the embedded game with some notational differences.\n\n\n\n\nPA\n\n\nPB\n\n\nA\n\n\n6\n\n\n0\n\n\nB\n\n\n3\n\n\n4\n\n\nI’ve written PA and PB in the columns to indicated that A or B is Predicted. But in this game that makes little difference, since Doctor will do whatever they predict Amsterdam will do. Things are a little different for Brussels. If Doctor predicts that Brussels has written B, they will flip a coin to decide whether to opt-out, or opt-in. So we can’t write Brussels’s return in actual dollars, since we don’t know how the coin lands. But we can write the return in expected dollars, and we assume that Brussels is after all trying to maximise expected dollars. (We’ll come back to this assumption in the next section.) So the table Brussels faces looks like this.\n\n\n\n\nPA\n\n\nPB\n\n\nA\n\n\n6\n\n\n50\n\n\nB\n\n\n3\n\n\n52\n\n\nIf Doctor predicts B, then Player has a 1 in 2 chance of getting $100, and a 1 in 2 chance of getting the payout from the previous game. So their average payout is $50 if they play A, and $52 if they play B. Hence the values in the right hand column here.\nSo what The Core Premise says is that if each of these games has a uniquely rational choice, it must be the same choice. As we’ll see, a lot of theories do not satisfy this constraint.\nEvidential Decision Theory\nGiven a perfect predictor, Evidential Decision Theory says that the only payout values that matter are those in the main diagonal, running from northwest to southeast. So Amsterdam should choose A, since they’ll expect to get $6 from A and $4 from B. But Brussels should choose B, since they’ll expect to get $6 from A and $52 from B. So Evidential Decision Theory violates The Core Premise, and hence is mistaken.\nWhen I introduced Cardiff’s case, I said we had to check what a particular theory said about what Cardiff would pay for the earplugs. So let’s do that for Evidential Decision Theory. If Cardiff thinks they would be told that Doctor has opted-in, they would pay up to $46 to avoid that information, since they think they will get $52 without the information and $6 with it. But maybe they would be told that Doctor opted-out. It turns out the assumption they would be told that is incoherent, and Cardiff knows it. If they are told Doctor has opted-out, Doctor will know they are indifferent between the options. And in that case, Doctor will flip a coin to decide what ‘prediction’ to make. But if Doctor thinks it is 50/50 what Cardiff will do, they have an expected return of $2 from opting-in and choosing A, but an expected return of $1 from opting-out. So they will opt-in and choose A, contradicting the assumption that Cardiff will be told they opted-out. And Cardiff can do all this reasoning. So Cardiff can predict that if they are told anything, it will be that Doctor has opted-in, putting them in the same position as Amsterdam. But if they are told nothing, they are in the same position as Brussels. And they prefer, by $46, being in the same position as Brussels.\nNow this is not really a new objection to Evidential Decision Theory. You can find similar points being made about the strange behaviour of Evidential Decision Theory in dynamic choice settings as far back as Gibbard and Harper (1978). The details of my argument are a bit different, but ultimately they rest on the same foundations. I think those are perfectly solid foundations, but given how long the arguments have been around, clearly not everyone agrees. So I want to note one internal tension within Evidential Decision Theory this case brings up.\nAs Edward Elliott (2019) notes, within contemporary decision theory there is little overlap between work on what to do when a predictor is around, and work on the nature of risk. All parties to the former dispute take for granted the orthodox view that when there is no predictor, one should maximise expected utility. But that’s very controversial within the debates about risk. There the big question is whether the heterodox risk-weighed utility theory developed by Quiggin (1982) and Buchak (2013) is preferable to orthodoxy.\nThe Quiggin/Buchak view raises a dilemma for Evidential Decision Theory. If they reject the view and stick with orthodoxy, as most do, they should have an argument against the risk-weighted view. But the strongest such arguments turn on the fact that the risk-weighted view violates the Sure Thing Principle, and leads to people paying to avoid information. Evidential Decision Theorists can’t complain about it on those grounds, since their theory does the same thing. Alternatively, they can modify their theory to incorporate the Quiggin/Buchak view. But then they wouldn’t have a decisive decision theory, since on that view what to do in a decision problem depends on something not typically specified in the problem, namely the chooser’s attitude towards risk. So even if the Evidential Decision Theorist rejects the arguments behind The Core Premise, as I suspect most will, they need to either find a new objection to risk-weighted theories of choice, or modify their theory in a way that abandons decisiveness. I think the arguments for The Core Premise are sound, but even if they aren’t, it seems unlikely that there is a plausible decisive theory that can be derived from Evidential Decision Theory.\nStag Hunt\nIt’s possible to transform any decision problem involving a predictor into a game. David Lewis (1979) already noted the relationship between Prisoners’ Dilemma and Newcomb’s Problem. And William Harper (1986) noted that you could turn any problem involving a predictor into a game by assuming the predictor wants to make correct predictions and acts in their own interest.\nIt’s also frequently possible to do the reverse transformation, to turn a game into a decision problem involving a predictor. Start with any one-shot two person game, where each player has the same number of choices in front of them. Then change the payout for Column so that they get 1 if Row and Column make the ‘same’ choice (for some mapping between Row’s and Column’s choices), and 0 otherwise. Then just treat the Column player as a known to be accurate predictor, either an agent making predictive choices or a state of the world that tracks the agent’s choices in some way. Now Row’s choice is just a familiar kind of decision problem.\nIf you plug various famous games into the recipe from the previous paragraph, you get some familiar examples from modern decision theory. If you start with Prisoners’ Dilemma and apply this recipe, you get Newcomb’s Problem. If you start with Matching Pennies2 , you get Death in Damascus (Gibbard and Harper 1978). If you start with Battle of the Sexes, you get Asymmetric Death in Damascus (Richter 1984). If you start with Chicken, you get the Psychopath Button (Egan 2007). But there hasn’t been quite as much attention paid to what happens if you start with Stag Hunt and run this recipe. The game you get turns out to be very useful for classifying decisive decision theories that choose two boxes in Newcomb’s Problem.\nHere is an abstract form of a Stag Hunt game, where the options are G/g for Gather or H/h for Hunt. Actually, this is a table for a generic symmetric game; what makes it a Stag Hunt are the four constraints listed below.3\n\n\n\n\ng\n\n\nh\n\n\nG\n\n\n\\(x, x\\)\n\n\n\\(y, z\\)\n\n\nH\n\n\n\\(z, y\\)\n\n\n\\(w, w\\)\n\n\n\\(x > z\\)\n\\(w > y\\)\n\\(w > x\\)\n\\(x + y > z + w\\)\nThe first two constraints imply that \\(\\langle G, g \\rangle\\) and \\(\\langle H, h \\rangle\\) are both equilibria. This isn’t like Prisoners’ Dilemma, that only has one equilibrium. But it is like Prisoners’ Dilemma in that there is a cooperative solution, in this case \\(\\langle H, h \\rangle\\), but it isn’t always easy to get to it. It isn’t easy because there are at least two kinds of reasons to play \\(G\\).\nFirst, one might play \\(G\\) because one wants to minimise regret. Each play is a guess that the other player will do the same thing. If one plays \\(G\\) and guesses wrong, one loses \\(w - y\\) compared to what one could have received. If one plays \\(H\\) and guesses wrong, one loses \\(x - z\\). And the last constraint entails that \\(x - z > w - y\\). So playing \\(G\\) minimises possible regret.\nSecond, one might want to maximise expected utility, given uncertainty about what the other player will do. Since one has no reason to think the other player will prefer \\(g\\) to \\(h\\) or vice versa - both are equilibria - maybe one should give each of them equal probability. And then it will turn out that \\(G\\) is the option with highest expected utility. Intuitively, \\(H\\) is a risky option and \\(G\\) is a safe option, and when in doubt, perhaps one should go for the safe option.\nWhat I’ll call a Stag Decision is basically a Stag Hunt game where the other player is a predictor. So the decision looks like this, where the above four constraints on the values still hold, and PX means the predictor predicts X will be chosen.\n\n\n\n\nPG\n\n\nPH\n\n\nG\n\n\n\\(x\\)\n\n\n\\(y\\)\n\n\nH\n\n\n\\(z\\)\n\n\n\\(w\\)\n\n\nThese kinds of decisions are important in the history of game theory because they illustrate in the one game the two most prominent theories of equilibrium selection: risk dominance and payoff dominance (Harsanyi and Selten 1988). Risk dominance recommends gathering; payoff dominance recommends hunting. And most contemporary proponents of decisive decision theories in philosophy fall into one of these two camps.\nIn principle, there are three different views that a decisive theory could have about Stag Decisions: always Hunt, always Gather, or sometimes do one and sometimes the other. A decisive theory has to give a particular recommendation on any given Stag Decision, but it could say that the four constraints don’t settle what that decision should be. Still, in practice all existing decisive theories fall into one or other of the first two categories.\nOne approach, endorsed for rather different reasons by Richard Jeffrey (1983) and Frank Arntzenius (2008), says to hunt because it says in decisions with multiple equilibria, one should choose the equilibria with the best payout. This approach will end up agreeing with everything the Evidential Decision Theorist says about the choices facing Amsterdam and Brussels, and should be rejected for the same reason. It treats differently choices that are fundamentally the same, it violates Sure Thing, and it says Cardiff should pay $46 to avoid finding out what Doctor selected. And the same will be true for any decisive theory that says to always Hunt in Stag Decisions.\nAnother family of approaches says to always Gather in Stag Decisions. For very different reasons, this kind of view is endorsed by Ralph Wedgwood (2013), Dmitri Gallow (2020) and Abelard Podgorski (2022). These three views differ from each other in how they motivate Gathering, and in how they extend the view to other choices, but they all agree that one should Gather in any Stag Decision. And this leads to the reverse problem to that facing the always Hunt view.\nBoth Amsterdam and Brussels are facing Stag Decisions. But for Amsterdam, choosing A is Hunting and choosing B is Gathering, while for Brussels, choosing A is Gathering and choosing B is Hunting. So any view which says to always Gather will say that Amsterdam should choose B, and Brussels should choose A. Again, this treats differently choices that are fundamentally the same, and violates Sure Thing. But does it mean Cardiff will pay to avoid information? Here things are a little trickier, because Cardiff has four possible choices: Receive information or pay to decline it, and then choose A or B. And the different approaches to Gathering say different things about how to make decisions in four-way choices. So let’s set that argument for The Core Premise aside - the first two arguments for it still seem like decisive objections to any view that one should always Gather.\nWhat about views that deny that all Stag Decisions should be treated alike? As I’ve said, I don’t think any such view is in the literature, but it’s good to think about other views. Let’s drop the assumption that we’re even looking at a Stag Decision (though it will turn out that we are), and think about what to do in general in cases where there are two strict equilibria. That is, think about what our imaginary decisive decision theory will say about the following case, where we just have the constraints \\(x > z\\) and \\(w > y\\), and again \\(PX\\) means the predictor predicts \\(X\\).\n\n\n\n\nPE\n\n\nPF\n\n\nE\n\n\n\\(x\\)\n\n\n\\(y\\)\n\n\nF\n\n\n\\(z\\)\n\n\n\\(w\\)\n\n\nAny coherent solution must be invariant under redescriptions of the problem. So if you take a real world example that fits this category, and relabel which option is E and which is F, the recommendation should flip. And if you rescale the utilities by multiplying by a positive constant or adding a constant, the verdict should be unchanged, since utilities are only defined up to positive affine transformation. The only theories that meet these constraints say that a choice has a ‘score’ \\(x + my\\), where \\(x\\) is the equilibrium payoff, and \\(y\\) is the other possible payoff, and \\(m\\) is a free variable the theory sets which reflects how much it cares about the value of the non-equilibrium payoff. The theory then says to pick the option with the higher score, or to be indifferent otherwise. So it says to strictly prefer E to F iff \\(x + my > w + mz\\) and to be indifferent between the choices if that’s an equality not an inequality. Setting \\(m\\) to 0 gives you the view that says one should always Hunt, since one should always pick the equilibrium with the highest equilibrium value. Setting \\(m\\) to 1 gives you the view that you should always Gather, since you should maximise the sum (or, equivalently, the average) of the two payouts you might get with the choice. And both of these views violate The Core Premise. But what should we say about views that give \\(m\\) other values?\nThe first thing to say is that it is very hard to see any good philosophical motivation for values of \\(m\\) other than 0 or 1. Both these values make a certain amount of sense, but the reasons behind any other value are harder to understand. Still, if coherence required some other value for \\(m\\), I’m sure someone would come up with a motivation.\nThe second thing to say is that we have done more already than object just to the theories that set \\(m\\) to 1 or 0. Any theory that has \\(m < \\frac{2}{3}\\) will say that Amsterdam should choose A and Brussels should choose B, violating The Core Premise. And any view that has \\(m > \\frac{46}{47}\\) will say that Amsterdam should choose B and Brussels should choose A, also violating The Core Premise. But we don’t yet have an objection to theories on which \\(\\frac{2}{3} \\leq m \\leq \\frac{46}{47}\\).\nTo see what’s wrong with those theories, keep the structure of the game the same, but change the rewards as follows (all rewards are in dollars).\nHuman Pick\nDoctor Pick\nHuman Reward\nDoctor Reward\nNone\nOpt-out\n0\n1\nA\nA\n4\n4\nA\nB\n0\n0\nB\nA\n3\n0\nB\nB\n2\n1\nThen the ‘late game’ that Amsterdam faces will look like this:\n\n\n\n\nPA\n\n\nPB\n\n\nA\n\n\n4\n\n\n0\n\n\nB\n\n\n3\n\n\n2\n\n\nSince \\(2 + 3m > 4 + 0m\\) for any value of \\(m\\) satisfying \\(\\frac{2}{3} \\leq m \\leq \\frac{46}{47}\\), the theory will say Amsterdam should choose B.\nThe ‘early game’ that Brussels faces will look like this:\n\n\n\n\nPA\n\n\nPB\n\n\nA\n\n\n4\n\n\n0\n\n\nB\n\n\n3\n\n\n1\n\n\nSince in this game Player gets nothing if Doctor opts-out, and there is a 50/50 chance the Doctor will opt-out if they predict B, the returns in the right-hand column are half what they are in the late game. Since \\(4 + 0m > 1 + 3m\\) for any value of \\(m\\) satisfying \\(\\frac{2}{3} \\leq m \\leq \\frac{46}{47}\\), the theory will say Brussels should choose A.\nSo any decisive theory will violate The Core Premise for some choice pair or other. Hence all decisive theories are mistaken.\nWhere To Next?\nDecision theory cannot be everything that some of its proponents want it to be. It cannot be a guide that tells us what to do in every situation, even if we allow it to sometimes say that options are tied. So what can decision theory be? A natural answer is that it can tell us which options are rationally permissible, knowing that there will often be a plurality of options that are permissible. I think the way to finding a plausible indecisive theory goes via answering the following five questions.\nFirst, does decision theory start with what the chooser believes, or with what they should believe? If Player is certain that the red box has more money, but they have conclusive evidence that the blue box has more money, which box does decision theory say that they should choose? If decision theory is the theory of which actions “most effectively serve one’s desires according to one’s beliefs” (Lewis 2020c, 465), then it is the red box. If it is the theory of which choices are rational, then it is the blue box. I’m sympathetic to the arguments that Nomy Arpaly (2002) makes that the theory of rational choice should not pay any special attention to the agent’s beliefs. What’s rational to choose in a situation is a function of what’s rational to believe in that situation, not what one actually believes.4\nSecond, in a given situation, how many different beliefs are rational? The Uniqueness thesis says the answer is one. Permissivism says that Uniqueness is false, and for some propositions in some situations, there are multiple rational attitudes to have. See Kopec and Titelbaum (2016) for a good survey of the issues, Schultheis (2018) for a recent argument for Uniqueness, and Callahan (2021) for a recent argument for Permissivism.5 I’m on the Permissivist side of this debate.\nNow if you think decision theory should be sensitive to rational beliefs rather than actual beliefs, and you think Permissivism is true, you’re committed to indecisiveness. You won’t even need demons. After all, any situation where any credence in \\(p\\) between \\(x\\) and \\(y\\) is permissible will mean there are multiple bets at distinct odds on \\(p\\) that rationality neither requires taking nor requires passing. I think this is a perfectly sound argument for indecisiveness, but I didn’t lean on it here because the premises are considerably less secure than the ones I’ve appealed to.\nBut there is a third question that needs answering before we can offer a plausible indecisive theory: what is a mixed strategy? Relatedly, what role do mixed strategies have in the correct decision theory? This is a rather vexed question, and an important one. Almost all recent arguments against causal decision theory seem, to my eyes at least, to turn on attributing a bad theory of mixed strategies to the causal decision theorist. You can see this from the fact that almost all recent papers on decision theory involve problems that, when converted into games, have no pure strategy equilibria, but do have mixed strategy equilibria. We can’t offer a full decision theory, even an indecisive one, without resolving these problems, and that means having a theory of mixed strategies. And that’s very much a theory for another paper.6\nNote one thing I haven’t said so far, and won’t say in what follows. I don’t say that the way to find the correct indecisive theory is to come up with a bunch of cases, consult our intuitions about them, and then see which theory can match at least 80% of those intuitions. (Or whatever percentage we are working with this week.) That is a dubious approach in general, but around here it is close to incoherent.\nMost contemporary work in decision theory starts with the assumption that when there are no demons around (or anything else vaguely demonic), expected utility maximisation is the correct decision theory. And then theorists will start rolling out fantastic cases involving demons or predictors or lesions or genes or twins or triplets or whatever is in fashion. And they will ask what extension of expected utility theory best tracks intuitions about these cases. But this seems like a very dubious strategy, since intuitions about cases will not lead one to expected utility theory in the first place. Trying to match intuitions about cases like the Allais or Ellsberg paradoxes will lead one to prefer some non-standard theory like the one developed by John Quiggin (1982) or Lara Buchak (2013). It seems very unlikely that the best way to extend a counterintuitive theory like expected utility maximisation is by consulting intuitions about puzzle cases. It is much better to ask what principles we want our theory to endorse, and work towards a theory that satisfies those principles. And that is the methodology I have adopted here.\nI’ve relied heavily in this paper on two such principles: The Sure Thing principle, and the principle that information has non-negative value. I’ll end by describing one more principle, and noting two questions it raises. The principle is that a decider should be a probabilist, and that they should maximise expected utility. More precisely, it says that if the states are \\(\\{S_1, \\dots, S_m\\}\\), and the choices are \\(\\{O_1, \\dots, O_n\\}\\), then \\(O_i\\) is a permissible choice just in case there is some probability function \\(Pr\\) such that\n\\[\n\\sum_{k = 1}^m V(S_k \\wedge O_i)Pr(S_k) \\geq \\sum_{k = 1}^m V(S_k \\wedge O_j)Pr(S_k)\n\\]\nfor all \\(j \\in \\{1, \\dots, n\\}\\). Even if the subjective probability of the state is affected by the choice one makes, there should be some probability function that the chooser ends up with, and their choice should make sense by the lights of that probability function. Note that if we assume that the chooser can select any mixed strategy from among their choices, there is guaranteed to be at least one strategy that satisfies this requirement, even if one thinks the states are choices of a demon who can predict one’s strategy.7\nSo that seems to me like a minimal constraint on choices. As Pearce (1984) shows, it is equivalent to the requirement that one not make a choice that is strictly dominated by some other choice, or by some mixture of other choices. (This result is hardly obvious, but it turns out to be a reasonably straightforward consequence of the existence of Nash equilibria for all finite zero-sum games.) That’s hardly an uncontroversial principle, but it is also one I’m happy to adopt. If you’re still on board, there are two more questions that we need to answer before we finish our decision theory.\nAre all further constraints on rational decisions representable as constraints on the \\(Pr\\) in this principle? There surely are some further constraints on rational decisions. If you’re offered a bet at even money on whether I will become Canadian President next week, the only rational thing to do is to decline it. And that’s true even though there is a \\(Pr\\) such that taking the bet maximises expected utility. But that \\(Pr\\) is completely irrational given your evidence. So Do something that maximises expected utility given some probability is too liberal a rule; we need to say something about the \\(Pr\\). Do we need to say more than that? My answer is no, though I’m not even going to start defending that here.8\nThe Canadian Presidency examples suggests that there are constraints on \\(Pr\\) that are external to decision theory. You shouldn’t take that bet because you shouldn’t have probability above 0.5 that I’ll become Canadian President next week. The order of explanation runs from the (ir)rationality of the credal state to the (ir)rationality of the decision. Our fifth and final question is, are there any cases where the order of explanation goes the other way? Arntzenius (2008) argued that one should have credences such that the highest value equilibrium was also the choice that maximised expected utility. That’s an example of a constraint on \\(Pr\\) where the order of explanation runs from decisions to beliefs. I argued against that principle, but not because of a systematic reason to think that the order of explanation can’t run that way. Instead I argued that this particular principle was dynamically incoherent. That leaves open the general question of whether any such principles, where constraints on decisions explain constraints on belief, are right.\nThe long term goal of the project behind this paper is to argue that there are no such principles. The only constraints on rational decision are that one should maximise expected utility given some \\(Pr\\), and this \\(Pr\\) should satisfy independently motivated epistemic requirements. Now I haven’t come close to arguing for that here, and it’s a very strong claim. Given everything else I’ve said, it basically amounts to the claim that the theory of equilibrium selection has no role to play in normative decision theory. It may have a central role to play in descriptive decision theory, in explaining why people end up at a certain equilibrium. But it can’t justify that equilibrium, since any equilibrium could be rationally justified.9\nBut all of this is for future work. The aim of this paper has been to open up the possibility of an indecisive, i.e., permissive, decision theory. Decisive decision theories have to take a stand on Stag Decisions, and there is no coherent way for them to do that. So no decisive theory is correct, and the correct decision theory is indecisive.\n\n\n\nArntzenius, Frank. 2008. “No Regrets; or, Edith Piaf Revamps Decision Theory.” Erkenntnis 68 (2): 277–97. https://doi.org/10.1007/s10670-007-9084-8.\n\n\nArpaly, Nomy. 2002. “Moral Worth.” Journal of Philosophy 99 (5): 223–45. https://doi.org/10.2307/3655647.\n\n\nBonanno, Giacomo. 2018. “Game Theory.” Davis, CA: CreateSpace Independent Publishing Platform. 2018. http://faculty.econ.ucdavis.edu/faculty/bonanno/GT_Book.html.\n\n\nBuchak, Lara. 2013. Risk and Rationality. Oxford: Oxford University Press.\n\n\nCallahan, Laura Frances. 2021. “Epistemic Existentialism.” Episteme. https://doi.org/10.1017/epi.2019.25.\n\n\nChang, Ruth. 2002. “The Possibility of Parity.” Ethics 112 (4): 659–88. https://doi.org/10.1086/339673.\n\n\nEgan, Andy. 2007. “Some Counterexamples to Causal Decision Theory.” Philosophical Review 116 (1): 93–114. https://doi.org/10.1215/00318108-2006-023.\n\n\nElliott, Edward. 2019. “Normative Decision Theory.” Analysis 79 (4): 755–72. https://doi.org/10.1093/analys/anz059.\n\n\nGallow, J. Dmitri. 2020. “The Causal Decision Theorist’s Gudie to Managing the News.” The Journal of Philosophy 117 (3): 117–49.\n\n\nGibbard, Allan, and William Harper. 1978. “Counterfactuals and Two Kinds of Expected Utility.” In Foundations and Applications of Decision Theory, edited by C. A. Hooker, J. J. Leach, and E. F. McClennen, 125–62. Dordrecht: Reidel.\n\n\nHarper, William. 1986. “Mixed Strategies and Ratifiability in Causal Decision Theory.” Erkenntnis 24 (1): 25–36. https://doi.org/10.1007/BF00183199.\n\n\nHarsanyi, John C., and Reinhard Selten. 1988. A General Theory of Equilibrium Selection in Games. Cambridge, MA: MIT Press.\n\n\nJeffrey, Richard. 1983. “Bayesianism with a Human Face.” In Testing Scientific Theories, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.\n\n\nKopec, Matthew, and Michael G. Titelbaum. 2016. “The Uniqueness Thesis.” Philosophy Compass 11 (4): 189–200. https://doi.org/10.1111/phc3.12318.\n\n\nLewis, David. 1979. “Prisoners’ Dilemma Is a Newcomb Problem.” Philosophy and Public Affairs 8 (3): 235–40.\n\n\n———. 2020a. “Letter to D. H. Mellor, 14 October 1981.” In Philosophical Letters of David K. Lewis, edited by Helen Beebee and A. R. J. Fisher, 2:432–34. Oxford: Oxford University Press.\n\n\n———. 2020b. “Letter to Gregory Kavka, 10 July 1979.” In Philosophical Letters of David K. Lewis, edited by Helen Beebee and A. R. J. Fisher, 2:423–24. Oxford: Oxford University Press.\n\n\n———. 2020c. “Letter to Huw Price, 17 May 1988.” In Philosophical Letters of David k. Lewis, edited by Helen Beebee and A. R. J. Fisher, 2:464–66. Oxford: Oxford University Press.\n\n\n———. 2020d. “Letter to William j. Talbott, 22 June 1984.” In Philosophical Letters of David k. Lewis, edited by Helen Beebee and A. R. J. Fisher, 2:448–49. Oxford: Oxford University Press.\n\n\nPearce, David G. 1983. “A Problem with Single Valued Solution Concepts.” 1983. https://sites.google.com/a/nyu.edu/davidpearce/.\n\n\n———. 1984. “Rationalizable Strategic Behavior and the Problem of Perfection.” Econometrica 52 (4): 1029–50. https://doi.org/10.2307/1911197.\n\n\nPodgorski, Aberlard. 2022. “Tournament Decision Theory.” Noûs. https://doi.org/10.1111/nous.12353.\n\n\nQuiggin, John. 1982. “A Theory of Anticipated Utility.” Journal of Economic Behavior & Organization 3 (4): 323–43. https://doi.org/10.1016/0167-2681(82)90008-7.\n\n\nRichter, Reed. 1984. “Rationality Revisited.” Australasian Journal of Philosophy 62 (4): 393–404. https://doi.org/10.1080/00048408412341601.\n\n\nSchultheis, Ginger. 2018. “Living on the Edge: Against Epistemic Permissivism.” Mind 127 (507): 863–79. https://doi.org/10.1093/mind/fzw065.\n\n\nSkyrms, Brian. 1990. The Dynamics of Rational Deliberation. Cambridge, MA: Harvard University Press.\n\n\n———. 2001. “The Stag Hunt.” Proceedings and Addresses of the American Philosophical Association 75 (2): 31–41. https://doi.org/10.2307/3218711.\n\n\n———. 2004. The Stag Hunt and the Evolution of Social Structure. Cambridge: Cambridge University Press.\n\n\nWedgwood, Ralph. 2013. “A Priori Bootstrapping.” In The a Priori in Philosophy, edited by Albert Casullo and Joshua C. Thurow, 225–46. Oxford: Oxford University Press.\n\n\nMy personal preference is to understand states historically. For any proposition relevant to the decision, a state determines its truth value if it is about the past, or its chance at the start of deliberation if it is about the future. And then causal independence comes in from a separate presupposition that there is no backwards causation. But I definitely won’t assume this picture of states here.↩︎\nYou can see examples of all these games, and all the game theoretic machinery I use throughout this paper, in any standard game theory textbook. My favorite such textbook is Bonanno (2018), which has the two advantages of being philosophically sophisticated and open access. I’m not going to include citations for every bit of textbook game theory I use; that seems about as appropriate as citing an undergrad logic textbook every time I use logic. But if you want more details on anything unfamiliar in this paper, that’s where to look.↩︎\nI’ve listed the constraints as strict inequalities, but that might be over the top. Sometimes you’ll see one or other of these constraints weakened to an inclusive inequality. This difference won’t matter for current purposes.↩︎\nSee also Lewis (2020a) where Lewis sketches a view that would say that each choice is rational in a way, and there need not be anything more to say about which is rational all-things-considered. I take it he means decision theory is the theory of the part of rationality that the red box chooser does well on. A similar point is suggested in Lewis (2020d).↩︎\nInterestingly, Callahan connects Permissivism to existentialism. I suspect there are deep and unexplored connections between existentialism and decision theory, especially concerning the questions about the priority of strategies or individual choices. But that’s for another paper.↩︎\nFor what it’s worth, I think that theory must include the following two factors. First, playing a mixed strategy is just what Lewis (2020b) calls using a tie-breaking procedure. Second, the output of such a tie-breaking procedure is in principle unpredictable by anything that doesn’t time travel.↩︎\nIf the demon can predict what one will do on a given occasion while playing a mixed strategy, this guarantee may fail. But assuming what I said in the last footnote about mixed strategies, that would mean we’re in the realm of backwards causation, and the states are not causally independent of the actions.↩︎\nNote that if you say no to this question, and you think that probabilities have to be real-valued, then you’re committed to weak dominance not having a role to play in decision theory. So this is a non-trivial question.↩︎\nBut note here that what I’m calling an equilibrium is just a coherent set of beliefs that is grounded in the evidence. It doesn’t include the requirement, typical in game-theory, that the chooser has true beliefs about some aspect of the world around them.↩︎\n",
    "preview": "posts/2021-03-14-indecisive-decision-theory/bear_hunt.jpg",
    "last_modified": "2021-11-17T09:24:39-05:00",
    "input_file": "indecisive-decision-theory.knit.md"
  },
  {
    "path": "posts/2021-03-30-mixing-expert-opinion/",
    "title": "Mixing Expert Opinion",
    "description": "This paper contributes to the project of articulating and defending the supra-Bayesian approach to judgment aggregation. I discuss three cases where a person is disposed to defer to two different experts, and ask how they should respond when they learn about the opinion of each. The guiding principles are that this learning should go by conditionalisation, and that they should aim to update on the evidence that the expert had updated on. But this doesn't settle how the update on pairs of experts should go, because we also need to know how the experts are related. I work through three examples showing how the results change given different prior beliefs about this relationship.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2021-03-30",
    "categories": [
      "games and decisions",
      "unpublished"
    ],
    "contents": "\n\nContents\nCase One: Conditionally Independent Evidence\nCase Two: Common Marbles\nCase Three: Differentially Informed Experts\nSummary\n\nWhat should you do if two experts, each of whom you are disposed to defer to, disagree? The answer depends on what you know about the relationship between the experts’ evidence. I’m going to argue for this dependence claim, and work through three examples that start the process of illustrating the nature of the dependence. The first example concerns a case where the evidence the experts have is maximally independent. This case has been well analysed by Easwaran et al. (2016), and my main contribution is to offer a new (and perhaps more explanatory) proof of their primary conclusion. The second case is where you know what proportion of the experts’ evidence is shared. And the third is where you know that one expert is more informed, but you don’t know which. In each of the last two cases I’ll show the computed exact values of the posterior probabilities after conditionalising on the expert credences, and also show some simple methods for approximating these exact values. The approximations are, I suspect, a little more robust when we move from the simple examples I’ll describe to more realistic ones.\n\nChairs by François-Honoré-Georges Jacob-Desmalter. Picture via Louvre.\nSo let’s get more precise about the question we’re asking, and also give names to the characters in the story. (It feels weird to talk about you when I don’t know who you are, so I prefer having named characters.) Assume Player regards Ivy and Zack as experts about \\(p\\) in the following sense.\nIf Player learns that Ivy’s credence in \\(p\\) is \\(x\\), and nothing else, he will change his credence in \\(p\\) to \\(x\\).\nIf Player learns that Zack’s credence in \\(p\\) is \\(x\\), and nothing else, he will change his credence in \\(p\\) to \\(x\\).\nGiven that, what is the answer to this question.\nIf Player learns that Ivy’s credence in \\(p\\) is \\(y\\), and Zack’s credence in \\(p\\) is \\(z\\), and nothing else, what should his credence in \\(p\\) become?\nFollowing Baccelli and Stewart (2021), let’s distinguish two kinds of answers to this question. The supra-Bayesian says that this case, like every other case, calls for conditionalisation. This is going to be the kind of answer I defend. Here’s how we spell this answer out. First, we rewrite (1) and (2) as (4) and (5)\n\\(\\forall x: Cr_P(p | Cr_I(p) = x) = x\\)\n\\(\\forall x: Cr_P(p | Cr_Z(p) = x) = x\\)\nWhere \\(Cr_P, Cr_I\\) and \\(Cr_Z\\) are Player, Ivy and Zack’s credence functions respectively. Then (3) gets rephrased as a request for the value of\n\\(Cr_P(p | Cr_I(p) = y \\wedge Cr_I(p) = z)\\)\nThat’s good as far as it goes, but it raises two natural questions. First, what reasonable credal functions make (4) and (5) true, and what do they tend to say about (6)? Second, given the massive computational difficulty in calculating values like (6) in real time, are there heuristics for approximating its value in realistic cases? This paper aims to make progress on both questions. It offers some examples of reasonable credal functions satisfying (4) and (5), and uses them to suggest some heuristics for approximating (6) in somewhat realistic cases.\nBut before we get to those answers, we should look at the other kind of answer Baccelli and Stewart (2021) mention: pooling answers. A pooling answer to (3) says that we should find some function that in some way ‘pools’ \\(y\\) and \\(z\\) to answer (3). One obvious such function is the arithmetic mean. The answer to (3) is just \\((y + z)/2\\). Unfortunately, this won’t do for three reasons. One reason, as proven independently by Gallow (2018) and Bradley (2017) is that it is incompatible with supra-Bayesianism. A second reason, as stressed by Russell, Hawthorne, and Buchak (2015), is that it is in cases where Player defers to Ivy and Zack across a range of questions, this answer is incompatible with Player, Ivy and Zack all updating on external evidence by conditionalisation.1 A third reason, as stressed by Levinstein (2015) and Easwaran et al. (2016) is that in some cases the intuitively correct answer to (3) is not between \\(y\\) and \\(z\\).\nThe last of these reasons is most pressing. The natural response to the first two reasons is to move to some other kind of pooling. Both Russell, Hawthorne, and Buchak (2015) and Baccelli and Stewart (2021) suggest that we should use some kind of geometric pooling instead of linear pooling. In this context, to use geometric pooling is to give an answer to (3) something like2\n\\[\n\\frac{\\sqrt{yz}}{\\sqrt{yz} + {\\sqrt{(1-y)(1-z)}}}\n\\]\nAnd that pooling function can be shown to avoid the first two reasons for not using linear pooling. But it can’t avoid the third, and that’s what I’m going to focus on here.\nThere are three somewhat distinct reasons you might use pooling to answer (3).\nFirst, you might use it as a replacement for supra-Bayesianism. I’m going to argue that if you do this, you also have to give up on Bayesianism across the board. Sometimes the recipient of expert opinion can reliably infer the evidence behind the opinion reliably. In those cases, regular Bayesianism implies that the recipient should update on just that evidence. And that regular, not supra, Bayesian principle is enough to dispose of pooling answers.\nThere are two more plausible uses for a pooling answer. Second, you might use it as a constraint on supra-Bayesianism. You could argue that if the values that (6) takes for various \\(y, z\\) do not look like some kind of pooling function, that’s evidence the prior \\(Cr_P\\) was irrational to start with. And third, you might use it as an approximation for supra-Bayesianism. It’s a lot easier to calculate linear or geometric means than to work out precisely the value of (6). Both of the last two uses are intuitively very plausible. One of the arguments of this paper is that they are, unfortunately, ultimately untenable. There just isn’t much use around here for pooling.\nPooling answers to (3) look a lot like conciliationist approaches to peer disagreement. Indeed, the form of pooling that uses linear averaging is sometimes thought to be a application of the Equal Weight View (Elga 2007). Supra-Bayesian answers look like evidentialist approaches to peer disagreement. In particular, they look a lot like the Total Evidence View (Lackey 2010). I’m going to use an even older motivation for them: the evidentialist approach to testimony defended by Frank Jackson (1987). On Jackson’s view, testimony that \\(p\\) is evidence that the speaker has evidence for \\(p\\). The way to rationally update on it depends on what kind of evidence you think the speaker is likely to have, given they’ve concluded \\(p\\), and what you would (rationally) do with that evidence. Typically, the answer is Conclude p. Jackson argues that while this is typical, it isn’t always the right answer. And it fails to be the right answer in just the cases you shouldn’t accept the speaker’s testimony.\nSo to simplify here, I’m going to look at some cases where Player can simply deduce, given one of the experts’ credences, what their evidence must have been. And then Player will update on that evidence. As we’ll see, different assumptions about how the evidence of the experts interacts leads to different answers to (3).\nTwo quick notes. First, I’m only going to look at cases where the experts are treated symmetrically. That’s a restriction, but it’s a useful one for letting us see the range of cases. Second, I’m going to be agreeing with Easwaran et al. (2016) a lot, especially in the first half of the paper. I’m ultimately going to consider some different kinds of cases to what they consider - but that’s a difference in focus, not a difference in conclusions. (They look at a bunch of kinds of cases that I won’t consider as well; it’s not like I’m going strictly beyond their work.) This paper is intended as a complement to theirs, not at all a substitute. But I think it’s a valuable complement, because I’ll show how some very realistic cases require a generalisation of their model, and make some suggestions for what that generalisation should look like.\nCase One: Conditionally Independent Evidence\nIn our first case, the experts’ evidence is as independent as possible. Here’s a story to think about how that could be. Carmen has an urn with 50 marbles, 25 black and 25 white. She draws one at random and marks it with invisible ink. She has a scanner that can detect which marble is marked, but no one else can tell it apart from the other marbles. Let \\(p\\) be the proposition that the marked marble is white - that’s what we’ll focus on from now on.\nAfter selecting one marble to be marked, she puts together a jar containing the marked marble and 9 other marbles drawn at random from the urn. (I’ll use ‘urn’ for where Carmen keeps all the unused marbles, and ‘jar’ for what she constructs to show the experts.) She shows that to one of the experts, let’s say Ivy. She gets to inspect the jar, i.e., count how many marbles in it are white and black. She then reports to Player, but crucially not to Zack, her credence in \\(p\\).\nIn this example, the next thing that happens is that Carmen takes the jar back, removes the 9 unmarked marbles, puts them back in the urn, and draws a new set of 9 marbles. (That set may overlap with the first set of course.) She puts these 9 in the jar, along with the marked marble, and shows the jar to Zack. He examines the jar, and reports to Player his credence in \\(p\\).\nNow in this case we can work out precisely how Player should update on these two pieces of information. When one expert reports a credence of \\(x\\) in \\(p\\), Player can infer that they saw \\(10x\\) white marbles. After all, what the expert knows is just that the marked marble is equally likely to be any of the marbles in the jar they see. So given \\(Cr_I(p) = y\\) and \\(Cr_Z(p) = z\\), Player can infer how many white marbles were in each jar. And he can work out the probability of each of those jars turning up given \\(p\\) and given \\(\\neg p\\). And that’s enough to plug into Bayes’s Theorem to work out a posterior probability for \\(p\\). When you do that, you get the following result.\n\\(Cr_P(p | Cr_I(p) = y \\wedge Cr_Z(p) = z) = \\frac{yz}{yz + (1-y)(1-z)}\\)\nI’m not going to work through the derivation of this, because it’s a straightforward consequence of something I will derive below. If you do want to check it for yourself, the key input is that the probability of drawing \\(x\\) white balls in \\(t\\) draws without replacement from an urn with \\(w\\) white balls and \\(b\\) black balls is\n\\[\n\\frac{\\binom{w}{x} \\binom{b}{t-x}}{\\binom{w+b}{t}}\n\\]\nMore importantly, (7) looks just like a special case of the central formula (Upco) that Easwaran et al. (2016) use. And that’s not surprising, since this case uses the same conditional independence assumption that they make through much of their paper. To say that \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) is just to say that \\(\\Pr(A \\wedge B | C) = \\Pr(A | C)\\Pr(B | C)\\). In this case, any pair of claims about how many white balls are in the jars shown to Ivy and to Zack are conditionally independent, both conditional on \\(p\\) and on \\(\\neg p\\).\nThe right hand side of (7) also looks a lot like the geometric means described above. The big difference is that the square root signs have disappeared. And that makes a difference, because it means the result violates what Baccelli and Stewart (2021) call Unanimity. This principle requires that \\(Cr_P(p | Cr_I(p) = y \\wedge Cr_I(p) = y) = y\\). If (7) is true then Unanimity is violated in every case except where \\(y\\) equals 0, 0.5 or 1. But this is bad news for Unanimity, because the case for (7) in this case seems very strong. Player really knows how many white marbles were in each jar, and it’s just a bit of algebra to get from there to (7) via conditionalisation. And it’s very plausible that conditionalisation is the right way to update on evidence about how many marbles are in a jar. So any principle incompatible with (7) is false.\nIt turns out that varying how many marbles are in the urn Carmen starts with does not change (7). But changing the ratio of white marbles to black marbles in the urn does change the formula. If the proportion of the initial urn that is white is \\(r\\), then the general result is\n\\(Cr_P(p | Cr_I(p) = y \\wedge Cr_I(p) = z) = \\frac{yz(1-r)}{yz(1-r) + (1-y)(1-z)r}\\)\nAgain, this isn’t a new result; Easwaran et al. (2016, 27) derive an even more general formula from which this falls out as a special case. But my way of deriving it is just different enough to be worth including.\nLet \\(I_x\\) be the disjunction of all possible evidence propositions that would lead Ivy to have credence \\(x\\) in \\(p\\). In this case \\(I_x\\) is a simple proposition that there are \\(10x\\) white marbles in the jar, but we don’t need to assume that \\(I_x\\) will be anything like that simple. Everything that follows about \\(I_x\\) also holds for \\(Z_x\\), the disjunction of all possible evidence propositions that would lead Ivy to have credence \\(x\\) in \\(p\\), but I won’t repeat the derivations. Since Player defers to Ivy, i.e., (4) is true, we have the following proof. (All credences are Player’s, so I’ll drop the subscripts.)\n\\[\\begin{align*}\nCr(p | I_x) &= x &&\\therefore \\\\\nCr(p \\wedge I_x) &= x \\cdot Cr(I_x) \\\\\n &= x (Cr(p \\wedge I_x) + Cr(\\neg p \\wedge I_x)) &&\\therefore \\\\\n(1-x)Cr(p \\wedge I_x) &= x \\cdot Cr(\\neg p \\wedge I_x)) &&\\therefore \\\\\nCr(\\neg p \\wedge I_x)) &= \\frac{1-x}{x} Cr(p \\wedge I_x) &&\\therefore \\\\\nCr(I_x | \\neg p) &= \\frac{(1-x)Cr(p)}{x\\cdot Cr(\\neg p)}Cr(I_x | p)\n\\end{align*}\\]\nSo we know the ratio of \\(Cr(I_x | p)\\) to \\(Cr(I_x | \\neg p)\\). That will become useful in what follows. Assuming evidentialism, what matters for (6) is working out the value of \\(Cr(p | I_y \\wedge Z_z)\\). But we now know enough to do that.\n\\[\nCr(p | I_y \\wedge Z_z) = \\frac{Cr(p \\wedge I_y \\wedge Z_z)}{Cr(I_y \\wedge Z_z)}\n\\]\nUsing the general fact that \\(X\\) is equivalent to \\((p \\wedge X) \\vee (\\neg p \\wedge X)\\), and that Player’s credences are probabilistic, so his credence in an exclusive disjunction equals the sum of the credence in the disjuncts, we know this equals.\n\\[\n\\frac{Cr(p \\wedge I_y \\wedge Z_z)}{Cr(p \\wedge I_y \\wedge Z_z) + Cr(\\neg p \\wedge I_y \\wedge Z_z)}\n\\]\nSince \\(Cr(p \\wedge X) = Cr(X | p)Cr(p)\\), we can rewrite this as\n\\[\n\\frac{Cr(I_y \\wedge Z_z | p) Cr(p)}{Cr(I_y \\wedge Z_z | p)Cr(p) + Cr(I_y \\wedge Z_z | \\neg p)Cr(\\neg p)}\n\\]\nAnd since \\(I_y\\) and \\(Z_z\\) are independent given both \\(p\\) and \\(\\neg p\\), this becomes\n\\[\n\\frac{Cr(I_y| p) Cr(Z_z | p) Cr(p)}{Cr(I_y| p) Cr(Z_z | p) Cr(p) + Cr(I_y| \\neg p) Cr(Z_z | \\neg p) Cr(\\neg p)}\n\\]\nIf we assume the initial value of \\(Cr(p) = r\\), and use the earlier derived fact that \\(Cr(I_x | \\neg p) = \\frac{(1-x)r}{x(1-r)}Cr(p)\\) this becomes\n\\[\n\\frac{Cr(I_y| p) Cr(Z_z | p)r}{Cr(I_y| p) Cr(Z_z | p)r + \\frac{(1-y)r}{y(1-r)} Cr(I_y| p) \\frac{(1-z)r}{z(1-r)} Cr(Z_z | p) (1-r)}\n\\] Now we can finally eliminate \\(Cr(I_y| p) Cr(Z_z | p)r\\) from the top and bottom, so this becomes\n\\[\n\\frac{1}{1 + \\frac{(1-y)(1-z)r}{yz(1-r)}}\n\\]\nOr in other words\n\\[\n\\frac{yz(1-r)}{yz(1-r) + (1-y)(1-z)r}\n\\]\nAnd that’s the completely general result when the evidence the experts has is conditionally independent of both \\(p\\) and \\(\\neg p\\), and Player starts with credence \\(r\\) in \\(p\\).\nBut this case is surely rare. Experts typically have some training in common that isn’t shared by non-experts. So their reasons for having a credence in \\(p\\) that differs from our prior will not be completely independent. Easwaran et al. (2016) note that sometimes we can adjust for the common evidence by conditionalising on the common evidence to come up with a new ‘prior,’ or perhaps I should say ‘intermediate’ credence, \\(r\\), then applying this formula. This is slightly more general, but still not a lot. Part of what makes us non-experts be non-experts is that we don’t have this common training, so we can’t identify what’s common between the experts. Let’s see if we can come up with a slightly more general case.\nCase Two: Common Marbles\nIn our second case, Carmen once again has an urn with 50 marbles, 25 black and 25 white. She draws one at random and marks it with invisible ink. She can tell which one this is, but no one else can. And \\(p\\) is still the proposition that the marked marble is white - that’s what we’ll focus on from now on. After selecting the marble to be marked, she puts together a jar containing the marked marble and 9 other marbles drawn at random from the urn. She shows that to one of the experts, let’s say Ivy. She gets to inspect the jar, i.e., count how many marbles in it are white and black. She then reports to Player, but crucially not to Zack, her credence in \\(p\\).\nSo far, it’s just like the last case. But what happens next is (possibly) different. In this case, Carmen removes \\(m\\) unmarked marbles from the jar, puts them back in the urn, and draws a new set of \\(m\\) marbles to put in the jar. It’s all random, so this could include some of the marbles she just removed. She shows the jar to Zack, he inspects it, and reports his credence in \\(p\\) to Player. And, crucially, player knows \\(m\\), the number of marbles that are in common between the jars. So \\(m\\) is a measure of the independence of the experts’ opinions.\nOnce again, we can work out precisely what Player’s credence should be given \\(m\\), and the two credences. Unfortunately, it’s just a long formula that doesn’t seem to reduce nicely. But if you’ve got a machine that’s good at calculating hypergeometric distributions, and you dear reader are probably reading this paper on a machine that’s good at calculating hypergeometric distributions, it’s not that hard to calculate the values by brute force. I won’t list all the values, there are several hundred of them, but I’ll present them graphically here. (Note I’ll leave off the case where one or other expert announces a credence of 0 or 1; in that case Player knows whether \\(p\\) is true, so the question of how to merge the credences is easy.)\n\n\n\nHere is how to read the graph. Each row corresponds to a partiular credence announced by Ivy; the credence is shown on the right. Each column corresponds to a partiular credence announced by Zack; that credence is shown on the top. The x-axis of the individual graphs shows the value for \\(m\\), the number of marbles removed. And the y-axis shows Player’s final credence in \\(p\\). There are more dots on some graphs than others because some combinations of Ivy credence, Zack credence and \\(m\\) are impossible. The announced credences can’t, by the rules of the game, differ by more than \\(0.1m\\).\nOne notable feature of that graph is that as \\(m\\) gets larger, the final credence tends to move away from 0.5; it tends to get more opinionated. Another notable feature, though probably not one you can see in this resolution, is that this move towards greater opinionation happens in a surprisingly linear fashion. To a first approximation, Player’s credence moves away from 0.5 roughly the same amount for each addition to \\(m\\), at least holding \\(y\\) and \\(z\\) fixed.\nIt’s not perfectly linear, but it’s much closer than I would have guessed looking at how really quite non-linear the inputs are. Let’s zoom in on a part of the graph to see this more vividly.\n\n\n\nThe curve in the bottom right panel is not really linear; it definitely curves downwards. But as you move your eye upwards and leftwards in the table, the curves look much much straighter. The panel where they both announce 0.7 is really remarkably straight. If we focus on the middle of the big graph, this is even more striking. (I’ve left off the cases where Zack announces a credence under 0.5 because those graphs are just mirror images of graphs already shown.)\n\n\n\nWhy does this matter? Because pooling functions are easy to use, and the supra-Bayesian needs something to match that ease of use. It’s a cliche that for every problem there is a solution that is simple, intuitive, and wrong. And the version of the pooling approach that uses linear averages is very simple, very intuitive, and very wrong. The version that uses geometric averages strikes most people as less simple and intuitive (or maybe I’m just bad at explaining it), but it is less wrong. But still, sometimes simple, intuitive and wrong is exactly what you need! Computation is hard, life is short, precision is overrated. Why not just average if you are just looking to get something roughly right?\nThe supra-Bayesian can exploit the more-or-less linearity of the graphs above graphs to come up with an approximation to these ideal Bayesian credence. And the approximation isn’t that much harder to calculate than the geometric average. Intuitively, it works like this. If the experts have exactly the same evidence, we take the geometric average of their opinions.3 If the experts’ evidence is conditionally independent, we use the formula from Easwaran et al. (2016) that I rederived in the last section. In between, we just need a guess \\(k\\) about what proportion of the evidence they share, and what is independent. And we use that guess to come up with an average of those two things, the geometric average and the formula for conditionally independent evidence. So our estimation of the new credence is this, where \\(y\\) and \\(z\\) are the announced credences, and \\(k\\) is the measure of independence of the evidence.\n\\[\n(1-k)\\frac{\\sqrt{yz}}{\\sqrt{yz} + \\sqrt{(1-y)(1-z)}} + k\\frac{yz}{yz + (1-y)(1-z)}\n\\]\nLet’s check visually how this does against the exact calculations. In the graphs that follow, I’ll use circles for the ideally calculated posterior credences, and triangles for the estimates made using this formula.\n\n\n\nThat looks pretty good. There is a tiny bit of separation in the bottom right panel, but otherwise the estimate tracks the calculated credences pretty closely. Let’s look at the middle of the graph.\n\n\n\nAnd all through here the dots are overlapping. That’s close enough. So at least in this special case, the supra-Bayesian can produce an estimate that is very close to the ideally calculated credence. So we don’t need to resort to pooling even as an approximation device.\nBut the simplifications here are dire. Here are six ways we might want to generalise the model.\nHave the prior probabilities of \\(p\\) and \\(\\neg p\\) vary.\nHave more colors for the marbles, and have each expert announce credences over all the colors.\nHave the person doing the merger be uncertain about \\(k\\).\nHave the experts sample the jars they are given, not inspect them fully.\nHave more than two experts.\nAllow that some experts are more informed than others.\nThe first two points are not that hard. I could produce a string of graphs for different priors over the colors, or for more colors, and the typical story is not that different to what we’ve seen so far. It just gets messy because we have more degrees of freedom than is consistent with a concise graphical display.\nThe next two points are harder. It’s not that they are harder to come up with the idea value. For any prior over \\(k\\), or sampling technique that’s available to the expert, it’s pretty easy to write code to come up with the optimal calculated credences. It’s rather that the number of degrees of freedom are so great that it gets a little harder to eyeball how good any given approximation is. The big point is that the posterior distribution of \\(k\\) will usually be different to the prior. In extreme cases, the announced expert credences might rule out some hypotheses about \\(k\\). So it won’t just be a matter of calculating the values of the above formula for each value of \\(k\\), and averaging them out using the prior probabilities of \\(k\\). There is a lot of possible future research here.\nHaving more than two experts raises both computational questions, like what we’ve just discussed, and conceptual questions. Consider even what happens when we go to three experts. Imagine the balls are numbered, and say \\(M_i\\) is the proposition that ball \\(i\\) is numbered, and \\(J_{i, e}\\) is that ball \\(i\\) is in the jar shown to expert \\(e\\). To represent the degree of connectedness of one expert’s evidence to the other(s) in the two expert case, we really just need to specify one variable: \\(\\Pr(J_{i, 1} | J_{i, 2} \\wedge \\neg M_i)\\). But in the three expert case, we need three variables to be specified.\n\\(\\Pr(J_{i, 1} | J_{i, 2} \\wedge J_{i, 3} \\wedge \\neg M_i)\\)\n\\(\\Pr(J_{i, 1} | J_{i, 2} \\wedge \\neg J_{i, 3} \\wedge \\neg M_i)\\)\n\\(\\Pr(J_{i, 1} | \\neg J_{i, 2} \\wedge J_{i, 3} \\wedge \\neg M_i)\\)\nAnd if there are four experts, there are seven of these variables. And this number grows exponentially as the number of experts rises.\nThe point is not just that the compututations of the ideal supra-Bayesian credence require an exponentially increasing number of inputs as the number of experts rises. It’s that even thinking about how to approximate this ideal calculation, we need a good way to conceptualise this space whose dimensionality rises exponentially with the number of experts in a way that lets us even think about what a good approximation would look like. I don’t have an answer to this; it feels like a question for future research.\nWhat I will try to make some headway on instead is the last question, what happens if we do not assume the experts are just as well informed as each other.\nCase Three: Differentially Informed Experts\nIn our last case, one expert is better informed than the other. Carmen first fills the jar with the marked marble and 19 randomly chosen unmarked marbles. She flips a coin to decide which expert to show this jar to. They inspect the jar, and record their credence in \\(p\\) to the nearest 0.1. (We’ll come back very soon to why this is rounded.) Carmen then removes 10 unmarked marbles from the jar, chosen at random, and then shows it to the other expert. They inspect it, and come up with a new credence in \\(p\\). Then both these recorded numbers are reported to Player, without any indication about who saw the larger jar and who saw the smaller one.\nThere is a weird thing in this setup in that one of the experts reports something other than their precise credence. The reason I set up the example this way is to make it impossible for the recipient of the expert opinion to infer who saw the smaller jar. If they both reported their actual credence, it would be possible for the recipient to be told one of them has credence 0.75 in \\(p\\) and the other has credence 0.6. And then it would be obvious that the hearer should have credence 0.6 in \\(p\\), since that’s the credence of the more informed person. So I made the first person round to the nearest 0.1 to make it harder to make such inferences.\nGiven all that setup we can work out what Player’s credence in \\(p\\) should be given the two announcements. (I’m rounding to three decimal places to save space.I’m leaving off the cases where one or other party announces an extremal credence - the hearer agrees with those credences, at least to three decimal places. And the ‘NA’ values are where it is impossible given the setup for those to be the announced values.)\n\n\nIvy/Zack\n\n\n0.1\n\n\n0.2\n\n\n0.3\n\n\n0.4\n\n\n0.5\n\n\n0.6\n\n\n0.7\n\n\n0.8\n\n\n0.9\n\n\n0.1\n\n\n0.100\n\n\n0.103\n\n\n0.100\n\n\n0.100\n\n\n0.100\n\n\n0.100\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n0.2\n\n\n0.103\n\n\n0.200\n\n\n0.208\n\n\n0.203\n\n\n0.202\n\n\n0.200\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n0.3\n\n\n0.100\n\n\n0.208\n\n\n0.300\n\n\n0.320\n\n\n0.325\n\n\n0.348\n\n\n0.500\n\n\nNA\n\n\nNA\n\n\n0.4\n\n\n0.100\n\n\n0.203\n\n\n0.320\n\n\n0.400\n\n\n0.439\n\n\n0.500\n\n\n0.652\n\n\n0.800\n\n\n0.900\n\n\n0.5\n\n\n0.100\n\n\n0.202\n\n\n0.325\n\n\n0.439\n\n\n0.500\n\n\n0.561\n\n\n0.675\n\n\n0.798\n\n\n0.900\n\n\n0.6\n\n\n0.100\n\n\n0.200\n\n\n0.348\n\n\n0.500\n\n\n0.561\n\n\n0.600\n\n\n0.680\n\n\n0.797\n\n\n0.900\n\n\n0.7\n\n\nNA\n\n\nNA\n\n\n0.500\n\n\n0.652\n\n\n0.675\n\n\n0.680\n\n\n0.700\n\n\n0.792\n\n\n0.900\n\n\n0.8\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n0.800\n\n\n0.798\n\n\n0.797\n\n\n0.792\n\n\n0.800\n\n\n0.897\n\n\n0.9\n\n\nNA\n\n\nNA\n\n\nNA\n\n\n0.900\n\n\n0.900\n\n\n0.900\n\n\n0.900\n\n\n0.897\n\n\n0.900\n\n\nAnd a striking thing about this table is how close it comes to verifying a strong form of what Levinstein (2015) calls Thrasymachus’ Principle. The hearer defers to the expert with the strongest view, i.e., the view that’s furthest from the prior. In contemporary terms, the hearer listens to the expert with the hottest take. It isn’t an unvarnished form of that. When one says 0.5 and the other says 0.6 you end up with 0.561, not 0.6. But that’s in large part because there’s a good chance that the person who said 0.6 was merely rounding up as the result of a coin flip. In general, the rule in this case is find the expert credence that is furthest from the prior, and adopt it.\nThere is a reason that a case like this should follow Thrasymachus’ Principle. If the experts are rational, hotter takes should correspond to stronger evidence. And while it isn’t impossible for the person with more evidence to have in a sense weaker evidence, the extra evidence may be full of defeaters for the first obtained evidence, it is pretty unlikely. In general, if someone is worthy of deference, and they have a strong view, they have strong evidence. If someone else has a weaker view, i.e., a view closer to the prior, the best explanation is that they simply don’t have the evidence that the person with stronger view does.\nSo again, we shouldn’t pool the opinions in any interesting sense. The table shows the optimal response by supra-Bayesian lights. And the simple approximation is, “When one expert has clearly stronger views, listen to them. Otherwise take the geometric mean.”\nSummary\nLet’s take stock of what’s been covered so far.\nI’ve argued against all three uses of pooling answers to the question of how to merge expert opinions. Sometimes the pooling answer is clearly wrong, often it won’t be a good constraint on priors, and there are better ways to approximate the correct supra-Bayesian answer.\nI’ve connected supra-Bayesianism to some familiar positions in epistemology, the view on testimony in Jackson (1987) and the view on disagreement in Lackey (2010).\nI’ve shown that if you take that approach, that conditionalising on someone else’s credence is just conditionalising on the fact that they have evidence that rationalises such a credence by their lights, then the principle Easwaran et al. (2016) recommend for updating on the credences of others follows directly from the assumptions that each expert is independently worthy of deference, and the evidence the experts have is conditionally independent.\nI’ve developed a toy example that lets us think about cases where the hearer doesn’t know which parts of the evidence are in common, but does know how much is in common.\nAnd I’ve shown that in that case, the correct supra-Bayesian answer is nicely approximated by a linear average of two familiar formulas.\nI developed a toy example that lets us think about the case where one expert is known to be more informed, but we aren’t sure which it is.\nAnd in that case I showed that what Levinstein (2015) calls Thrasymachus’ Principle is approximately right; we should defer to the ‘stronger,’ i.e., more opinionated, expert.\nAt the end of section 2 I mentioned six ways in which we might make the model even more general. This is very much not meant to be the last word. But I suspect these kinds of examples can be used to provide useful approximations, or guides, to real life situations where we know something about the relationship between the experts. The general lesson is that by looking at toy cases, we can provide practical advice for how to emulate, or at least approximate, the supra-Bayesian approach for merging expert opinion. And this advice will be better than the advice that anyone who ignores the relationship between the experts can offer.\nBut there is one last kind of relationship between experts that I haven’t made any progress on modeling, and it is a big one. What should we say about cases where the experts know each others credences? This is an old and, to my mind, open question. For reasons that trace back to Aumann (1976), in anything like the kind of model I’ve used here, if the experts know each other’s credences, they have to agree. And someone who knows both credences should agree with them. But the real world obviously contains experts who do agree to disagree. What to say about those cases is the biggest open questions around here, and I’m not sure whether this approach can help. Gallow (2018) ends his paper by raising doubts about whether it is rational to be disposed to defer to two different experts. I’m not worried about that in general; I’ve described three very different kinds of cases where it is rational. But I suspect one could not be rationally disposed to defer to two experts who one knows are themselves disposed to agree to disagree. That, however, is a story for another paper. This paper has described a number of cases where the hearer knows something the experts doesn’t know: namely what other experts think. And it has described both precise and approximate answers for what to do in those interesting cases.\n\n\n\nAumann, Robert J. 1976. “Agreeing to Disagree.” The Annals of Statistics 4 (6): 1236–39. https://doi.org/10.1214/aos/1176343654.\n\n\nBaccelli, Jean, and Rush T. Stewart. 2021. “Support for Geometric Pooling.” Review of Symbolic Logic forthcoming. https://doi.org/doi:10.1017/S1755020320000416.\n\n\nBradley, Richard. 2017. “Learning from Others: Conditioning Versus Averaging.” Theory and Decision 85 (1): 5–20. https://doi.org/10.1007/s11238-017-9615-y.\n\n\nEaswaran, Kenny, Luke Fenton-Glynn, Christopher Hitchcock, and Joel D. Velasco. 2016. “Updating on the Credences of Others: Disagreement, Agreement, and Synergy.” Philosophers’ Imprint 16 (11): 1–39.\n\n\nElga, Adam. 2007. “Reflection and Disagreement.” Noûs 41 (3): 478–502. https://doi.org/10.1111/j.1468-0068.2007.00656.x.\n\n\nGallow, J. 2018. “No One Can Serve Two Epistemic Masters.” Philosophical Studies 175 (10): 2389–98. https://doi.org/10.1007/s11098-017-0964-8.\n\n\nJackson, Frank. 1987. Conditionals. Blackwell: Oxford.\n\n\nLackey, Jennifer. 2010. “What Should We Do When We Disagree.” Oxford Studies in Epistemology 3: 274–93.\n\n\nLevinstein, Benjamin Anders. 2015. “With All Due Respect: The Macro-Epistemology of Disagreement.” Philosophers’ Imprint 15 (13): 1–20.\n\n\nRussell, Jeffrey Sanford, John Hawthorne, and Lara Buchak. 2015. “Groupthink.” Philosophical Studies 172 (5): 1287–1309. https://doi.org/10.1007/s11098-014-0350-8.\n\n\nNote that supra-Bayesianism is the view that Player should update on expert testimony by conditionalisation. This objection does not assume supra-Bayesianism, but does assume that conditionalisation is the right rule for normal, non-testimonial, updating.↩︎\nI say ‘something like’ because you might want to allow some extra parameters in the answer if, for example, you want to give different weights to the two experts. That kind of detail won’t matter to the argument here; we’re just going to focus on cases where the experts are treated symmetrically.↩︎\nWe are working with cases so far where there is a unique rational credence for each evidence, so if they have the same evidence they have the same credence, and which kind of averaging we use is redundant. What matters about the geometric average is how it enters into mixtures, as we’re about to see.↩︎\n",
    "preview": "posts/2021-03-30-mixing-expert-opinion/two_chairs.jpg",
    "last_modified": "2021-03-30T12:12:18-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-the-sporting-attitude/",
    "title": "The Sporting Attitude",
    "description": "Contribution to a symposium on Steffen Borge's \"The Philosophy of Football\".",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2020-12-17",
    "categories": [
      "book symposium",
      "on books",
      "games and decisions",
      "unpublished"
    ],
    "contents": "\nSteffen Borge’s The Philosophy of Football (Borge 2019) is a really great contribution to philosophy of sport. More than that, it shows how questions in metaphysics, aesthetics, and philosophy of mind can be illuminated by looking at them through the perspective of sport. I’m going to focus on one particular question he raises, primarily in chapter 3. What attitude towards a game should players take? What is, to use Suits’s terminology (Suits 1978), the lusory attitude that goes along with playing a sport.\n\nImage by IQRemix via Creative Commons.\nThere are actually three distinct questions here that are worth separating. I’m going to start with the first, but as we’ll see, I’m going to end up having more to say about the second and the third.\nWhat attitude to the game must players have if they are to play the game?\nWhat attitude to the game should players have if they are playing the game?\nWhat attitude must players generally have if the game is to be the game that it is?\nBorge argues that Suits’s answer to question 1 is much too strong, and I’m mostly inclined to agree. But I think the answers he gives to 1 and 3 are too weak. And thinking about question 2 will help us see why.\nTo start, let’s think about why we might be interested in question one in the first place. Imagine someone, call him George, who stands on a football field, but doesn’t act like a footballer. When the ball comes to him, he catches it, or picks it up, and runs towards the opposite goal line. If he makes it there, he places the ball over the goal line in celebration. George isn’t playing football - he’s playing rugby. (Or, perhaps, he’s trying to play rugby and not really succeeding at playing anything, since there clearly isn’t a rugby game going on.) Now imagine someone else, call him Webb, who tries to be like George, but fails. He really wants to pick the ball up and run with it. And he tries to do this repeatedly. But he fails every time. He never lays a hand on the ball in fact. Is Webb playing football?\nI think he’s not, or at least that there is an important sense in which he is not. And this is a hard thing to capture. Webb isn’t playing football well, since he isn’t ever involved with the play. But actually kicking the ball, or even doing any kind of football like move, isn’t essential to playing football. (See, for example, some of the less impressive performances of Mesut Özil’s Arsenal career.) The problem with Webb is that he’s trying to play a completely distinct game. It was easy to say why George was not playing football; he was gratuitously breaking the rules. But Webb is not breaking the rules. What’s wrong with Webb, what makes him a non-footballer, is something mental.\nNow at this stage you might be tempted to say that the problem with Webb is that he isn’t trying to follow the rules. But, as Borge points out, this can’t be the story1. A defender who grabs an opponent’s jersey - just hard enough to not get penalised - is still playing football. A winger who drags an opponent back to prevent a counterattack - and knows that a yellow card will follow - is still playing football. To use an example we’ll come back to a bit, Luis Suárez was playing football when he pulled off that impressive, but totally illegal, save in the 2010 World Cup. You can play football while deliberately, knowingly, breaking the rules of football. So if the problem with Webb is that he has the wrong attitude, what attitude that he lacks must you have to count as a football player?\nI’m going to argue that playing a sport requires taking the rules of that sport as providing reasons against certain actions. To play football is, among other things, to regard oneself as having a reason to not handle the ball. (Except as a goalkeeper, or during a throw-in, etc.) This reason can be outweighed, but never defeated. Even if handling the ball is the right thing to do all things considered, one has an outweighed but undefeated reason to not do it. That’s the attitude that’s essential to playing football. Or, more precisely, playing football requires being part of a game where almost all the players have that attitude almost all the time.\nTo get to this conclusion, I’m going to start by looking at Suits’s view that being a player requires treating the rules as binding; that one is not playing the game is the rules are broken. This requires reconsidering what the rules are, and I’m going to broadly agree with Borge’s critique of this reconsideration. Then I’m going to go over my positive view, that being a player requires treating rules as providing reasons. Then I’ll compare this view to Borge’s view; the views might not be that far apart. And finally I’ll talk about the view of rules as reasons can be strengthened by incorporating D’Agastino’s view that games have an ethos, and playing the game requires upholding that ethos.\nA simple way to relate rules to player attitudes is to say that playing the game requires treating the rules as binding. On the face of it this is absurd; players commit fouls, even intentional fouls, in every game. A way to make it plausible is to reinterpret the rules so that they are more or less never broken. Now this might seem absurd - a defender grabbing an attacker who is running by is breaking the rules. But as Borge discusses2, you don’t have to think about rules this way. You cold say the rule is not Don’t grab other players. Instead, the rule is If you grab another player then (ceteris paribus), the other team gets a free kick. The defender isn’t breaking that rule. It’s true that they do something that leads to the other team being awarded something by the referee. But a defender who kicks the ball into touch to stop an attack also does something that leads to the other team being awarded something by the referee, and they aren’t breaking any rules. On this way of thinking, all rules are like the rules about what happens when the ball goes out of play, and players do not deliberately break those rules.\nWe can put the same point in Kantian terms. We ordinarily think of rules as being categorical imperatives, like Don’t grab other players, that players break. The view I’m interested in here is that rules are hypothetical imperatives, like If you grab another player then (ceteris paribus), the other team gets a free kick. And while these might be broken too, some fouls are never called, players do not intend to break them. Indeed, it’s not clear that a player could intend to break them. The few categorical imperatives there are, like Don’t use a sword while playing football, are clearly followed by the players.\nThis is a plausible model for some sports. In particular, it seems like a not absurd model for cricket. At first it might look like cricket has a number of rules for proper bowling, like that you must not overstep the crease when bowling, and you must not straighen your arm when bowling. But on closer look, it is plausible that some of these are hypothetical imperatives. The overstepping rule is really a conditional - if you overstep then the batting team is awarded a run (and some other things). The bowler isn’t breaking a rule when they overstep, they are just doing something that results by rule in good results for the other team. In that respect, they are just like a fielder whose overthrow goes to the boundary. On the other hand, the rule about straightening your arm is a categorical imperative: you must not do that. And we can see that from the fact that the match officials’ duty is not to penalise this kind of bowling, but to prevent it. So it’s plausible that in cricket, most rules should be understood as conditionals, and players intend to conform to them.\nBut this is not a particularly plausible model for football. We can see this by considering a pair of cases. In each case, an attacker has the ball at the corner flag, and is about to cross to an unmarked teammate near the penalty box. In the first case, defender Ellie prevents the cross by sliding in and cleanly kicking the ball over the goal line. In the second case, attacker Sam, who isn’t as good at this, prevents the cross by sliding in and bundling the ball, the attacker, the corner flag and the watching sideline official over the goal line. In both cases, the immediate thing the officials should do is award an unobstructed kick to the attacking team by the corner flag. If rules are hypothetical imperatives, then in an important sense Sam and Ellie did the same thing. They triggered a hypothetical that leads to the other team getting a reward of an unobstructed kick by the corner flag. But surely that leaves something out. What Ellie did was great defending, and what Sam did was foul play. This suggests that we want some notion of rules in which Sam was breaking the rules, and Ellie was not. If the Suitsian model says otherwise, it is wrong. But the Suitsian can’t go on to say that what Sam did was against the rules, because then it would imply that being a player means intending to not do what Sam did, and that is clearly wrong.\nBorge discusses some examples like this one, and gives two further arguments as to why the view I’m discussing gets the case wrong. I’m sympathetic to Borge’s conclusions, but both the arguments seem to need further refinement. And working through is interesting because it reveals how hard it is to put one’s finger on what distinguishes the cases.\nBorge’s first argument3 is that we need to say Sam broke the rules to explain why we added extra penalties in the 1980s and 1990s against this kind of foul play. But I suspect this is easier to explain than Borge thinks. Sports, especially football codes, change rules all the time to discourage behaviour they want to see less of. In Australian football, if a defender carries a ball into their own goal, the other team gets one point, and traditionally the defending team got a goal kick. Since the alternative might be giving up a goal, worth six points, this was often a sensible play. It was so sensible, and became so prevalent, that the rules were changed to discourage it, replacing the goal kick with a jump ball near the goal. The fact that administrators of the game changed the penalties for certain tackles doesn’t show that those tackles were against the rules, it might just show they wanted less of those kinds of things. (Compare too the change to the rules of football to ban handling back passes.) And the fact that Sam might get a red card for this tackle doesn’t even show it is against the rules. Dangerous play can get a red card even if it isn’t a rule violation. There isn’t a rule against kicking the ball hard and straight at an annoying fan pitchside, but it could be a red card if the kick was too hard and straight.\nBorge’s second argument is that the I’m imagining is too revisionist. We talk as if there are laws of the game, rules, that Sam broke and Ellie did not. And while this is true, I don’t think we should be too concerned about this. That’s in part because there are sports like cricket where this kind of revisionism seems on reflection plausible. But it is in part because of things internal to football. We talk about tackles like Sam’s being against the Laws of the game. But we also talk about being offside as against the Laws. It certainly triggers the exception clause (unless there has been a violation of the Laws) in the clause about a definition of a goal. And the story I’m telling seems fine, and perhaps quite plausible, for off side. You can’t get carded for repeatedly being offside, even if like Inzaghi you were born in an offside position. If there is a distinction between Sam’s case and Ellie’s case, it doesn’t just feel like we talk as if Sam’s action was against the rules (or the Laws), and Ellie’s was not.\nStill, I think is a key difference between the cases. And I suspect it does cause a problem for this view. Here is one way to see the difference between the cases. Imagine Sam gets away with just a yellow card for her tackle, so both versions of the story continue with the defending team gathering in the penalty box to defend a set piece. In a normal football game, the reactions of the defending team would be different in the two cases. Ellie would be getting fist bumps or other signs of appreciation at a job well done. But it would be very poor form for Sam’s teammates to react in the same way. That’s true even though doing what Sam did,triggering the condition of a hypothetical imperative, improved her team’s position just as much as what Ellie did. Being a football player involves taking a certain attitude towards actions, and that attitude requires distinguishing Sam and Ellie’s attitude.\nThere is a famous real life example of this: Luis Suárez’s handball on the goal line against Ghana in the 2010 World Cup. On the rules as hypothetical imperatives model, the rules played out to perfection in this case. A penalty was awarded against Suárez’s team, and he was given a red card and a suspension. But this benefited his team, since the penalty was missed, and his team went on to win a game they surely would have lost otherwise. On the view that rules are hypothetical imperatives, then what Suárez did was great football, just like Ellie in the fictional example. But that all seems wrong. A lot of people in the game thought that it was unseemly of Suárez to be so proud of what he did. Yet why shouldn’t he have been proud?\nBoth of these cases can be explained if we understand rules as categorical imperatives, and the players’ attitude towards them not as binding constraints, but as providing reasons. Football includes a rule against handling the ball, and a rule against kicking other players. It also provides penalties for breaching these rules. But the force of the rules is not exhausted by penalties. The rules provide reasons that can be outweighed by other considerations, but never defeated. That’s why we don’t celebrate tackles like Sam’s, or saves like Suárez’s. They have done something that may have increased the team’s win probability, but which they had reason not to do. And their teammates share those reasons. Celebrating the action is a kind of complicity in wrongdoing.\nBorge’s view about the lusory attitude is similar to this, but I think a little different. He says that football players have to “endure, obey or accept the arbitration of the rules of football” (150). Or, as he’d put it previously, the players have to “defer to the referee and … respect his decisions”4. (I’m simplifying a bit here, not least by blurring the participant/practitioner distinction.)\nNow there is an obvious objection to this view. Players clearly do not respect the authority of the referee. It is a commonplace to see them surrounding the referee after an adverse decision complaining about it, and trying to cajole the referee to change their mind. If a defendant in a criminal trial reacted to a judge’s verdict this way, they’d be held in contempt of court. And it is hard to square respect with contempt.\nBorge should, I think, say that respecting the authority of the referee is better understood not in its ordinary usage, but just in the sense that the players do what the referee says. Maybe they complain about the mistaken award of a corner, but they don’t just take a goal kick if the referee is unmoved. That’s to say, the term ‘endure’ in the first quote above is important; it’s what players most often do.\nBut even this would be too strong a claim. Let me give just one amusing example. In 2002, I was watching the Germany-Ireland World Cup game in a bar in London. It ended with a stoppage time equaliser by Robbie Keane which brought the house down. But before that the most striking moment was an otherwise routine Ireland free kick. Germany lined up a wall, and the referee clearly said where they were supposed to stand. The camera operator, in a moment of genius, focused on the feet of the German players as the referee walked away. And as soon as his back was turned, four pairs of feet started shuffling forward in unison. The bar erupted in laughter. The lesson for us is that the players don’t have to respect the referee in the sense of doing what he says, or even endure his decisions; if they can get away with it they will just do something else.\nA better idea, not far from Borge’s I think, is to say that the players don’t have to respect the referee, but they do have to respect the rules of the game. Now this might seem absurd, in light of the examples of gratuitous rule breaking that we’ve used. But I think we can see why something like it is right if we step away from Germans and Uruguayans at World Cups, and imagine a park game. Thinking about games that are low stakes, and so the incentive to win at all costs is reduced, will help us get a better sense of what’s permissible.\nImagine Lisa is playing a game where there is a wall running down one sideline not far from the field of play. At one stage, Lisa is trapped with the ball near that sideline. She realises that a clever little bounce pass to herself off the wall will let her get out of the trap, and she executes it with aplomb. Now this might be a fun thing to do in practice, but it’s really not compatible with playing. When she does this, she has to some extent ceased to be a football player, and instead become someone who likes to show off football skills.\nOf course, Lisa won’t get any advantage from this, because the referee will simply award a throw in to the opposition. At least, the referee will probably do that. But maybe the referee will be unsighted, or incompetent, and will not award the throw. Still, it was wrong for Lisa to do that. It’s part of football that walls are not in play, and being a player requires acting as if that’s true.\nIf we imagine an incompetent referee, then we can push intuitions about cases like this even further. Imagine that Lisa goes on to notice that the referee either can’t or won’t penalise players for using their arms to control passes that come in at chest height. So every time she receives a pass to her chest, she uses her arm to help cushion the ball. The opposition are infuriated, she isn’t being subtle about it, but the referee doesn’t stop her, so she keeps on doing it. And eventually she gets a goal.\nI think she’s doing something wrong here. And I suspect, though perhaps cultural norms will vary a bit on this point, that if it is too blatant and the stakes are low enough, her teammates won’t be impressed either. They came to play a football game, and she’s making a mockery of it. Maybe they won’t celebrate the goal she gets by cheating this way, or maybe they will join in the opposition’s remonstration. Why don’t they just applaud her contribution to winning? The picture of rules as reasons explains this nicely I think. The rule against handball provides a reason for every player to not handle the ball. Maybe in a game with a huge amount at stake, the stakes override that reason. But in a park game, where the benefit of rule breaking is merely that Lisa gets a bit better control over the ball, that reason should be decisive. To the extent that she doesn’t treat it as decisive, she is undermining the sense in which they are playing football. And this can be true even if the referee won’t call this kind of foul.\nI think, and again I could be wrong, that the players would react very differently to Lisa than they’d react to the kind of ordinary shirt pulling and soft fouling that goes on at most corners. There is something particularly disrespectful about what Lisa is doing that doesn’t extend to fouls that everyone does all the time. And this is true even if Lisa would, were the referee to call her for a foul, be willing to shrug and hand the ball to the opposition for a free kick. (And then stand a foot closer than the referee said was allowed.) This is a puzzle, and I am not convinced Borge’s theory of what it is to play football can account for it.\nThe right thing to say here draws on a view of Fred D’Agostino’s that Borge discusses5. A sport has an ethos. This can’t be derived from the written rules of the game, but is something like the collective spirit in which it is played. In D’Agostino’s version, this provides the unbreakable rules of the game. The ethos says that if you do this or this, you’re no longer playing the game. This is too strong, as Borge points out. But something like it is right. My preferred version is that the ethos of the game provides the strength of reasons that go along with each rule. Currently the ethos says that shirt tugging at corners is something one has little reason to avoid, handball is something one has strong reason to avoid, and tackles from behind one has stronger reason still to avoid. But these aren’t essential to playing football; it was the same game when the strength of reasons were different.\nIn most cases in football, the strength of reasons is just what you might expect from a minimal familiarity with the game, combined with the fact that player safety is in everyone’s interests. But in other sports you need something like an ethos to explain a lot of what we see. In both cricket and baseball, a player on the batting side is out if they hit the ball and it is caught by a fielder before touching the ground. And in both sports there are hard cases where the ball, the fielder, and the ground come together almost simultaneously. But the sports treat these cases very differently. In cricket it is very poor form to appeal for a catch unless you are confident you caught the ball, and if you believe you did not catch it, you should say so to the officials. In baseball, you appeal for everything and leave it up to the officials to make the decisions. These principles are followed from the lowest levels of the game to the highest. You couldn’t derive them from the rules of the game, or from the idea that players should respect the rules and the officials. You need to appeal to something like D’Agostino’s idea of ethos to explain the difference between the sports.\nBut if an ethos is so essential to a sport, does that mean that players in communities with a different ethos are literally playing different games? As Borge points out, this would be an absurd result6. His example involves a World Cup team not used to the stricter refereeing in international games. But you don’t need to go that far afield. I’ve heard that in England it can be a debacle when a Premier League referee takes charge of a Championship game, because the players just one level down are used to getting away with much heavier tackles than a referee who has to look after superstars in the top flight will allow. Now here’s the objection. If the ethos is essential to the game, and the ethos is different in the Premiership and the Championship, then it follows they are playing a different sport in the Premiership and the Championship. And that’s a reductio of the position.\nThis criticism relies on reading too much into the notion of an ethos. It’s true that in a colloquial sense, the game has a different ethos in a place where a certain tackle is commonplace to what it has in a place where that tackle is routinely penalised. But this isn’t what D’Agastino meant by “ethos,” and it isn’t what I mean. In D’Agastino’s version, it concerned what was simply not to be done. The teams who are used to lighter refereeing typically won’t do things that teams used to stricter refereeing simply won’t do. The things they get penalised for all the time are part of the repetoire of the more mannered teams; it’s just that those teams don’t do them as often. So I’m not sure these are examples of difference in ethos in D’Agastino’s sense. And they need not be differences in my sense either. As I’m using the term, the ethos of a game tells you what reasons you have to not do certain actions beyond what penalties will be applied to those actions. Changing the penalties doesn’t even look like something that changes the non-penalty reasons.\nBut the bigger point to make in reply turns on the fact, much stressed by Borge, that football is social. Indeed, it is social twice over. Whether one is playing football at a given moment is a social fact. Whether I am reading a book at a moment is largely up to me. But there is literally nothing I could do right now, sitting at my computer with no one around, that would make it the case that I was playing a game of football. For that I would need teammates, and opponents (and for that matter a field) and none of them are to hand. But that doesn’t exhaust how social football is. As Borge argues in chapter two, what makes it the case that various token games are tokens of the kind football consists largely of social facts as well. Once we take these things into account, we can see that appeal to something like an ethos of football won’t make it the case that people with different attitudes are playing different games.\nThere is an objection to the whole project of this paper that you might have been considering, and which it is finally time to address. I’ve been asking what attitude is required to play football. And at some level the answer is that literally anything goes. If there is a field of the right kind, and 22 other people on it - 10 of them your teammates, 11 of them opponents, and a referee - and they are doing paradigmatically football type things, then as long as you’re in uniform you’re playing football. Short of pulling out a weapon and assaulting people with the weapon, there is little you could do that would count as not playing football, as opposed to playing badly. So how can we talk about the attitude that is necessary for playing football?\nWell, we can still talk generically about what the players in general must think and feel in order for there to be a game. Exceptions can be tolerated. It is easy to come up with extreme cases. Imagine an East German player playing in France in the 1960s, and spending the whole game looking for the safest moment to defect. Or imagine a girl from an area where scouts never venture, finally getting a chance to play in front of a scout, and for this game only caring about how impressive her play is. It will be hard to come up with any plausible story about the attitude of football players that covers their attitudes, yet they are still playing football. But those exceptions can be tolerated, as long as they are exceptions. If everyone is looking for a chance to defect, it isn’t really a game, it’s an escape attempt. If everyone is just looking to impress the scouts, it’s an exhibition or a scrimmage, not a game. What we’re after here is what must be true in general.\nBecause to a pretty close approximation, all it takes to be playing football is to be part of a football game. And being part of it might literally just mean wearing the right kit, and being on the right field. And it being a football game is a matter of this game standing in the right social relations to games of football across space and time. Neither requires any player have attitudes of any kind. But we can ask what attitudes, if any, are necessary to be generic across the players in this game for it to stand in the right relations to the class of all football games. And we can ask what attitudes, if any, are necessary to be generic across the players in all games if those games are to be, collectively, football.\nAnd like as above, I think an account in terms of reasons is basically right. What makes the players across all the football games the world over players of the same game? I think it’s because they are, generally, taking the rules to provide reasons to act, and not act, in certain ways. There is massive variation within these. At a junior enough level, they are barely cognisant of the rules, and so cannot take them as reasons. At a high enough level, they might be so focused on winning that they care little for the rules beyond the fact that rule violations might lead to penalties. But it would have to be a very jaded team that celebrates Sam just as much as they celebrate Ellie; even at the highest level, players’ reactive attitudes tend to generally acknowledge the reason-giving force of the rules.\nI’ll close by considering two related problems. If the games are associated with the way players take the rules to be reasons, that suggests that games are individuated much too finely. If here we regard the rule against handball as having just this strength as a reason, and over there they regard it as having a little less strength, then are we not both playing football? That would be absurd. And if little kids don’t understand the rules as having reason-giving force, because perhaps they don’t understand the rules at all, are they playing a different game? This seems wrong, since we can talk about someone having played football since they were four.\nThere are two points to note about ‘football’ that are relevant to both of these objections. The term is vague. Whether these five year olds kicking a ball around a small field, with no throw ins, goal keepers, headers, or offside rules, are playing football is a bit vague. There is a sense in which they are, and a sense in which they are not. And even given a precisification, the question of whether two people are playing the same sport, or the same game, doesn’t always correspond to the meaning of the name of the game, or games, they play. The same thing happens with language. How widely is English spoken? Do folks speak the same language in Glasgow, Pittsburgh and Sydney? There is a sense in which they are speaking different languages - they certain have a very different lexicon. But there is a perhaps more important sense in which they are speaking the same language, and that language is English. Is 50-over cricket the same sport, or the same game, as 20-over cricket? There is a sense in which the answer is yes, and a sense in which the answer is no. (I’m actually kind of surprised at how much the infrastructure around cricket supposes the no answer - the games are more similar to each other than either is to junior cricket.) The same happens here. It’s true on my view that there is a sense in which players who differ in what strength they give to the rules of football are playing different games, just like 50-over and 20-over cricket might be different games. But just like those are both games of cricket, and like the folks in Glasgow, Pittsburgh and Sydney are all speaking English, the players might all be playing football.\nWhile I’ve disagreed, at least on points of emphasis, with Borge, I want to close by expressing again my appreciation for his book. Philosophy is richer when it engages with real life, especially with those aspects of real life that make less sense the more you think about them. And his book is a great example of this kind of engagement, and is rewarding reading for anyone who cares about either football or philosophy, and especially for those of us who care about both.\n\n\n\nBorge, Steffen. 2010. “In Defence of Maradona’s Hand of God.” In Philosophy of Sport: International Perspectives, edited by A. Hardman and C. Jones, 154–79. Cambridge Scholars Publishing.\n\n\n———. 2019. The Philosophy of Football. London: Routledge.\n\n\nD’Agostino, Fred. 1981. “The Ethos of Games.” Journal of the Philosophy of Sport, 7–18.\n\n\nSuits, Bernard. 1978. The Grasshopper: Games, Life and Utopia. Toronto: University of Toronto Press.\n\n\nSee the ‘Fistful of Fouls’ example on page 139.↩︎\nSee the ideas for how to flesh out a Suitsian view on pages 154ff.↩︎\nAgain, I’m focussing on the discussion on pages 143ff.↩︎\nBorge (2010, 164), as cited on page 150↩︎\nSee D’Agostino (1981) for the original, and pages 144-148 of Borge’s book for the discussion.↩︎\nI’m drawing here on his discussion of Ghafoor Jahani at the 1978 World Cup, on page 148.↩︎\n",
    "preview": "posts/2021-01-04-the-sporting-attitude/matildas.jpg",
    "last_modified": "2021-03-04T10:22:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-deliberation-costs/",
    "title": "Deliberation Costs",
    "description": "Our theory of rational choice should be sensitive to deliberation costs. It is irrational to take into account minor differences between goods, if the cost of taking those differences into account is greater than the expected gain from doing so. It has often been held in economics that this line of reasoning will lead to an infinite regress. I argue that the regress can be stopped if we take the rational chooser to be skilled at attending to the right information. On the appropriate model of skill, the rational agent will attend to the right information without reasoning about whether this is the right information to attend to.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2020-06-17",
    "categories": [
      "games and decisions",
      "epistemology",
      "philosophy of economics",
      "unpublished"
    ],
    "contents": "\nHumans making decisions face two big limitations. First, we are informationally limited. We don’t know everything and sometimes we don’t know what we need to know in order to make the optimal decision. Second, we are computationally limited. We can’t process all of the information that we have available to us before a decision needs to be made. Or at least, we can’t do this in a costless manner.\n\nUnpublished draft. Thanks to audiences at Michigan, Toronto, and the Arizona Philosophy Workshop for valuable feedback, not all of which I’ve yet incorporated.\nOrthodox decision theory treats these two limitations very differently. To a first approximation, the whole point of orthodox decision theory is to handle the question of how to make decisions without full information. But on the other hand, orthodox decision theory simply assumes away the computational limitations. Orthodox decision theory is a theory of rational choice, and rationality is here understood to involve not being subject to these pesky computational limitations.\n\nPicture by Andy Warhol via Creative Commons.\nI think this is a serious mistake. In particular, I think there are several cases where our theory of rational choice can only give us the correct verdict if we allow it to be sensitive to both kinds of limitation. In this paper, I will discuss three such kinds of cases, and describe how rational choice theory might be revised so as to handle them.\nI’m far from the first to notice this asymmetry in how orthodox decision theory handles the two limitations. For approximately as long as decision theory has existed, there have been people who have noted the oddity of ignoring computational limitations. But there has always been a powerful argument against taking computational limitations seriously. It is long been thought that attempt to do this would lead to a nasty kind of regress. It isn’t entirely clear how the regress argument here is supposed to run; the argument is more often alluded to than carefully stated. But it is a major challenge and I will have something to say about it.\nThe short version of what I’m going to say is that while we should take both kinds of limitations seriously, we should treat them differently in our final theory. We should, as orthodox decision theory says, take a broadly evidentialist approach to informational limitations. That is, good decision makers should have credence distributions over the possibilities left open by their evidence, those credences should be sensitive to the evidence they have, and the choices they make should maximize expected value given those credences. On the other hand, we should take a broadly reliabilist approach to computational limitations. Good decision makers will adopt procedures for managing their own limitations that reliably produce good outcomes. There is no requirement that they adopt the procedures that are best supported by their evidence. The reason there is no requirement they do that is that figuring out what those reliable procedures might be is even more computationally taxing then the problem of deciding what to do. And if we’re going to respect the fact that people can’t always complete difficult computational tasks, we shouldn’t expect them to perform the incredibly difficult task of figuring out how to adjust their decision procedures in light of the evidence about their own limitations.\nYou might think that the reason orthodox theory treats computational limitations this way is that it is simply trying to provide a theory of ideal decision making. There is a separate question, to be sure, of how non-ideal agents should make decisions. But the thought, or at least the hope, is that clearly stating what the ideal looks like will help the non-ideal agents in this task. I think there is a little reason to believe that this hope will be realized. In general, knowing what the ideal looks like provides us with very little guidance as to how to get better. Knowing that any ideal outcome has a certain attribute does not provide a reason, even a defeasible reason, for trying to to acquire that attribute (Lipsey and Lancaster 1956).\nWe can see this by simply thinking about the one limitation that orthodox theory does take seriously. A good decision maker without full information will in general behave nothing like a good decision maker with full information. For example, if you put the informationally limited agent in a casino they will do the exact opposite of what an informationally unlimited agent will do. The informationally unlimited agent will play every game and do quite well at them. The informationally limited agent, on the other hand, will play none of the games because they all have negative expected returns. I think is the general case. It’s a bad idea to emulate the ideal agent, because us non ideal agents often have to act so as to minimize the damage that have other limitations can do. Everyone agrees that is true in the case of informational limitations, and I am going to try and argue that it’s also true for computational limitations.\nSo here’s the plan for the paper. First, in sections 1-2, I will introduce the three kinds of cases but I think motivate taking computational limitations seriously. Then, in sections 3-4, I will introduce the regress argument that is alleged to show that any attempt to do this will end badly. In sections 5-6, I will show how the broadly reliabilist approach to handling computational limitations that I favor can be motivated, and can avoid the regress. Sections 7 and 8 are contingent speculations about how non-ideal agents might choose reliably, and observations on how these debates connect to other philosophical debates\nBefore I start on this, it’s helpful to get clear on exactly what I am taking my orthodox opponent to be committed to. I take them to endorse the following three constraints on a theory of rational choice.\nRational agents have credences, and these credences are responsive to evidence.\nThese credences also respect the probability calculus.\nRational agents take actions that maximize their expected utility given these credences.\nThere are a lot of questions that I do not take my orthodox opponent to have a settled view on, though of course many orthodox theorists will have one view or another on one or other of these questions. These questions include\nWhether rationality puts any constraints on what can be valued;\nWhether our theory of rationality divides up failures to make rational choices into epistemic failures, axiological failures, and practical failures, and if it does make such a division, exactly how it should be made;\nWhether rationality requires that agent be self-aware, i.e., whether they know what their own credences and utilities are; and\nExactly what evidence is, or what it means for credences to be responsive to evidence.\nMy hope is that I can provide an objection to orthodoxy that is insensitive to how orthodox theorists answered these questions. That’s a rather ambitious project, since the answers one gives to these questions will help provide responses to some of the objections I shall offer. But I’m not going to try to anticipate every possible response the orthodox theorist could make. Indeed, I don’t think that I’ve got anything like a knock down watertight argument against all possible versions of orthodoxy. What I think I do have is a set of reasons to consider an alternative, and an outline of what that alternative may look like.\nThree Puzzles\nPuzzle One - Close Calls\nLet’s start with an example from a great thinker. It will require a little exegesis, but that’s not unusual when using classic texts.\n\nWell Frankie Lee and Judas Priest\nThey were the best of friends\nSo when Frankie Lee needed money one day\nJudas quickly pulled out a roll of tens\nAnd placed them on the footstool\nJust above the potted plain\nSaying \"Take your pick, Frankie boy,\nMy loss will be your gain.\"\n          (“The Ballad of Frankie Lee and Judas Priest,” 1968.\n           Lyrics from Bob Dylan (2016) 225)\n\nOn a common reading of this, Judas Priest isn’t just asking Frankie Lee how much money he wants to take, but which invididual notes. Let’s simplify, and say that it is common ground that Frankie should only take $10, so the choice Frankie Lee has is which of the individual notes he will take. This will be enough to set up the puzzle.\nAssume something else that isn’t in the text, but which isn’t an implausible addition to the story. The world Frankie Lee and Judas Priest live in is not completely free of counterfeit notes. And it would be bad for Frankie Lee to take a counterfeit note. It won’t matter just how common these notes are, or how bad it would be. But our puzzle will be most vivid if each of these are relatively small quantities. So there aren’t that many counterfeit notes in circulation, and the (expected) disutility to Frankie Lee of having one of them is not great. There is some chance that he will get in trouble, but the chance isn’t high, and the trouble isn’t any worse than he’s suffered before. Still, other things exactly equal, Frankie Lee would prefer a genuine note to a counterfeit one.\nNow for some terminology to help us state the problem Frankie Lee is in. Assume there are \\(k\\) notes on the footstool. Call them \\(n_1, \\dots, n_k\\). Let \\(c_i\\) be the proposition that note \\(n_i\\) is counterfeit, and its negation \\(g_i\\) be that it is genuine. And let \\(t_i\\) be the act of taking note \\(n_i\\). Let \\(U\\) be Frankie Lee’s utility function, and \\(Cr\\) his credence function.\nIn our first version of the example, we’ll make two more assumptions. Apart from the issue of whether the note is real or counterfeit, Frankie Lee is indifferent between the notes, so for some \\(h, l\\), \\(U(t_i | g_i) = h\\) and \\(U(t_i | c_i) = l\\) for all \\(i\\), with of course \\(h > l\\). If we add an extra assumption that Frankie Lee thinks the probability that each of the notes is genuine is the same, we get the intuitive result back that he is indifferent between the banknotes.\nBut is that really a plausible move? Here is one way to start worrying about it. Change the example so that the country Frankie Lee and Judas Priest live in is very slowly modernising its currency. It is getting rid of old fashioned, and somewhat easy to counterfeit, paper money, and joining the civilised countries that use plastic money. Moreover, plastic bank notes are, for all intents and purposes, impossible to counterfeit. (At least, no one has yet figured out how to do it, and Frankie Lee knows this.)\nSome of the notes Judas Priest offers are the new plastic notes, and some are the old paper notes. Now it seems clear that Frankie Lee should take one of the new notes, and not merely on aesthetic grounds. Rather, the fact that the plastic notes are less likely to be counterfeit is a reason to prefer to take them. And this is true no matter how unlikely it is that the paper notes are counterfeit, as long as this likelihood is non-zero.\nBut now go back to the base case, where all the money is paper. A small change in probability of being counterfeit seems to be enough to give Frankie Lee a reason to prefer some of them to the others. Indeed, the only way for him to be indifferent between the notes is if the probability of any one being counterfeit is exactly the same as the probability of any other being counterfeit. But that two of the notes have exactly the same probability of being counterfeit is a measure zero event. It isn’t happening. So Frankie Lee shouldn’t be indifferent between the notes.\nOf course, if the notes look exactly the same, then the probability that each is counterfeit is exactly the same. But that’s only because that probability is one. In that case Frankie Lee should run away as fast as possible. That’s not the realistic case.\nThe realistic case is that the notes look a little different to each other in ever so many respects. (Including, one hopes, their serial numbers.) Some will be a little more faded, or a little more torn, or a little more smudged or crumpled, than the others. It is overwhelmingly likely that these fades, tears, smudges, spills etc are the result of the normal wear and tear on the currency - wear and tear that paper notes tend to wear on their face. But every imperfection in every note is some evidence, very very marginal evidence but still evidence, that the note is counterfeit. And since Frankie Lee’s evidence, on any extant theory of evidence, includes visible things like the tears, smudges etc on the notes, they are pieces of evidence that affect the evidential expected utility of taking each note. So if Frankie Lee wants to maximize evidential expected utility, there is precisely one note he should take. Though it probably won’t be obvious to him which one it is, so rationality requires Frankie Lee to spend some time thinking about which note is best.\nThis is intuitively the wrong result. (Though it is what happens in the song.) Frankie Lee should just make a choice more or less arbitrarily. Since expected utility theory does not say this, expected utility theory is wrong.\nThe Frankie Lee and Judas Priest case is weird. Who offers someone money, then asks them to pick which note to take? And intuitions about such weird cases cases are sometimes deprecated. Perhaps the contrivance doesn’t reveal deep problems with a philosophical theory, but merely a quirk of our intuitions. I am not going to take a stand on any big questions about the epistemology of intuitions here. Rather, I’m going to note that cases with the same structure as the story of Frankie Lee and Judas Priest are incredibly common in the real world. Thinking about the real world examples can both show us how pressing the problems are, and eventually show us a way out of those problems.\nSo let’s leave Frankie Lee for now, just above the potted plain, and think about a new character. We will call this one David, and he is buying a few groceries on the way home from work. In particular, he has to buy a can of chickpeas, a bottle of milk, and a carton of eggs. To make life easy, we’ll assume each of these cost the same amount: five dollars.1 None of these purchases is entirely risk free. Canned goods are pretty safe, but sometimes they go bad. Milk is normally removed from sale when it goes sour, but not always. And eggs can crack, either in transit or just on the shelf. In David’s world, just like ours, each of these risks is greater than the one that came before.\nDavid has a favorite brand of chickpeas, of milk, and of eggs. And he knows where in the store they are located. So his shopping is pretty easy. But it isn’t completely straightforward. First he gets the chickpeas. And that’s simple; he grabs the nearest can, and unless it is badly dented, or leaking, he puts in in his basket. Next he goes onto the milk. The milk bottles have sell-by dates printed in big letters on the front. And David checks that he isn’t picking up one that is about to expire. His store has been known to have adjacent bottles of milk with sell-by dates 10 days apart, so it’s worth checking. But as long as the date is far enough in the future, he takes it and moves on. Finally, he comes to the eggs. (Nothing so alike as eggs, he always thinks to himself.) Here he has to do a little more work. He takes the first carton, opens it to see there are no cracks on the top of the eggs, and, finding none, puts that in his basket too. He knows some of his friends do more than this; flipping the carton over to check for cracks underneath. But the one time he tried that, the eggs ended up on the floor. And he knows some of his friends do less; just picking up the carton by the underside, and only checking for cracks if the underside is sticky where the eggs have leaked. He thinks that makes sense too, but he is a little paranoid, and likes visual confirmation of what he’s getting. All done, he heads to the checkout, pays his $15, and goes home.\nThe choice David faces when getting the chickpeas is like the choice Frankie Lee faces. He has to choose from among a bunch of very similar seeming options. In at least the chickpeas example, he should just pick arbitrarily. But for very similar reasons to Frankie Lee, expected utility theory won’t say that.\nThe standard model of practical rationality that we use in philosophy is that of expected utility maximization. But there are both theoretical and experimental reasons to think that this is not the right model for choices such as that faced by Frankie or David. maximizing expected utility is resource intensive, especially in contexts like a modern supermarket, and the returns on this resource expenditure are unimpressive. What people mostly do, and what they should do, is choose in a way that is sensitive to the costs of adopting one or other way.\nThere are two annoying terminological issues around here that I mostly want to set aside, but need to briefly address in order to forestall confusion.\nI’m going to assume maximizing expected utility means taking the option with the highest expected utility given facts that are readily available. So if one simply doesn’t process a relevant but observationally obvious fact, that can lead to an irrational choice. I might alternatively have said that the choice was rational (given the facts the chooser was aware of), but the observational process was irrational. But I suspect that terminology would just add needless complication.\nI’m going to spend more time on another point that is partially terminological, but primarily substantive. That’s whether we should identify the choice consequentialists recommend in virtue of the fact that it maximizes expected utility with one of the options (in the ordinary sense of option), or something antecedent. I’m going to stipulate (more or less) that it is consistent with consequentialism that the choice can be something antecedent - it can be something like a choice procedure. And I’m going to argue that this is what the rational consequentialist should choose.\nI’m going to call any search procedure that is sensitive to resource considerations a satisficing procedure. This isn’t an uncommon usage. Charles Manski (2017) uses the term this way, and notes that it has rarely been defined more precisely than that. But it isn’t the only way that it is used. Mauro Papi (2013) uses the term to exclusively mean that the chooser has a ‘reservation level,’ and they choose the first option that crosses it. This kind of meaning will be something that becomes important again in a bit. And Chris Tucker (2016), following a long tradition in philosophy of religion, uses it to mean any choice procedure that does not optimize. Elena Reutskaja et al (2011) contrast a ‘hybrid’ model that is sensitive to resource constraints with a ‘satisficing’ model that has a fixed reservation level. They end up offering reasons to think ordinary people do (and perhaps should) adopt this hybrid model. So though they don’t call this a satisficing approach, it just is a version of what Manski calls satisficing. Andrew Caplin et al (2011), on the other hand, describe a very similar model to Reutskaja et al’s hybrid model - one where agents try to find something above a reservation level but the reservation level is sensitive to search costs - as a form of satisficing. So the terminology around here is a mess. I propose to use Manski’s terminology: agents satisfice if they choose in a way that is sensitive to resource constraints.\nIdeally they would maximize, subject to constraints, but saying anything more precise than this brings back the regress problem that we started with. Let’s set it aside just a little longer, and go back to David and the chickpeas.\nWhen David is facing the shelf of chickpeas, he can rationally take any one of them - apart perhaps from ones that are seriously damaged. How can expected utility theory capture that fact? I think if it identifies David’s choices with the cans on the shelf, and not with a procedure for choosing cans, then it cannot.\nIt says that more than one choice is permissible only if the choices are equal in expected utility. So the different cans are equal in expected utility. But on reflection, this is an implausible claim. Some of the cans are ever so slightly easier to reach. Some of the cans will have ever so slight damage - a tiny dint here, a small tear in the label there - that just might indicate a more serious flaw. Of course, these small damages are almost always irrelevant, but as long as the probability that they indicate damage is positive, it breaks the equality of the expected utility of the cans. Even if there is no visible damage, some of the labels will be ever so slightly more faded, which indicates that the cans are older, which ever so slightly increases the probability that the goods will go bad before David gets to use them. Of course in reality this won’t matter more than one time in a million, but one in a million chances matter if you are asking whether two expected utilities are strictly equal.\nThe common thread to the last paragraph is that these objects on the shelves are almost duplicates, but the most careful quality control doesn’t produce consumer goods that are actual duplicates. There are always some differences. It is unlikely that these differences make precisely zero difference to the expected utility of each choice. And even if they do, discovering that is hard work.\nSo it seems likely that, according to the expected utility model, it isn’t true that David could permissibly take any can of chickpeas that is easily reachable and not obviously flawed. Even if that is true, it is extremely unlikely that David could know it to be true. But one thing we know about situations like David’s is that any one of the (easily reached, not clearly flawed) cans can be permissibly chosen, and David can easily know that. So the expected utility model, as I’ve so far described it, is false.\nPuzzle Two - Psychic Costs of Bias\nIn all but a vanishingly small class of cases, the different cans will not have the same expected utility. But figuring out which can has the highest expected utility is going to be work. It’s possible in principle, I suppose, that someone could be skilled at it, in the sense that they could instinctively pick out the can whose shape, label fading, etc., reveal it to have the highest expected utility. Such a skill seems likely to be rare - though I’ll come back to this point below when considering some other skills that are probably less rare. For most people, maximizing expected utility will not be something that can be done through skill alone; it will take effort. And this effort will be costly, and almost certainly not worth it. Although one of the cans will be ever so fractionally higher in expected utility than the others, the cost of finding out which can this is will be greater than the difference in expected utility of the cans. So aiming to maximize expected utility will have the perverse effect of reducing one’s overall utility, in a predictable way.\nThe costs of trying to maximize expected utility go beyond the costs of engaging in search and computation. There is evidence that people who employ maximizing strategies in consumer search end up worse off than those who don’t. A. A. M. Schwartz Barry AND Ward (2002) reported that consumers could be divided in ‘satisficers’ and ‘maximizers.’ And once this division is made, it turns out that the maximizers are less happy with individual choices, and with their life in general. This finding has been extended to work on career choice (S. S. I. A. R. E. W. A. B. Schwartz 2006), where the maximizers end up with higher salaries but less job satisfaction, and to friend choice (Newman et al. 2018), where again the maximizers seem to end up less satisfied.\nThere are two things that can go wrong when you try to maximize. Maximising requires considering the strengths and weaknesses of each of the choices. That means, it requires giving at least some consideration to the negative attributes of what you end up choosing. And these can cause you to be less happy with the actual choice when those negative attributes are realized. And it also means giving consideration to the positive attributes of the choices not made. And this could lead to regret when you have to adopt a choice that lacks those positive attributes. So there are two very natural paths by which the attempt to maximize could backfire, any incurs costs that wouldn’t have been incurred by the person who simply makes an arbitrary choice.\nThere is evidence here that both these paths are realised, and that maximisers do indeed end up psychically worse off than satisficers. Now to be sure, there are both empirical and theoretical reasons to be cautious about accepting these results at face value. Whether the second path, from consideration of positive attributes of the non-chosen option to felt regret, is psychologically significant seems to be tied up with the ‘paradox of choice’ (B. Schwartz 2004), the idea that sometimes giving people even more choices makes them less happy with their outcome, because they are more prone to regret. But it is unclear whether such a paradox exists. One meta-analysis (Todd 2010) did not show the effect existing at all, though a later meta-analysis finds a significant mediated effect (Goodman 2015). But it could also be that the result is a feature of an idiosyncratic way of carving up the maximizers from the satisficers. Another way of dividing them up produces no effect at all (Diab 2008).\nThe theoretical reasons relate to Newcomb’s problem. Even if we knew that maximizers were less satisfied with how things are going than satisficers, it isn’t obvious that any one person would be better off switching to satisficing. They might be like a two-boxer who would get nothing if they took one-box. There is a little evidence in S. S. I. A. R. E. W. A. B. Schwartz (2006) that this isn’t quite what is happening, but the overall situation is unclear.\nBut the philosophical questions here are a bit simpler than the psychological questions. Whether maximisers in general are subject to these two kinds of costs is a tricky empirical question. Whether there could be one maximiser who is subject to them, and who knows that they are, is a much easier question. Of course someone could be like that. Indeed, it seems beyond dispute that many real people are subject to these costs. The only empirical question is whether these people are a significant minority or a significant majority.\nAnd all it takes for the philosophical question to be pressing is that some choosers are, and know that they are, disposed to incur these psychological costs if they consciously try to maximise expected value. Our theory of choice should have something to say to them, and orthodox theory is silent. Especially for choices that are intended to produce happiness, the happiness effects of the choice procedure itself should be taken into account. But orthodox theory ignores it.\nPuzzle Three - Mathematical Challenges\nFor a final case, let’s consider Kyla, a student taking a mathematics exam. It’s getting towards the end of the exam and she’s facing quite a bit a time pressure. She comes to a true false question, and she knows that she knows how to solve questions like it. But she also knows that there are other kinds of questions that she is better at solving under time pressure. And while this is just a true false question, the exam is set up so that she gets a large negative score if she gets the question wrong. The expected return of simply guessing is strongly negative.\nThe rational thing for Kyla to do is to go on to other questions and come back to this one if she has time. But orthodox theory doesn’t allow for this. The probability of any mathematical truth is one. And it’s part of orthodoxy that credences are supposed to be probability functions. So whatever the correct answer is, offering it will have positive expected utility given Kyla’s credences, assuming those credences are rational.\nSo orthodoxy gets this choice, and all other choices that turn on mathematical ignorance, badly wrong. The case where Kyla simply has to decide whether to answer the question now or come back to it later is in some ways a relatively easy case. The really hard decisions are about how much time to allocate to solving various mathematical problems, when there are both costs to spending time, and rewards to solving as many problems as possible. These can often be important decisions, and ones that our theory should have something to say about. But orthodoxy does not have anything to say. It’s time to look for something else.\nDialectical Interludes\nInterlude One - The Obvious Answer\nBy this time you might be expecting a relatively simple answer to this question. The problem is that the orthodox theorist was focussing on the wrong choice. We shouldn’t focus on the choice to take this can of chickpeas or that one, or to answer true or false to this question. Rather, we should focus on the choice to choose one procedure or another. And the rational chooser will choose the procedure that is on average best.\nThat solves our cases quite well. The best procedure for Frankie Lee or David to adopt is to choose arbitrarily. Any other procedure will take time, and it’s not going to be time well spent. The best procedure for the person wracked by regret at choices they didn’t make is also to choose somewhat arbitrarily, before the regrets have time to embed. Conversely, the best procedure for Kyla is to skip any questions that she can’t do quickly, and come back to them if it turns out she has time.\nGiven some very weak assumptions, Maximise expected utility will not be an optimal procedure in this sense. Actually it’s ambiguous what it means to say someone should adopt the procedure Maximise expected utility, but however you disambiguate that, it’s wrong. The procedure Calculate what maximises expected utility then choose it is not optimal, because the calculations may not be worth the effort. The procedure Instinctively choose what maximises expected utility is a very efficient procedure if it is available, but for most agents it isn’t available. We should no more criticise agents who don’t adopt it than we criticise agents who don’t get to work by apparating.\nI’m going to adopt a version of the view that the rational choice is the outcome of an optimal procedure. But I’m not going to adopt the most obvious version of this obvious answer. In particular, I’m not going to say that agents should adopt the procedure such that adopting that procedure maximises expected value. Rather, I think, they should adopt the procedure that maximises something like average value. We’ll return to this in a bit, but first I want to clear up some other dialectical points.\nDialectical Point Two - Possible Orthodox Solutions\nThere are ways of tinkering with orthodoxy to avoid some of the problems that I raised in the previous section. For example, dropping the constraint that credences are probabilities would avoid giving the wrong answer in Kyla’s case. And maybe, just maybe, there is a theory of evidence, or of evidential support, such that the evidential expected utility of each of Frankie Lee’s choices are not distinct. I’m certainly not going to try to go through every possible theory of evidence, or of evidential support, to show that this isn’t the case.\nBut I do want to note three constraints on an orthodox solution to the problems that I have raised.\nFirst, the solution must handle all the cases. This is not a completely trivial point. The reason orthodoxy fails in the three cases is a little different in each case. There is not, at least as far as I can tell, a simple way to handle all of them simultaneously while staying roughly within orthodoxy.\nSecond, the solution must not introduce any more complications of its own. For example, you could try to solve some of the problems by saying that the decision maker’s evidence includes just those facts that are immediately available to her. Perhaps there is some sense of ‘immediacy’ in which this provides the start of a solution to the first two puzzles. (I think the third puzzle won’t be solved this way, but the first two might.) But this solution introduces problems of its own. For example, a decision can be irrational in virtue of the fact that a moment’s thought would’ve revealed to the decision maker that it will lead to disaster. If we restrict evidence to what is available at less than a moment’s thought, then we get this case wrong. If we don’t put such a restriction in place, then we’re back to having problems with the first two puzzles I mentioned above. I don’t want to clean there is nothing for the orthodox theorist to do here, but I do think it will be tricky to handle the puzzles without licencing irrational thoughtlessness.\nThird, any orthodox solution should be just as simple and as well motivated as the obvious answer I just discussed. Saying that we should focus on procedures and not on the choices they lead to on an occasion resolves all of these puzzles in a simple and natural way. Even if an orthodox solution can be found to all three puzzles, if it requires three different changes to the orthodox view, it’s hard to believe that it will be preferable to a simple solution in terms of procedures.\nDialectical Point Three - Terminology\nAt this point, some people might want to simply stipulate that the word “rational” picks out the choice that a computationally ideal actor would take. Even if it’s good in some sense for David to choose arbitrarily, there is still an ideal can to choose, and he only deserves the honorific rational if he chooses it.\nI am not going to get into a fight over terminology here. If people want to continue inquiring into what David would ideally do, then I’m not going to get in their way. But I found this inquiry unmotivated for three reasons. First, if we’re going to consider what David would ideally do, then I’m more interested in what he’d do if he were really ideal, and knew everything. I don’t see the appeal of investigating what he would do given one, but only one, kind of idealisation. Second, I don’t think the ideal is a particularly good guide. Knowing what the gods do doesn’t help the mortals, for mortals just get burned if they try to be like gods.\nBut the biggest reason concerns a purpose that I think is a central function of the concept of rationality. We have a need to make the people around us intelligible and predictable. And the best way we have to do this is to understand the constraints and the motivations of people around us, and feed those into a theory of rational choice that outputs a decision given constraints and motivations. It doesn’t always work, especially if you are trying to make predictions. But it beats most of the alternatives by a comfortable margin.\nIf that’s the reason for having a theory of rational choice, then the orthodox theory is not fit for purpose. The person who stands in the grocery store aisle deliberating over which can to get is neither intelligible nor predictable. The theory that says rational agents adopt procedures that do well on average, given their constraints and motivations, does make the ordinary behavior of supermarket shoppers intelligible and predictable.\nWhen I say ‘we’ need to make folks around us intelligible and predictable, I mean this to work at two different levels. From a very early age, we do this kind of reasoning about particular individuals to learn about the world (Scott and Baillargeon 2013). If a child sees a competent seeming adult use a particular method to solve a problem, and the adult does not seem to have any constraints that the child is free of, the child will copy what the adult does (Levy and Alfano 2020). This makes perfect sense; the adult is rational (and better informed than the child), so probably the adult’s procedure is optimal for the child. If we know that children do this, we can exploit it to trick them. For example, we can demonstrate sub-optimal procedures, and children will mimic them for a surprisingly long time. But this isn’t because the child is a fool; it’s because they have a clever way of learning about the world that can misfire when people set out to confound it.\nBut I also mean this to work at the level of social analysis. The whole point of game theoretic explanations of social phenomena is that we can make a pattern of behavior intelligible by simply presenting the constraints and motivations of the choosers, and then showing how rational behavior on everyone’s part produces the outcome. The research program this paper is a part of is motivated by the hope, and it is a hope more than a theory, that the same theory of rationality can serve both the child who is selectively imitating those around them, and the social scientist with their game theoretic models. Whether that’s true in general or not, I think both the child and the theorist are better served by a theory of rational choice that is sensitive to computational limitations and deliberation costs. And it’s their perspectives that I’m most interested in when theorising about rationality.\nThere is one other terminological dispute that I have no interest in entering into, but I need to make explicit in order to set aside. Some philosophers use ‘decision theory’ to refer to the study of purely procedural aspects of rationality. On this picture, there are three parts to rational choice: epistemology, axiology and decision theory. A rational agent will comply with all three. Compliance with the first is manifest in rational credences. Compliance with the second is manifest in rational values. And compliance with the third is manifest in choices that are rational given the first two. I don’t much care for this highly factorised model of rational choice theory. Imagine we see someone punching themselves in the head, and ask why they are doing this. If they say, “I want to bring about world peace, and I believe this is the best way to do it,” we don’t reply, “Well, I guess two out of three isn’t bad.” We just think they are irrational. But for current purposes I don’t want to debate this. This paper is about the theory of rational choice. If you think that encompasses more than decision theory, that it also includes epistemology and axiology, then this isn’t a paper in decision theory strictly speaking. But even someone who thinks the theory of rational choice can be factorised in this way still thinks there is a theory of rational choice. And my plan here is to offer a rival theory. Whether what I offer is a rival theory of decision turns on terminological questions about ‘decision theory’ that I’m hereby setting aside.\nHistory and Regresses\nThe idea that rational people are sensitive to their own computational limitations has a long history. It is often traced back to a footnote of Frank Knight’s. Here is the text that provides the context for the note.\n\nLet us take Marshall’s example of a boy gathering and eating berries … We can hardly suppose that the boy goes through such mental operations as drawing curves or making estimates of utility and disutility scales. What he does, in so far as he deliberates between the alternatives at all*, is to consider together with reference to successive amounts of his “commodity,” the utility of each increment against its “cost in effort,” and evaluate the net result as either positive or negative (Knight 1921, 66–67)\n\nAnd the footnote attached to ‘at all’ says this\n\nWhich, to be sure, is not very far. Nor is this any criticism of the boy. Quite the contrary! It is evident that the rational thing to do is to be irrational, where deliberation and estimation cost more than they are worth. That this is very often true, and that men still oftener (perhaps) behave as if it were, does not vitiate economic reasoning to the extent that might be supposed. For these irrationalities (whether rational or irrational!) tend to offset each other. (Knight 1921, 67fn1)\n\nKnight doesn’t really give an argument for the claim that these effects will offset. And as John Conlisk (1996) shows in his fantastic survey of the late 20th century literature on bounded rationality, it very often isn’t true. Especially in game theoretic contexts, the thought that other players might think that “deliberation and estimation cost more than they are worth” can have striking consequences. But our aim here is not to think about economic theorising, but about the nature of rationality.\nThere is something paradoxical, almost incoherent, about Knight’s formulation. If it is “rational to be irrational,” then being “irrational” can’t really be irrational. There are two natural ways to get out of this paradox. One, loosely following David Christensen (2007), would be to say that “Murphy’s Law” applies here. Whatever one does will be irrational in some sense. But still some actions are less irrational than others, and the least irrational will be to decline to engage in deliberation that costs more than it is worth. I suspect what Knight had in mind though was something different (if not obviously better). He is using ‘rational’ as more or less a rigid designator of the the property of choosing as a Marshallian maximizer does. And what he means here is that the disposition to not choose in that way will be, in the long run, the disposition with maximal returns.\nThis latter idea is what motivates the thought that rational agents will take what John Conlisk calls “deliberation costs” into account. And Conlisk thinks that this is what rational agents will do. But he also raises a problem for this view, and indeed offers one of the clearest (and most widely cited) statements of this problem.\n\nHowever, we quickly collide with a perplexing obstacle. Suppose that we first formulate a decision problem as a conventional optimization based on the assumption of unbounded rationality and thus on the assumption of zero deliberation cost. Suppose we then recognize that deliberation cost is positive; so we fold this further cost into the original problem. The difficulty is that the augmented optimization problem will itself be costly to analyze; and this new deliberation cost will be neglected. We can then formulate a third problem which includes the cost of solving the second, and then a fourth problem, and so on. We quickly find ourselves in an infinite and seemingly intractable regress. In rough notation, let \\(P\\) denote the initial problem, and let \\(F(.)\\) denote the operation of folding deliberation cost into a problem. Then the regress of problems is \\(P, F(P), F^2(P), \\ldots\\) (Conlisk 1996, 687)\n\nConlisk’s own solution to this problem is not particularly satisfying. He notes that once we get to \\(F^3\\) and \\(F^4\\), the problems are ‘overly convoluted’ and seem to be safely ignored. This isn’t enough for two reasons. First, even a problem that is convoluted to state can have serious consequences when we think about solving it. (What would Econometrica publish if this weren’t true?) Second, as is often noted, \\(F^2(P)\\) might be a harder problem to solve than \\(P\\), so simply stopping the regress there and treating the rational agent as solving this problem seems to be an unmotivated choice.\nAs Conlisk notes, this problem has a long history, and is often used to dismiss the idea that folding deliberation costs into our model of the optimising agent is a good idea. I use ‘dismiss’ advisedly here. As he also notes, there is very little discussion of this infinite regress problem in the literature before 1996. The same remains true after 1996. What is done is that instead people appeal to the regress in a sentence or two to set aside approaches that incorporate deliberation cost in the way that Conlisk suggests.\nUp to around the time of Conlisk’s article, the infinite regress problem was often appealed to by people arguing that we should, in effect, ignore deliberation costs. After his article, the appeals to the regress comes from a different direction. It is usually from theorists arguing that deliberation costs are real, but the regress means it will be impossible to consistently incorporate them into a model of an optimizing agent. So we should instead rely on experimental techniques to see how people actually handle deliberation costs; the theory of optimization has reached its limit. This kind of move is found in writers as diverse as Selten (2001), Odell (2002), Pingle (2006), Slack (2010), Tanaka (2017) and Chakravarti (2017). And proponents of taking deliberation costs seriously within broadly optimizing approaches, like Miles Kimball (2015), say that solving the regress problem is the biggest barrier to having such an approach taken seriously by economists. So let’s turn to how we might solve it.\nFour Non-Solutions\nMy solution, as I’ve mentioned a couple of times, is a form of reliabilism. The rational choice is the one that would be produced by using the procedure that does best on average. That procedure will just be maximising expected utility when computational costs are zero, and will involve appeal to expected utility maximisation in many other cases. But it won’t, in general, simply be expected utility maximisation.\nTo get clearer on what the reliabilist solution is, and how it is motivated, I want to first go through three other solutions that I don’t think work.\nFirst, we could just say that the rational choice is simply the choice that produces the best actual result. This gets some cases intutively wrong; it says that it is never rational to leave a casino without gambling. And it eliminates a type of choice that we think is real: the lucky guess. We want lucky guesses to be cases that produce good outcomes, but are not rational. If the rational choice just is the best choice, this is impossible. Since lucky guesses are not impossible, this theory can’t be right.\nSecond, we could say that the rational choice is the choice that maximises expected value. But I’ve already gone over why that is wrong. There are really two things we could mean by saying the rational choice is the one that maximises expected value, and both of them are wrong. We could say that the rational choice is to compute what has the highest expected value, and then choose it. But this gives the wrong result in all the cases that I discussed at the beginning. Or we could say that the rational choice is to insinctively pick the one with the highest expected value. But there is no more reason to think this is something that choosers can do than there is to think that choosers can instinctively pick the choice with the highest actual value. So this is unrealistic.\nThird, we could say the choice is the output of the procedure such that adopting that procedure maximises expected value, given one’s evidence about the world and about the nature of procedures. Here, I think, the regress has bite because the same arguments from the previous paragraph still apply. We really need to distinguish two possible things we might mean by saying that one should adopt the procedure such that adopting it maximises expected value. First, we could mean that choosers should compute which procedure will maximise expected value, and then adopt it. But this will get the wrong result in Frankie Lee’s case, and in David’s. They shouldn’t be doing any computation at all. So perhaps instead we could say that the rational chooser will instinctively choose the procedure with the highest expected value. But there is no more reason to think that choosers could always do that there is to think that they can simply choose the first-order option with the highest actual or expected value. So this idea fails, and it fails for just the same reasons as the suggestion of the previous paragraph. That doesn’t prove a regress is looming, but it doesn’t look good.\nThe fourth option I’ll discuss is designed to avoid this problem, and it is going to look somewhat more promising2. Maybe we can’t all at once choose the best procedure. But we can do it piecemeal.\nIn general, here’s a way to adopt a complicated procedure. When faced with a certain class of problems, adopt the simplest procedure that agree with the complicated procedure over the range of choices you face. Then, as the problems expand, start either complicating the procedure, or adopt a meta-procedure for choosing which simple procedure to adopt on an occasion. Over time, if all goes well, you’ll eventually adopt something like the complicated procedure, and do it without having to solve impossibly hard calculations about procedural effectiveness, or having miraculously good instincts.\nOne appeal of this approach is that it blocks the regress. If one selects a procedure piecemeal in this way, there is a good sense in which \\(F(P) = F^2(P) = F^3(P) = \\dots\\). After all, there won’t be a difference between adopting a procedure, and adopting a procedure for adopting that procedure. Both of them will just involve making the choices you have to make on a given day, and looking for the opportunity to integrate those choices into a larger and more systematic theory. By adopting a first-order procedure piecemeal, you also adopt a second-order procedure piecemeal. And if \\(F(P) = F^2(P) = F^3(P) = \\dots\\), then the regress doesn’t get going.\nThe problem is that this is too demanding. We want choosers to maximise. We don’t expect them to be able to maximise over every possible choice situation, just over the one in front of them. If I’m buying chickpeas, and I arbitrarily choose one of the cans, that’s all to the good. It’s a rational choice. And, crucially, it stays being a rational choice even if I have dispositions to choose badly in other choice situations. But on the ‘piecemeal’ model being considered here, those dispositions to choose badly are partially constitutive of my choice procedure. And rational choice is a matter of choosing in virtue of adopting the correct choice procedure. So someone who is irrational somewhere is, it turns out, irrational everywhere. This is a bad result. There is something right about the idea that the rational chooser will just choose what’s in front of them, and do so in a sensible way. But we shouldn’t go on to say that rational choice requires that the global procedure one thereby implicitly adopts is the right one; that’s too high a bar.\nSkilled Choice\nThe way to see what’s right about the last proposal, and to see our way to the correct solution, is to somewhat reconceptualise rational choice. We shoudl conceive of the rational chooser as a skilled chooser. And we should think skills are a matter of reliably doing well across realistic situations.\nThe justification for conceiving of rational choice as skilled choice is largely pragmatic. Thinking of rationality that ways results in a plausible theory of rationality, and other ways of thinking about rationality resulted in implausible theories. So rather than argue for the conception of rationality as skill, I’m going to more or less assume it, and hope to justify this assumption by its fruitfulness. What I will argue for is the idea that skill involves reliably succeeding across realistic situations.3\nThink for a bit about skilled atheletes, or skilled players of chess or other games. Part of being skilled is succeeding. But it isn’t just about success. Some people win due to luck. The skilled player won’t always win, but they will reliably win across a range of situations.\nWhich situations are those? They are the situations that are normal enough for the kind of activity being engaged in. These might be dependent on highly contingent features of the activity. A chess player who wins international tournaments must be very skilled. We wouldn’t retract that assessment if it turned out they only played well in quiet enviornments, and frequently lost chess games in noisy pubs. High level chess is played in quiet environments, so that’s what matters.\nA football player whose instincts only go right when there is no wind around is not particuarly skilled. Someone who doesn’t know how to adjust their passes when the wind changes is not skilled; it is lucky that they get ever connect on a pass. Conversely, a football player whose instincts are finely calibrated to the actual gravitational field strength around here could be highly skilled. It’s not part of footballing skill that one is able to adjust to changes in gravitational field strength. Some kinds of flexibility, such as ability to adjust to wind conditions, matter, while others, such as ability to adjust to a different gravitational field, do not. There are intermediate cases where the importance of the ability to adjust is dependent on contingent attributes of the activity. Top level Australian Rules Football is almost always played at sea level. An Australian Rules Footballer whose instincts are calibrated for play at sea level, and who has no ability to adjust to changes in altitude, might still be highly skilled. But in a sporting competition where top flight games are frequently played in Mexico City or Quito, an inability to adjust to changing altitudes is a substantial limitation on one’s skill. It is luck, not skill, that causes one to succeed in contests at one’s favored altitude. But it isn’t luck that the Australian Rules Footballer is playing at sea level; that’s a stable generalisation about the sport.\nThe same kind of story holds true for the skilled chooser. They have to do well, and not by chance. But that can involve having instincts that are calibrated to the enviornment one is actually in, and which would misfire in other environments. A skilled supermarket shopper need not be applying procedures that would do well in a medieval market. But they must be applying procedures that will keep working if the shelving of various items is changed.\nThat’s to say, the skilled chooser will adopt a procedure that will, on average, produce the best results in circumstances like the ones they are in.There is an implicit notion of probability in that definition. But it isn’t the notion of credence, or even of rational credence. Rather, it is the notion of how likely it is, or how frequent it is, that different circumstances obtain. That’s the sense in which the theory is reliabilist.\nWhen I say ‘produce the best results,’ I mean the best results of the available procedures. Just like we don’t require rational commuters to apparate, we don’t require rational choosers to instinctively maximise utility, or expected utility. They (just) have to do the best they can.\nSkilled action frequently involves doing things where one has no evidence for the utility of such performances. It can even involve doing things where one has evidence against the utility of what one is doing. To see this, imagine a junior athelete who is thriving against competitors their own age with an unusual technique. They are told, by seemingly trustworthy coaches, that to thrive at higher levels, they have to adopt a more orthodox technique. But though they have reason to believe these coaches, they keep instinctively lapsing back into their unusual techniques. And, amazingly, the coaches are wrong, and what looked like a technique for winning against kids in parks ends up working at international level competition. (This isn’t entirely unlike the story of Australian cricketer Steve Smith.) Such an athelet may be highly skilled. And their skill consists in, among other things, their instincts to do things that they have (misleading) evidence will not work. Their skill, that is, involves deploying a procedure that is actually reliable, even after they get evidence it is unreliable. I think the same is true of skilled choice. Sometimes, the skilled chooser will deploy a technique that they think is defective, and even one that they think is defective on reasonable grounds. As long as it works, it can still be the basis for skilled, and hence rational, choosing.\nRegress Blocking\nWith all that in place, let’s return to the regress problem, and in particular to Conlisk’s statement of it. Why should we think the rational agent solves \\(F(P)\\), and not \\(F^n(P)\\) for some \\(n > 1\\)? I want to say that’s just what rational choice is; it’s skillfully managing one’s own computational and informational limitations. And skill in this sense involves getting it right, and doing so reliably, not necessarily thinking through the problem. This suggests two questions.\nWhy should we allow this kind of unreflective rule-following in our solution to the regress?\nWhy should we think that \\(F(P)\\) is the point where this consideration kicks in, as opposed to \\(P\\), or anything else?\nThere are a few ways to answer 1. One motivation traces back to the work by the artificial intelligence researcher Stuart Russell (1997). (Although really it starts with the philosophers Russell cites as inspiration, such as Cherniak (1986) and Harman (1973).) He stresses that we should think about the problem from the outside, as it were, not from inside the agent’s perspective. How would we program a machine that we knew would have to face the world with various limitations? We will give it rules to follow, but we won’t necessarily give it the desire (or even the capacity) to follow those rules self-consciously. That might be useful some of the time - though really what’s more useful is knowing the limitations of the rules. And that can be done without following the rules as such. It just requires good dispositions to complicate the rules one is following in cases where such complication will be justified.\nAnother motivation is right there in the quote from Knight that set this literature going. Most writers quote the footnote, where Knight suggests it might be rational to be irrational. But look back at what he’s saying in the text. The point is that it can be perfectly rational to use considerations other than drawing curves and making utility scales. What one has to do is follow internal rules that (non-accidentally) track what one would do if one was a self-consciously perfect Marshallian agent. That’s what I’m saying too, though I’m saying it one level up.\nFinally, there is the simple point that on pain of regress any set of rules whatsoever must say that there are some rules that are simply followed. This is one of the less controversial conclusions of the debates about rule-following that were started by Wittgenstein (1953). That we must at some stage simply follow rules, not follow them in virtue of following another rule, say the rule to compute how to follow the first rule and act accordingly, is an inevitable consequence of thinking that finite creatures can be rule followers.\nSo question 1 is not really a big problem. But question 2 is more serious. Why \\(F(P)\\), and why not something else? The short answer will be that any reason to think that rational actors maximize expected utility, as opposed to actual utility, will also be a reason to think that they solve \\(F(P)\\) and not \\(P\\).\nStart by stepping back and thinking about why we cared about expected utility instead of actual utility in the first place. Why not just say that the best thing to do is to produce the best outcome, and be done with it? Well, we don’t say that because we take it as a fixed point of our inquiry that agents are informationally limited, and that the best thing to do is what is best given that limitation. Given some plausible assumptions, the best thing for the informationally limited agent to do would be to maximize expected utility. This is a second-best option, but the best is unavailable given the limitations that we are treating as unavoidable.\nBut agents are not just informationally limited, they are computationally limited too. And we could have instead treated that as the core limitation to be modelled. As Conlisk says, it is “entertaining to imagine” theorists who worked in just this way, taking the agents in their models to have computational but not informational limitations (Conlisk 1996, 691). Let’s imagine that when we meet the Martian economists, that’s how they reason. Conlisk notes a few things that the Martian economists might do. They might disparage their colleagues who take informational limitations seriously as introducing ad hoc stipulations into their theory. They might argue that informational limitations are bound to cancel out, or be eliminated by competition. They might argue that apparent informational limitations are really just computational ones, or at least can be modelled as computational ones. And so on.\nWhat he doesn’t add is that they might suggest that there is a regress worry for any attempt to add informational constraints. Let \\(Q\\) be the initial problem as the Martians see it. That is, \\(Q\\) is the problem of finding the best outcome given full knowledge of the situation, but the actual computational limitations of the agent. Then we suggest that we should also account for the informational limitations. Let’s see if this will work, they say. Let \\(I()\\) be the function that transforms a problem into one that is sensitive to the informational limitations of the agent. But if we’re really sensitive to informational limitations, we should note that \\(I(Q)\\) is also a problem the agent has to solve under conditions of less than full information.4 So the informationally challenged agent will have to solve not just \\(I(Q)\\), but \\(I^2(Q)\\), and \\(I^3(Q)\\) and so on.5\nOrthodox defenders of (human versions of) rational choice theory have to think this is a bad argument. And I think most of them will agree with roughly the solution I’m adopting. The right problem to solve is \\(I(Q)\\), on a model where \\(Q\\) is in fact the problem of choosing the objectively best option. If one doesn’t know precisely what one’s knowledge is, then one has to maximize expected utility somewhat speculatively. But that doesn’t mean that one shouldn’t maximize expected utility.\nBut the bigger thing to say is that neither we nor the Martians really started with the right original problem. The original problem, \\(O\\), is the problem of choosing the objectively best option. The humans start by considering the problem \\(I(O)\\), i.e., \\(P\\), and then debate whether we should stick with that problem, or move to \\(F(I(O))\\). The Martians start by considering the problem \\(F(O)\\), i.e., \\(Q\\), then debate whether we should stick with that or move to \\(I(F(O))\\). And the answer in both cases is that we should move.\nGiven the plausible commutativity principle, that introducing two limitations to theorising has the same effect whichever order we introduce them, \\(I(F(O)) = F(I(O))\\). That is, \\(F(P) = I(Q)\\). And that’s the problem that we should think the rational agent is solving.\nBut why solve that, rather than something more or less close to \\(O\\)? Well, think about what we say about an agent in a Jackson case who tries to solve \\(O\\) not \\(I(O)\\). (A Jackson case, in this sense, is a case where the choice with highest expected value is known to not have the highest objective value. So trying to get the highest objective value will mean definitely not maximizing expected value.) We think it will be sheer luck if they succeed. We think in the long run they will almost certainly do worse than if they tried to solve \\(I(O)\\). And in the rare case where they do better, we think it isn’t a credit to them, but to their luck. In cases where the well-being of others is involved, we think aiming for the solution to \\(O\\) involves needless, and often immoral, risk-taking.\nThe Martians can quite rightly say the same things about why \\(F(O)\\) is a more theoretically interesting problem than \\(O\\). Assume we are in a situation where \\(F(O)\\) is known to differ from \\(O\\), such as the case Kyla was in. Or, for a different example, imagine the decision maker will get a reward if they announce the correct answer to whether a particular sentence is a truth-functional tautology, and they are allowed to pay a small fee to use a computer that can decide whether any given sentence is a tautology. The solution to \\(O\\) is to announce the correct answer, whatever it is. The solution to \\(F(O)\\) is to pay to use the computer. And the Martians might point out that in the long run, solving \\(F(O)\\) will yield better results. That if the agent does solve problems like \\(O\\) correctly, even in the long run, this will just mean they were lucky not rational. That if the reward is that a third party does not suffer, then it is immorally reckless to not solve \\(F(O)\\), i.e., to not consult the computer. And in general, whatever we can say that motivated “Rational Choice Theory,” as opposed to “Choose the Best Choice Theory,” they can say too.\nBoth the human and the Martian arguments look good to me. We should add in both computational and informational limitations into our model of the ideal agent. And that’s the solution to the regress. It is legitimate to think that there is a rule that rational creatures follow immediately, on pain of thinking that all theories of rationality imply regresses. And thinking about the contingency of how Rational Choice Theory got to be the way it is suggests that the solution to what Conlisk calls \\(F(P)\\), or what I’ve called \\(F(I(O))\\), will be that point.\nThe Nature of Good Procedures\nSince this is meant to be a theory of rational choice for real people, it would be helpful to say a few words about what these reliable procedures that stop the regress might be. In principle they could be anything, but in practice I think three kinds of procedures are particularly important: instincts, planning, and modelling. I’ll say a bit about each of these in turn.\nHumans are surprisingly good at instinctively allocating reasonable amounts of cognitive resources to computational tasks. In artificial intelligence research, one of the big challenges is trying to make machines be as good as humans at figuring out which problems to allocate cognitive resources to. This is sometimes known as the frame problem. Here’s a typical description of this from a recent survey article.\n\nAnd, more generally, how do we account for our apparent ability to make decisions on the basis only of what is relevant to an ongoing situation without having explicitly to consider all that is not relevant? (Shanahan 2016)\n\nNote that this assumes is that humans are actually very good at this rather hard task - setting aside the irrelevant without first thinking that it is irrelevant. This has to be instinctive. We don’t go around thinking about how much time to spend thinking on various subjects. That would be self-defeating. Obviously we are far from perfect at this, but it is striking how good we are at it.\nRecent work on ‘vigilance’ has illustrated how good we are at one aspect of this problem (Sperber et al. 2010). Somehow, and I don’t think it is clear how, we manage to keep track of our environment in a comprehensive enough away that it allows us to focus on those things that need focusing on. For example, when walking down a busy street, we don’t make a model of the expected movements of each of the individuals around us. That would be too computationally taxing. But we do pay enough attention to each of those individuals for us to be able to focus on any one of them if they seem to pose a particular challenge or threat. If one of them is weaving in a drunken manner, or carrying a sword, we are able to focus on them very quickly. To do this we must be paying at least background attention to every one of them. I think this turns out to be a common phenomenon. There are many situations where we don’t have the ability to carefully consider everything that’s going on, but we do manage to pick out the things around us that need close attention. And that requires monitoring of the entire environment, and doing some very quick and dirty processing of the resulting inputs. As I said, it’s a bit of a mystery how we do this. But whatever we do, it’s an amazing feat of insinctively solving a cognitive resource allocation problem.\nWhen I say we do some of these things instinctively, I don’t mean that our ability to do them is innate. We might pick them up by learning from those around us. This learning need not be conscious. It might happen by imitation. It is sometimes thought that humans’ disposition to over imitate those around them is a kind of irrationality (Levy and Alfano 2020). But my guess is that it is part of what grounds our skill in solving these hard cognitive resource allocation problems.\nBut rather than speculate further about what future research will show about the range and limits of human instinct, let’s turn to two ways of consciously adopting reliable procedures. In his discussion of the regress, Miles Kimball (2015) suggests a few options that might work. I want to focus on two of them: planning and modelling.\n\nLeast transgressive are models in which an agent sits down once in a long while to think very carefully about how carefully to think about decisions of a frequently encountered type. For example, it is not impossible that someone might spend one afternoon considering how much time to spend on each of many grocery-shopping trips in comparison shopping. In this type of modelling, the infrequent computations of how carefully to think about repeated types of decisions could be approximated as if there were no computational cost, even though the context of the problem implies that those computational costs are strictly positive. (Kimball 2015, 174)\n\nAnd that’s obviously relevant to David in the supermarket. He could, in principle, spend one Saturday afternoon thinking about how carefully to check each of the items in the supermarket before putting it in his shopping cart. And then in future trips, he could just carry out this plan. In general, planning as a device for incurring computational costs at a time when those costs are less costly.\nThis isn’t a terrible strategy, but I suspect it’s rarely optimal. For one thing, there are much better things to do with Saturday afternoons. For another, it suggests we are back in the business of equating solving \\(F(P)\\) with approximately solving \\(P\\). And that’s a mistake. Better to just say that David is rational if he just does the things that he would do were he to waste a Saturday afternoon this way, and then plan it out. And that thought leads to Kimball’s more radical suggestion for how to avoid the regress,\n\n[M]odelling economic actors as doing constrained optimization in relation to a simpler economic model than the model treated as true in the analysis. This simpler economic model treated as true by the agent can be called a “folk theory” (Kimball 2015, 175)\n\nIt’s this last idea I plan to explore in more detail. (It has some similarities to the discussion of small worlds in Joyce (1999) 70-77.) The short version is that David can, and should, have a little toy model of the supermarket in his head, and should optimize relative to that model. The model will be false, and David will know it is false. And that won’t matter, as long as David treats the model the right way.\nThere are a lot of things that could have gone wrong with a can of chickpeas. They could have gone bad inside the can. They could have been contaminated, either deliberately or through carelessness. They could have been sitting around so long they have expired. All these things are, at least logically, possible.\nBut these possibilities, while serious, have two quite distinctive features. One is that they are very rare. In some cases they may have never happened. (I’ve never heard of someone deliberately contaminating canned chickpeas, though other grocery products like strawberries have been contamination targets.) The other is that there are few easy ways to tell whether they are actualised. You can scan each of the cans for an expiry date, but it is really uncommon that this is relevant, and it takes work since the expiry dates are normally written in such small type. If a can is really badly dented, I guess that weakens the metal and raises ever so slightly the prospect of unintentional contamination. But it’s common to have shelves full of cans that have no dents, or at most very minor ones.\nGiven these two facts - the rarity of the problems and the difficulty in getting evidence that significantly shifts the probability that this is one of the (rare) problems - the rational thing to do is choose in a way that is insensitive to whether those problems are actualised. Or, perhaps more cautiously, one should be vigilant, in the sense of Sperber et al. (2010), to some of these problems, and ignore the rest. But being vigilant about a problem means, I take it, being willing to consider it if and only if you get evidence that it is worth considering. In the short run, you still ignore the potential problem.\nAnd to ignore a potential problem is to choose in a way that is insensitive to evidence for the problem. That makes sense for both the banknotes and the chickpeas, because engaging in a choice procedure that is sensitive to the probability of the problem will, in the long run, make you worse off.\nIn Kimball’s terms, the rational shopper will have a toy model of the supermarket in which all cans of chickpeas that aren’t obviously damaged are safe to eat. This will be a defeasible model, but on a typical grocery trip, it won’t be defeated. In Joyce’s terms, the small worlds the shopper uses in setting up the decision problem they face will all be ones in which the chickpeas are safe.\nSo the suggestion is that very often, the way to be rational is to have right model in your head, and apply it correctly. A choice is the rational choice in your situation iff it is the recommendation of the right model. And the right model includes just as much information, and just as many complications, as the situation demands. The regress is blocked, on this picture, because you don’t have to have computed, or even be in a position to compute, that the right model is the right model. Here I am following Knight. Rational agents don’t have to have worked through Marshall’s Principles; they just have to think and act as if they had. But crucially, they don’t have to even act as if they are applying the Principles to the world. They could apply them to a good model of the world, and that’s good enough.\nThree Philosophical Postscripts\nIdealisation\nThe story I’m telling here about how rational agents use models is very similar, and indeed draws heavily on, the story that Michael Strevens (2008) tells about how scientists use idealisations. On that story, to use an idealisation is to set some messy value to a computationally more simple value (often 0 or 1), and to (implicitly) assert that the difference between the actual value and the computationally simpler value is irrelevant for current purposes.\nOne benefit Strevens gets from this is that he is spared saying that scientists use falsehoods in their reasoning. After all, it is often true that the difference between the messy value and the simple value is irrelevant for current purposes - and that’s all that the scientist is committing to.\nThe same is true in this picture. Frankie Lee can’t know that the banknotes are all equally likely to be genuine; because that’s not strictly true. But he can know that the right model to use in his current situation is one that sets the probability of any note’s genuineness to 1. That’s both true - assuming that our picture of what makes a model right is one that takes deliberation costs seriously - and well supported by his evidence.\nEpistemic Luck\nOn the story I’m telling, whether decision makers are rational or irrational will often be a matter of luck. This is as you should expect if rationality is a matter of successfully applying a skill. Most skills are not infallible. Being skilled at an activity means one usually succeeds, or at least one succeeds at a higher rate than is normal, but on any given day one could fail. The epistemic failures I call irrationality, even though the person in some sense does the same thing as they do in cases where they act rationally.\nHere is one version of that. Recall the version of the Frankie Lee example where the country has just started modernising its financial system by introducing plastic banknotes. Frankie Lee knows that plastic banknotes are genuine - no one has figured out how to forge them yet. So if some of them are on offer, he should take one. But, let’s imagine, he’s temporarily forgotten this fact. So he takes one of the paper notes. This is irrational.\nBut it’s also bad luck. It’s not normally required that we scour our memories for any relevant information before making a decision like this. Normally, Frankie Lee could have put in this much cognitive effort, and ended up rational. But the world did not cooperate, and he ended up irrational.\nI think any story that connects rationality to succeeding via skill will have the consequence that sometimes whether one is rational is in part a matter of luck. But the possibility of epistemic luck shouldn’t surprise us. Assume that what one should, rationally, do and believe is a function of what one knows. And assume that the right epistemic logic is weaker than S5. Then one won’t always know what one knows. So one won’t always know what one should do or believe. So if one believes what one should, or does what one should, this will be in some sense a matter of luck. And surely the right epistemic logic is weaker than S5. Even if you think the anti-luminosity arguments are bad, and the right epistemic logic is stronger than S4, you shouldn’t think that people know what it is they don’t know. (False beliefs, for example, are typically pieces of non-knowledge that are not known to be not knowledge.) So we shouldn’t be surprised that there is epistemic luck.\nKnowledge and Rational Choice\nSo my preferred picture of rational action in cases where there are deliberation costs is that the chooser has a model of the decision problem in their head, and they know it is a good model. That’s a constraint on rationality, but it’s also a constraint on knowledge. If the chooser knows that \\(p\\), they can’t be using a model where it might be that \\(\\neg p\\). That, I think, is the core way that practical factors encroach on knowledge - sometimes one is in a practical situation where the best model allows for the possibility of \\(\\neg p\\), and being in such a situation defeats any putative knowledge that \\(p\\).\nBut I used to say something different about how practical factors affected knowledge. I used to say something like the following.\nOne knows \\(p\\) only if the rational choice (or choices) conditional on \\(p\\) are the rational choice (or choices) unconditionally for any choice one is considering.\nThe rational choice, either conditional or unconditional, is the one with the highest expected utility, or if there are ties, then all of them are rational choices.\nAnd it turns out that combination of views is untenable. This was shown independently twice over, once by Alex Zweber (2016) and then, separately, by Charity Anderson and John Hawthorne (2019). They considered situations like the original Frankie Lee example, and noted that my view had the implausible consequence that Frankie Lee did not know, of each note, that it was genuine. After all, as it stands Frankie Lee should be indifferent between the notes, but conditional on one of the notes being genuine, he should prefer that one. And that’s implausible. Both papers go on to note other implausibilities that purportedly follow, but already we should acknowledge this is a problem. (Whether my view was really committed to the other implausibilities is something I could argue about, but it doesn’t matter because this is already a perfectly good counterexample.)\nThe solution, I now think, is to qualify the second bullet point above. What I should have said instead is\nThe rational choice is the one with the highest expected utility on the model that the chooser is rationally using, or if there are ties, then all of them are rational choices.\nAnd now the problem goes away. It is rational for Frankie Lee to use the model where all the notes are genuine - it isn’t worth the cost of using a more complicated model. And on that model, conditionalising on the hypothesis that one of the notes is genuine doesn’t change anything. So if Frankie Lee is using that model, he knows the notes are genuine. If he isn’t using that model then he doesn’t know the notes are genuine. But this isn’t because of any pragmatic theory of knowledge - it’s simply that to know \\(p\\) requires one actually take \\(p\\) as given, and Frankie Lee fails that criteria.\nSo cases like Frankie Lee, or David and the chickpeas, are perfectly good counterexamples to the version of epistemic pragmatic encroachment I used to endorse. But they don’t show that pragmatic theories are false in general; they just show I got an important detail wrong. To get these details right we need a better theory of when people (rationally) ignore the details.\n\n\n\nAnderson, Charity, and John Hawthorne. 2019. “Knowledge, Practical Adequacy, and Stakes.” Oxford Studies in Epistemology 6: 234–57.\n\n\nBeddor, Bob, and Carlotta Pavese. 2020. “Modal Virtue Epistemology.” Philosophy and Phenomenological Research 101 (1): 61–79. https://doi.org/10.1111/phpr.12562.\n\n\nChakravarti, Ashok. 2017. “Imperfect Information and Opportunism.” Journal of Economic Issues 51 (4): 1114–36. https://doi.org/10.1080/00213624.2017.1391594.\n\n\nCherniak, Christopher. 1986. Minimal Rationality. Cambridge, MA: MIT Press.\n\n\nChristensen, David. 2007. “Does Murphy’s Law Apply in Epistemology? Self-Doubt and Rational Ideals.” Oxford Studies in Epistemology 2: 3–31.\n\n\nConlisk, John. 1996. “Why Bounded Rationality?” Journal of Economic Literature 34 (2): 669–700.\n\n\nDiab, Michael A. AND Highhouse, Dalia L. AND Gillespie. 2008. “Are Maximizers Really Unhappy? The Measurement of Maximizing Tendency.” Judgment and Decision Making 3 (5): 364–70. http://journal.sjdm.org/8320/jdm8320.pdf.\n\n\nDylan, Bob. 2016. The Lyrics: 1961-2012. New York: Simon & Schuster.\n\n\nElster, Jon. 1979. Ulysses and the Sirens: Studies in Rationality and Irrationality. Cambridge: Cambridge University Press.\n\n\nGoodman, Alexander Chernev AND Ulf Böckenholt AND Joseph. 2015. “Choice Overload: A Conceptual Review and Meta-Analysis.” Journal of Consumer Psychology 25 (2): 333–58. https://doi.org/10.1016/j.jcps.2014.08.002.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\nJoyce, James M. 1999. The Foundations of Causal Decision Theory. Cambridge: Cambridge University Press.\n\n\nKimball, Miles. 2015. “Cognitive Economics.” The Japanese Economic Review 66 (2): 167–81. https://doi.org/10.1111/jere.12070.\n\n\nKnight, Frank. 1921. Risk, Uncertainty and Profit. Chicago: University of Chicago Press.\n\n\nLevy, Neil, and Mark Alfano. 2020. “Knowledge from Vice: Deeply Social Epistemology.” Mind 129 (515): 887–915. https://doi.org/10.1093/mind/fzz017.\n\n\nLipsey, R. G., and Kelvin Lancaster. 1956. “The General Theory of Second Best.” Review of Economic Studies 24 (1): 11–32. https://doi.org/10.2307/2296233.\n\n\nManski, Charles F. 2017. “Optimize, Satisfice, or Choose Without Deliberation? A Simple Minimax-Regret Assessment.” Theory and Decision 83 (2): 155–73. https://doi.org/10.1007/s11238-017-9592-1.\n\n\nMartin, Andrew Caplin AND Mark Dean AND Daniel. 2011. “Search and Satisficing.” American Economic Review 101 (7): 2899–2922. https://doi.org/10.1257/aer.101.7.2899.\n\n\nNewman, David B., Joanna Schug, Masaki Yuki, Junko Yamada, and John B. Nezlek. 2018. “The Negative Consequences of Maximizing in Friendship Selection.” Journal of Personality and Social Psychology 114 (5): 804–24. https://doi.org/10.1037/pspp0000141.\n\n\nOdell, John S. 2002. “Bounded Rationality and World Political Economy.” In Governing the World’s Money, edited by David M. Andrews AND C. Randall Henning AND Louis W. Pauly, 168–93. Ithaca: Cornell University Press.\n\n\nPapi, Mario. 2013. “Satisficing and Maximizing Consumers in a Monopolistic Screening Model.” Mathematical Social Sciences 66 (3): 385–89. https://doi.org/10.1016/j.mathsocsci.2013.08.005.\n\n\nPingle, Mark. 2006. “Deliberation Cost as a Foundation for Behavioral Economics.” In In Handbook of Contemporary Behavioral Economics: Foundations and Developments, edited by Morris Altman, 340–55. New York: Routledge.\n\n\nRangel, Elena Reutskaja AND Rosemarie Nagel AND Colin F. Camerer AND Antonio. 2011. “Search Dynamics in Consumer Choice Under Time Pressure: An Eye-Tracking Study.” American Economic Review 101 (2): 900–926. https://doi.org/10.1257/aer.101.2.900.\n\n\nRussell, Stuart J. 1997. “Rationality and Intelligence.” Artificial Intelligence 94 (1-2): 57–77. https://doi.org/10.1016/S0004-3702(97)00026-X.\n\n\nSchwartz, Andrew AND Monterosso, Barry AND Ward. 2002. “Maximizing Versus Satisficing: Happiness Is a Matter of Choice.” Journal of Personality and Social Psychology 83 (5): 1178–97. https://doi.org/10.1037/0022-3514.83.5.1178.\n\n\nSchwartz, Barry. 2004. The Paradox of Choice: Why More Is Less. New York: Harper Collins.\n\n\nSchwartz, Sheena S. Iyengar AND Rachael E. Wells AND Barry. 2006. “Doing Better but Feeling Worse: Looking for the ‘Best’ Job Undermines Satisfaction.” Psychological Science 17 (2): 143–50. https://doi.org/10.1111/j.1467-9280.2006.01677.x.\n\n\nScott, Rose M., and Renée Baillargeon. 2013. “Do Infants Really Expect Agents to Act Efficiently? A Critical Test of the Rationality Principle.” Pscyhological Science 24 (4): 466–74. https://doi.org/10.1177/0956797612457395.\n\n\nSelten, Gerd Gigerenzer AND Reinhard. 2001. Bounded Rationality: The Adaptive Toolbox. Cambridge, MA: MIT Press.\n\n\nShanahan, Murray. 2016. “The Frame Problem.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Spring 2016. Metaphysics Research Lab, Stanford University.\n\n\nSlack, Jean Mangan AND Amanda Hughes AND Kim. 2010. “Student Finance, Information and Decision Making.” Higher Education 60 (5): 459–72. https://doi.org/10.1007/s10734-010-9309-7.\n\n\nSperber, Dan, Fabrice Clément, Christophe Heintz, Olivier Mascaro, Hugo Mercier, Gloria Origgi, and Deirdre Wilson. 2010. “Epistemic Vigilance.” Mind and Language 25 (4): 359–93. https://doi.org/10.1111/j.1468-0017.2010.01394.x.\n\n\nStrevens, Michael. 2008. Depth: An Account of Scientific Explanations. Cambridge, MA: Harvard University Press.\n\n\nTanaka, Masao Ogaki AND Saori C. 2017. Behavioral Economics: Toward a New Economics by Integration with Traditional Economics. Singapore: Springer.\n\n\nTodd, Benjamin Scheibehenne AND Rainer Greifeneder AND Peter M. 2010. “Can There Ever Be Too Many Options? A Meta-Analytic Review of Choice Overload.” Journal of Consumer Research 37 (3): 409–25. https://doi.org/10.1086/651235.\n\n\nTucker, Chris. 2016. “Satisficing and Motivated Submaximization (in the Philosophy of Religion).” Philosophy and Phenomenological Research 93 (1): 127–43. https://doi.org/10.1111/phpr.12191.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nWittgenstein, Ludwig. 1953. Philosophical Investigations. London: Macmillan.\n\n\nZweber, Adam. 2016. “Fallibilism, Closure, and Pragmatic Encroachment.” Philosophical Studies 173 (10): 2745–57. https://doi.org/10.1007/s11098-016-0631-5.\n\n\nIf that sounds implausible to you, make the can/bottle/carton a different size, or change the currency to some other dollars than the one you’re instinctively using. But I think this examples works tolerably well when understand as involving, for example, East Carribean dollars.↩︎\nIndeed, in early drafts of this paper, I didn’t distinguish this proposal from the one I’ll endorse in the next section. And those ‘early drafts’ were circulated in late 2019. So they included the first draft of the paper that I sent to Ted. I’m sorry for doing something between clarifying and changing my view so late.↩︎\nThis is very similar to the modal understanding of skill in Beddor and Pavese (2020).↩︎\nAt this point the Martians might note that while they are grateful that Williamson (2000) has highlighted problems with the KK principle, and these problems show some of the reasons for wanting to idealise away from informational limitations, they aren’t in fact relying on Williamson’s work. All they need is that agents do not exactly what they know. And that will be true as long as the correct epistemic logic is weaker than S5. And that will be true as long as someone somewhere has a false belief. And it would just be weird, they think, to care about informational limitations but want to idealise away from the existence of false beliefs.↩︎\nAt this point, some of the Martians note that the existence of Elster (1979) restored their faith in humanity.↩︎\n",
    "preview": "posts/2021-01-04-deliberation-costs/warhol_soup.jpg",
    "last_modified": "2021-02-04T15:33:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-accuracy-and-the-imps/",
    "title": "Accuracy and the Imps",
    "description": "Recently several authors have argued that accuracy-first epistemology ends up licensing problematic epistemic bribes. They charge that it is better, given the accuracy-first approach, to deliberately form one false belief if this will lead to forming many other true beliefs. We argue that this is not a consequence of the accuracy-first view. If one forms one false belief and a number of other true beliefs, then one is committed to many other false propositions, e.g., the conjunction of that false belief with any of the true beliefs. Once we properly account for all the falsehoods that are adopted by the person who takes the bribe, it turns out that the bribe does not increase accuracy.",
    "author": [
      {
        "name": "James Joyce",
        "url": "http://www-personal.umich.edu/~jjoyce/"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2019-01-01",
    "categories": [
      "epistemology",
      "accuracy"
    ],
    "contents": "\nAccuracy, Bribes and Scoring Rules\nBelief aims at the truth. So at least in some sense, an agent is doing better at believing the closer they are to the truth. When applied to individual beliefs, this generates epistemic advice that is literally platitudinous: if you know that a change in your attitude towards p will make your attitude towards p more accurate, make that change! When applied to collective bodies of belief though, the advice turns out to be more contentious. Call epistemic consequentialism the view that if an agent knows that a change in their overall belief state will make their belief state more accurate, they should make that change, if they have the power to do so.\nThanks to Alejandro Pérez Carballo, Richard Pettigrew, and the participants in the Arché Epistemology Seminar for helpful comments.\nImage by tanakawho via Creative Commons.\nHilary Greaves (2013) has recently argued that epistemic consequentialism is false because it licences certain epistemic ‘bribes,’ and these should not be licenced. We’ll argue that the best forms of epistemic consequentialism do not licence some of these bribes after all.1 Here is the key case Greaves uses.2\n\nEmily is taking a walk through the Garden of Epistemic Imps. A child plays on the grass in front of her. In a nearby summerhouse are \\(n\\) further children, each of whom may or may not come out to play in a minute. They are able to read Emily’s mind, and their algorithm for deciding whether to play outdoors is as follows. If she forms degree of belief 0 that there is now a child before her, they will come out to play. If she forms degree of belief 1 that there is a child before her, they will roll a fair die, and come out to play iff the outcome is an even number. More generally, the summerhouse children will play with chance \\((1-\\frac{q(C_0)}{2})\\), where \\(q(C_0)\\) is the degree of belief Emily adopts in the proposition \\(C_0\\) that there is now a child before her. Emily’s epistemic decision is the choice of credences in the proposition \\(C_0\\) that there is now a child before her, and, for each \\(j = 1, \\ldots, n\\) the proposition \\(C_j\\) that the jth summerhouse child will be outdoors in a few minutes’ time.\n\n\n\\(\\ldots\\) if Emily can just persuade herself to ignore her evidence for \\(C_0\\), and adopt (at the other extreme) credence 0 in \\(C_0\\), then, by adopting degree of belief 1 in each \\(C_{j} (j = 1, ... , 10)\\), she can guarantee a perfect match to the remaining truths. Is it epistemically rational to accept this ‘epistemic bribe?’ Greaves (2013, 918)\n\nThe epistemic consequentialist says that it is best to have credences that are as accurate as possible. We will focus on believers who assign probabilistically coherent credences (degrees of belief) to the propositions in some “target set” \\(\\mathscr{X}\\), and we will think of the “degree of fit” between her beliefs and the truth as being measured by a strictly proper scoring rule. This is a function \\(\\mathbf{I}_{\\mathscr{X}}\\) which associates each pair \\(\\langle \\mathbf{cred}, @ \\rangle\\) consisting of a credence function \\(\\mathbf{cred}\\) whose domain includes \\(\\mathscr{X}\\) and a consistent truth-value assignment @ for elements of \\(\\mathscr{X}\\) with a non-negative real number \\(\\mathbf{I}_{\\mathscr{X}}(@, \\mathbf{cred})\\). Intuitively, \\(\\mathbf{I}_{\\mathscr{X}}\\) measures the inaccuracy of the credences that cred assigns to the propositions in \\(\\mathscr{X}\\) when their truth-values are as described by @. Note that higher \\(\\mathbf{I}_{\\mathscr{X}}\\)-values indicate higher levels of epistemic disutility, so that lower is better from a consequentialist perspective. One popular scoring rule is the Brier score, which identifies inaccuracy with the average squared distance between credences and truth-values. (Greaves calls this the ‘quadratic scoring rule,’ which is a useful description too.) More formally, we have:\n\\[\\mathbf{Brier}_{\\mathscr{X}}(@, \\mathbf{cred}) = \\frac{1}{|\\mathscr{X}|}\\sum_{X \\in \\mathscr{X}} (\\mathbf{cred}(X) - @(X))^2\\] where \\(|\\mathscr{X}|\\) is the number of propositions in \\(\\mathscr{X}\\) and \\(@(X)\\) is either zero or one depending upon whether X is true or false.\nAnother common score is the logarithmic rule, which defines inaccuracy as:\n\\[\\mathbf{Log}_{\\mathscr{X}}(@, \\mathbf{cred}) = \\frac{1}{|\\mathscr{X}|}\\sum_{X \\in \\mathscr{X}} -\\text{log}(\\mathbf{cred}(X)) \\cdot @(X)\\] For now we will follow Greaves in assuming that our epistemic consequentialist uses the Brier score to measure epistemic disutility, but we will relax that assumption in a little while.\nNow let’s think about the ‘bribe’ that Greaves offers, from the point of view of the epistemic consequentialist. The choices are to have one of two credal states, which we’ll call cred1 and cred2. We’ll say cred1 is the one that best tracks the initial evidence, so \\(\\mathbf{cred1}(C_0) = 1\\), and \\(\\mathbf{cred1}(C_i) = 0.5\\) for \\(i \\in {1, ..., 10}\\). And cred2 is the credence Emily adopts if she accepts the bribe, so \\(\\mathbf{cred2}(C_0) = 0\\), while \\(\\mathbf{cred2}(C_i) = 1\\) for \\(i \\in {1, ..., 10}\\). Which state is better?\nThinking like an epistemic consequentialist, you might ask which state is more accurate? It seems like that would be cred2. While cred1 gets \\(C_0\\) exactly right it does not do very well on the other propositions. In contrast, while cred2 gets \\(C_0\\) exactly wrong, it is perfect on the other ten propositions. So overall, cred2 looks to have better epistemic consequences: when compared to being right about one proposition and off by 0.5 on ten others, being right on ten is surely worth one false belief. The Brier score seems to bear this out. If we let \\(\\mathscr{X}\\), the target set, consist of \\(C_0, C_1, ..., C_{10}\\), then we have \\[\\begin{aligned}\n\\mathbf{Brier}_\\mathscr{X}(\\mathbf{cred1}, @) &= \\frac{1}{11}[(1-\\mathbf{cred1}(C_0))^2 + \\sum_{i = 1}^{10} (@(C_i) - \\frac{1}{2}) ^2] = \\frac{10}{44} \\\\\n\\mathbf{Brier}_\\mathscr{X}(\\mathbf{cred2}, @) &= \\frac{1}{11}[(1-\\mathbf{cred2}(C_0))^2 + \\sum_{i = 1}^{10} (@(C_i) - cred(C_i)) ^2] = \\frac{1}{11} \\end{aligned}\\] So, it seems that a good epistemic consequentialist will take the bribe. But, doesn’t that seem like the height of epistemic irresponsibility? It means choosing to believe that \\(C_0\\) is certainly false when you have conclusive evidence for thinking that it is true. If you see the child on the lawn in front of you, how can you sanction believing she is not there?\nAs Greaves admits, intuitions are divided here. Some consequentialists might think that “epistemic bribes” are at least sometimes worth taking, while those of a more deontological bent will always find such trade-offs “beyond the pale”  (Berker 2013a, 363). We will largely sidestep these contentious issues here, though our argument will offer comfort to epistemic consequentialists who feel queasy about accepting the bribe offered in Imps. We contend that, when inaccuracy is measured properly, the consequences of adopting the cred2 credences are strictly worse than the consequences of adopting cred1.\nThe basic problem is that Imps cherry-picks propositions in a way no consequentialist should condone. Its persuasive force rests on the assumption that, for purposes of epistemic evaluation, nothing matters except the accuracies of the credences assigned to propositions in the target set \\(\\mathscr{X}\\). But \\(\\mathscr{X}\\) is the wrong target! By confining attention to it Greaves ignores the many other credences to which Emily becomes committed as a consequence of adopting cred1 or cred2. Any (coherent) agent who invests credence zero in \\(C_0\\) must also invest credence zero in any proposition \\(C_0 \\wedge Y\\), where \\(Y\\) is any conjunction or disjunction of elements from \\(\\mathscr{X}\\). Likewise, anyone who invests credence one in \\(C_n\\) must invest credence one in any proposition \\(C_n \\vee Y\\), where \\(Y\\) is any conjunction or disjunction from \\(\\mathscr{X}\\). In the current context (where the probabilities of the various \\(C_i\\) are independent), when Emily adopts a credence function over \\(\\mathscr{X}\\) she commits to having a credence for (i) every atomic proposition ±\\(C_0 \\wedge\\)± \\(C_1 \\wedge\\)±\\(C_2 \\wedge \\ldots \\wedge\\)±\\(C_{10}\\), where ‘±’ can be either an affirmation or a negation, and (ii) every disjunction of these atomic propositions. In short, she commits to having credences over the whole Boolean algebra \\(\\mathscr{A}_\\mathscr{X}\\) generated by \\(\\mathscr{X}\\). Since each event of a child coming out is independent, adopting cred1 will commit her to setting cred1(±\\(C_0 \\wedge\\)± \\(C_1 \\wedge\\)±\\(C_2 \\wedge \\ldots \\wedge\\)±\\(C_{10}) = \\frac{1}{1024}\\) when \\(C_0\\) is affirmed, and 0 when it is negated. While adopting cred2 commits her to setting cred2(±\\(C_0 \\wedge\\)± \\(C_1 \\wedge\\)±\\(C_2 \\wedge \\ldots \\wedge\\)±\\(C_{10}\\)) equal to 1 when \\(C_0\\) is negated and the rest of the \\(C_i\\) are affirmed, and to 0 otherwise. In this way, each of these probability assignments over the 2048 atoms determine a definite probability for every one of the \\(2^{2048}\\) propositions in \\(\\mathscr{A}_\\mathscr{X}\\).\nIt is our view that consequentialists should reject any assessment of epistemic utility that fails to take the accuracies of all these credences into account. All are consequences of adopting cred1 or cred2, and so all should be part of any consequentialist evaluation of the quality of those credal states. The right “target set” to use when computing epistemic disutility is not \\(\\mathscr{X}\\) but \\(\\mathscr{A}_\\mathscr{X}\\). If we don’t do that, we ignore most of the ways in which cred1 and cred2 differ in accuracy. If Emily takes the bribe, she goes from having credence 0.5 in \\(C_0 \\leftrightarrow C_1\\) to having credence 0 in it. And that’s unfortunate, because the chance of \\(C_0 \\leftrightarrow C_1\\) goes from 0.5 to 1. This is another proposition, as well as \\(C_0\\), that Emily acquires a false belief in by taking the bribe. Of course, there are other propositions not counted that go the other way. Originally, Emily has a credence of 0.25 in \\(C_1 \\wedge C_2\\), and its chance is also 0.25. After taking the bribe, this has a chance of 1, and her credence in it is 1. That’s an improvement in accuracy. So there are a host of both improvements and deteriorations that are as yet unaccounted for. We should account for them, and making the target set be \\(\\mathscr{A}_\\mathscr{X}\\) does that.\nWhen seen from this broader perspective, it turns out the seeming superiority of cred2 over cred1 evaporates. The rest of this section (and the appendix) is dedicated to demonstrating this. We’ll make the calculations a little easier on ourselves by relying on a theorem concerning Brier scores for coherent agents. Assume, as is the case here, that Emily’s credences are defined over an atomic Boolean alegbra of propositions. The atoms are the ‘worlds,’ or states that are maximially specific with respect to the puzzle at hand. In this case there are 2048 states, which we’ll label \\(s_0\\) through \\(s_{2047}\\). In \\(s_k\\), the first child is on the lawn iff \\(k \\leq 1023\\), and summerhouse child \\(i\\) comes out iff the (\\(i\\) + 1)th digit in the binary expansion of \\(k\\) is 1. Let \\(\\mathscr{S}_\\mathscr{X}\\) be the set of all these states. That’s not a terrible target set; as long as Emily is probabilistically coherent it is comprehensive. The theorem in question says that for any credence function cred defined over a partition of states \\(\\mathscr{S}\\), and over the algebra \\(\\mathscr{A}\\) generated by those states,\n\nTheorem-1 \\[\\mathbf{Brier}_{\\mathscr{A}}(\\mathbf{cred}, @) = \\frac{|\\mathscr{S}|}{4}\\mathbf{Brier}_{\\mathscr{S}}(\\mathbf{cred}, @)\\]\n\n(The proof of this is in the appendix.) So whichever credence function is more accurate with respect to \\(\\mathscr{S}_{\\mathscr{X}}\\) will be more accurate with respect to \\(\\mathscr{A}_{\\mathscr{X}}\\). So let’s just work out \\(\\mathbf{Brier}_{\\mathscr{S}_{\\mathscr{X}}}\\) for cred1 and cred2 at the actual world.\nFirst, cred1 will appropriately assign credence 0 to each \\(s_k (k \\in {0, ..., 1023})\\). Then it assigns credence \\(\\frac{1}{1024}\\) to every other \\(s_k\\). For 1023 of these, that is off by \\(\\frac{1}{1024}\\), contributing \\(\\frac{1}{2^{20}}\\) to the Brier score. And for 1 of them, namely @, it is off by \\(\\frac{1023}{1024}\\), contributing \\(\\frac{1023^2}{2^{20}}\\). So we get: \\[\\begin{aligned}\n\\mathbf{Brier}_{\\mathscr{S}_{\\mathscr{X}}}(\\mathbf{cred1}, @) &= \\frac{1}{2048} [1024 \\cdot 0 + 1023 \\cdot \\frac{1}{2^{20}} + \\frac{1023^2}{2^{20}}] \\\\\n&= \\frac{1}{2048} \\cdot \\frac{1023 + 1023 ^2}{2^{20}} \\\\\n&= \\frac{1}{2048} \\cdot \\frac{1023 \\cdot 1024}{2^{20}} \\\\\n&= \\frac{1}{2048} \\cdot \\frac{1023}{1024} \\\\\n&= \\frac{2^{10}-1}{2^{21}}\\end{aligned}\\] It’s a bit easier to work out \\(\\mathbf{Brier}_{\\mathscr{S}_{\\mathscr{X}}}(\\mathbf{cred2}, s_{2047})\\). (We only need to work out the Brier score for that state, because by the setup of the problem, Emily knows that’s the state she’ll be in if she adopts cred2). There are 2048 elements in \\(\\mathscr{S}_{\\mathscr{X}}\\). And cred2 assigns the perfectly accurate credence to 2046 of them, and is perfectly inaccurate on 2, namely \\(s_{1023}\\), which it assigns credence 1, and \\(s_{2047}\\) which it assigns credence 0. So we have \\[\\begin{aligned}\n\\mathbf{Brier}_{\\mathscr{S}_{\\mathscr{X}}}(\\mathbf{cred2}, s_{2047}) &= \\frac{1}{2048} (2046 \\cdot 0 + 1 + 1) \\\\\n&= \\frac{1}{1024} \\\\\n&= \\frac{2^{11}}{2^{21}}\\end{aligned}\\] In fact, it isn’t even close. If Emily adopts cred2 she becomes a little more than twice as inaccurate.\nIt is tedious to calculate \\(\\mathbf{Brier}_{\\mathscr{A}_{\\mathscr{X}}}(\\mathbf{cred1}, @)\\) directly, but it is enlightening to work through the calculation of \\(\\mathbf{Brier}_{\\mathscr{A}_{\\mathscr{X}}}(\\mathbf{cred2}, s_{2047})\\). Note that there are two crucial states out of the 2048: \\(s_{2047}\\), the actual state where all children come out, and state \\(s_{1023}\\) where child 0 does not come out, but the other 10 children all do. There are \\(2^{2^{11}-2}\\) propositions in each of the following four sets:\n\\(\\{p: s_{2047} \\vDash p\\) and \\(s_{1023} \\vDash p\\}\\)\n\\(\\{p: s_{2047} \\vDash p\\) and \\(s_{1023} \\nvDash p\\}\\)\n\\(\\{p: s_{2047} \\nvDash p\\) and \\(s_{1023} \\vDash p\\}\\)\n\\(\\{p: s_{2047} \\nvDash p\\) and \\(s_{1023} \\nvDash p\\}\\)\nIf Emily takes the bribe, she will have perfect accuracy with respect to all the propositions in class 1 (which are correctly believed to be true), and all the propositions in class 4 (which are correctly believed to be false). But she will be perfectly inaccurate with respect to all the propositions in class 2 (which are incorrectly believed to be false), and all the propositions in class 3 (which are incorrectly believed to be true). So she is perfectly accurate on half the propositions, and perfectly inaccurate on half of them, so one’s average inaccuracy is \\(0.5 \\cdot 0 + 0.5 \\cdot 1 = 0.5\\). And that’s an enormous inaccuracy. It is, in fact, as inaccurate as one can possibly be while maintaining probabilistic coherence.\n\nTheorem-2: When inaccuracy over \\(\\mathscr{A}\\) is measured using the Brier score, the least accurate credal states are those which assign credence 1 to some false atom of \\(\\mathscr{A}\\).\n\n(The proof is in the appendix.) So taking the bribe is not a good deal, even by consequentialist lights. And that isn’t too surprising; taking the bribe makes Emily have maximally inaccurate credences on half of the possible propositions about the children.\nSo far we have followed Greaves in assuming that inaccuracy is measured by the quadratic, or Brier, rule. It turns out that we can drop that assumption. We actually only need some very weak conditions on accuracy rules to get the result that Greaves style bribes are bad deals, though the proof of this becomes a trifle more complicated.\nLet \\(\\mathscr{A}\\) be an algebra of propositions generated by a partition of \\(2N\\) atoms \\(a_1, ..., a_{2N}\\). Suppose \\(a_1\\) is the truth, and consider two probability functions, \\(P\\) and \\(Q\\) defined in \\(\\mathscr{A}\\). \\(P\\) assigns all its mass to the first \\(N\\) atoms, so that \\(P(a_k) = 0\\) for all \\(k > N\\). We also assume that \\(P\\) assigns some positive probability to the true atom \\(a_1\\). \\(Q\\) assigns all its mass to the false atom \\(a_{2N}\\). Note that this will be a good model of any case where an agent is offered a bribe of the form: drop the positive confidence you have in proposition \\(p_0\\), instead assign it credence 0, and you’ll be guaranteed a maximally accurate credence in \\(j\\) other logically independent propositions \\(p_1, ..., p_j\\). The only other assumptions needed to get the model to work are that \\(p_0\\) is actually true, and \\(N = 2^j\\).\nImagine that the accuracy of a probability function \\(\\pi\\) over \\(\\mathscr{A}\\) is measured by a proper scoring rule of the form\n\\[\\mathbf{I}(a_n, \\pi) = 2^{-2N}\\sum_{X \\in \\mathscr{A}} \\mathbf{i}(v_n(X), \\pi(X))\\] where \\(v_n(X)\\) is \\(X\\)s truth value when \\(a_n\\) is the true atom, and i is a score that gives the accuracy of \\(\\pi(X)\\) in the event that \\(X\\)s truth value is \\(v_n(X)\\). We shall assume that this score has the following properties.\nTruth Directedness\nThe value of \\(\\mathbf{i}(1, p)\\) decreases monotonically as \\(p\\) increases. The value of \\(\\mathbf{i}(0, p)\\) increases monotonically as \\(p\\) decreases.\n\nExtensionality\n\\(\\mathbf{i}(v_n(X), \\pi(X))\\) is a function only of the truth-value and the probability; the identity of the proposition does not matter.\n\nNegation Symmetry\n\\(\\mathbf{i}(v_n(\\neg X), \\pi(\\neg X)) = \\mathbf{i}(v_n(X), \\pi(X))\\) for all \\(x, n, \\pi\\).\n\n\nTheorem-3: Given these assumptions, \\(P\\)’s accuracy strictly exceeds \\(Q\\)’s.\n\nAgain, the proof is in the appendix.\nTheorem-3 ensures that taking the deal that Greaves offers in Imps will reduce Emily’s accuracy relative to any proper scoring rule satisfying Truth Directedness, Extensionality and Negation Symmetry. To see why, think of Emily’s credences as being defined over an algebra generated by the atoms ±\\(C_0 \\wedge\\)± \\(C_1 \\wedge\\)±\\(C_2 \\wedge \\ldots \\wedge\\)±\\(C_{10}\\), where it is understood that some \\(C_0\\) atom is true and all the \\(\\neg C_0\\) atoms are false. Since Emily is convinced of \\(C_0\\) and believes that every other \\(C_n\\) has some chance of occurring, and since the various \\(C_n\\) are independent of one another, her credence function cred1 will assigns a positive probability to each \\(C_0\\) atom, including the true atom (whichever that might be). Now, let \\(Q\\) be a credence function that places all its weight on some false atom \\(\\neg C_0 \\wedge\\)± \\(C_1 \\wedge\\)±\\(C_2 \\wedge \\ldots \\wedge\\)±\\(C_{10}\\). Theorem-3 tells us that Emily’s cred1 is more accurate than \\(Q\\), and that this is true no matter which \\(C_0\\) atom is true or which \\(\\neg C_0\\) atom \\(Q\\) regards as certain. By taking the bribe Emily will guarantee the truth of \\(C_0 \\wedge C_1 \\wedge \\dots \\wedge C_{10}\\), but the cost will be that she must adopt the cred2 credences, which assign probability one to the false atom \\(\\neg C_0 \\wedge C_1 \\wedge \\dots \\wedge C_{10}\\). Extensionality ensures that any two credence functions that assign probability one to a false atom will have the same inaccuracy score, and that this score will not depend on which atom happens to be the true one. The upshot is that cred2 will have the same inaccuracy when Emily accepts the bribe as \\(Q\\) does when she rejects it. Thus, since cred1 is more accurate than \\(Q\\), it is also more accurate than cred2, which means that Emily should reject the bribe in order to promote credal accuracy.\nWe do not want to oversell this conclusion. Strictly speaking, we have only shown that consequentialists should reject epistemic bribes when doing so requires them to go from being confident in a truth to being certain of some maximally specific falsehood. This is a rather special situation, and there are nearby cases to which our results do not apply, and in which consequentialists may sanction bribe-taking. For example, if Emily only has to cut her credence for \\(C_0\\) in half, say from \\(\\frac{1}{2}\\) to \\(\\frac{1}{4}\\), to secure knowledge of \\(C_1 \\wedge \\dots \\wedge C_{10}\\), then Theorem-3 offers us no useful advice. Indeed, depending on the scoring rule and the nature of the bribe, we suspect that believers will often be able to improve accuracy by changing their credences in ways not supported by their evidence, especially when these changes affect the truth-values of believed propositions. The only thing we insist upon is that, in all such cases, credal accuracy should be measured over all relevant propositions, not just over a select salient few. But that’s something that is independently plausible. Perhaps it might be pragmatically justified to become more accurate on salient propositions at the expense of becoming very inaccurate over hard to state compounds of those propositions, but it is never epistemically justified.\nFour Caveats\nGreaves’s Imps Argument May Work Against Some Forms of Consequentialism\nWe said above that no consequentialist should accept Greaves’s setup of the Imps puzzle, since they should not accept an inaccuracy measure that ignores some kind of introduced inaccuracy. That means that, for all we have said, Greaves’s argument works against those consequentialists who do not agree with us over the suitability of target sets that are neither algebras or partitions. And, at least outside philosophy, some theorists do seem to disagree with us.\nFor instance, it is common in meteorology to find theorists who measure the accuracy of rain forecasts over an \\(n\\) day period by just looking at the square of the distance between the probability of rain and the truth about rain on each day. To pick an example almost literally at random, Mark Roulston (2007) defends the use of the Brier score, calculated just this way, as a measure of forecast accuracy. So Greaves’s target, while not including all consequentialists, does include many real theorists.\nThat said, it seems there are more mundane reasons to not like this approach to measuring the accuracy of weather forecasts. Consider this simple case. Ankita and Bojan are issuing forecasts for the week that include probabilities of rain. They each think that there is a 0% chance of rain most days. But Ankita thinks there will be one short storm come through during the week, while Bojan issues a 0% chance of rain forecast for each day. Ankita thinks the storm is 75% likely to come on Wednesday, so there’s a 75% chance of rain that day, and 25% likely to come Thursday, so there’s a 25% chance of rain that day.\nAs it happens, the storm comes on Thursday. So over the course of the week, Bojan’s forecast is more accurate than Ankita’s. Bojan is perfectly accurate on 6 days, and off by 1 on Thursday. Ankita is perfectly accurate on 5 days, and gets an inaccuracy score of \\(0.75^2 = 0.5625\\) on Wednesday and Thursday, which adds up to more than Bojan’s inaccuracy. But this feels wrong. There is a crucial question that Ankita was right about and Bojan was wrong about, namely will there be a storm in the middle of the week. Ankita’s forecast only looks less accurate because we aren’t measuring accuracy with respect to this question. So even when we aren’t concerned with magical cases like Greaves’s, there is a good reason to measure accuracy comprehensively, i.e., with respect to an algebra or a partition.\nSeparateness of Propositions\nThere is a stronger version of the intuition behind the Imps case that we simply reject. The intuition is well expressed by Selim Berker (2013a, 365, emphasis in original)\n\nThe more general point is this: when determining the epistemic status of a belief in a given proposition, it is epistemically irrelevant whether or not that belief conduces (either directly or indirectly) toward the promotion of true belief and the avoidance of false belief in other propositions beyond the one in question.\n\nLet’s put that to the test by developing the Ankita and Bojan story a little further. They have decided to include, in the next week’s forecast, a judgment on the credibility of rain. Bojan thinks the evidence is rather patchy. And he has been reading Glenn Shafer (1976), and thinks that when the evidence is patchy, credences in propositions and their negations need not add to 1. So if \\(p\\) is the proposition It will rain next week, Bojan has a credence of 0.4 in both \\(p\\) and \\(\\neg p\\).\nAnkita thinks that’s crazy, and suggests that there must be something deeply wrong with the Shafer-based theory that Bojan is using. But Bojan is able to easily show that the common arguments against Shafer’s theory are blatantly question begging  (Maher 1997; Weatherson 1999). So Ankita tries a new tack. She has been reading Joyce (1998), from which she got the following idea. She argues that Bojan will be better off from the point of view of accuracy in having credence 0.5 in each of \\(p\\) and \\(\\neg p\\) than in having credence 0.4 in each. As it stands, one of Bojan’s credences will be off by 0.4, and the other by 0.6, for a Brier score of \\((0.4^2 + 0.6^2)/2 = 0.26\\), whereas switching would give him a Brier score of \\((0.5^2 + 0.5^2)/2 = 0.25\\).\nBut Bojan resists. He offers two arguments in reply.\nFirst, he says, for all Ankita knows, one of his credences might be best responsive to the evidence. And it is wrong, always and everywhere, to change a credence away from one that is best supported by the evidence in order to facilitate an improvement in global accuracy. That, says Bojan, is a violation of the “separateness of propositions”  (Berker 2013a).\nSecond, he says, even by Ankita’s accuracy-based lights, this is a bad idea. After all, he will be making one of his credences less accurate in order to make an improvement in global accuracy. And that’s again a violation of the separateness of propositions. It’s true that he won’t be making himself more inaccurate in one respect so as to secure accuracy in another, as in the bribes case. But he will be following advice that is motivated by the aim of becoming, in total, more accurate, at the expense of accuracy for some beliefs.\nWe want to make two points in response. First, if the general point that Berker offers is correct, then these are perfectly sound replies by Bojan. Although Bojan is not literally in a bribe case, like Emily, he is being advised to change some credences because the change will make his overall credal state better, even if it makes it locally worse in one place. It does not seem to matter whether he can identify which credence gets made worse. Berker argues that the trade offs that epistemic consequentialism makes the same mistake ethical consequentialism makes; it authorises inappropriate trade-offs. But in the ethical case, it doesn’t matter whether the agent can identify who is harmed by the trade-off. If it is wrong to harm an identifiable person for the greater good, it is wrong to harm whoever satisfies some description in order to produce the greater good.\nSo if the analogy with anti-consequentialism in ethics goes through, Bojan is justified in rejecting Ankita’s advice. After all there is, according to Berker, a rule against making oneself doxastically worse in one spot for the gain of an overall improvement. And that’s what Bojan would do if he took Ankita’s advice. But, we say, Bojan is not justified in rejecting Ankita’s advice. In fact, Ankita’s advice is sound advice, and Bojan would do well to take it. So Berker’s general point is wrong.\nOur second point is a little more contentious. We suspect that if Bojan has a good reason to resist this move of Ankita’s, he has good reason to resist all attacks on his Shafer-based position. So if Berker’s general point is right, it means there is nothing wrong with Bojan’s anti-probabilist position. Now we haven’t argued for this; to do so would require going through all the arguments for probabilism and seeing whether they can be made consistent with Berker’s general point. But our suspicion is that none of them can be, since they are all arguments that turn on undesirable properties of global features of non-probabilistic credal states. So if Berker is right, probabilism is wrong, and we think it is not wrong.\nIs this Consequentialism?\nSo far we’ve acquiesed with the general idea that Greaves’s and Berker’s target should be called consequentialism. But there are reasons to be unhappy with this label. In general, a consequentialist theory allows agents to make things worse in the here and now, in return for future gains. A consequentialist about prudential decision making, in the sense of Hammond (1988), will recommend exercise and medicine taking. And they won’t be moved by the fact that the exercise hurts and the medicine is foul-tasting. It is worth sacrificing the welfare of the present self for the greater welfare of later selves.\nNothing like that is endorsed, as far as we can tell, by any of the existing ‘epistemic consequentialists.’ Certainly the argument that Ankita offers Bojan does not rely on this kind of reasoning. In particular, epistemic consequentialists do not say that it is better to make oneself doxastically worse off now in exchange for greater goods later. Something like that deal is offered to the reader of Descartes (1641/1996), but it isn’t as popular nowadays.\nRather, the rule that is endorsed is Right now, have the credences that best track the truth! This isn’t clearly a form of consequentialism, since it really doesn’t care about the consequences of one’s beliefs. It does say that it is fine to make parts of one’s doxastic state worse in order to make the whole better. That’s what would happen if Bojan accepted Ankita’s advice. But that’s very different from doing painful exercise, or drinking unpleasant medicine. (Or, for that matter, to withdrawing belief in any number of truths.)\nWhen Greaves tries to flesh out epistemic consequentialism, she compares it to evidential and causal versions of prudential decision theory. But it seems like the right comparison might be to something we could call constitutive decision theory. The core rule, remember, is that agents should form credences that constitute being maximally accurate, not that cause them to be maximally accurate.\nThe key point here is not the terminological one about who should be called consequentialist. Rather, it is that the distinction between causation and constitution is very significant here, and comparing epistemic utility theory to prudential utility theory can easily cause it to be lost. Put another way, we have no interest in defending someone who wants to defend a causal version of epistemic utility theory, and hence thinks it could be epistemically rational to be deliberately inaccurate now in order to be much more accurate tomorrow. We do want to defend the view that overall accuracy right now is a prime epistemic goal.\nOther Bribes\nAs already noted, we have not offered a general purpose response to bribery based objections to epistemic consequentialism. All we’ve shown is that some popular examples of this form of objection misfire, because they offer bribes that are bad by the consequentialists’ own lights. But there could be bribes that are immune to our objection.\nFor example, imagine that Ankita has, right now, with credence 0.9 in \\(D_0\\), and 0.5 in \\(D_1\\). These are good credences to have, since she knows those are the chances of \\(D_0\\) and \\(D_1\\). She’s then offered an epistemic bribe. If she changes her credence in \\(D_0\\) to 0.91, the chance of \\(D_1\\) will become 1, and she can have credence 1 in \\(D_1\\). Taking this bribe will increase her accuracy.\nWe could imagine the anti-consequentialist arguing as follows.\nIf epistemic consequentialism is true, Ankita is epistemically justified in accepting this bribe.\nAnkita is not epistemically justified in accepting this bribe.\nSo, epistemic consequentialism is not true.\nWe’re not going to offer a reply to this argument here; that is a task for a much longer paper. There are some reasons to resist premise one. It isn’t clear that it is conceptually possible to accept the bribe. (It really isn’t clear that it is practically possible, but we’re not sure whether that’s a good reply on the part of the consequentialist.) And it isn’t clear that the argument for premise one properly respects the distinction between causation and constitution we described in the last section.\nEven if those arguments fail, the intuitive force of premise two is not as strong as the intuition behind Greaves’s, or Berker’s, anti-bribery intuitions. And that’s one of the main upshots of this paper. It’s commonly thought that for the consequentialist, in any field, everything has its price. The result we proved at the end of section one shows this isn’t true. It turns out that no good epistemic consequentialist should accept a bribe that leads them to believing an atomic proposition they have conclusive evidence is false, no matter how strong the inducements. Maybe one day there will be a convincing bribery based case that epistemic consequentialism is unacceptably corrupting of the epistemic soul. But that case hasn’t been made yet, because we’ve shown a limit on how corrupt the consequentialist can be.\nAppendix: Proofs of Theorems 1, 2, 3\n\nTheorem-1: Brier\\(_{\\mathscr{A}}(\\mathbf{c},@) = \\frac{N}{4}\\text{Brier}_{\\mathscr{S}}(\\mathbf{c},@)\\) where \\[\\begin{aligned}\n\\text{Brier}_{\\mathscr{S}}(\\mathbf{c},@) &= \\frac{\\sum_{s \\in \\mathscr{S}} (@(s) - c(s))^2}{N}\\end{aligned}\\]\n\nTo prove this we rely on a series of lemmas.\n\nAlejandro Pérez Carballo gives a more direct and elegant proof of this result in a recent manuscript. We have kept our inefficient proof since its structure provides a guide for the proof of Theorem-3.\nLet \\(\\mathscr{A}\\) be the algebra generated by a finite partition of states \\(\\mathscr{S}= \\{s_1, s_2, \\dots, s_N\\}\\). @ is a truth-value assignment for propositions in \\(\\mathscr{A}\\). For simplicity, assume \\(s_1\\) is the true state, so that @(\\(s_1\\)) = 1 and @(\\(s_n\\)) = 0 for \\(n > 1\\). The credence function c assigns values of \\(c_1, c_2, \\dots, c_{N-1}, c_N\\) to the elements of \\(\\mathscr{S}\\), where \\(\\sum^{N}_{n=1} c_n = 1\\) in virtue of coherence.\nIt will be convenient to start by partitioning \\(\\mathscr{A}\\) into four “quadrants.” Let \\(B\\) range over all disjunctions with disjunctions drawn from \\(\\mathscr{B}= \\{s_2, s_3, \\dots, s_{N-1}\\}\\) (including the empty disjunction, i.e., the logical contradition \\(\\bot\\)). Then, \\(\\mathscr{A}\\) can be split into four disjoint parts:\n\\(\\mathscr{A}_1 = \\{B \\vee s_1 \\vee s_N: B\\) a disjunction of the elements of \\(\\mathscr{B}\\}\\)\n\\(\\mathscr{A}_2 = \\{B \\vee s_1: B\\) a disjunction of the elements of \\(\\mathscr{B}\\}\\)\n\\(\\mathscr{A}_3 = \\{B \\vee s_N: B\\) a disjunction of the elements of \\(\\mathscr{B}\\}\\)\n\\(\\mathscr{A}_4 = \\{B: B\\) a disjunction of the elements of \\(\\mathscr{B}\\}\\)\nNotice that:\n\\(\\mathscr{A}_1 \\cup \\mathscr{A}_2\\) contains all and only the true propositions in \\(\\mathscr{A}\\).\n\\(\\mathscr{A}_3 \\cup \\mathscr{A}_4\\) contains all and only the false propositions in \\(\\mathscr{A}\\).\n\\(\\mathscr{A}_1\\) and \\(\\mathscr{A}_4\\) are complementary sets, i.e., all elements of \\(\\mathscr{A}_4\\) are negations of elements of \\(\\mathscr{A}_1\\), and conversely.\n\\(\\mathscr{A}_2\\) and \\(\\mathscr{A}_3\\) are also complementary.\n\\(\\mathscr{A}_1 \\cup \\mathscr{A}_4\\) is the subalgebra of \\(\\mathscr{A}\\) generated by \\(\\{s_1 \\vee s_N, s_2, s_3, \\dots, s_{N-1}\\}\\).\nAll four quadrants have the same cardinality of \\(2^{N-2}\\).\nFor an additive scoring rule \\(\\mathbf{I}(\\mathbf{c}, @) = \\sum_{A \\in \\mathscr{A}}\\mathbf{i}(\\mathbf{c}(A), @(A))\\) and \\(j = 1, 2, 3, 4\\), define \\(\\mathbf{I}_j = \\sum_{A \\in \\mathscr{A}_j}\\mathbf{i}(\\mathbf{c}(A), @(A))\\), and note that \\(\\mathbf{I}(\\mathbf{c}, @) = 2^{-N}(\\mathbf{I}_1 + \\mathbf{I}_2 + \\mathbf{I}_3 + \\mathbf{I}_4)\\).\n\nLemma-1.1: If \\(\\textbf{I}\\) is negation symmetric, i.e., if \\(\\mathbf{i}(\\mathbf{c}(\\neg A), @(\\neg A)) = \\mathbf{i}(\\mathbf{c}(A), @(A))\\) for all \\(A\\), then \\(\\mathbf{I}_1 = \\mathbf{I}_4\\) and \\(\\mathbf{I}_2 = \\mathbf{I}_3\\), and \\(\\mathbf{I}(\\mathbf{c},@) = 2^{1-N}(\\mathbf{I}_2 + \\mathbf{I}_4)\\).\n\nProof: This is a direct consequence of the fact that \\(\\mathscr{A}_1\\) is complementary to \\(\\mathscr{A}_4\\) and that \\(\\mathscr{A}_2\\) is complementary to \\(\\mathscr{A}_3\\) since this allows us to write\n\\[\\begin{aligned}\n\\mathbf{I}_1(\\mathbf{c},@) &= \\sum_{A \\in \\mathscr{A}_1} \\mathbf{i}(\\mathbf{c}(A), @(A)) = \\sum_{A \\in \\mathscr{A}_1} \\mathbf{i}(\\mathbf{c}(\\neg A), @(\\neg A)) = \\mathbf{I}_4(\\mathbf{c},@). \\\\\n\\mathbf{I}_3(\\mathbf{c},@) &= \\sum_{A \\in \\mathscr{A}_3} \\mathbf{i}(\\mathbf{c}(A), @(A)) = \\sum_{A \\in \\mathscr{A}_3} \\mathbf{i}(\\mathbf{c}(\\neg A), @(\\neg A)) = \\mathbf{I}_2(\\mathbf{c},@). \\text{ QED}\\\\\\end{aligned}\\] Applying Lemma 1.1 with I = Brier we get\n\\[\\begin{aligned}\n(\\#)\\quad \\mathbf{Brier}_{\\mathscr{A}}(\\mathbf{c}, @) &= 2^{1-N} \\sum_{A \\in \\mathscr{A}} (@(A) - c(A))^2 \\\\\n &= 2^{1-N} \\sum_B [(1-c_1)^2 - 2(1-c_1)\\mathbf{c}(B) + \\mathbf{c}(B)^2]\\end{aligned}\\] since\n\\[\\begin{aligned}\n\\mathbf{Brier}_2 &= \\sum_B[1 - \\mathbf{c}(B \\vee s_1)]^2 &&= \\sum_B[(1 - c_1) - \\mathbf{c}(B)]^2 \\\\\n& &&= \\sum_B [(1-c_1)^2 - 2(1-c_1)\\mathbf{c}(B) + \\mathbf{c}(B)^2] \\\\\n\\mathbf{Brier}_4 &= \\sum_B \\mathbf{c}(B)^2 && \\quad\\end{aligned}\\]\n\nLemma-1.2 \\[\\begin{aligned}\n(\\sum_{n=2}^{N-1} c_n)^2 &= \\sum_{n=2}^{N-1} c{_n}^2 + 2 \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j\\end{aligned}\\]\n\nProof by induction. Easy.\n\nLemma-1.3 \\[\\begin{aligned}\n\\mathbf{Brier}_{\\mathscr{S}}(\\mathbf{c},@) &= \\frac{2}{N}[(1-c_1)^2 + \\sum_{n=2}^{N-1} c{_n}^2  - (1-c_1)(\\sum_{n=2}^{N-1}c_n) + \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j]\\end{aligned}\\]\n\nProof: Using the definition of the Brier score and the fact that \\(s_1\\) is true, we have\n\\[\\begin{aligned}\n\\mathbf{Brier}_{\\mathscr{S}}(\\mathbf{c},@) &= \\frac{1}{N}[(1 - c_1)^2 + \\sum_{n=2}^{N-1} c{_n}^2 + (1 - \\sum_{n=1}^{N-1}c_n)^2] \\\\\n&= \\frac{1}{N}[(1 - c_1)^2 + \\sum_{n=2}^{N-1} c{_n}^2 + ((1 -c_1) - \\sum_{n=2}^{N-1}c_n)^2] \\\\\n&= \\frac{1}{N}[(1 - c_1)^2 + \\sum_{n=2}^{N-1} c{_n}^2 + (1 -c_1)^2 - 2(1 - c_1) \\sum_{n=2}^{N-1}c_n + (\\sum_{n=2}^{N-1}c_n)^2] \\\\\n&= \\frac{1}{N}[(1 - c_1)^2 + \\sum_{n=2}^{N-1} c{_n}^2 + (1 -c_1)^2 - 2(1 - c_1) \\sum_{n=2}^{N-1}c_n  \\\\\n&\\quad \\quad +\\sum_{n=2}^{N-1} c{_n}^2 + 2 \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j] \\quad \\text{(Lemma-1.2)}\\end{aligned}\\] Then grouping like terms and factoring out 2 yields the desired result. QED\n\nLemma-1.4 \\[\\begin{aligned}\n\\sum_{n=2}^{N-1}c_n &= 2^{3-N}\\sum_{B \\in \\mathscr{B}}\\mathbf{c}(B)\\end{aligned}\\]\n\nProof: For each \\(n = 2, 3, \\dots, N-1\\), each \\(s_n\\) appears in half of the \\(2^{N-2}\\) disjunctions with disjuncts drawn from \\(\\mathscr{B}\\). As a result, each \\(c_n\\) appears as a summand \\(2^{N-3}\\) times among the sums that express the various \\(\\mathbf{c}(B)\\). So \\(\\sum_{B \\in \\mathscr{B}}\\mathbf{c}(B) = 2^{N-3}\\sum_{n=2}^{N-1}c_n\\). QED\n\nLemma-1.5 \\[\\begin{aligned}\n\\sum_{B \\in \\mathscr{B}}\\mathbf{c}(B)^2 &= 2^{N-3}[\\sum_{n=2}^{N-1} c{_n}^2 + \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j]\\end{aligned}\\]\n\nProof: We proceed by induction starting with the first meaningful case of \\(N=4\\), where calculation shows \\(\\sum_B\\mathbf{c}(B)^2 = (c_2 + c_3)^2 + c{_2}^2 + c{_3}^2 = 2[c{_2}^2 + c{_3}^2 + c_2c_3]\\). Now, assume the identity holds for disjunctions \\(B\\) of elements of \\(\\mathscr{B}\\) and show that it holds for disjunctions \\(A\\) of elements of \\(\\mathscr{B}\\cup \\{s_N\\}\\).\n\\[\\begin{aligned}\n\\sum_A\\mathbf{c}(A)^2 &= \\sum_B\\mathbf{c}(B)^2 + \\sum_B\\mathbf{c}(B \\vee s_N)^2 \\\\\n&= \\sum_B\\mathbf{c}(B)^2 + \\sum_B(\\mathbf{c}(B)^2 + 2c_N\\mathbf{c}(B) + c{_N}^2)\\\\\n&= 2\\sum_B\\mathbf{c}(B)^2 + 2c_N\\sum_B\\mathbf{c}(B) + \\sum_Bc{_N}^2 \\\\\n&= 2 \\cdot 2^{N-3}[\\sum_{n=2}^{N-1} c{_n}^2 + \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j] + 2c_N\\sum_B\\mathbf{c}(B) + \\sum_Bc{_N}^2 &&\\text{(Induction Hypothesis)}\\\\\n&= 2^{N-2}[\\sum_{n=2}^{N-1} c{_n}^2 + \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j] + 2^{N-2}c_N\\sum_{n=2}^{N-1}c_n + \\sum_Bc{_N}^2 &&\\text{(Lemma-1.4)} \\\\\n&= 2^{N-2}[\\sum_{n=2}^{N-1} c{_n}^2 + \\sum_{n=2}^{N-2} \\sum_{j>n}^{N-1}c_nc_j] + 2^{N-2}c_N\\sum_{n=2}^{N-1}c_n + 2^{N-2}c{_N}^2 &&\\text{Since $|\\mathscr{B}| = 2^{N-2}$} \\\\\n&= 2^{N-2}[\\sum_{n=2}^{N} c{_n}^2 + \\sum_{n=2}^{N-1} \\sum_{j>n}^{N}c_nc_j] && \\text{ QED}\\end{aligned}\\] Plugging the results of the last two lemmas into Lemma-1.3 produces a result of\n\\[\\begin{aligned}\n\\mathbf{Brier}_{\\mathscr{S}}(\\textbf{c},@) &= \\frac{2}{N}[(1-c_1)^2 + 2^{3-N}\\sum_{B \\in \\mathscr{B}}\\mathbf{c}(B)^2 - 2^{3-N}(1-c_1)\\sum_{B \\in \\mathscr{B}}\\mathbf{c}(B)] \\\\\n&= \\frac{2}{N}\\sum_{B \\in \\mathscr{B}}[2^{2-N}(1-c_1)^2 + 2^{3-N}\\mathbf{c}(B)^2 - 2^{3-N}(1-c_1)\\mathbf{c}(B)] \\\\\n&= \\frac{2^{3-N}}{N}\\sum_{B \\in \\mathscr{B}}[(1 - c_1)^2 + 2\\mathbf{c}(B)^2 - 2(1-c_1)\\mathbf{c}(B)]\\end{aligned}\\] Comparing this to (#) we see that it is just \\(\\frac{N}{4}\\) times Brier\\(_\\mathscr{S}(\\mathbf{c},@)\\), as we aimed to prove. QED.\n\nTheorem-2. When inaccuracy over \\(\\mathscr{A}\\) is measured using the Brier score, the least accurate credal states are those which assign credence 1 to some false atom of \\(\\mathscr{A}\\).\n\nProof: As before, suppose that \\(@(s_1) = 1\\), and let c be a credence function that assigns credence 1 to some false atom \\(s_2, s_3,..., s_N\\) of \\(\\mathscr{A}\\). In light of Theorem-1 it suffices to show that \\(\\mathbf{Brier}_\\mathscr{S}(\\mathbf{c}, @) > \\mathbf{Brier}_\\mathscr{S}(\\mathbf{b}, @)\\) where b does not assign credence 1 to any false atom. Start by noting that for any credence function \\(\\pi\\) defined on the atoms of \\(\\mathscr{A}\\) one has\n\\[\\begin{aligned}\n\\mathbf{Brier}_\\mathscr{S}(\\pi,@) &= \\frac{1}{N}[(1-\\pi_1)^2 + \\sum_{n=2}^{N-1}\\pi{_n}^2+ (1-\\sum_{n=1}^{N-1}\\pi{_n})^2] \\\\\n&= \\frac{1}{N}[1 - 2\\pi_1 + \\sum_{n=1}^{N-1}\\pi{_n}^2+ (1-\\sum_{n=1}^{N-1}\\pi{_n})^2]\\end{aligned}\\]\nBut, since each \\(\\pi_n \\in [0, 1]\\) is non-negative, it follows that \\(\\pi_1 \\geq \\pi{_1}^2, \\pi_2 \\geq \\pi{_2}^2, \\dots, \\pi_N \\geq \\pi{_N}^2\\) with the inequality strict in each case unless \\(\\pi_n\\) is either 1 or 0.\nThis means that the sum \\(\\sum_{n=1}^{N-1}\\pi{_n}^2+ (1-\\sum_{n=1}^{N-1}\\pi{_n})^2\\) is less than or equal to 1, with equality if and only if exactly one of the atoms \\(s_n\\) is assigned probability 1 (and the rest have probability zero). As a result, Brier\\(_\\mathscr{S}(\\pi, @) \\leq \\frac{2}{N}(1 - \\pi_1)\\) with equality if and only if exactly one of the atoms \\(s_n\\) is assigned probability 1. So, there are three relevant cases:\nIf \\(\\pi\\) assigns some false atom probability 1, Brier\\(_\\mathscr{S}(\\pi, @) = \\frac{2}{N}\\cdot(1 - 0) = \\frac{2}{N}\\).\nIf \\(\\pi\\) assigns the true atom probability 1, Brier\\(_\\mathscr{S}(\\pi, @) = \\frac{2}{N}\\cdot(1 - 1) = 0\\).\nIf \\(\\pi\\) does not assign any atom probability 1, Brier\\(_\\mathscr{S}(\\pi, @) < \\frac{2}{N}\\cdot(1 - c_1) \\leq \\frac{2}{N}\\).\nSo, since c fits case (i) and b fits case (ii) or (iii) we have the desired result. QED\n\nTheorem-3: Let \\(\\mathscr{A}\\) be an algebra of propositions generated by atoms \\(a_1, ..., a_{2N}\\), where \\(a_1\\) is the truth. Let \\(P\\) and \\(Q\\) be probability functions defined on \\(\\mathscr{A}\\). \\(P\\) assigns all its mass to the first \\(N\\) atoms, so that \\(P(a_1 \\vee \\dots \\vee a_N) = 1\\), and it also assigns some positive probability to \\(a_1\\). \\(Q\\) assigns all its mass to the false atom \\(a_{2N}\\), so that \\(Q(a_{2N}) = 1\\). Then, for any proper score I satisfying Truth-directedness, Extensionality and Negation Symmetry we have \\(\\mathbf{I}(v_1, P) < \\mathbf{I}(v_1, Q)\\) where \\(v_1\\) is the truth-value assignment associated with \\(a_1\\) (i.e., where \\(v_1(X) = 1\\) if and only if \\(a_1\\) entails \\(X\\)).\n\nProof: We can divide the algebra \\(\\mathscr{A}\\) into four quadrants \\[\\begin{aligned}\n\\mathscr{A}^1 &= \\{X \\in \\mathscr{A}: a_1 \\vDash X \\text{ and } a_{2N} \\vDash X\\} \\\\\n\\mathscr{A}^2 &= \\{X \\in \\mathscr{A}: a_1 \\vDash X \\text{ and } a_{2N} \\nvDash X\\} \\\\\n\\mathscr{A}^3 &= \\{X \\in \\mathscr{A}: a_1 \\nvDash X \\text{ and } a_{2N} \\vDash X\\} \\\\\n\\mathscr{A}^4 &= \\{X \\in \\mathscr{A}: a_1 \\nvDash X \\text{ and } a_{2N} \\nvDash X\\}\\end{aligned}\\] We know the following:\n\\(Q\\) is maximally accurate on \\(\\mathscr{A}^1 \\cup \\mathscr{A}^4\\). Every proposition in \\(\\mathscr{A}^1\\) is true, and \\(Q\\) assigns it a probability of 1. Every proposition in \\(\\mathscr{A}^4\\) is false, and \\(Q\\) assigns it a probability of 0.\n\\(Q\\) is maximally inaccurate on \\(\\mathscr{A}^2 \\cup \\mathscr{A}^3\\). Every proposition in \\(\\mathscr{A}^2\\) is true, and \\(Q\\) assigns it a probability of 0. Every proposition in \\(\\mathscr{A}^3\\) is false, and \\(Q\\) assigns it a probability of 1.\n\\(P\\) is maximally accurate on \\(\\mathscr{A}^3 \\cup \\mathscr{A}^4\\). Every proposition in \\(\\mathscr{A}^3 \\cup \\mathscr{A}^4\\) is false, and \\(P\\) assigns it a probability of 0.\nEach quadrant has \\(2^{2N-2}\\) elements.\n\nLemma-3.1: When \\(a_1\\) is true, the accuracy score of \\(P\\) over the propositions in \\(\\mathscr{A}^1\\) is identical to the accuracy score of \\(P\\) over the propositions in \\(\\mathscr{A}^2\\).\n\nProof: Note first that the function \\(F: \\mathscr{A}^1 \\rightarrow \\mathscr{A}^2\\) that takes \\(X\\) to \\(X \\wedge \\neg a_{2N}\\) is a bijection of \\(\\mathscr{A}^1\\) onto \\(\\mathscr{A}^2\\). Since every proposition in \\(\\mathscr{A}^1 \\cup \\mathscr{A}^2\\) is true, we can then write the respective accuracy scores of \\(\\mathscr{A}^1\\) and \\(\\mathscr{A}^2\\) as \\[\\begin{aligned}\n\\mathbf{I}_{\\mathscr{A}^1}(a_1, P) &= 2^{2-2N} \\cdot \\sum_{X \\in \\mathscr{A}^1} \\mathbf{I}(1, P(X)) \\\\\n\\mathbf{I}_{\\mathscr{A}^2}(a_1, P) &= 2^{2-2N} \\cdot \\sum_{X \\in \\mathscr{A}^1} \\mathbf{I}(1, P(X \\wedge \\neg a_{2N}))\\end{aligned}\\] Note: \\(X\\) ranges over \\(\\mathscr{A}^1\\) in both summations. But since \\(P(a_{2N}) = 0\\) we have \\(P(X) = P(X \\wedge a_{2N})\\) for each \\(X\\) in \\(\\mathscr{A}^1\\). Since I is extensional, this means that \\(\\mathbf{I}(1, P(X)) = \\mathbf{I}(1, P(X \\wedge a_{2N}))\\) for each \\(X\\) in \\(\\mathscr{A}^1\\). And, it follows that \\(\\mathbf{I}_{\\mathscr{A}^1}(a_1, P)\\) and \\(\\mathbf{I}_{\\mathscr{A}^2}(a_1, P)\\) are identical. (Note that even if \\(P(a_{2N}) > 0\\), Truth-directedness entails that \\(\\mathbf{I}_{\\mathscr{A}^1}(a_1, P) < \\mathbf{I}_{\\mathscr{A}^2}(a_1, P)\\).)\n\nLemma-3.2: When \\(a_1\\) is true, the accuracy score of \\(Q\\) over \\(\\mathscr{A}^2\\) is identical to the accuracy score of \\(Q\\) over \\(\\mathscr{A}^3\\).\n\nProof: To see this, note first that the function \\(G: \\mathscr{A}^2 \\rightarrow \\mathscr{A}^3\\) that takes \\(X\\) to \\(G(X) = \\neg X\\) is a bijection (i.e., the negation of everything in \\(\\mathscr{A}^2\\) is in \\(\\mathscr{A}^3\\) and vice-versa). This, together with the fact that \\(\\mathscr{A}^2\\) contains only truths and \\(\\mathscr{A}^3\\) contains only falsehoods, lets us write \\[\\begin{aligned}\n\\mathbf{I}_{\\mathscr{A}^2}(a_1, Q) &= 2^{2-2N} \\cdot \\sum_{X \\in \\mathscr{A}^2} \\mathbf{I}(1, Q(X)) \\\\\n\\mathbf{I}_{\\mathscr{A}^3}(a_1, Q) &= 2^{2-2N} \\cdot \\sum_{X \\in \\mathscr{A}^2} \\mathbf{I}(0, Q(\\neg X))\\end{aligned}\\] But since I is negation symmetric, \\(\\mathbf{I}(1, Q(X)) = \\mathbf{I}(0, Q(\\neg X))\\) for every \\(X\\), which means that \\(\\mathbf{I}_{\\mathscr{A}^2}(a_1, Q) = \\mathbf{I}_{\\mathscr{A}^3}(a_1, Q)\\). (Note that this proof made no assumptions about \\(Q\\) except that it was a probability.)\n\nLemma-3.3: If \\(P(a_1) > 0\\), the accuracy score of \\(P\\) over \\(\\mathscr{A}^2\\) is strictly less than the accuracy score of \\(Q\\) over \\(\\mathscr{A}^2\\).\n\nProof: Since \\(Q(X) = 0\\) everywhere on \\(\\mathscr{A}^2\\) we have \\[\\begin{aligned}\n\\mathbf{I}_{\\mathscr{A}^2}(a_1, P) &= 2^{2-2N} \\cdot \\sum_{X \\in \\mathscr{A}^2} \\mathbf{I}(1, P(X)) \\\\\n\\mathbf{I}_{\\mathscr{A}^2}(a_1, Q) &= 2^{2-2N} \\cdot \\sum_{X \\in \\mathscr{A}^2} \\mathbf{I}(1, 0) \\end{aligned}\\] But, by Truth Directedness \\(\\mathbf{I}(1, 0) > \\mathbf{I}(1, P(X))\\) since \\(P(a_1) > 0\\) implies that \\(P(X) > 0\\) for all \\(X \\in \\mathscr{A}^2\\). Thus \\(\\mathbf{I}_{\\mathscr{A}^2}(a_1, Q) > \\mathbf{I}_{\\mathscr{A}^2}(a_1, P)\\).\nTo complete the proof of the theorem we need only note that \\[\\begin{aligned}\n\\mathbf{I}_{\\mathscr{A}}(a_1, P) &= \\frac{\\mathbf{I}_{\\mathscr{A}^1}(a_1, P)}{4} + \\frac{\\mathbf{I}_{\\mathscr{A}^2}(a_1, P)}{4} &\\text{(since }P\\text{ is perfect on }\\mathscr{A}^3 \\cup \\mathscr{A}^4) \\\\\n&= \\frac{\\mathbf{I}_{\\mathscr{A}^2}(a_1, P)}{2} &\\text{Lemma-3.1} \\\\\n&< \\frac{\\mathbf{I}_{\\mathscr{A}^2}(a_1, Q)}{2} &\\text{Lemma-3.3} \\\\\n&= \\frac{\\mathbf{I}_{\\mathscr{A}^2}(a_1, Q)}{4} + \\frac{\\mathbf{I}_{\\mathscr{A}^3}(a_1, Q)}{4} &\\text{Lemma-3.2} \\\\\n&= \\mathbf{I}_{\\mathscr{A}}(a_1, Q) &\\text{(since }Q\\text{ is perfect on }\\mathscr{A}^1 \\cup \\mathscr{A}^4)\\end{aligned}\\]\n\n\n\nBerker, Selim. 2013a. “Epistemic Teleology and the Separateness of Propositions.” Philosophical Review 122 (3): 337–93. https://doi.org/10.1215/00318108-2087645.\n\n\n———. 2013b. “The Rejection of Epistemic Consequentialism.” Philosophical Issues 23 (1): 363–87. https://doi.org/10.1111/phis.12019.\n\n\nDescartes, René. 1641/1996. Meditations on First Philosophy, Tr. John Cottingham. Cambridge: Cambridge University Press.\n\n\nGreaves, Hilary. 2013. “Epistemic Decision Theory.” Mind 122 (488): 915–52. https://doi.org/10.1093/mind/fzt090.\n\n\nHammond, Peter J. 1988. “Consequentialist Foundations for Expected Utility.” Theory and Decision 25: 25–78. https://doi.org/10.1007/BF00129168.\n\n\nJenkins, C. S. 2007. “Entitlement and Rationality.” Synthese 157 (1): 25–45. https://doi.org/10.1007/s11229-006-0012-2.\n\n\nJoyce, James M. 1998. “A Non-Pragmatic Vindication of Probabilism.” Philosophy of Science 65 (4): 575–603. https://doi.org/10.1086/392661.\n\n\nMaher, Patrick. 1997. “Depragmatised Dutch Book Arguments.” Philosophy of Science 64 (2): 291–305. https://doi.org/10.1086/392552.\n\n\nRoulston, Mark S. 2007. “Performance Targets and the Brier Score.” Meterological Applications 14: 185–94. https://doi.org/10.1002/met.21.\n\n\nShafer, Glenn. 1976. A Mathematical Theory of Evidence. Princeton: Princeton University Press.\n\n\nWeatherson, Brian. 1999. “Begging the Question and Bayesians.” Studies in the History and Philosophy of Science Part A 30: 687–97.\n\n\nThough they do licence others; see section 2.4 for more discussion.↩︎\nGreaves has four other cases, but the Imps case is the only one that is a problem for all forms of consequentialism she discusses. Similar cases have suggested by Selim Berker (2013a, 2013b) and C. S. Jenkins (2007), but we’ll focus on Greaves’s discussion since she engages more fully with the literature on scoring rules. We’ll return briefly to Berker’s discussion in section 2.↩︎\n",
    "preview": "posts/2021-01-04-accuracy-and-the-imps/imp.jpg",
    "last_modified": "2021-02-04T15:31:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-freedom-of-research-area/",
    "title": "Freedom of Research Area",
    "description": "Some writers have said that academic freedom should extend to giving academics complete freedom over what they choose to research. I argue against this: it is consistent with academic freedom for universities to hire people to research particular subjects, and to make continued employment conditional on at least some of the academic’s research being in the areas they were hired to work in. In practice, many academics think that their fellow academics should be free to choose to work on anything that’s within the disciplinary boundaries of the department they were hired into. I argue that’s both too narrow and too broad. Academic freedom implies that researchers should be allowed to have their research focus drift over time. But the boundaries of permissible drift do not correspond to anything like the boundaries of contemporary academic departments.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2018-12-25",
    "categories": [],
    "contents": "\nImagine that a young designer gets hired by Uber. The company has decided that their smartphone logo is terrible, and it needs to be replaced. And rather than using a design agency, they think they can do a better job in house. So they get a designer. And the new employee is, in a sense, really good. They aren’t really good at designing logos; in fact they don’t make any progress on the logo at all. But they are really good at researching the history of transportation regulation, and writing about this history in a crisp and timely manner. After a while, they spend all their time on this historical research and writing, and the new logo languishes.\nPublished in Academic Freedom, edited by Jennifer Lackey, Oxford 102-115.\nPicture by MCAD Library via Creative Commons.\nI haven’t worked for Uber, or any company like much like them, so I can’t say for sure what would happen. But my impression is that the new employee would find themselves fired rather promptly. It wouldn’t matter how good their work was. In fact, I stipulated that it was good. And it wouldn’t matter how important that work was to the overall mission of the company. I don’t know Uber’s inner workings, but from the outside I suspect that anything that can help them deal with regulatory challenges is considerably more important to their long-term profitability than the look of their smartphone icon. None of this matters. The employee was hired to do a job, they were conspicuously not doing it, and in many businesses, that will mean you get fired.\nAcademia is, in crucial respects, not like that. Indeed, it is a key aspect of academic freedom that researchers get a rather large degree of freedom in choosing what they want to work on. A department may conduct a search in a very specialised area, and hire someone on the strength of their work in just that area, but if that person conducts research in a somewhat different area when they arrive at the job, there is little the department can do about it. And, some say, this is how things should be. Anything otherwise, they say, would be an unacceptable restriction on academic freedom. Here, for instance, is how Cary Nelson puts the point,\n\nAcademic freedom gives both students and faculty the right to study and do research on the topics they choose and to draw what conclusions they find consistent with their research, though it does not prevent others from judging whether their work is valuable and their conclusions sound. (Nelson 2010)\n\nThe idea that academics are not constrained in their research topics has some history. Here is Alexander Bickel, describing an ideal university.\n\nIn universities, professionals of many disciplines can follow lines of inquiry determined by themselves, individually and collectively, and dictated by no one else, on grounds either ideological or practical.  (Bickel 1975, 127)\n\nAnd we see something similar in some university regulations. Here, for example, is what the University of Chicago has to say about research topics.\n\nThe basic policies of The University of Chicago include complete freedom of research and the unrestricted dissemination of information.1\n\nThere is a principle that seems to be running through these quotes, and that I mean to focus on here. I’ll call it FRA.\nFreedom of Research Area (FRA)\nIf part of an academic’s job involves doing research, then the academic themselves gets to choose which areas they shall perform that research in. And provided solely that the quality of the work is sufficiently high, this research in their self-chosen area shall count as adequately discharging their duties to their academic employer, at least as they pertain to research.\n\nI think FRA is false, or at least that it should be false. That is, I think academics should not have complete free choice of what they research on. I’m not really sure how many people think FRA is true, though I probably some people think something like it is true. Perhaps more importantly, I think the appropriate qualifications that one needs to add to FRA to make it true are neither obvious, nor reflected in practice. And that’s why I think it’s worth discussing.\nIn particular, while FRA does not seem to me to be reflected in the regular practice of academic life, a related principle I’ll call FRAD is. That is, I think FRAD is both widely believed, and many people act as if it is true.\nFreedom of Research Area within Department (FRAD)\nIf part of an academic’s job involves doing research, then the academic themselves gets to choose which areas they shall perform that research in, provided it is within the disciplinary boundaries of their home department. And provided solely that the quality of the work is sufficiently high, and that they work within these disciplinary boundaries, this research in their self-chosen area shall count as adequately discharging their duties to their academic employer, at least as they pertain to research.\n\nFRAD, I’ll argue, is also false. Something close to FRAD, however, is true. Academics should have ‘elbow room’ in their research; they should be able to move from one research project to nearby projects. And if they make such a move, they should count as having fulfilled their research responsibilities provided their research is of a high enough quality. FRAD is similar to the elbow room thesis, but not quite the same as it. And the differences matter in some important cases.\nIn focussing on FRA and FRAD, I’m setting to one side most of the questions usually thought central to debates about academic freedom. (Though I trust these questions will get plenty of discussion in the rest of this volume.) The focus here is on which questions academics ask, not on what answers they give. Questions about how free academics should be in answering questions (E.g., Is it ok to defend Pol Pot? Is it ok to use seances to motivate historical interpretations?) are left for others to address. And I’m exclusively focussing on what academics do in the conduct of their work. Questions about whether they should be subject to professional sanction for research activities outside work, and, assuming they are immune from such sanction, whether this immunity ought be related to their status as academics or simply to their status as employees are also being set aside.\nSo we’re focussed on what questions academics ask in the course of their work as academics. There is one more distinction to make to really focus the discussion. Academics have, to greater or lesser extents, both the freedom to tackle different research topics, and the responsibility to tackle certain topics. I’m interested in the responsibility side. What kind of research counts as suitably discharging one’s professional responsibility to research? Put more bluntly, the focus here is not on what research questions an academic may ask, but on what questions they must ask.\nThe American academy has a rather odd structure when in comes to enforcing this responsibility. Junior academics get reviewed after roughly six years, and if their performance is satisfactory, they are awarded tenure. If not, they are fired. Just what counts for tenure varies a lot between institutions, but at research institutions, whether one has adequately discharged one’s research responsibilities is a huge part of the equation.\nIt’s not completely true that there is no other point in the American academic’s career where there will be an inquiry into how well they are discharging their research responsibilities. But at no other point are the stakes nearly as high. For instance, many departments have a small pot of money to distribute in the form of annual raises each year, and often enough research performance is a factor in that distribution. But the sums involved, especially in cash-strapped times, are tiny. Since it is very rare for one’s nominal salary to fall in this process, and inflation is so low, the worst that happens if one completely fails to discharge all research responsibilities is that one’s salary falls by a percent or so per year. That can add up over time, but compared to being fired, it’s a minor penalty.\nSo I’m going to focus mostly on that tenure decision here. That isn’t because I have any sympathy for the current structure, with the stakes being so high here and so low elsewhere. But it’s what we have to work with, so it’s what is relevant here and now.2\nSo imagine the following case. A young scholar gets hired in a US university3 on a tenure-track line. Six years later, they are up for tenure review. And in the interim they have done high quality work, with the quality and quantity of the work being sufficient for promotion to tenure. But the work is in a different area to the work they did before being hired, and this work is not at all what the department had in mind when they were hired. Assuming their promotion file is adequate in other respects (especially concerning teaching and service), should they be promoted to tenure? Or, perhaps more precisely, what further details of the case matter to whether they should be promoted to tenure? Should it matter, for instance, whether the work was inside or outside the disciplinary purview of the department?\nI’m assuming here that principles of academic freedom apply at all to pre-tenured faculty. This doesn’t seem too controversial, though it is striking how some universities talk about tenure and academic freedom. Here, for instance, is a passage from the University of Michigan’s Tenure Guidelines:\n\nThe University safeguards academic freedom through its policy that no person who has been awarded tenure by the Regents or who has been employed by the University for a total of ten years at the rank of a full-time instructor or higher may, thereafter, be dismissed, demoted or recommended for terminal appointment without adequate cause and an opportunity for a review…  (The University of Michigan 2016a)\n\nIt isn’t hard to read that as saying that it is through tenure that academic freedom is protected, and conclude from that that faculty without tenure don’t have academic freedom. But I’m assuming that conclusion is false; academic freedom does extend to untenured faculty. And the question is what it covers.\nThe particular puzzle case I’m interested in is not unique to philosophy, but philosophy is considerably more prone to it than other fields. In many fields, a central part of the tenure file consists of the book that results from the dissertation. In such cases, there is little danger that the tenure file will look radically different from the research profile that was submitted in the candidate’s original job application. In many other fields, research is closely connected to getting and spending grant money. And the mechanics of grants make it hard for someone’s research to take a sharp change of direction at a very early stage of their career. This is not to say the case I’m interested in cannot arise in such disciplines. But it is much more likely to arise in disciplines that are neither grant-based nor book-based. There are few such disciplines in existence right now, but philosophy, at least in the US, is one. Philosophy is also the discipline I know the most about and, to be honest, care the most about, so it doesn’t bother me that I’m writing about a problem that is more common here than elsewhere.\nI’m also going to write exclusively about fictional cases. I looked into using some real life cases to make the discussion more vivid. But they ended up being more of a distraction than a helpful illustration. In particular, it was hard to find a case where a candidate for tenure was uncontroversially doing high quality work, but there were concerns about the area it was in. Rather than re-litigating the tenure files of these human beings, I think it most appropriate to focus here on the abstract case.4\nTo be sure, it is hard to precisely imagine a case just like the one I am describing. Doing high level research in any field is hard. If there is no sign of one having worked on something before being hired, the probability that one will be able to acquire sufficient knowledge and skills to do top quality research in that field is not high. And perhaps it will even be hard to get unbiased reports on the quality of the work, if the candidate did not get into the field through the usual channels. But it’s not so unrealistic as to be unimaginable.\nI hope everyone would agree that doing extra work, well away from what one was hired to do, is not a bad thing. It shouldn’t count against the candidate for tenure. Indeed, it is good to show some ability to stretch out. When I say FRA and FRAD are false, I very much do not mean that one is obliged to not do any other kind of research.5 The more research the better! But first things first.\nSo why do I think FRA is false? And why do I think it would be reasonable for universities to insist that their candidates for tenure do some research on what they were hired to do.6 Well, let’s start by looking at an extreme case that might be thought to motivate something like FRA. The following would be unreasonable behaviour on the part of a hiring department. A candidate is hired on the basis of an excellent dissertation on peer disagreement, which has led to two publications in good journals, and there are two more papers under review from the dissertation. The hiring department expects her to keep doing just this kind of work.\nBut soon after she arrives at her new job, she surveys the most recent work on peer disagreement and decides the debate is dead. There is, she thinks, nothing more to say about this debate. It is an ex-debate, it has ceased to be, it is no more, it has shuffled off this mortal coil and is now pushing up the roofs of the libraries.7 So rather than scream into the void, she decides to take what she has learned in debates about disagreement and apply them to more vibrant debates about testimony, and about judgment aggregation. And between getting hired and coming up for tenure, she writes a series of high quality, widely cited, papers on these topics in respected journals.\nBut then the hiring department gets upset at time for tenure review. We hired you to work on peer disagreement, they say, and what have we here? Nothing at all on peer disagreement, but all this stuff on these distinct, though admittedly related, fields. That’s not enough, we say, for promotion to tenure.\nThis is poor behaviour on the part of the hiring department, and so unreasonable that I find anything like this happening in a real department almost inconceivable. (Though some departments do have an impressively dogged commitment to unreasonableness, so perhaps I should be careful here about the link between conceivability and possibility.) But we can say more than just why it is unreasonable. There is a good story about what makes it unreasonable.\nAcademic debates die. Everything that needs to be said is said, and it’s time to start talking about something new. When that happens, it is wrong to keep plowing these barren fields. And the people best positioned to spot the death of a debate are experts, with dedicated knowledge. Outsiders may suspect that if nothing new is happening, the participants are just tired following a prolonged squawk. Or, perhaps more likely, outsiders will confuse mere squawking for actual progress. The people best positioned to determine whether it is worth investing more resources in a debate are participants to it. If our imagined candidate has decided that the debate is dead, then it will usually be reasonable to defer to her expert judgment. I’m assuming here that when it comes to particular areas of debate, an assistant professor will be an expert, even compared to her senior colleagues. She will know, and they won’t know, the details of what has been happening in the very recent literature, and how much those details matter. Sometimes that won’t be true; her colleagues will be experts. But it will be usually true, and so it is safe to assume it is true when considering hypothetical cases for the purposes of policy development.\nOne other relevant fact about academic debates is that they are not isolated. An expert on one debate won’t automatically become an expert on all related questions, but she won’t be a novice either. In the peer disagreement example, it is very natural to think that our expert will know a lot about testimony and about judgment aggregation. Those debates are both highly relevant to disagreement. So it is reasonable to expect that if our candidate slid into those debates, she would produce excellent work. And, recall, that is exactly what happened in the example.\nPutting these two thoughts together, we get the following conclusions. Allowing people to drift between nearby areas of research will not, on the whole, reduce the quality of their research. And allowing the people who are experts in a particular debate to choose when to move between nearby fields, we can leverage their knowledge of those fields to ensure that their work remains relevant to lively debates.\nThese considerations support a freedom to drift, to move from one area of research to adjacent areas without needing approval from a central authority. And that’s already a kind of academic freedom. The motivation here has a family resemblance to Hayek’s argument that a virtue of markets is that they provide a way for the system to leverage the expertise that market participants typically have, at least about areas immediately relevant to them  (Hayek 1945). And this Hayekian flavour to the argument shouldn’t be surprising. The alternative to a model where academics have some freedom to choose the direction of their research is one where a central planner chooses everyone’s research topic for them. And arguing against the success of such central planning models was a central concern of Hayek’s throughout his career.\nSo academics, even junior ones, should have elbow room (to borrow a metaphor from Daniel Dennett). But it’s a long way from this to endorsing FRA. Indeed, the considerations that supported a freedom to drift could not possibly support FRA. For one thing, the fact that one was good enough to be hired in one particular sub-field does not indicate that one will have particularly expert judgment on whether it is a good use of university resources to have (more) research conducted on a particular field distant from one’s own. And for another thing, the fact that one was hired in one field is little to no evidence that one would be the right person to conduct that research, even if it were in the university’s interests. So if there is a wide ranging freedom to research on whatever takes one’s fancy, it will need radically different justification to this.\nIt is hard to see what that justification could possibly be. There are people who are given awards that are meant to support any kind of research that they find interesting. The MacArthur Fellows Program, the so-called ‘genius grant,’ is like this. And it seems suitable for people who have made spectacular contributions, and will likely continue to do so. It seems particularly suitable for people who have already shown an ability to create great works that require leaping between seemingly distant fields. If, for example, you can use hip-hop to turn the story of the most elitist of the founding fathers into a popular phenomenon, then someone should probably give you untied funding to just see what happens next.\nBut a junior professorship is not a MacArthur Fellowship. Indeed, it is dangerous to think that it is, or that it should be. It encourages the idea that universities should be looking to hire geniuses, rather than hiring people who have put in the hard work to get to where they are in their field, and are likely to keep getting further results by a continued application of just that kind of hard work. At least some of the time, perhaps most of the time, the question of just how smart the candidate is should be considerably less relevant to a job search than the question of what they have achieved, and what those achievements signal about their likely future research contributions.8 Yet if everyone who was hired was been given a free rein to work on anything whatsoever, if every hire was the equivalent to bestowing a MacArthur Fellowship, then whether the candidate was some kind of genius would be a central, perhaps sole, criteria.\nIf we were going to say any academic should work on whatever they like, or even whatever they like in their department’s research purview, we need to do one of two things:\nShow how this freedom is consistent with the idea that departments can, in hiring, take area of research into account; or\nArgue that the very widespread practice of taking area of research into account in hiring is indefensible.\nI actually have some sympathy for option 2 here, but it would be an incredibly radical step.9 So let’s investigate the prospects for option 1. I think they are rather dim.\nThe motivation for hiring by research field seems straightforward. Departments have (allegedly) an interest in having researchers working on diverse fields. And hiring people who have worked in diverse fields is one way to meet that interest. But given FRA, or even FRAD, there is a big gap in this motivation. All that we can know by looking at a job application file is what a person has worked on. The department, presumably, has an interest in there being diversity in what its members will work on. And we need a bridge between past work and future work here.\nOne way of bridging this gap would be to insist that the newly hired academic work continue to do (some) research on (roughly) the areas they were hired to work on. I think that’s the right way to bridge the gap, but it is inconsistent with FRA and FRAD.\nAnother way would be to take past research interests as noisy indicators of future research interests. So if you want to hire in philosophy of biology, you might hire someone who has worked in philosophy of biology to increase the probability that that’s what they’ll work in. The problem with this reasoning is that hiring the person who is most likely to do the best work in the area you want to hire in will lead to some bad choices in realistic scenarios. Imagine you want to hire in philosophy of biology, and you have three candidates.\nA is the best philosopher of the bunch, but has at best a passing interest in philosophy of biology.\nB is the best philosopher of biology, but also has a very strong interest (including a book manuscript in progress) on a completely different field.\nC is nearly as good as B at philosophy of biology, and has no other philosophical interests.\nIf you want to maximise the expected value of research your department does on philosophy of biology, and FRA or FRAD are in place, the best thing to do hire C. After all, there is a non-trivial chance that B will just work on their book manuscript and related papers, and indeed use it to get tenure. If you want to maximise the expected value of research your department does in philosophy, the best thing to do is to hire A. They are the best philosopher. What’s hard to see is the motivation for making what intuitively is the right choice here, hiring B. The solution, I think, is to ditch FRAD, and hire B with the explicit requirement that they do a certain amount of work in philosophy of biology.\nLet’s say that FRA is false then, and conclude with a more focussed look at FRAD as it applies to tenure cases. This is a somewhat more practical matter, since FRAD is more like the rule that is applied in actual tenure cases. Indeed, here is what the handbook at the University of Michigan (my employer) says about tenure review,\n\nAfter the appropriate probationary period (see section 6.C “Tenure Probationary Period”), tenure may be granted to those instructional faculty members whose professional accomplishments indicate that they will continue to serve with distinction in their appointed roles. Tenure is awarded to those who demonstrate excellent teaching, outstanding research and scholarship, and substantial additional service, each of which must be relevant to the goals and needs of the University, college and department. The award of tenure is based on the achievement of distinction in an area of learning and the prediction of continued eminence throughout the individual’s professional career  (The University of Michigan 2016b emphasis added)\n\nAs far as I can tell, in practice the bolded clause is interpreted in line with something like FRAD. The wording is ambiguous; it could just as easily be read as supporting the elbow room standard that I prefer. But I don’t believe that’s how things work in practice.\nWe have two questions to answer then. First, how similar are FRAD and the elbow room standard? And second, in cases where they differ, which provides a better model for building a university. I think they are not particularly similar, and the elbow room standard is much better.\nThere is, of course, a certain similarity between the two standards. FRAD says work on whatever you like, provided it is in the same discipline as the work that got you hired. The elbow room standard says work on whatever you like, provided it is sufficiently similar (along some salient dimension) to the work that got you hired. And being in the same department is a dimension of similarity. But it isn’t, ultimately, a particularly important one. Making it of central importance, as FRAD does, leads to numerous avoidable errors.\nFor one thing, FRAD gives some academics more freedom to switch fields than the elbow room standard could possibly justify. Philosophy is a very broad field. Just because one is doing really excellent work in one field is very little evidence that one will be able to do excellent work in another field. Thinking that it is evidence is to just relapse into a restricted version of the myth, or cult, of genius. So at least in some cases, the elbow room standard will be more restrictive than FRAD. But in other cases it will be less restrictive, and those are perhaps more important in practice.\nDisciplines have boundaries. Those boundaries are vague, but they are there. Some people work on topics that are very near to a boundary, and some work on topics that are far from a boundary. FRAD impacts these two groups in very different ways, and the difference is unfair. Someone whose work is near a boundary can’t just drift into any nearby field, since the nearby fields may be outside the disciplinary bounds. To take one clear example, a researcher hired for work on the semantics of modals can easily drift into other areas of semantics, or onto modal fallacies in argumentation, but not into work on the syntax of modals. For whatever reason, we’ve decided the boundary between philosophy and not-philosophy around here is very close to the the syntax-semantics boundary. And this is a violation of the elbow room principle, since this kind of move from the semantics to the syntax of a particular class of expressions is just the kind of move to a closely related field that the elbow room principle is designed to protect.\nIt won’t help here to say that there should be overlapping areas of research concern between departments. The crucial question is the boundary between X and not-X, not the boundary between X and Y. If someone is near the X/not-X boundary, they could have a tenure home in X, and then drift into not-X. They deserve protection in their research, and FRAD wouldn’t provide it. It would perhaps help if literally every possible area of research was such that there was some department it was not just in, but in without being near the boundary. But a world with academic departments organised that way feels very different to the one we are in.\nIn won’t help to say we should just make the boundaries larger. Unless we abolish the boundaries altogether, the problem will persist. And abolishing all the boundaries would create more problems than it solves. The boundaries play useful roles right now. It is good that hiring and tenure decisions for, say, a position in metaphysics are made by people with a broad range of philosophical backgrounds, and not (in the first instance) by an arbitrary collection of people from across the university. The ideal here is not no boundaries, but porous boundaries.10 Boundaries exist so that local experts, and people with special local interests, get extra say on questions of local concern, but they are porous so they get in the way of freedoms. Replacing FRAD with the elbow room principle gets the balance right.\nIt also doesn’t help that the boundaries are vague. In general, if something is true no matter how a vague term is made precise, it is a good bet that it is true.11 But we can say a bit more about why vagueness doesn’t matter in this particular case. The following principle looks both true, and the best bet for why we might think vague disciplinary boundaries make FRAD more palatable.\nIf someone’s work is clearly within discipline X, then any related area they could reasonably drift into under the elbow room principle will not be clearly outside discipline X.\nIf everyone who is hired is clearly working within X, then FRAD might be no more restrictive than the elbow room principle. After all, any permissible drift will not take one clearly outside one’s home discipline.\nThe problem is that only hiring people whose work is clearly within the hiring discipline is a terrible idea. Indeed, it is a worse idea than FRAD. It ensures that one will only hire in safe, traditional areas of research. And that’s a plan for stagnation, not for doing the best research. Sometimes we have to hire people on the frontiers, and sometimes their work will drift clearly outside one’s discipline. That’s just a cost of having a dynamic research program, and attempts to avoid paying this cost will make things even worse.\nThere is one problem that vague disciplinary boundaries does help with. Vague boundaries are easier to shift than precise boundaries. That’s because there was never any agreement on where they were in the first place, so no agreement has to be overturned. And if enough people work who were hired in X start working on an area that used to be outside X, we should just start treating that area as inside X. But we don’t have to make this conceptual shift every time a good philosopher does good work on a nearby topic. Some good work is in other fields, and that’s ok.\nThere are other odd features of FRAD. If applied consistently, it would lead to treating some like cases in very unlike ways. There are several fields that are set up as departments in some universities, and as programs in other universities. For example, in America right now there is a divide among universities about whether to set up things like Women’s Studies, Cognitive Science, and PPE as independent departments, or as programs run collaboratively by a number of different departments. There are administrative considerations on either side of this choice, and these considerations vary somewhat between different universities. If they are set up as programs, then anyone hired in to them will have a tenure home in one of the constituent departments. And given FRAD, that will put certain limitations on their research. Those limitations will be very different to what they would face if the unit were its own department. But it seems very wrong to think the administrative decision to set up a unit as a department or a program impact the freedoms of people hired to work in those unites. Where FRAD applies, however, this difference is dramatic. Someone hired in a philosophy department to support a Cognitive Science program could move to work on philosophy of religion, but not experimental developmental psychology. Someone hired by a Cognitive Science department would have the opposite set of freedoms. I’m not sure what is optimal here, but it is very odd that an administrative decision should impact researchers in this way.\nBut the biggest problem with FRAD is that it makes the disciplinary boundaries too important. Young researchers shouldn’t have to second guess whether a particular development of their research is inside or outside a vague, shifting, boundary. The solution isn’t to abolish these boundaries, any more than rights of free movement across an area is a reason to abolish all political boundaries within that area.12 Rather, the solution is to downplay them. If exercising their elbow room rights takes an academic outside the purview of their home department, that’s just a cost of having a dynamic research program.\nAnd it is really the effect on these younger scholars, trying to pre-judge what their reviewers at tenure time will think, that I’m most interested in here. As I noted above, I don’t know of any clear cases where someone was turned down for tenure because their research was in the wrong area. But I know many cases of academics who have put off more speculative research projects until after their tenure review. And the reason, typically, has been that they are nervous that the outputs of the new project would be discounted, merely in virtue of their subject matter, at the time of tenure review. This feels like an undesirable feature of the status quo, and one that could be remedied by rethinking why we care about what an individual academic works on.\nSo while I disagree with the strongest statements of academic freedom, I think the position I’m endorsing allows greater freedom in practice than existing practices like FRAD. There are, I would guess, many more people who are worried that their research is drifting away from what their colleagues will regard as really part of the discipline than there are people who would like the freedom to jump to an area they have no training, expertise or background in. Defending the elbow room principle, or freedom to drift, will take care of those concerns. And the principle is much easier to defend in theory than FRA or FRAD. So it, I think, is the core important principle concerning academics’ rights to direct their own research.\nConclusion\nI’ve focussed here exclusively on research, and not at all on teaching. But in many ways what I’m saying here could be summarised as the view that the norms concerning topic choice are fairly similar in research and in teaching. If I’m given a course on history of political philosophy to teach, then I better teach history of political philosophy, and not, say, formal logic, or Australian geography, or baseball statistics.13 It isn’t in any way a violation of academic freedom if I’m required to teach the subject I signed up to teach.\nBut in practice, and in theory, there is a lot of freedom within the boundaries of a course. If I want my history of political philosophy course to include thinkers who are not commonly central to the story Anglophone political philosophy tells about itself, I should (and typically would) be free to include them. If I think the most relevant secondary literature is by people in departments other than philosophy (e.g., history, political science, women’s studies, etc) then I should be able to base my syllabus around such writers. Now I personally haven’t taught history of political philosophy since I was a post-doctoral fellow who was too nervous to consider any such plan. But it’s exactly the kind of thing academic freedom should protect - and I suspect in most cases it is what academic freedom would protect.\nThe same I think should go for research. If one is hired to research history of political philosophy, then it is reasonable for the university to require that one do just that. It isn’t reasonable to require one do only that; people should be allowed to explore what they want. But it is reasonable to require some work on what one was hired to do. Yet if doing that takes one outside the bounds of what is (hereabouts) considered philosophy, that should be fine too. Do what you’re hired to do is a good principle; FRAD is not.\nThe picture of academia I’m trying to promote is one where more units are free to operate the way that Paul Romer describes the Marron Institute of Urban Management at NYU as operating.\n\n[I]nstead of giving its faculty members the usual freedom to study anything that that seems interesting, the institute lets the problems that cities face set its research agenda. Because these choices are not the usual ones on campus, many people complained.  (Romer 2016)\n\nIf universities want to give people complete freedom to set their own research agenda, I’m not going to complain about that (much). What I want to deny is that setting up things like the Marron Institute is a violation of academic freedom. There is a lot to be gained by hiring people for relatively specific research tasks, and it isn’t a violation of their freedom to hire them in this way. And that’s especially true if the constraints on their research agenda are set just by the questions that their research team is focussed on, and not by the disciplinary homes that house the thinkers they engage with.\n\n\nBickel, Alexander. 1975. The Morality of Consent. New Haven, CT: Yale University Press.\n\n\nCole, Jonathan R., Stephen Cole, and Christopher C. Weiss. 2015. “Academic Freedom: A Pilot Study of Faculty Views.” In Who’s Afraid of Academic Freedom?, edited by Akeel Biglrami and Jonathan R. Cole, 343–94. New York: Columbia University Press.\n\n\nHayek, F. A. 1945. “The Use of Knowledge in Society.” American Economic Review 35 (4): 519–30.\n\n\nKeynes, John Maynard. 1936. The General Theory of Employment, Interest and Money. London: Macmillan.\n\n\nLeslie, Sarah-Jane, Andrei Cimpian, Meredith Meyer, and Edward Freeland. 2015. “Expectations of Brilliance Underlie Gender Distributions Across Academic Disciplines.” Science 347 (6219): 262–65. https://doi.org/10.1126/science.1261375.\n\n\nMeyer, Meredith, Andrei Cimpian, and Sarah-Jane Leslie. 2015. “Women Are Underrepresented in Fields Where Success Is Thought to Require Brilliance.” Frontiers in Psychology 6: 1–12. https://doi.org/10.3389/fpsyg.2015.00235.\n\n\nNelson, Cary. 2010. “Defining Academic Freedom.” Inside Higher Ed. https://www.insidehighered.com/views/2010/12/21/defining-academic-freedom.\n\n\nRomer, Paul. 2016. “Everybody Wants Progress; Nobody Wants Change.” https://paulromer.net/progress-change/.\n\n\nShweder, Richard A. 2015. “To Follow the Argument Where It Leads: An Antiquarian View of the Aim of Academic Freedom at the University of Chicago.” In Who’s Afraid of Academic Freedom?, edited by Akeel Biglrami and Jonathan R. Cole, 190–238. New York: Columbia University Press.\n\n\nStorage, Daniel, Zachary Horne, Andrei Cimpian, and Sarah-Jane Leslie. 2016. “The Frequency of Words Like ‘Brilliant’ and ‘Genius’ in Teaching Evaluations Predicts the Representation of Women and African Americans Across Academia.” PLoS One 11 (3): 1–17. https://doi.org/10.1371/journal.pone.0150194.\n\n\nThe University of Michigan. 2016a. “The University of Michigan Faculty Handbook, 6.a General Principles.” http://provost.umich.edu/faculty/handbook/6/6.A.html.\n\n\n———. 2016b. “The University of Michigan Faculty Handbook, 6.b Criteria for Tenure.” http://provost.umich.edu/faculty/handbook/6/6.B.html.\n\n\nRetrieved from https://provost.uchicago.edu/handbook/research/research-policies. Both this quote and the Bickel quote are cited by Richard A. Shweder (2015).↩︎\nJust to be clear, I’m not denying that the arguments given below could apply to tenured academics just as easily as non-tenured ones. But doing so would be a very radical break from current practice, and motivating such a radical break would need much more careful discussion than I have space to do here.↩︎\nI will use the term ‘university’ here for any post-secondary educational institution that hires professors with an expectation they will produce some research. Many of these institutions have ‘college’ rather than ‘university’ in their name, but I’m calling them all universities.↩︎\nAs I’ll return to below, the main way the issues I’m discussing impact everyday academic life is that expectations of how tenure reviews will be conducted affect how pre-tenured academics structure their research profiles. As any game theorist knows, the nature of non-equilibrium outcomes can be profoundly important to the actual world, even if they are never reached.↩︎\nI wasn’t hired to write articles like this one, but there isn’t, I hope, anything wrong with my writing it.↩︎\nThere is an important caveat here. It would be completely unreasonable for a university to decide, just as a candidate comes up for tenure, that they really wanted the candidate to have been doing different research for the past six years. The question is what behaviour on the part of universities would be reasonable if it were clearly communicated well in advance.↩︎\nFull disclosure: I have a book manuscript under review with a long discussion of peer disagreement.↩︎\nSarah-Jane Leslie’s work (with various colleagues) has shown that there is a strong correlation between how strongly people think that brilliance is required for producing good work in a field, and the gender distribution of faculty in the field  (Leslie et al. 2015; Meyer, Cimpian, and Leslie 2015; Storage et al. 2016). Thinking that whoever is hired can work on anything, and it will probably be good if the work they originally did was good, seems similar to me to taking raw talent to be the primary requisite for successful work.\nThe primary argument of this paper has been the requiring academics to do at least a certain amount of work in a particular area is not a violation of academic freedom. It is no violation of academic freedom to set up something like NYU’s Marron Institute (discussed further below). The considerations of this paragraph suggest something stronger, that it is positively bad to not require (most) academics to do work in a particular area, because to not do this encourages an invidious cult of genius. I’m not endorsing this stronger claim, but these considerations do look like the germ of an argument for it.↩︎\nAs of November 16, 2016, there were 209 jobs advertised on PhilJobs.org, and by my count only 36 did not put some restrictions or desiderata on the research area of the hired candidate.↩︎\nAcademia is hardly the only place where this is the ideal.↩︎\nThis principle traces back at least to Keynes (1936Ch. 6).↩︎\nBy analogy, it’s a good thing that Ann Arbor has a city council, and it’s a good thing that there are no barriers to moving in or out of Ann Arbor.↩︎\nCompare the discussion of academic freedom and the requirement to teach the topic of the class in Cole, Cole, and Weiss (2015). The example of teaching Australian geography in a class that is not on that is taken from one of their survey respondents.\n\n↩︎\n",
    "preview": "posts/2021-03-12-freedom-of-research-area/mcad.jpg",
    "last_modified": "2021-03-12T23:41:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-02-interests-evidence-and-games/",
    "title": "Interests, Evidence and Games",
    "description": "Pragmatic encroachment theories have a problem with evidence. On the one hand, the arguments that knowledge is interest-relative look like they will generalise to show that evidence too is interest-relative. On the other hand, our best story of how interests affect knowledge presupposes an interest-invariant notion of evidence. The aim of this paper is to sketch a theory of evidence that is interest-relative, but which allows that 'best story' to go through with minimal changes. The core idea is that the evidence someone has is just what evidence a radical interpreter says they have. And a radical interpreter is playing a kind of game with the person they are interpreting. The cases that pose problems for pragmatic encroachment theorists generate fascinating games between the interpreter and the interpretee. They are games with multiple equilibria. To resolve them we need to detour into the theory of equilibrium selection. I'll argue that the theory we need is the theory of **risk-dominant equilibria**. That theory will tell us how the interpreter will play the game, which in turn will tell us what evidence the person has. The evidence will be interest-relative, because what the equilibrium of the game is will be interest-relative. But it will not undermine the story we tell about how interests usually affect knowledge.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2018-06-29",
    "categories": [
      "epistemology",
      "interest-relativity",
      "games and decisions"
    ],
    "contents": "\n\nContents\nEncroachment, Reduction and Explanation\nThe Problems with Evidence\nA Simple, but Unsatisfying, Solution\nGamifying the Problem\nEquilibrium Selection Principles\nObjections and Replies\nAppendix One: Carlsson and van Damme’s Game\nAppendix Two: The Modified Game\n\nPragmatic encroachment theories have a problem with evidence. On the one hand, the arguments that knowledge is interest-relative look like they will generalise to show that evidence too is interest-relative. On the other hand, our best story of how interests affect knowledge presupposes an interest-invariant notion of evidence.\nPublished in Episteme 15: 329-344.\nImage by Paul Wordingham via Creative Commons.\nThe aim of this paper is to sketch a theory of evidence that is interest-relative, but which allows that ‘best story’ to go through with minimal changes. The core idea is that the evidence someone has is just what evidence a radical interpreter says they have. And a radical interpreter is playing a kind of game with the person they are interpreting. The cases that pose problems for pragmatic encroachment theorists generate fascinating games between the interpreter and the interpretee. They are games with multiple equilibria. To resolve them we need to detour into the theory of equilibrium selection. I’ll argue that the theory we need is the theory of risk-dominant equilibria. That theory will tell us how the interpreter will play the game, which in turn will tell us what evidence the person has. The evidence will be interest-relative, because what the equilibrium of the game is will be interest-relative. But it will not undermine the story we tell about how interests usually affect knowledge.\nEncroachment, Reduction and Explanation\nI will start with an argument for a familiar disjunctive conclusion: either knowledge is interest-relative, or scepticism is true. The argument will resemble arguments to the same disjunctive conclusion in Hawthorne (2004) and Fantl and McGrath (2009). Indeed, it is inspired by those discussions. But it uses less controversial premises than previous versions.\nThe argument starts by considering a game, one I’ll call the red-blue game. Here are the rules of the game.\nTwo sentences will be written on the board, one in red, one in blue.\nThe player will make two choices.\nFirst, they will pick a colour, red or blue.\nSecond, they say whether the sentence in that colour is true or false.\nIf they are right, they win. If not, they lose.\nIf they win, they get $50, and if they lose, they get nothing.\nOur player is Parveen. She is an epistemologist who works on pragmatic encroachment, and (as will become important in a minute), she has frequently cited both Knowledge and Lotteries  (Hawthorne 2004), and Knowledge and Practical Interests  (Stanley 2005). She knows the rules of the game, and no other relevant facts about the game. When the game starts, the following two sentences are written on the board, the first in red, the second in blue.\nTwo plus two equals four.\nKnowledge and Lotteries was published before Knowledge and Practical Interests.\nIntuitively, there is a unique rational play in this game: Red-True. That is, Parveen announces that she will evaluate the truth value of the red sentence, and then announce that it’s true. That’s a sure $50.\nOn the other hand, in normal circumstances, we would say that Parveen does know that Knowledge and Lotteries was published before Knowledge and Practical Interests. After all, she has looked up their publication dates many times in checking over her papers.\nThere is a puzzle in reconciling these intuitions. The pragmatic encroachment theorist has a solution to these puzzles. In normal circumstances, Parveen does know that Knowledge and Lotteries was published before Knowledge and Practical Interests. But these are not normal circumstances. Right now, it matters whether her reason to believe that Knowledge and Lotteries was published before Knowledge and Practical Interests is as strong as her reason to believe that two plus two equals four. And (unless something very weird is happening), that isn’t true for Parveen. So she knows that red-true will win, she doesn’t know any other play will win, so she should play Red-True.\nIf we reject pragmatic encroachment, and we are not sceptics, we should say that Parveen does know that Knowledge and Lotteries was published before Knowledge and Practical Interests. And then it is a mystery why playing Red-True is more rational than playing Blue-True. After all, Parveen knows the rules of the game, and she knows (by hypothesis) the blue sentence is true, so if she can do even basic logical reasoning in a knowledge preserving way, she knows she will get as good a result as possible by playing Blue-True. So it is a bit of a mystery why it would be anything other than maximally rational to play Blue-True.\nOne way we might try to resolve this mystery is by saying that although Parveen knows that Blue-True will win $50, she super-knows that Red-True will win $50. What do we mean here by super knowledge? Think of this as a placeholder for certainty, or knowledge that one knows, or anything other epistemic state that you think might be relevant to her practical decision making. Perhaps the fact that she super-knows what two plus two is, but doesn’t super-know when the epistemology books were published, could be the explanation for why Red-True is the unique rational play.1\nBut no such explanation can work, because Parveen doesn’t super-know that playing Red-True will win $50. She super-knows that two plus two is four. But we have not assumed that she super-knows the rules of the game. So she doesn’t super-know that Red-True will win, she just knows it. And she also, by hypothesis, knows that Blue-True will win. So looking at any kind of super-knowledge can’t break the intuitive asymmetry between Red-True and Blue-True.\nPut another way, if Parveen knows that Knowledge and Lotteries was published before Knowledge and Practical Interests, then she knows that she is playing the following game.\nTwo sentences will be written on the board, one in red, one in blue.\nThe player chooses to play either Blue-True, Blue-False, Red-True, or Red-False.\nIf they play Blue-True, they win $50.\nIf they play Blue-False, they win nothing.\nIf they play Red-True, they win $50 if the red sentence is true, and nothing otherwise.\nIf they play Red-False, they win $50 if the red sentence is false, and nothing otherwise.\nAnd is is rational to play Blue-True in that game. (It might also be rational to pay Red-True depending on what the red sentence is, but it is always rational to play Blue-True.) Yet it is not rational to play Blue-True in the original game. So Parveen does not know, when she plays the original game, that Knowledge and Lotteries was published before Knowledge and Practical Interests.\nSo to avoid pragmatic encroachment here we must deny that Parveen ever knew that Knowledge and Lotteries was published before Knowledge and Practical Interests. On its own, that’s not a sceptical conclusion: lots of people don’t know that. But once we go down that path, it looks like not much knowledge will be left. After all, we can repeat the game with any number of different things in the place of the blue sentence. If we adopt the constraint that Parveen only knows p, right now, if it is rationally permissible for her to play Blue-True when p is the blue sentence, no matter what the red sentence is, then either we have to say very unintuitive things about rational plays of the game, or we have to say she knows very little.\nSo we’ve got the conclusion that either pragmatic encroachment is true, or scepticism is true. Since I’m not a sceptic, I’m happy to conclude that pragmatic encroachment is true. But note that we’ve done this without any reference to high stakes situations. The stakes in Parveen’s game are just $50. That’s not nothing, but it’s not ‘high stakes’ in the way that phrase is normally used.\nThe version of pragmatic encroachment we get is that what matters for knowledge are not the stakes involved in any bet on p, but the odds.2 Parveen loses knowledge because she is being asked, in effect, to make a super long odds bet on a fact about publication schedules. She is in no position to rationally make a bet at those odds. So she doesn’t know the fact about publication schedules.\nAnd that’s the general principle: agents only know a proposition if they are in a position to rationally bet on that proposition at the odds currently being offered to them. In practice, high stakes situations tend to feature bets at long odds, so in practice much knowledge dissipates in high stakes cases. But the explanation of the dissipation is the odds the agent faces, not the stakes.\nMore precisely, I endorse these principles as constraints on knowledge:\nIf the agent knows that p, then for any question they have an interest in, the answer to that question is identical to the answer to that question conditional on p.\nWhen an agent is considering the choice between two options, the question of which option has a higher expected utility given their evidence is a question they have an interest in.\nThose principles are meant to not merely be extensionally adequate. They are meant to explain why agents lose knowledge when considering some sets of options, like in the Red-Blue game. In some sense, they are meant to be part of reductive explanations. These reductive explanations take as primitive inputs facts about the agent’s evidence, and facts about evidential probability. I’m going to set aside worries about the metaphysics of evidential probability, and just focus on evidence. Because it turns out that there is a real problem in getting a plausible theory of evidence that can function as an input to that reductive explanation.\nThe Problems with Evidence\nGo back to the red-blue game. Consider a version of the game where:\nThe red sentence is that two plus two equals four.\nThe blue sentence is something that, if known, would be part of the agent’s evidence.\nI’m going to argue that there are cases where the only rational play is Red-True, but the blue sentence is something we want to say that, ordinarily, the subject knows. And I’ll argue that this is a problem for the kind of reductive explanation I just sketched. If pragmatic effects matter to what the evidence is, we can’t take the evidence as a fixed input into an explanation of how and when pragmatic effects matter.\nLet’s have Parveen play the game again. She’s going to be playing the game in a restaurant, one in Ann Arbor where she lives. Just before the game starts, she notices an old friend, Rahul, across the room. Rahul is someone she knows well, and can ordinarily recognise, but she had no idea he was in town. She thought Rahul was living in Italy. Still, we would ordinarily say that she now knows Rahul is in the restaurant; indeed that he is in the restaurant. It would be perfectly acceptable for her to say to someone else, “I saw Rahul here,” for example. Now the game starts.\nThe red sentence is Two plus two equals four.\nThe blue sentence is Rahul is in this restaurant.\nNow we have a problem. On the one hand, there is only one rational play here: Red-True. If you haven’t seen someone for a long time, then you can’t be completely certain it’s them when you spot them across a restaurant. It would be foolish to be as confident that it’s Rahul as that two and two make four. It looks like this is a case where pragmatic effects defeat knowledge.\nOn the other hand, our story for why Parveen loses knowledge here has run into problems. I wanted to tell a story roughly like the following. She can’t play Blue-True when the probability of the blue sentence, given her evidence, is less than the probability of the red sentence, given her evidence. That explanation can only go through if the blue sentence is itself not part of her evidence, since the probability of anything given itself is one. So we need a story about how it is that it is not part of Parveen’s evidence that Rahul is not in the restaurant.\nThat story can’t be the one that presupposes facts about what is in Parveen’s evidence. So it can’t use facts about the probability of some proposition given her evidence; at least not in any simple way. If we can independently identify Parveen’s evidence, then we can go back to using evidential probability. But until we’ve done that, we’re stuck.\nThere are two options here that seem possible for the pragmatic encroachment theorist, but not particularly attractive.\nOne is to say that propositions like Rahul is in this restaurant are never part of Parveen’s evidence. Perhaps her evidence just consists of things like I am being appeared to Rahul-like. Such an approach is problematic for two reasons. The first is that it is subject to all the usual objections to psychological theories of evidence  (Williamson 2007). The second is that we can re-run the argument with the blue sentence being some claim about Parveen’s psychological state, and still get the result that the only rational play is Red-True. A retreat to a psychological conception of evidence will only help with this problem if agents are infallible judges of their own psychological states, and that is not in general true  (Schwitzgebel 2008).\nAnother option is to deny that a reductive explanation is needed here. Perhaps pragmatic effects, like the particular sentences that are chosen for this instance of the Red-Blue game, mean that Parveen’s evidence no longer includes facts about Rahul, but this isn’t something we can give a reductive account of. We shouldn’t assume that everything will have a simple reductive explanation, so this isn’t so bad in theory. The problem in practice is that without a reductive explanation, we don’t have a predictive theory of when pragmatic effects matter. And that seems to be a bad thing. For instance, the following theory is completely consistent with Parveen’s case as described.\nE=K; i.e., one’s evidence is all and only what one knows.\nSomeone does not know p if the evidential probability of p is not close enough to one for current purposes.\nSince it is part of Parveen’s evidence that Rahul is in the restaurant, the probability that he is there is one, so it is close enough to one for current purposes.\nSo this is not a case where pragmatic effects change what she knows.\nThat theory seems to me to be badly mistaken, since it goes on to predict that it is rationally permissible to play Blue-True. But we need a pragmatic account that says that it is mistaken, and says something about which alternative situations would not threaten Parveen’s knowledge. We don’t yet, as far as I can see, have such an account. The aim of the rest of this paper is to provide one.3\nA Simple, but Unsatisfying, Solution\nLet’s take a step back and look at the puzzle more abstractly. We have an agent S, who has some option O, and it really matters whether or not the value of O, i.e., \\(V(O)\\) is at least \\(x\\). It is uncontroversial that the agent’s evidence includes some background \\(K\\), and controversial whether it includes some contested proposition \\(p\\). It is also uncontroversial that \\(V(O | p) \\geq x\\), and we’re assuming that for any proposition \\(q\\) that is in the agent’s evidence, \\(V(O | q) = V(O)\\). That is, we’re assuming the relevant values are conditional on evidence. We can capture that last assumption with one big assumption that probably isn’t true, but is a harmless idealisation for these purposes. Say there is a prior value function \\(V^-\\), with a similar metaphysical status to the mythical, mystical prior probability function. Then for any choice \\(C\\), \\(V(C) = V^-(C | E)\\), where \\(E\\) is the evidence the agent has.\nNow we’re in a position to state a simple, but unsatisfying, solution. Let \\(p\\) be the proposition that the agent might or might not know, and the question of whether \\(V(O) \\geq x\\) be the only salient one that \\(p\\) is relevant to. Then the agent knows \\(p\\) only if the following is true:\n\n\\(\\frac{V^-(O | K) + V^-(O | K \\wedge p)}{2} \\geq x\\)\n\nThat is, we work out the value of \\(O\\) with and without the evidence \\(p\\), and if the average is greater than \\(x\\), good enough!\nThat solves the problem of Parveen and Rahul. Parveen’s evidence may or may not include that Rahul is in the restaurant. If it does, then Blue-True has a value of $50. If it does not, then Blue-True’s value is somewhat lower. Even if the evidence includes that someone who looks a lot like Rahul is in the restaurant, the value of Blue-True might only be $45. Averaging them out, the value is less than $50. But you’d only play Blue-True if it was worthwhile it play it instead of Red-True, which is worth $50. So you shouldn’t play Blue-True.\nGreat! Well, great except for two monumental problems. The first problem is that what we’ve said here really only helps with very simple cases, where there is a single decision problem that a single contested proposition is relevant to. We need some way to generalise the case to less constrained situations. The second (and bigger) problem is that the solution is completely ad hoc. Why should we use the arithmetic mean of these two things rather than any other formula that would have implied the intuitively correct result in the Parveen-Rahul case? Pragmatic encroachment starts with a very elegant, very intuitive, principle: you only know the things you can reasonable take to be settled for the purposes of current deliberation. And that deliberation should be driven by the aim of maximising expected utility. It does not look like any such elegant, intuitive, principles will lead to some theorem about averaging out the value of an option with and without new evidence.\nHappily, the two problems have a common solution. But the solution requires a detour into some technical work. It’s time for some game theory.\nGamifying the Problem\nWe can usefully think of some philosophical problems as games, and hence subjects for study using game theoretic techniques. This is especially when the problems involve interactions of rational agents. Here, for example, is the game table for Newcomb’s problem, with the human who is usually the focus of the problem as Row, and the demon as Column.4\n\n\nPredict 1 Box\nPredict 2 Boxes\nChoose 1 Box\n1000, 1\n0,0\nChoose 2 Boxes\n1001, 0\n1, 1\n\nThis game has a unique Nash equilbrium; the bottom right corner.5 And that’s one way of motivating the view that (a) the game is possible, and (b) the rational move for the human is to choose two boxes.\nLet’s look at a more complicated game. I’ll call it The Interpretation Game. The game has two players. Just like in Newcomb’s problem, one of them is a human, the other is a philosophical invention. But in this case the invention is not a demon, but The Radical Interpreter.6 To know the payouts for the players, we need to know their value function. More colloquially, we need to know their goals.\nThe Radical Interpreter assigns mental states to Human in such a way as to predict Human’s actions given Human rationality. We’ll assume here that evidence is a mental state, so saying what evidence Human has is among Radical Interpreter’s tasks. (Indeed, in the game play to come, it will be their primary task.)\nHuman acts so as to maximise the expected utility of their action, conditional on the evidence that they have. Human doesn’t always know what evidence they have; it depends on what The Radical Interpreter says.\nThe result is that the game is a coordination game. The Radical Interpreter wants to assign evidence in a way that predicts rational Human action, and Human wants to do what’s rational given that assignment of evidence. Coordination games typically have multiple equilibria, and this one is no exception.\nLet’s make all that (marginally) more concrete. Human is offered a bet on p. If the bet wins, it wins 1 util; if the bet loses, it loses 100 utils. Human’s only choice is to Take or Decline the bet. The proposition p, the subject of the bet, is like the claim that Rahul is in the restaurant. It is something that is arguably part of Human’s evidence. Unfortunately, it is also arguable that it is not part of Human’s evidence. We will let \\(K\\) be the rest of Human’s evidence (apart from \\(p\\), and things entailed by \\(K \\cup \\{p\\}\\)), and stipulate that \\(\\Pr(p | K) = 0.9\\). Each party now faces a choice.\nThe Radical Interpreter has to choose whether p is part of Human’s evidence or not.\nHuman has to decide whether to Take or Decline the bet.\nThe Radical Interpreter achieves their goal if human takes the bet iff p is part of their evidence. If p is part of the evidence, then The Radical Interpreter thinks that the bet has positive expected utility, so Human will take it. And if p is not part of the evidence, then The Radical Interpreter thinks that the bet has negative expected utility, so Human will decline it. Either way, The Radical Interpreter wants Human’s action to coordinate with theirs. And Human, of course, wants to maximise expected utility. So we get the following table for the game.\n\n\n\\(p \\in E\\)\n\\(p \\notin E\\)\nTake the bet\n1, 1\n-9.1, 0\nDecline the bet\n0, 0\n0, 1\n\nWe have, in effect, already covered The Radical Interpreter’s payouts. They win in the top-left and lower-right quadrants, and lose otherwise. Human’s payouts are only a little trickier. In the bottom row, they are guaranteed 0, since the bet is declined. In the top-left, the bet is a sure winner; their evidence entails it wins. So they get a payout of 1. In the top-right, the bet wins with probability 0.9, so the expected return7 of taking it is \\(1 \\times 0.9 - 100 \\times 0.1 = -9.1\\).\nThere are two Nash equilibria for the game - I’ve bolded them below.\n\n\n\\(p \\in E\\)\n\\(p \\notin E\\)\nTake the bet\n1, 1\n-9.1, 0\nDecline the bet\n0, 0\n0, 1\n\nThe mathematical result that there are two equilibria to this game should not come as a surprise. In discussing games like this earlier, we said that general principles connecting evidence, knowledge and action are not predictive; they are consistent both with p being part of the evidence, and with it not being part of the evidence. The general principles we had stated rule out, in effect, non-equilibrium solutions to games like this one. But they are not predictive in cases where there are multiple equilibria.\nTo make more progress, we need to turn to more contested areas of game theory. In particular, we need to look at some work on equilibrium choice. We’ll introduce this material via a game that is inspired by an example of Rousseau’s.\nEquilibrium Selection Principles\nAt an almost maximal level of abstraction, a two player, two option each game looks like this.\n\n\n\\(a\\)\n\\(b\\)\n\\(A\\)\n\\(r_{11}\\), \\(c_{11}\\)\n\\(r_{12}\\), \\(c_{12}\\)\n\\(B\\)\n\\(r_{21}\\), \\(c_{21}\\)\n\\(r_{22}\\), \\(c_{22}\\)\n\nWe’re going to focus on games that have the following eight properties:\n\\(r_{11} > r_{21}\\)\n\\(r_{22} > r_{12}\\)\n\\(c_{11} > c_{12}\\)\n\\(c_{22} > c_{21}\\)\n\\(r_{11} > r_{22}\\)\n\\(c_{11} \\geq c_{22}\\)\n\\(\\frac{r_{21}+r_{22}}{2} > \\frac{r_{11}+r_{12}}{2}\\)\n\\(\\frac{c_{12}+c_{22}}{2} \\geq \\frac{c_{11}+c_{21}}{2}\\)\nThe first four clauses say that the game has two (strict) Nash equilibria: \\(Aa\\) and \\(Bb\\). The fifth and sixth clauses say that the \\(Aa\\) equilibria is Pareto-optimal: no one prefers the other equilibria to it. In fact it says something a bit stronger: one of the players strictly prefers the \\(Aa\\) equilibria, and the other player does not prefer \\(Bb\\). The seventh and eighth clauses say that the \\(Bb\\) equilibria is risk-optimal. Risk-optimality is a somewhat complicated notion in general; see Harsanyi and Selten (1988) for more details. But for our purposes, we can focus on a simple characterisation of it. Neither player would prefer playing \\(A\\)/\\(a\\) to playing \\(B\\)/\\(b\\) if they thought it was a coin flip which equilibrium the other player was aiming for.\nI’m going to offer an argument from Hans Carlsson and Eric van Damme (1993) for the idea that in these games, rational players will end up at \\(Bb\\). The game that Human and The Radical Interpreter are playing fits these eight conditions, and The Radical Interpreter is perfectly rational, so this will imply that in that game, The Radical Interpreter will say that \\(p \\notin E\\), which is what we aimed to show.\nGames satisfying these eight inequalities are sometimes called Stag Hunt games. There is some flexibility, and some vagueness, in which of the eight inequalities need to be strict, but that level of detail isn’t important here. The name comes from a thought experiment in Rousseau’s Discourse on Inequality.\n\n[T]hey were perfect strangers to foresight, and were so far from troubling themselves about the distant future, that they hardly thought of the morrow. If a deer was to be taken, every one saw that, in order to succeed, he must abide faithfully by his post: but if a hare happened to come within the reach of any one of them, it is not to be doubted that he pursued it without scruple, and, having seized his prey, cared very little, if by so doing he caused his companions to miss theirs.  (Rousseau 1913, 209–10)\n\nIt is rather interesting to think through which real-life situations are best modeled as Stag Hunts, especially in situations where people have thought that the right model was a version of Prisoners’ Dilemma. This kind of thought is one way in to appreciating the virtues of Rousseau’s political outlook, and especially the idea that social coordination might not require anything like the heavy regulatory presence that, say, Hobbes thought was needed. But that’s a story for another day. What we’re going to be interested in is why Rousseau was right to think that a ‘stranger to foresight,’ who is just focussing on this game, should take the rabbit.\nTo make matters a little easier, we’ll focus on a very particular instance of Stag Hunt, as shown here. (From here I’m following Carlsson and van Damme very closely; this is their example, with just the labelling slightly altered.)\n\n\n\\(a\\)\n\\(b\\)\n\\(A\\)\n4, 4\n0, 3\n\\(B\\)\n3, 0\n3, 3\n\nAt first glance it might seem like \\(Aa\\) is the right choice; it produces the best outcome. This isn’t like Prisoners Dilemma, where the best collective outcome is dominated. In fact \\(Aa\\) is the best outcome for each individual. But it is risky, and Carlsson and van Damme show how to turn that risk into an argument for choosing \\(Bb\\).\nEmbed this game in what they call a global game. We’ll start the game with each player knowing just that they will play a game with the following payout table, with \\(x\\) to be selected at random from a flat distribution over \\([-1, 5]\\).\n\n\n\\(a\\)\n\\(b\\)\n\\(A\\)\n4, 4\n0, x\n\\(B\\)\nx, 0\nx, x\n\nBefore they play the game, each player will get a noisy signal about the value of \\(x\\). There will be signals \\(s_R\\) and \\(s_C\\) chosen (independently) from a flat distribution over \\([x - 0.25, x + 0.25]\\), and shown to Row and Column respectively. So each player will know the value of \\(x\\) to within \\(\\frac{1}{4}\\), and know that the other player knows it to within \\(\\frac{1}{4}\\) as well. But this is a margin of error model, and in those models there is very little that is common knowledge. That, they argue, makes a huge difference.\nIn particular, they prove that iterated deletion of strictly dominated strategies (almost) removes all but one strategy pair.8 Each player will play \\(A\\)/\\(a\\) if the signal is greater than 2, and \\(B\\)/\\(b\\) otherwise.9 Surprisingly, this shows that players should play the risk-optimal strategy even when they know the other strategy is Pareto-optimal. When a player gets a signal in \\((2, 3.75)\\), then they know that \\(x < 4\\), so \\(Bb\\) is the Pareto-optimal equilibrium. But the logic of the global game suggests the risk-dominant equilibrium is what to play.\nCarlsson and van Damme go on to show that many of the details of this case don’t matter. As long as (a) there is a margin of error in each side’s estimation of the payoffs, and (b) every choice is a dominant option in some version of the global game, then iterated deletion of strongly dominant strategies will lead to each player making the risk-dominant choice.\nI conclude from that that risk-dominant choices are rational in these games. There is a limit assumption involved here; what’s true for games with arbitrarily small margins of error is true for games with no margin of error. (We’ll come back to that assumption below.) And since The Radical Interpreter is rational, they will play the strategy that is not eliminated by deleting dominant strategies. That is, they will play the risk-dominant strategy.\nIn game with Human, the rational (i.e., risk-dominant) strategy for The Radical Interpreter is to say that \\(p \\notin E\\). And in the case of Parveen and Rahul, rational (i.e., risk-dominant) strategy for The Radical Interpreter is to say that it is not part of Parveen’s evidence that Rahul is in the restaurant. And this is an interest-relative theory of evidence; had Parveen been playing a different game, The Radical Interpreter would have said that it is part of Parveen’s evidence that Rahul was in the restaurant.\nAnd from this point we can say all the things we wanted to say about the case. If it is part of Parveen’s evidence that Rahul is in the restaurant, then she knows this. Conversely, if she knows it, then The Radical Interpreter would have said it is part of her evidence, so it is part of her evidence. Parveen will perform the action that maximises expected utility given her evidence. And she will lose knowledge when that disposition makes her do things that would be known to be sub-optimal if she didn’t lose knowledge.\nIn short, this model gives us a way to keep what was good about the pragmatic encroachment theory, while also allowing that evidence can be interest-relative. It does require a slightly more complex theory of rationality than we had previously used. Rather than just say that agents maximise evidential expected utility, we have to say that they play risk-dominant strategies in coordination games. But it turns out that this is little more than saying that they maximise evidential expected utility, and they expect others (at least perfectly rational abstract others) to do the same, and they expect those others to expect they will maximise expected utility, and so on.\nObjections and Replies\nWe’ll end the body of the paper with some objections that might be raised to this model. And then the appendix will contain proofs of a couple of the formal claims.\nObjection: The formal result of the previous section only goes through if we assume that the agents do not know precisely what the payoffs are in the game. We shouldn’t assume that what holds for arbitrarily small margins of error will hold in the limit, i.e., when they do know the payoffs.\nReply: If pushed, I would defend the use limit assumptions like this to resolve hard cases like Stag Hunt. But I don’t need that assumption here, What we really need is that Parveen doesn’t know precisely the probability of Rahul being in the restaurant given the rest of her evidence. Given that evidence is not luminous, as Williamson (2000) shows, this is a reasonable assumption. So the margin of error assumption that Carlsson and van Damme make is not, in our case, an assumption that merely makes the math easier; it is built into the case.\nObjection: Even if Parveen doesn’t know the payoffs precisely, The Radical Interpreter does. They are an idealisation, so they can be taken to be ideal.\nReply: It turns out that Carlsson and van Damme’s result doesn’t require that both parties are ignorant of the precise values of the payoffs. As long as one party doesn’t know the exact value of the payoff, the argument goes through. I prove this in Appendix Two.\nObjection: The formal argument requires that in the ‘global game’ there are values for \\(x\\) that make \\(A\\) the dominant choice. These cases serve as a base step for an inductive argument that follows. But in Parveen’s case, there is no such setting for \\(x\\), so the inductive argument can’t get going.\nReply: What matters is that there are values of \\(x\\) such that \\(A\\) is the strictly dominant choice, and Human (or Parveen) doesn’t know that they know that they know, etc., that those values are not actual. And that’s true in our case. For all Human (or Parveen) knows that they know that they know that they know…, the proposition in question is not part of their evidence under a maximally expansive verdict on The Radical Interpreter’s part. So the relevant cases are there in the model, even if for some high value of \\(n\\) they are known\\(^n\\) not to obtain.\nObjection: This model is much more complex than the simple motivation for pragmatic encroachment.\nReply: Sadly, this is true. I would like to have a simpler model, but I don’t know how to create one. The argument I gave earlier that our simple principles underdetermine what to say in cases like Parveen and Rahul’s seems fairly compelling. So more complexity will be needed, one way or another. I think paying this price in complexity is worth it overall, but I can see how some people might think otherwise.\nObjection: Change the case involving Human so that the bet loses 15 utils if p is false, rather than 100. Now the risk-dominant equilibrium is that Human takes the bet, and The Radical Interpreter says that p is part of Human’s evidence. But note that if it was clearly true that p was not part of Human’s evidence, then this would still be too risky a situation for them to know p. So whether it is possible that p is part of Human’s evidence matters.\nReply: This is all true, and it shows that the view I’m putting forward is incompatible with some programs in epistemology. In particular, it is incompatible with E=K, since the what it takes to be evidence on this story is slightly different from what it takes to be knowledge. I don’t think E=K is so intuitively obvious that this refutes the theory, but it is potentially a cost that I have to give it up.\nObjection: Carlsson and van Damme discuss one kind of global game. But there are other global games that have different equilibria. For instance, changing the method by which the noisy signal is selected would change the equilibrium of the global game. So this kind of argument can’t show that the risk-dominant equilibrium is the one true solution.\nReply: This is somewhat true. There are other ways of embedding the game involving Human and The Radical Interpreter in global games that lead to different outcomes. They are usually somewhat artificial; e.g., by having the signal be systematically biased in one way. But what really matters is the game where the error in Human’s knowledge of the payoffs is determined by their actual epistemic limitations. I think that will lead to something like the model we have here. But it is possible that the final result will differ a bit from what I have here, or (more likely) have some indeterminacy about just how interests interact with evidence and knowledge. The precise details are ultimately less important to me than whether we can provide a motivated story of how interests affect knowledge and evidence that does not presuppose we know what the agent’s evidence is. And the method I’ve outlined here shows that we can do that, even if we end up tinkering a bit with the details.\nAppendix One: Carlsson and van Damme’s Game\nTwo players, Row (or R) and Column (or C) will a version of the following game.\n\n\n\\(a\\)\n\\(b\\)\n\\(A\\)\n4, 4\n0, x\n\\(B\\)\nx, 0\nx, x\n\nThey won’t be told what \\(x\\) is, but they will get a noisy signal of \\(x\\), drawn from an even distribution over \\([x - 0.25, x + 0.25]\\). Call these signals \\(s_R\\) and \\(s_C\\). Each player must then choose \\(A\\), getting either 4 or 0 depending on the other player’s choice, or choose \\(B\\), getting \\(x\\) for sure.\nBefore getting the signal, the players must choose a strategy. A strategy is a function from signals to choices. Since the higher the signal is, the better it is to play \\(B\\), we can equate strategies with ‘tipping points,’ where the player plays \\(B\\) if the signal is above the tipping point, and \\(A\\) below the tipping point. Strictly speaking, a tipping point will pick out not a strategy but an equivalence class of strategies, which differ in how they act if the signal is the tipping point. But since that happens with probability 0, the strategies in the equivalence class have the same expected return, and so we won’t aim to distinguish them.\nAlso, strictly speaking, there are strategies that are not tipping points, because they map signals onto probabilities of playing \\(A\\), where the probability decreases as \\(A\\) rises. I won’t discuss these directly, but it isn’t too hard to see how these are shown to be suboptimal using the argument that is about to come. It eases exposition to focus on the pure strategies, and to equate these with tipping points. And since my primary aim here is to explain why the result holds, not to simply repeat an already existing proof, I’ll mostly ignore these mixed strategies.\nCall the tipping points for Row and Column respectively \\(T_R\\) and \\(T_C\\). Since the game is symmetric, we’ll just have to show that in conditions of common knowledge of rationality, \\(T_R = 2\\). It follows by symmetry that \\(T_C = 2\\) as well. And the only rule we’ll use is iterated deletion of strictly dominated strategies. That is, we’ll assume players won’t play strategies where another strategy does better no matter what the opponent chooses, and they won’t play strategies where another strategy does better provided the other player does not play a dominated strategy, and they won’t play strategies where another strategy does better provided the other player does not play a strategy ruled out by these first two conditions, and so on.\nThe return to a strategy is uncertain, even given the other player’s strategy. But given the strategies of each player, we can work out an expected return for each player. And that’s what we’ll assume is the return to a strategy pair.\nNote first that \\(T_R = 4.25\\) strictly dominates any strategy where \\(T_R = y > 4.25\\). If \\(s_R \\in (4.25, y)\\), then \\(T_R\\) is guaranteed to return above 4, and the alternative strategy is guaranteed to return 4. In all other cases, the strategies have the same return. And there is some chance that \\(s_R \\in (4.25, y)\\). So we can delete all strategies \\(T_R = y > 4.25\\), and similarly all strategies \\(T_C = y > 4.25\\). By similar reasoning, we can rule out \\(T_R < -0.25\\) and \\(T_C < -0.25\\).\nIf \\(s_R \\in [-0.75, 4.75]\\), then it is equally likely that \\(x\\) is above \\(s_R\\) as it is below it. Indeed, the posterior distribution of \\(x\\) is flat over \\([s_R - 0.25, s_R + 0.25]\\). From this it follows that the expected return of playing \\(B\\) after seeing signal \\(s_R\\) is just \\(s_R\\).\nNow comes the important step. Assume that we know that \\(T_C \\leq y > 2\\). Now consider the expected return of playing \\(A\\) given various values for \\(s_R > 2\\). Given that the lower \\(T_C\\) is, the higher the expected return is of playing \\(A\\), we’ll just work on the simple case where \\(T_C = y\\), realizing that this is an upper bound on the expected return of \\(A\\) given \\(T_C \\leq y\\). The expected return of \\(A\\) is 4 times the probability that Column will play \\(a\\), i.e., 4 times the probability that \\(s_C < T_C\\). Given all the symmetries that have been built into the puzzle, we know that the probability that \\(s_C < s_R\\) is 0.5. So the expected return of playing \\(A\\) is at most 2 if \\(s_R \\geq y\\). But the expected return of playing \\(B\\) is, as we showed in the last paragraph, \\(s_R\\), which is greater than 2. So it is better to play \\(B\\) than \\(A\\) if \\(s_R \\geq y\\). And the difference is substantial, so even if \\(s_R\\) is epsilon less than that \\(y\\), it will still be better to play \\(B\\). (This is hand-wavy of course, but we’ll make it rigorous in just a second.)\nSo if \\(T_C \\leq y > 2\\) we can prove that \\(T_R\\) should be lower still, because given that assumption it is better to play \\(B\\) even if the signal is just less than \\(y\\). Repeating this reasoning over and over again pushes us to it being better to play \\(B\\) than \\(A\\) as long as \\(s_R > 2\\). And the same kind of reasoning from the opposite end pushes us to it being better to play \\(A\\) than \\(B\\) as long as \\(s_R < 2\\). So we get \\(s_R = 2\\) as the uniquely rational solution to the game.\nLet’s make that a touch more rigorous. Assume that \\(T_C = y\\), and \\(s_r\\) is slightly less than \\(y\\). In particular, we’ll assume that \\(z = y - s_R\\) is in \\((0, 0.5)\\). Then the probability that \\(s_C < y\\) is \\(0.5 + 2z - 2z^2\\). So the expected return of playing \\(A\\) is \\(2 + 8z - 8z^2\\). And the expected return of playing \\(B\\) is, again, \\(s_R\\). These will be equal when the following is true. (The working out is a tedious but trivial application of the quadratic formula, plus some rearranging.)\n\\[s_R = y + \\frac{\\sqrt{145-32y} - 9}{16}\\] So if we know that \\(T_C \\geq y\\), we know that \\(T_R \\geq y + \\frac{\\sqrt{145-32y} - 9}{16}\\), which will be less than \\(y\\) if \\(y > 2\\). And then by symmetry, we know that \\(T_C\\) must be at most as large as that as well. And then we can use that fact to derive a further upper bound on \\(T_R\\) and hence on \\(T_C\\), and so on. And this will continue until we push both down to 2. It does require quite a number of steps of iterated deletion. Here is the upper bound on the threshold after \\(n\\) rounds of deletion of dominated strategies. (These numbers are precise for the first two rounds, then just to three significant figures after that.)\n\nRound\nUpper Bound on Threshold\n1\n4.250\n2\n3.875\n3\n3.599\n4\n3.378\n5\n3.195\n6\n3.041\n7\n2.910\n8\n2.798\n9\n2.701\n10\n2.617\n\nThat is, \\(T_R = 4.25\\) dominates any strategy with a tipping point above 4.25. And \\(T_R = 3.875\\) dominates any strategy with a higher tipping point than that, assuming \\(T_C \\leq 4.25\\). And \\(T_R \\approx 3.599\\) dominates any strategy with a higher tipping point than that, assuming \\(T_C \\leq 3.875\\). And so on.\nAnd similar reasoning shows that at each stage not only are all strategies with higher tipping points dominated, but so are strategies that assign positive probability (whether it is 1 or less than 1), to playing \\(A\\) when the signal is above the ‘tipping point.’ So this kind of reasoning rules out all mixed strategies (except those that respond probabilistically to \\(s_R = 2\\)).\nSo we’ve shown what was intended, namely that iterated deletion of dominated strategies will rule out all strategies except the risk-optimal equilibrium. We needed the possibility that \\(x\\) is greater than the maximal return for \\(A\\) to get the iterated dominance going. And we needed the signal to have an error bar to it, so that each round of iteration removes more strategies. But that’s all we needed; the particular values we chose are irrelevant to the proof.\nAppendix Two: The Modified Game\nThe aim of this section is to prove something that Carllson and van Damme did not prove, namely that the analysis of the previous appendix goes through with very little change if one party gets a perfect signal, while the other gets a noisy signal. That is, we’re going to consider the game that is just like the game of appendix one, but it is common knowledge that the signal Column gets, \\(s_C\\), equals \\(x\\).\nSince the game is no longer symmetric, we can’t appeal to the symmetry of the game as we frequently did in the previous appendix. But this only slows the proof down, it doesn’t stop it.\nWe can actually rule out slightly more at the first step in this game than in the previous game. Since Column could not be wrong about \\(x\\), Column knows that if \\(s_C > 4\\) then playing \\(b\\) dominates playing \\(a\\). So one round of deleting dominated strategies rules out \\(T_C > 4\\), as well as ruling out \\(T_R > 4.25\\).\nAt any stage, if we know \\(T_C \\leq y > 2\\), then \\(T_R = y\\) dominates \\(T_R > y\\). That’s because if \\(s_R \\geq y\\), and \\(T_C \\leq y\\), then the probability that Column will play \\(a\\) (given Row’s signal) is less than 0.5. After all, the signal is just as likely to be above \\(x\\) as below it (as long as the signal isn’t too close to the extremes). So if \\(s_R\\) is at or above \\(T_C\\), then it is at least 0.5 likely that \\(s_C = x\\) is at or above \\(T_C\\). So the expected return of playing \\(A\\) is at most 2. But the expected return of playing \\(B\\) equals the signal, which is greater than 2. So if Row knows \\(T_C \\leq y > 2\\), Row also knows it is better to play \\(B\\) if \\(s_R \\geq y\\). And that just means that \\(T_R \\leq y\\).\nAssume now that it is common knowledge that \\(T_R \\leq y\\), for some \\(y > 2\\). And assume that \\(x = s_C\\) is just a little less than \\(y\\). In particular, define \\(z = y -x\\), and assume \\(z \\in (0, 0.25)\\). We want to work out the upper bound on the expected return to Column of playing \\(a\\). (The return of playing \\(b\\) is known, it is \\(x\\).) The will be highest when \\(T_R\\) is lowest, so assume \\(T_R \\leq y\\). Then the probability that Row plays \\(A\\) is \\((1 + 2z)/2\\). So the expected return of playing \\(a\\) is \\(2 + 4z\\), i.e., \\(2 + 4(y - x)\\). That will be greater than \\(x\\) only when\n\\[x < \\frac{2 + 4y}{5}\\] And so if it is common knowledge that \\(T_R \\leq y\\), then it is best for Column to play \\(b\\) unless \\(x < \\frac{2 + 4y}{5}\\). That is, if it is common knowledge that \\(T_R \\leq y\\), then \\(T_C\\) must be at most \\(\\frac{2 + 4y}{5}\\).\nSo now we proceed in a zig-zag fashion. At one stage, we show that \\(T_R\\) must be as low as \\(T_C\\). At the next, we show that if it has been proven that \\(T_R\\) takes a particular value greater than 2, then \\(T_C\\) must be lower still. And this process will eventually rule out all values for \\(T_R\\) and \\(T_C\\) greater than 2.\nThis case is crucial to the story of the paper because The Radical Interpreter probably does not have an error bar in their estimation of the game they are playing. But it turns out the argument for risk-dominant equilibria being the unique solution to interpretation games is consistent with that. As long as one player has a margin of error, each player should play the risk-dominant equilibria.\n\n\nBrown, Jessica. 2008. “Subject-Sensitive Invariantism and the Knowledge Norm for Practical Reasoning.” Noûs 42 (2): 167–89. https://doi.org/10.1111/j.1468-0068.2008.00677.x.\n\n\nCarlsson, Hans, and Eric van Damme. 1993. “Global Games and Equilibrium Selection.” Econometrica 61 (5): 989–1018. https://doi.org/10.2307/2951491.\n\n\nFantl, Jeremy, and Matthew McGrath. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nHarsanyi, John C., and Reinhard Selten. 1988. A General Theory of Equilibrium Selection in Games. Cambridge, MA: MIT Press.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nIchikawa, Jonathan Jenkins, Benjamin Jarvis, and Katherine Rubin. 2012. “Pragmatic Encroachment and Belief-Desire Psychology.” Analytic Philosophy 53 (4): 327–43. https://doi.org/10.1111/j.2153-960X.2012.00564.x.\n\n\nLackey, Jennifer. 2010. “Acting on Knowledge.” Philosophical Perspectives 24: 361–82. https://doi.org/10.1111/j.1520-8583.2010.00196.x.\n\n\nReed, Baron. 2014. “Practical Matters Do Not Affect Whether You Know.” In Contemporary Debates in Epistemology, edited by Matthias Steup, John Turri, and Ernest Sosa, 2nd ed., 95–106. Chicester: Wiley-Blackwell.\n\n\nRousseau, Jean-Jacques. 1913. Social Contract & Discourses. Translated by G. D. H. Cole. New York: J. M. Dent & Sons.\n\n\nRubin, Katherine. 2015. “Total Pragmatic Encroachment and Epistemic Permissiveness.” Pacific Philosophical Quarterly 96: 12–38. https://doi.org/10.1111/papq.12060.\n\n\nSchwitzgebel, Eric. 2008. “The Unreliability of Naive Introspection.” Philosophical Review 117 (2): 245–73. https://doi.org/10.1215/00318108-2007-037.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\n———. 2007. The Philosophy of Philosophy. Blackwell.\n\n\nThat we need some kind of super-knowledge for action, and not mere knowledge, is a popular, and natural, explanation of the case. For versions of this explanation, obviously with more details than I’ve given here, see for example Jessica Brown (2008) and Jennifer Lackey (2010).↩︎\nJessica Brown (2008, 176) shows that pragmatic encroachment theories that rely just on the stakes involved are subject to serious counterexample. Katherine Rubin (2015) argues that if we have a ‘global’ version of pragmatic encroachment, where all our epistemic notions are interest-relative, then it is implausible that it is the stakes the subject faces that matter for knowledge. Since I’m defending such a global version of pragmatic encroachment, Rubin’s arguments show that it is important that I’m relying on odds, not stakes. Baron Reed (2014) argues that if it is stakes alone that matter to pragmatic encroachment, then agents who the pragmatic encroachment theorist takes to be perfectly rational would be subject to a Dutch Book.↩︎\nYou can read this paper as a reply to the challenge posed by Ichikawa, Jarvis, and Rubin (2012). They note that there are challenges facing the pragmatic encroachment theorist whether they make evidence interest-relative, or interest-invariant. I’m going to show how to have an interest-relative theory of evidence, and keep what was desirable about pragmatic encroachment theories.↩︎\nIn these games, Row chooses a row, and Column chooses a column, and that determines the cell that is the outcome of the game. The cells include two numbers. The first is Row’s payout, and the second is Column’s. The games are non-competitive; the players are simply trying to maximise their own returns, not maximise the difference between their return and the other player’s return.↩︎\nA Nash equilibrium is an outcome of the game where every player does as well as they can given the moves of the other players. Equivalently, it is an outcome where no player can improve their payout by unilaterally defecting from the equilibrium.↩︎\nThe Radical Interpreter feels like they should be a humanesque character in Alice in Wonderland or The Phantom Tollbooth, but for now they are resolutely abstract.↩︎\nI am making a large, if orthodox, assumption here: that the payouts that we use for equilibrium analysis should be expected returns, not actual returns. I think that’s the right thing to do, since it is usually impossible to say what the actual return of a game is. Even when we say that the payout is a certain number of dollars, we are really saying that the return is a certain kind of gamble. Maybe the value of the currency will deprecate quickly, and the dollars are not that valuable. Maybe the revolution will come and wealth will be a liability. Almost all games have probabilistic payouts, and this game is no different.↩︎\nA sketch of the proof is in Appendix One.↩︎\nStrictly speaking, we can’t rule out various mixed strategies when the signal is precisely 2, but this makes little difference, since that occurs with probability 0.\n\n↩︎\n",
    "preview": "posts/2021-02-02-interests-evidence-and-games/stag.jpg",
    "last_modified": "2021-02-05T20:43:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-notes-on-some-ideas-in-lloyd-humberstones-philosophical-applications-of-modal-logic/",
    "title": "Notes on Some Ideas in Lloyd Humberstone’s Philosophical Applications of Modal Logic",
    "description": "Lloyd Humberstone’s recently published _Philosophical Applications of Modal Logic_ presents a number of new ideas in modal logic as well explication and critique of recent work of many others. We extend some of these ideas and answer some questions that are left open in the book.",
    "author": [
      {
        "name": "Steven Kuhn",
        "url": "https://gufaculty360.georgetown.edu/s/contact/00336000014Rhs1AAC/steven-kuhn"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2018-04-14",
    "categories": [
      "logic"
    ],
    "contents": "\n\nContents\nLocal and Global Conditions\nFully Modalized Logics\n“Nothing in Between” and the Equivalence of Modal Logics\nS4 \\(\\oplus\\) 5\\(^\\prime\\) = S4\\(\\oplus\\)F\nAin’t Necessarily So\n\nLloyd Humberstone’s recently published Philosophical Applications of Modal Logic (Humberstone (2016)) presents a number of new ideas in modal logic as well explication and critique of recent work of many others. In this note we extend some of these ideas and answer some questions that are left open in the book. Numbers without other identification refer to pages in that book.\nPublished in Australasian Journal of Logic 15.\nPicture by Andy Images via Creative Commons\nLocal and Global Conditions\nOne theme sounded frequently in Humberstone (2016) is the relation between a local condition, which describes a point in a frame and a global condition, which concerns the frame as a whole. For example, the local conditions of being reflexive (\\(Rxx\\)) and being reflexive with reflexive successors (\\(Rxx \\land \\forall y(Rxy \\rightarrow Ryy)\\)) are distinct, but their universal possession by the points in a frame describes the same global condition of reflexivity. As a consequence, the non-equivalent modal axioms \\(\\Box p\\rightarrow p\\) and \\((\\Box p\\rightarrow p) \\land \\Box(\\Box q\\rightarrow q)\\) both define the class of reflexive frames. This example leads Humberstone to ask (189) whether there is a local property not implying that a point possessing it is reflexive whose universal possession makes the frame reflexive. Affirmative answers are supplied by the following formulas: \\(\\forall y(y{=}x\\wedge Rxx) \\vee (\\exists y(y\\ne x) \\wedge \\forall y(y\\ne x \\rightarrow Ryy))\\) (either x is the only world and it is reflexive or else there are other worlds, all of which are reflexive), and \\(\\exists zRxz \\wedge \\forall z(Rzx \\rightarrow Rzz)\\) (x has a successor and every world that can see x is reflexive). The second example implies that the tense-logical formulas \\({\\mathbf{F}}\\top \\wedge \\mathbf{H}(\\mathbf{G}p\\rightarrow p)\\) and \\(\\mathbf{G}p\\rightarrow p\\) both define the class of reflexive frames.\nFully Modalized Logics\nAnother topic that gets well-deserved attention in Humberstone (2016) is the property of logics that Humberstone calls being “fully modalized.” (See 290-304.) The idea is that in alethic modal systems the axiom \\(\\Box A\\rightarrow A\\) provides a logical connection between the modal and nonmodal formulas, whereas in a doxastic or deontic logic we expect that matters concerning what is believed or what ought to be the case should be logically independent of those concerning what is the case. The latter, but not the former, are fully modalized. But the idea needs to formulated with some care because we don’t want the presence of, for example, \\(A\\rightarrow \\Box \\top\\) as a theorem to count against a logic’s being fully modalized. As Humberstone puts it, in a fully modalized logic, “…we don’t expect…the forging of any…logical connections between \\(\\Box A\\) and \\(A\\) for any given \\(A\\) – other than those which hold…derivatively” (291). The notion is captured in a rather complicated way by E. Zolin in Zolin (2000) and Humberstone shows that the characterization there is equivalent to the following simpler one: if there is a theorem of the form \\(M\\vee N\\) where \\(M\\) is fully a fully modalized formula (i.e., containing no sentence letters not within the scope of a modal operator) and \\(N\\) is non-modal (i.e., containing no occurrences of modal operators) then either \\(M\\) or \\(N\\) is itself a theorem. In this section we show that Zolin’s characterization is also equivalent to an even simpler one that is closer in spirit to the motivating remarks in Humberstone (2016): every theorem is a tautological consequence of a fully modalized theorem. (Thus the theorems can be divided into two categoriesthe essentially nonmodal ones, i.e., the tautologies, and the essentially modal ones, i.e., the non-tautologies that are tautological consquences of fully modalized theorems).\nWe begin by restating Zolin’s definition in our own terminology. If \\(p_1,{\\ldots},p_n\\) are sentence letters, then a state description in \\({p_1,{\\ldots},p_n}\\) is a conjunction \\(p_1^*\\wedge {\\ldots}\\wedge p_n^*\\), where, for \\(1\\le i\\le n, p_i^*\\) is either \\(p_i\\) or \\(\\neg p_i\\). The (truth-functional) constituents of a formula \\(A\\) are the sentence-letters and \\(\\Box\\)-formulas occurring in \\(A\\) that do not properly occur within the scope of any \\(\\Box\\). Zolin observes that every formula \\(A\\) can be “decomposed” into a formula of the form \\({\\bigvee}\\{(\\vec{p}\\wedge B(\\vec{p}))\\!:\\vec{p}\\) is a state description in the sentence letter constituents of \\(A\\)}, where, for each \\(\\vec{p}\\) , \\(B(\\vec{p})\\) is some truth functional combination of the modal constituents of \\(A\\). By fixing on a particular ordering of formulas and taking the \\(B(\\vec{p})\\)’s to be in a disjunctive normal form that conforms to this ordering, we can single out a unique decomposition of this kind. Let’s call it the Zolin form of \\(A\\) and let’s call the formulas \\(B(\\vec{p})\\) that occur as right conjunct of a disjunct in the Zolin form of \\(A\\), the Zolin components of \\(A\\). Note that every formula is truth-funtionally equivalent to its Zolin form. Then, according to Zolin’s definition, a logic is fully modalized if \\(\\vdash A\\) implies \\(\\vdash B(\\vec{p})\\) for every \\(B(\\vec{p})\\) that is a Zolin component of \\(A\\).\n\nTheorem 1. A logic L is fully modalized (according to Zolin’s definition) iff every theorem of L is a tautological consequence of a fully modalized theorem.\n\n\nProof. (Left to right). Suppose L satisfies Zolin’s definition and \\(\\vdash_L A\\). Let \\((\\vec{p}_1\\wedge B(\\vec{p}_1))\\vee {\\ldots}\\vee (\\vec{p}_n\\wedge B(\\vec{p}_n))\\) be the Zolin form of \\(A\\). Then, according to Zolin’s definition, \\(\\vdash_L B(\\vec{p}_i)\\) for each \\(i\\), \\(1\\le i\\le n\\). Let \\(A^\\prime = B(\\vec{p}_1)\\wedge {\\ldots}\\wedge B(\\vec{p}_n)\\). \\(A^\\prime\\) is fully modalized and, since each of its conjuncts is provable in L, \\(A^\\prime\\) is as well. All that remains is to show that \\(A\\) is a truth-functional consequence of \\(A^\\prime\\). Let \\(\\alpha\\) be any assignment of truth values to the constituents of \\(A\\) such that \\(\\alpha\\models A^\\prime\\). Let \\(\\vec{p}_\\alpha\\) be the state description in the sentence letters that are truth-functional constiuents of \\(A\\) that corresponds to \\(\\alpha\\) in the sense that each conjunct of \\(\\vec{p}_\\alpha\\) is the literal \\(p\\) or \\(\\neg p\\) according to whether \\(\\alpha(p)\\) is true or false. Then \\(\\alpha\\) verifies \\((\\vec{p}_\\alpha \\wedge B(\\vec{p}_\\alpha))\\), which is a disjunct of \\(A\\) and so \\(\\alpha\\models A\\) as required.\n(Right to left). We are given that every theorem of L is a tautological consequence of some fully modalized theorem. Now suppose \\(\\vdash_L A\\) and \\(B(\\vec{p})\\) is a Zolin component of \\(A\\), with a view towards showing \\(\\vdash_L B(\\vec{p})\\). By the initial suppositions, \\(A\\) is a truth-functional consequence of some fully modalized formula \\(A^\\prime\\). Then the Zolin form of \\(A\\), call it \\((\\vec{p}_1\\wedge B(\\vec{p}_1))\\vee {\\ldots}\\vee (p_n\\wedge B(\\vec{p}_n))\\), is also a truth functional consequence of \\(A^\\prime\\), where for some \\(i\\), \\(1\\le i\\le n\\), \\(B(\\vec{p}_i)=B(\\vec{p})\\). We show that \\(B(\\vec{p})\\) is provable in L by showing that it is also a truth-functional consequence of the theorem \\(A^\\prime\\). To that end, let \\(\\alpha\\) be any assignment of truth values to the constituents of \\(A\\), such that \\(\\alpha\\models A^\\prime\\). Since \\(A^\\prime\\) is fully modalized, its truth value under an assignment is not affected by the truth assignments to sentence letters, so we can assume without loss of generality that these conform to the state description . Since the disjunction \\((\\vec{p}_1\\wedge B(\\vec{p}_1))\\vee {\\ldots}\\vee (\\vec{p}_n\\wedge B(\\vec{p}_n))\\) is a truth functional consequence of \\(A^\\prime\\), \\(\\alpha\\) must verify this disjunction. But \\(\\vec{p}_1,{\\ldots},\\vec{p}_n\\) are state descriptions, so \\(\\alpha\\) can verify only one disjunct of this formula, namely \\(\\wedge B(\\vec{p})\\). Hence \\(\\alpha\\models B(\\vec{p})\\) as required. ◻\n\nIt is possible that there are applications for which Zolin’s more detailed normal-form characterization of a fully modalized logic is more useful than the simple characterization given here. But the proof below shows that property that Humberstone extracts in Humberstone (2016) can be proved at least as easily from our simple characterization.\n\nTheorem 2. Suppose every theorem of L is a tautological consequence of a fully modalized theorem. Then \\(\\vdash_LM\\vee N\\) where \\(M\\) is fully modalized and \\(N\\) is modality-free implies either \\(\\vdash_LM\\) or \\(\\vdash_LN\\).\n\n\nProof. Assume the hypothesis of the claim and \\(\\vdash_LM\\vee N\\) for appropriate \\(M\\) and \\(N\\). Then there is some fully modalized L-theorem \\(A^\\prime\\) such that \\(M\\vee N\\) is a truth functional consequence of \\(A^\\prime\\). Suppose for reductio that neither \\(M\\) nor \\(N\\) is provable. Then neither \\(M\\) nor \\(N\\) is a truth functional consequence of \\(A^\\prime\\). So there is an assignment \\(\\alpha\\) of truth values to the constituents of \\(A^\\prime\\) and \\(M\\) that makes the former true and the latter false. Similarly, there is an assignment \\(\\beta\\) to the constituents of \\(A^\\prime\\) and \\(N\\) that makes \\(A^\\prime\\) true and \\(N\\) false. Now extend the assignment \\(\\alpha\\) to the sentence letters in \\(N\\) by assigning them the same truth values as \\(\\beta\\) does, and call the result \\(\\alpha^\\prime\\). Since \\(\\alpha^\\prime\\) agrees with \\(\\alpha\\) on the modal constituents it verifies \\(A^\\prime\\) and falsifies \\(M\\). Since it agrees with \\(\\beta\\) on sentence letters, it falsifies \\(N\\). This contradicts the earlier observation that \\((M\\vee N)\\) is a truth functional consequence of \\(A^\\prime\\). ◻\n\n“Nothing in Between” and the Equivalence of Modal Logics\nThe impetus for Section 4.4 of Humberstone’s book (304-324) is Arthur Prior’s observation that the logical structure of moral concepts appears to be unlike those of quantity and alethic modality:\n\nIn between “S must be P” and “S may be P” stands the simple “S is in fact P,” just as “This S is P stands in between”Every S is P and “Some S is P.” … But so far as I can see there is nothing among the moral or ‘deontic’ modalities that corresponds to these intermediary ‘existential’ or ‘alethic’ modalities. (Prior (1951) p145, quoted on 304 in Humberstone (2016).)\n\nEarly in the section,Humberstone notes that, in fact, there are strict logical intermediaries between “\\(A\\) is obligatory” and “\\(A\\) is permitted” or indeed between any sentences \\(A\\) and B of decreasing logical strength in any reasonably well behaved modal logic, whether their connectives are given a deontic reading or any other. For one can simply take as intermediary, any formula \\(A\\vee (B\\wedge p)\\) where \\(p\\) is a sentence letter that does not occur in \\(A\\) or \\(B\\). In this section we wish to point out that a consequence of this observation is that there is a sense in which all modal logics meeting certain minimal requirements are the same.\nWe identify a “logic” with a many-one deducibility relation on formulas satisfying the usual structural conditions. (So the logic with all tautologies as axioms and no rules of inference is distinct from a similar logic with modus ponens as a rule of inference.) The minimal requirements are just that logics are classically based and substitution-closed.By classically based we mean that their languages contain the Booleanconnectives (or at least some truth-functionally complete subset thereof) and that these behave classically under the deducibility relation, so that, for example \\(A \\wedge B \\vdash C\\) iff \\(A \\vdash (B \\rightarrow C)\\).1 If a logic is classical, we may safely identify it with the set of its theorems, knowing that these will determine the deducibility relation. By substitution-closed we mean that \\(A_1^\\prime,{\\ldots},A_n^\\prime \\vdash B^\\prime\\) whenever \\(A_1^\\prime,{\\ldots},A_n^\\prime\\) and \\(B^\\prime\\) are the result of uniformly replacing sentence letters by formulas in \\(A_1,{\\ldots},A_n\\) and \\(B\\) such that \\(A_1,{\\ldots},A_n \\vdash B\\). The requirement that the logic is classically based ensures that \\(A\\vee (B\\wedge p)\\) is a logical intermediary between \\(A\\) and \\(B\\). The requirement that it is substitution-closed implies that it is a strict intermediary. For if it provably implied \\(A\\), then its substitution instance \\(A\\vee (B\\wedge B)\\) would provably imply \\(A\\), and \\(A\\) would be provably equivalent to \\(B\\). And if it was provably implied by \\(B\\), then \\(A\\vee (B\\wedge A)\\) would be provably implied by \\(B\\) and again \\(A\\) would be equivalent to \\(B\\). By saying that these logics are the same we mean something close to what is sometimes called translationally equivalent.2 Let us say that logics \\(L_1\\) and \\(L_2\\) are weakly translationally equivalent if there is a map \\(s\\colon A{\\mapsto}A^s\\) from formulas of \\(L_1\\) to formulas of \\(L_2\\) and a map \\(t\\colon C{\\mapsto}C^t\\) from formulas of \\(L_2\\) to formulas of \\(L_1\\) satisfying the following conditions (where \\(\\vdash_i\\) is \\(\\vdash_{L_i}\\) for \\(i=1,2\\)):\n\\(A_1,{\\ldots},A_n \\vdash_1 B\\) implies \\({A_1}\\!^s,{\\ldots},{A_n}\\!^s \\vdash_2 B^s\\)\n\\(C_1,{\\ldots},C_m \\vdash_2 D\\) implies \\({C_1}^t,{\\ldots},{C_m}^t \\vdash_1 D^t\\)\n\\(A ~_1\\!{\\dashv}{\\vdash}\\!_1 (A^s)^t\\)\n\\(C ~_2\\!{\\dashv}{\\vdash}\\!_2 ~ (C^t)^s\\).\n\\(s\\) and \\(t\\) are to be thought of as translations between the logics. If \\(L_1\\) and \\(L_2\\) are weakly translationally equivalent then the word implies in i and ii can be strengthened to if and only if, so that \\(s\\) and \\(t\\) are faithful embeddings. For example, by condition ii, \\({A_1}^s,{\\ldots},{A_n}^s \\vdash_2 B^s\\) implies \\(({A_1}^s)^t,{\\ldots},({A_n}^s)^t \\vdash_1({B^s})^t\\), and so, by condition iii \\(A_1,{\\ldots},A_n \\vdash_1 B\\). But the strengthened versions of i and ii still do not imply iii and iv. (See, for example, French (2010) pp 111-124.) \\(L_1\\) and \\(L_2\\) are said to be translationally equivalent if the translations securing their weak equivalence meet some additional requirement, commonly that they be compositional, i.e., that they be maps \\(f\\) such that for every n-ary connective \\(\\#\\) in the source language there is formula schema \\(\\sigma\\) of the target language with \\(n\\) schematic variables such that \\(f(\\#A_1{\\ldots}A_n)=\\sigma(f(A_1),{\\ldots},f(A_n))\\). If we are interested in what can be said within a logic rather than the structure of the formulas saying it, however, the restriction to compositional translations seems unwarranted. A translation can be “sentence by sentence” rather than “symbol by symbol.” It is plausible to take formulas to be saying the same thing in a logic when they are provably equivalent. In that case the structure of the things that can be said in a logic is given by its Lindenbaum lattice. By this, we mean the structure \\((X,\\le )\\) where the members of X are the equivalence classes \\([A]_L\\) of formulas \\(A\\) under the relation \\(_L\\!{\\dashv}{\\vdash}\\!_L~\\) and \\([A]_L\\le [B]_L\\) iff \\(A\\vdash \\!_LB\\). In that case, we may say that two logics are the same with regards to what they can say if their Lindenbaum lattices are isomorphic. It is not difficult to show that under conditions of interest here, this condition coincides with weak translational equivalence.\n\nTheorem 3. (i) If \\(L_1\\) and \\(L_2\\) are weakly translationally equivalent then they have isomorphic Lindenbaum lattices. (ii) If \\(L_1\\) and \\(L_2\\) are classically based and they have isomorphic Lindenbaum lattices then they are weakly translationally equivalent.\n\n\nProof. Here and below, we drop the subscripts from the brackets and turnstile symbols, when the logic is intended is clear. To prove i, suppose \\(s\\) and \\(t\\) satisfy conditions i-iv defining weak translational equivalence. Let \\(\\Phi([A])=[A^s]\\). We show that \\(\\Phi\\) is an isomorphism. i)\\(\\Phi\\) is well defined. Suppose \\([A]=[B]\\). Then \\(A{\\dashv}{\\vdash}B\\). By condition i, this implies \\(A^s{\\dashv}{\\vdash}B^s\\). Hence \\([A^s] = [B^s]\\) and \\(\\Phi([A])=\\Phi([B])\\), as required. ii)\\(\\Phi\\) is 1-1. Suppose \\(\\Phi([A])=\\Phi([B])\\). Then \\([A^s]=[B^s]\\) and so \\(A^s{\\dashv}{\\vdash}B^s\\). By condition ii, \\((A^s)^t{\\dashv}{\\vdash}(B^s)^t\\). By condition iii, \\(A{\\dashv}{\\vdash}B,\\) and so \\([A]=[B]\\), as required. iii)\\(\\Phi\\) is onto. Take any \\(C\\) in the language of \\(L_2\\). By condition iv, \\(C{\\dashv}{\\vdash}(C^t)^s\\). Hence \\([C] = [(C^t)^s]\\). Therefore \\([C]=\\Phi([C^t])\\), and \\([C]\\) is in the range of \\(\\Phi\\), as required.\nThe proof of ii is facilitated by a lemma. Let us say that a translation \\(f\\colon A{\\mapsto}A^f\\) conforms to falsum if \\(\\bot^f {\\dashv}{\\vdash} \\bot\\); to negation if \\((\\neg A)^f {\\dashv}{\\vdash} \\neg A^f\\); to conjunction if \\((A\\wedge B)^f {\\dashv}{\\vdash}A^f \\wedge B^f\\) and similarly for all the other Boolean connectives. To prove part ii of the theorem, we use only that \\(s\\) and \\(t\\) conform to conjunction, but we take the opportunity to prove something more general.\n\nLemma 1. Suppose \\(s\\) and \\(t\\) are translations securing the weak equivalence of classically based modal logics \\(L_1\\) and \\(L_2\\). Then \\(s\\) and \\(t\\) conform to all the Boolean connectives.\n\n\nProof. . Suppose \\(s\\) and \\(t\\) satisfy the hypothesis of the lemma. We show that \\(s\\) and \\(t\\) conform to falsum (i) and the conditional (ii) and that it follows that they conform to all the other Boolean connectives (iii).\n\\(i\\) (We include the subscripts for clarity here.) Since \\(L_1\\) and \\(L_2\\) are classically based, \\(\\bot\\vdash_2 \\bot^s\\) and \\(\\bot \\vdash_1 \\bot^t\\). From the second of these it follows that \\(\\bot^s \\vdash_2 (\\bot^t)^s\\), and therefore that \\(\\bot^s \\vdash_2 \\bot\\). Hence \\(\\bot_2\\!{\\dashv}{\\vdash}\\!_2 \\bot^s\\) and so \\(s\\) conforms to \\(\\bot\\). The proof that \\(t\\) conforms to \\(\\bot\\) is similar.\n\\(ii\\) Since \\(L_1\\) is classically based, \\((A\\rightarrow B),A \\vdash B\\). By condition i of weak translational equivalence \\((A\\rightarrow B)^s, A^s \\vdash B^s\\). Since \\(L_2\\) is classically based, \\((A\\rightarrow B)^s \\vdash (A^s\\rightarrow B^s)\\). Similarly, since \\(L_2\\) is classically based, \\((A\\rightarrow B)^t,A^t \\vdash B^t\\). By condition (ii) of weak translational equivalence, \\((A^s\\rightarrow B^s)^t,(A^s)^t \\vdash (B^s)^t\\). By condition iii, \\((A^s)^t {\\dashv}{\\vdash}A\\) and \\((B^s)^t {\\dashv}{\\vdash}B\\), and so \\((A^s\\rightarrow B^s)^t,A \\vdash B\\). Since \\(L_1\\) is classically based, \\((A^s\\rightarrow B^s)^t \\vdash A\\rightarrow B\\). By condition i, \\(((A^s\\rightarrow B^s)^t)^s \\vdash (A\\rightarrow B)^s\\), which implies by condition iv that \\((A^s\\rightarrow B^s) \\vdash (A\\rightarrow B)^s\\). We have now shown that \\((A\\rightarrow B)^s{\\dashv}{\\vdash}(A^s\\rightarrow B^s)\\), and so \\(s\\) conforms to \\(\\rightarrow\\). The proof that \\(t\\) conforms to \\(\\rightarrow\\) is similar.\n\\(iii\\) It can be shown that \\(s\\) and \\(t\\) conform to each of the remaining Boolean connectives by expressing them in terms of falsum and the conditional. For example \\((\\neg A)^s {\\dashv}{\\vdash} (A\\rightarrow \\bot)^s\\). Since \\(s\\) conforms to the conditional and falsum, \\((\\neg A)^s {\\dashv}{\\vdash} A^s \\rightarrow \\bot\\). Since the logics are classically based, \\((\\neg A)^s {\\dashv}{\\vdash} \\neg A^s\\). The other cases are similar. ◻\n\nWe proceed to the proof of part ii of the theorem. Suppose \\(\\Phi\\) is an isomorphism between the Lindenbaum lattices \\((X_1,\\le _1)\\) and \\((X_2,\\le _2)\\) of \\(L_1\\) and \\(L_2\\). Let \\(s\\) map each formula \\(A\\) in the language of \\(L_1\\) to any member of \\(\\Phi([A])\\) and let \\(t\\) map each formula \\(C\\) in the language of \\(L_2\\) to any member of \\(\\Phi^{-1}([C])\\). We show that \\(s\\) and \\(t\\) meet the four conditions for weak translational equivalence. For i, suppose \\(A_1,{\\ldots},A_n \\vdash B\\). Since \\(L_1\\) is classically based \\(A_1\\wedge {\\ldots}\\wedge A_n \\vdash B\\), and so \\([A_1\\wedge {\\ldots}\\wedge A_n] \\le [B]\\). Since \\(\\Phi\\) is an isomorphism, \\(\\Phi([A_1\\wedge {\\ldots}\\wedge A_n]) \\le \\Phi([B])\\), and so \\((A_1\\wedge {\\ldots}\\wedge A_n)^s \\vdash B^s\\). Since \\(s\\) conforms to conjunction, \\(({A_1}^s\\wedge {\\ldots}\\wedge {A_n}^s) \\vdash B^s\\). Since \\(L_2\\) is classically based, \\({A_1}^s,{\\ldots},{A_n}^s \\vdash B^s\\). The proof that condition ii is satisfied is similar. For conditions iii and iv note that, since \\(A^s \\in \\Phi([A]), [A^s]= \\Phi([A])\\). Similarly, \\([C^t]= \\Phi^{-1}([C^t])\\). Together these two identities imply \\([(A^s)^t] = \\Phi^{-1}(\\Phi([A])\\) and \\([(C^t)^s]= \\Phi(\\Phi^{-1}[C])\\). It follows that \\([(A^s)^t] = [A]\\) and \\([(C^t)^s]=[C]\\) and therefore that conditions iii and iv are satisfied. ◻\n\nSince the modal logics under consideration are classically based, their Lindenbaum lattices are Boolean algebras, i.e., we can define from \\(\\le\\) operations \\(\\wedge\\),\\(\\vee\\), and \\(\\neg\\) satisfying the usual Boolean axioms. Humberstone’s observation that these logics provide strict intermediaries implies that they are dense. Using \\(X{<}Y\\) to mean \\(X\\le Y\\) and not \\(Y\\le X\\), we have that \\([A]{<}[B]\\) implies that there is some element \\([I]\\) such that \\([A]{<}[I]{<}[B]\\). But a Boolean algebra is dense iff it is atomless. (If the algebra is dense and \\(0{<}X\\) then there is is an element \\(I\\), that precedes \\(X\\), in the sense that \\(0{<}I{<}X\\), so \\(X\\) cannot be an atom. Conversely if the algebra has no atoms, then there is an intermediary \\(I\\) between \\(0\\) and \\(X\\), so if \\(X{<}Y\\), \\(X\\vee (I\\wedge Y)\\) is an intermediary between \\(X\\) and \\(Y\\).) A basic theorem of Boolean algebra states that the theory of atomless Boolean algebras is \\(\\aleph_0\\)-categorical, i.e., that any two countable atomless Boolean algebras are isomorphic.It follows that any two reasonably well-behaved modal logics are weakly translationally equivalent. There is a sense in which adding \\(\\Box\\) or any other non-Boolean connectives to the language of propositional logic and axioms and rules of derivation to the usual rules for classical logic adds nothing to what can be said. This observation contrasts starkly with what happens when translations are required to be compositional. In Pelletier and Urquhart (2003), it is shown that if well-behaved modal logics \\(L_1\\) and \\(L_2\\) are are translationally equivalent, then, for any number \\(n\\), the number of Kripke frames with \\(n\\) worlds validating \\(L_1\\) is the same as the number validating \\(L_2\\). It follows if two logics have the finite frame property (as all the most familiar modal logics do) and one is a sublogic of the other, they cannot be translationally equivalent. The observation here demonstrates the importance for the Pelletier/Urquhart result of the requirement that the translations be compositional.\nWe do know that the translations between classically based modal logics conform to the Boolean connectives. This allows us to sharpen the result slightly in the direction of Pelletier/Urquhart.\n\nTheorem 4. Suppose \\(L_1\\) and \\(L_2\\) are classically based modal logics. Then \\(L_1\\) and \\(L_2\\) are weakly translationally equivalent by way of translations \\(s^*\\) and \\(t^*\\) that preserve the Boolean connectives.\n\n\nProof. Since the logics are classically based they have isomorphic Lindenbaum lattices. By the previous theorem they are weakly translationally equivalent. Let \\(s\\) and \\(t\\) be the translations securing this similarity. We define \\(s^*\\) and \\(t^*\\) by cases:\n\\(s^*(A)=A^s\\) if \\(A\\) is a sentence letter or \\(A = {\\#}A_1{\\ldots}A_n\\) for \\({\\#}\\) an \\(n\\)-ary non-Boolean connective\n\\(s^{*}(\\bot)=\\bot\\)\n\\(s^{*}(\\neg A)= \\neg A^{s^{*}}\\)\n\\(s^{*}(A{\\#}B) = A^{s^{*}}\\!{\\#} B^{s^{*}}\\) if \\({\\#}\\) is \\(\\wedge ,\\vee ,\\rightarrow\\) or \\(\\leftrightarrow\\).\nThe clauses for \\(t^*\\) are similar.\nInduction using sentence letters and formulas \\({\\#}A_1{\\ldots}A_n\\) for \\({\\#}\\) non-Boolean as a base and appeal to the conformity property establishes that \\(s^*\\!(A){\\dashv}{\\vdash}A^s\\) and \\(t^*(C){\\dashv}{\\vdash}C^t\\). It follows that \\(s^*\\) and \\(t^*\\), which preserve the Boolean connectives, also satisfy the conditions for weak translational equivalence. ◻\n\nNote, however, that the result of Pelletier and Urquhart (2003) ensures that \\(s^*\\) and \\(t^*\\) are not in general compositional. So one should not presume, for example, that the \\(s^*\\)-translation of \\(\\Box (p\\wedge q)\\) is any function of the \\(s^*\\) translations of \\(p\\) and \\(q\\).\nS4 \\(\\oplus\\) 5\\(^\\prime\\) = S4\\(\\oplus\\)F\nConsider the following two axioms:\n5\\(^\\prime\\): \\((p \\wedge \\neg \\Box p \\wedge \\Box (p \\vee \\Box (p \\rightarrow \\Box p))) \\rightarrow \\Box \\neg \\Box p\\)\nF: \\((p\\wedge \\Diamond\\Box q) \\rightarrow \\Box (\\Diamond p\\vee q)\\)\nThese emerge in Humberstone’s survey (402-420) of the logical terrain between S4 and S5 for plausible epistemic logics. F figures prominently in Stalnaker (2006) and 5\\(^\\prime\\) in Voorbraak (1991). Humberstone (410) asks whether it is possible to derive F from S4 and 5\\(^\\prime\\). The point of this section is to argue that it is. We’ll also show something that is already clear in Humberstone’s text, which is that 5\\(^\\prime\\) can be proven in S4F, so S4F = S45\\(^\\prime\\). Humberstone in fact shows something considerably stronger, namely that S4F is complete with respect to the class of transitive, reflexive, semi-Euclidean frames, and 5\\(^\\prime\\) is sound with respect to the class of those frames. (The semi-Euclidean frames are those which satisfy \\(\\forall xyz((xRy \\wedge xRz) \\rightarrow (yRz \\vee zRx))\\). The term semi-Euclidean is taken from Voorbraak (1991).) From these results it follows there must be some proof of 5\\(^\\prime\\) in S4F. But the status of F in S45\\(^\\prime\\) was an open question.\nIt will be convenient to label three additional formulas that appear in the course of our derivation of F:\n5\\(^{\\prime\\prime}\\!: (p \\wedge \\Diamond\\neg p \\wedge \\Diamond\\Box p) \\rightarrow \\Diamond(\\neg p \\wedge \\Diamond(p\\wedge \\neg \\Box p))\\)\nA: \\((\\Diamond{p}\\vee \\Box q) \\wedge \\Diamond\\neg (\\Diamond{p}\\vee \\Box q) \\wedge \\Diamond\\Box (\\Diamond{p}\\vee \\Box q)\\)\nB: \\(\\neg (\\Diamond{p}\\vee \\Box q) \\wedge \\Diamond((\\Diamond{p}\\vee \\Box q) \\wedge \\neg \\Box (\\Diamond{p}\\vee \\Box q))\\)\nWe will show the following:\n\nTheorem 5.\n\n\n5\\(^{\\prime}~_{\\text{K}}\\!{\\dashv}{\\vdash}_\\text{K}~\\textbf{5}^{\\prime\\prime}\\)\n\\(\\vdash_\\text{KT4} ~ \\neg \\textbf{F}\\rightarrow \\textbf{A}\\)\n\\(\\vdash_{\\text{5}^{\\prime\\prime}} ~\\textbf{A}\\rightarrow \\Diamond\\textbf{B}\\)\n\\(\\vdash_\\text{S4} ~\\neg \\Diamond\\textbf{B}\\)\n\ni allows us to work within S45\\(^{\\prime\\prime}\\) rather than S45\\(^\\prime\\), and ii, iii, iv constitute a reductio proof of F within that system. It should be noted that Humberstone uses 5\\(^\\prime\\) as a label for the schema corresponding to the axiom given here. We work within a natural deduction system that allows us to use a rule of truth-functional consequence under assumptions, and to apply rules of necessitation and uniform substitution to formulas that are not under any assumptions.\n\nProof. To prove i we note the following chain of K-equivalent formulas:\n\\((p \\wedge \\neg \\Box p \\wedge \\Box (p \\vee \\Box (p \\rightarrow \\Box p))) \\rightarrow \\Box \\neg \\Box p~~~\\)(=5\\(^\\prime\\))\n\\((p \\wedge \\neg \\Box p \\wedge \\neg \\Box \\neg \\Box p) \\rightarrow \\neg \\Box (p \\vee \\Box (p \\rightarrow \\Box p))\\)\n\\((p \\wedge \\Diamond\\neg p \\wedge \\Diamond\\Box p) \\rightarrow \\Diamond\\neg (p \\vee \\Box (p \\rightarrow \\Box p))\\)\n\\((p \\wedge \\Diamond\\neg p \\wedge \\Diamond\\Box p) \\rightarrow \\Diamond(\\neg p \\wedge \\neg \\Box (p \\rightarrow \\Box p))\\)\n\\((p \\wedge \\Diamond\\neg p \\wedge \\Diamond\\Box p) \\rightarrow \\Diamond(\\neg p \\wedge \\Diamond(p \\wedge \\neg \\Box p))~~~\\)(=5\\(^{\\prime\\prime}\\))\nA derivation sketch establishing ii is given below. We make free use of K and truth functional logic, but we note steps that use T or 4.\n\\(\\neg ((p\\wedge \\Diamond\\Box q) \\rightarrow \\Box (\\Diamond{p}\\vee q))\\)(Assumption \\(\\neg\\)F)\n\\(p\\) (from 1)\n\\(\\Diamond{p}\\vee \\Box q\\)(from 2 using T)\n\\(\\Diamond(\\neg \\Diamond{p}\\wedge \\neg q)\\) (from 1)\n\\(\\Diamond(\\neg \\Diamond{p}\\wedge \\neg \\Box q)\\) (from 4 using T)\n\\(\\Diamond\\neg (\\Diamond{p}\\vee \\Box q)\\)(from 5)\n\\(\\Diamond\\Box q\\)(from 1)\n\\(\\Diamond\\Box \\Box q\\)(from 7 using 4)\n\\(\\Diamond\\Box (\\Diamond{p}\\vee \\Box q)\\) (from 8)\n\\((\\Diamond{p}\\vee \\Box q) \\wedge \\Diamond\\neg (\\Diamond{p}\\vee \\Box q)\\wedge \\Diamond\\Box (\\Diamond{p}\\vee \\Box q)\\)(from 3,6,9)\n\\(\\neg \\textbf{F} \\rightarrow \\textbf{A}\\)(from 1-10)\nFor iii note that a substitution of \\(\\Diamond{p}\\vee \\Box q\\) for \\(p\\) in 5\\(^{\\prime\\prime}\\) results in the formula A\\(\\rightarrow \\Diamond\\textbf{B}\\). Finally, we establish iv by the derivation sketch below.\n\\(\\neg (\\Diamond{p}\\vee \\Box q) \\wedge \\Diamond((\\Diamond{p}\\vee \\Box q) \\wedge \\neg \\Box (\\Diamond{p}\\vee \\Box q))\\)\n(Assumption B)\n\\(\\neg (\\Diamond{p}\\vee \\Box q)\\) (from 1)\n\\(\\Box \\neg p\\) (from 2)\n\\(\\Box \\Box \\neg p\\) (from 3 using 4)\n\\(\\Diamond((\\Diamond{p}\\vee \\Box q) \\wedge \\neg \\Box (\\Diamond{p}\\vee \\Box q))\\) (from 1)\n\\(\\Diamond(\\Box \\neg p \\wedge (\\Diamond{p}\\vee \\Box q) \\wedge \\neg \\Box (\\Diamond{p}\\vee \\Box q))\\) (from 4,5)\n\\(\\Diamond(\\Box \\neg p \\wedge \\Box q \\wedge \\neg \\Box (\\Diamond{p}\\vee \\Box q))\\) (from 6)\n\\(\\Diamond(\\Box \\neg p \\wedge \\Box \\Box q \\wedge \\neg \\Box (\\Diamond{p}\\vee \\Box q))\\) (from 7 using 4)\n\\(\\Diamond(\\Box \\neg p \\wedge \\Box \\Box q \\wedge \\neg \\Box \\Box q)\\) (from 8)\n\\(\\neg\\)B (from 1-9 by reductio)\n\\(\\Box \\neg\\)B (from 10 by necessitation)\n\\(\\neg \\Diamond\\)B (from 11)\n ◻\n\nAs we mentioned above, Humberstone shows that there must be a proof of 5\\(^\\prime\\) in S4F. For the sake of symmetry, we sketch that proof. As it turns out, only KF is required, which we could not have known from Humberstone’s completeness result.\n\\((p\\wedge \\Diamond\\Box q) \\rightarrow \\Box (\\Diamond{p}\\vee q)\\)(F)\n\\((\\neg (p\\rightarrow \\Box p) \\wedge \\Diamond\\Box p) \\rightarrow \\Box (\\Diamond\\neg (p\\rightarrow \\Box p)\\vee p)\\)(from 1 by substitution)\n\\(\\neg ((p \\wedge \\neg \\Box p \\wedge \\Box (p \\vee \\Box (p \\rightarrow \\Box p))) \\rightarrow \\Box \\neg \\Box p)\\)(Assumption \\(\\neg \\textbf{5}^\\prime)\\)\n\\(p \\wedge \\neg \\Box p \\wedge \\Box (p \\vee \\Box (p \\rightarrow \\Box p)) \\wedge \\Diamond\\Box p\\)(from 3)\n\\(\\neg (p\\rightarrow \\Box p) \\wedge \\Diamond\\Box p\\)(from 4)\n\\(\\Box (\\Diamond\\neg (p\\rightarrow \\Box p)\\vee p)\\)(from 2,5)\n\\(\\Diamond\\neg p\\)(from 4)\n\\(\\Diamond(\\neg p \\wedge \\Diamond\\neg (p\\rightarrow \\Box p)\\vee p)\\)(from 6,7)\n\\(\\Diamond(\\neg p \\wedge \\Diamond\\neg (p\\rightarrow \\Box p))\\)(from 8)\n\\(\\Box (p \\vee \\Box (p \\rightarrow \\Box p))\\)(from 4)\n\\(\\Diamond(\\neg (p\\vee \\Box (p\\rightarrow \\Box p)) \\wedge (p\\vee \\Box (p\\wedge \\Box p))\\)(from 9, 10)\n\\(\\textbf{5}^\\prime\\)(from 3-11 by reductio)\n\\(\\blacksquare\\)\nAin’t Necessarily So\nHumberstone’s “logic of coming about” (452-469) adds to the language of classical sentential logic a modal operator \\(\\mathbf{D}\\). \\(\\mathbf{D}A\\) is to be read as it comes about that \\(A\\) and understood as being something like Nuel Belnap’s a sees to it that \\(A\\), except that it abstracts from the idea of agency. Models are triples \\(\\langle U,f,V\\rangle\\) where \\(U\\) and \\(V\\) are sets and valuations of the kind familiar from modal logic and \\(f\\) is a unary function from \\(U\\) to \\(U\\). The truth definition has the usual clauses for the classical connectives and the additional clause:\n\\(\\langle U,f,V\\rangle \\models _x \\mathbf{D}A\\) iff \\(\\langle U,f,V\\!\\rangle \\models _x A\\) and not \\(\\langle U,f,V\\!\\rangle \\models _{f(x)} A\\).\nTruth in the model is truth at all \\(u{\\in}U\\), and validity is is truth in all models. Among the valid schemas and validity-preserving rules that he draws attention to are the following:\nD0: Substitution instances of tautologies\nD1: \\(\\mathbf{D}A\\rightarrow A\\)\nD2: \\((A\\wedge\\mathbf{D}B) \\rightarrow \\mathbf{D}(A\\wedge B)\\)\nD3: \\(\\mathbf{D}(A\\wedge B) \\rightarrow (\\mathbf{D}A\\vee \\mathbf{D}B)\\)\nMP: \\(A\\rightarrow B, A ~~/~~ B\\)\nRD\\(_{m,n}\\): \\((B_1 \\wedge {\\ldots}\\wedge B_m) \\rightarrow (A_1\\vee {\\ldots}\\vee A_n) ~~/ \\\\\\)\\((B_1 \\wedge {\\ldots}\\wedge B_m) \\rightarrow ((\\mathbf{D}A_1\\wedge {\\ldots}\\wedge \\mathbf{D}A_n) \\rightarrow (\\mathbf{D}B_1 \\vee {\\ldots}\\vee \\mathbf{D}B_m))\\)\nThe last rule schema is intended to include the cases \\(m=0\\) and \\(n=0\\) with the usual stipulation that an empty conjunction is \\(\\top\\) and an empty disjunction is \\(\\bot\\). The reader is asked to show that D2 and D3 are provable from the remaining schemas as an exercise and the valid formulas are then shown to be axiomatized by D0, D1, MP and all the RD rules. In this section we consider two additional schemas.\nD4: \\((\\mathbf{D}A\\wedge \\mathbf{D}B) \\rightarrow \\mathbf{D}(A\\vee B)\\)\nD5:\\(\\neg \\mathbf{D}\\top\\)\nWe show that the valid formulas of coming-about logic are axiomatized by D0, D1, D3, D4, D5 MP and RD\\(_{1,1}\\). Since we can always add a rule of substitution while replacing the schematic variables in D1, D3, D4 by sentence letters and replacing D0 by a finite set of axioms for sentential logic, this shows that Humberstone’s infinite axiomatization can be replaced by a simple finite one.\nD4 plays a special role among the axioms and rules considered. Suppose \\(\\mathbf{D}{A}\\) is interpreted as it is contingently true that \\(A\\) (or, as the section head suggests, that \\(A\\), while true, is not necessarily so). More precisely, replace the function \\(f\\) in Humberstone’s models by an accessibility of the usual kind and his truth clause for \\(\\mathbf{D}\\) by the following:\nCT: \\(\\langle U,R,V\\!\\rangle \\models _x \\mathbf{D}A\\) iff \\(\\langle U,R,V\\!\\rangle \\models _x A\\) and, for some \\(y\\) such that \\(xRy\\), not \\(\\langle U,R,V\\!\\rangle \\models _y A\\)\nIt is easy to check that D0, D1, D2, D3 and D5 all remain valid and MP and RD\\(_{1,1}\\) still preserve validity. But when \\(R\\) is not a function then D4 can be falsified: Let \\(U=\\{w,u,v\\}, R=\\{(w,u),(w,v)\\}, V(p)=\\{w,u\\}\\) and \\(V(q)=\\{w,v\\}\\). So D4 is independent of the five axioms and two rules just given. In fact, as we shall show, these axioms and rules are sufficient to axiomatize the contingently true operator under the interpretation CT.\nFirst, however, we turn to the connection between the formula schemas and the rule schema RD\\(_{m,n}\\) and the proof that the logic of coming about is finitely axiomatizable. Note that any logic containing D0 and closed under MP is closed under truth-functional consequence (TFC). This facilitates the proof of the following:\n\nClaim 1. In the presence of D0, D1 and MP: D2 is provable from RD\\(_{1,1}\\), D3 is provable from RD\\(_{2,1}\\), D4 is provable from RD\\(_{1,2}\\) and D5 is provable from RD\\(_{0,1}\\)\n\n\nProof. The required derivations are sketched below.\n\\((A\\wedge B)\\rightarrow B\\) by D0\n\\((A\\wedge B)\\rightarrow (\\mathbf{D}B\\rightarrow \\mathbf{D}(A\\wedge B))\\)from 1 by RD\\(_{1,1}\\)\n\\((A\\wedge B\\wedge \\mathbf{D}B)\\rightarrow \\mathbf{D}(A\\wedge B))\\)from 2 by TFC\n\\((A\\wedge \\mathbf{D}B)\\rightarrow \\mathbf{D}(A\\wedge B))\\)from 3 and D1 by TFC\n\\((A\\wedge B)\\rightarrow (A\\wedge B)\\) by D0\n\\((A\\wedge B)\\rightarrow (\\mathbf{D}(A\\wedge B)\\rightarrow (\\mathbf{D}A\\vee \\mathbf{D}B))\\)from 1 by RD\\(_{2,1}\\)\n\\(\\mathbf{D}(A\\wedge B)\\rightarrow (\\mathbf{D}A\\vee \\mathbf{D}B)\\)from 2 and D1 by TFC\n\\((A\\wedge B)\\rightarrow (A\\vee B)\\) by D0\n\\((A\\wedge B)\\rightarrow ((\\mathbf{D}A\\wedge \\mathbf{D}B)\\rightarrow \\mathbf{D}(A\\vee B))\\) from 1 by RD\\(_{1,2}\\)\n\\(\\mathbf{D}(A\\wedge B)\\rightarrow \\mathbf{D}(A\\vee B)\\)from 2 and D1 by TFC\n\\(\\top \\rightarrow \\top\\)by D0\n\\(\\top \\rightarrow (D\\top \\rightarrow \\bot)\\)from 1 by RD\\(_{0,1}\\)\n\\(\\neg D\\top\\)from 2 by TFC\n ◻\n\nSince Humberstone has already shown that D0 and D1 are valid and that modus ponens and every instance of RD\\(_{m,n}\\) preserves validity, the claim is sufficient to show that our new axiom system is sound. To prove sufficiency, it is sufficient to show that, for all \\(m,n\\!\\ge\\!0\\), RD\\(_{m,n}\\) is derivable in the new system. To this end, notice first that D3 and D4 generalize, i.e., if \\(\\vdash\\) indicates provability in the new axiom system then:\nD3*: For all \\(n{\\ge}1, \\vdash \\mathbf{D}(A_1\\wedge {\\ldots}\\wedge A_n) \\rightarrow (\\mathbf{D}A_1\\vee {\\ldots}\\vee \\mathbf{D}A_n)\\), and\nD4*: For all \\(n{\\ge}1, \\vdash (\\mathbf{D}A_1\\wedge {\\ldots}\\wedge \\mathbf{D}A_n) \\rightarrow \\mathbf{D}(A_1\\vee {\\ldots}\\vee A_n)\\).\nFor these claims to be sensible without grouping conjuncts and disjuncts our logic must allow replacement of truth-functional equivalents. Humberstone’s proof (453) of the claim that his logic satisfies the stronger property of being “congruential,” i.e., closed under replacement of provable equivalents uses only D0 and RD\\(_{1,1}\\) so we can help ourselves to this result. The claims can then be proved by induction on \\(n\\). In each case the basis case follows from D0. The inductive step for D3* uses D3 and that for D4* uses D4.\nThis allows us to show that RD\\(_{m,n}\\) is derivable for all positive \\(m\\) and \\(n\\): Suppose \\(\\vdash (B_1 \\wedge {\\ldots}\\wedge B_m) \\rightarrow (A_1\\vee {\\ldots}\\vee A_n)\\). Then by RD\\(_{1,1}\\),\\(\\vdash (B_1 \\wedge {\\ldots}\\wedge B_m) \\rightarrow (\\mathbf{D}(A_1\\vee {\\ldots}\\vee A_n) \\rightarrow \\mathbf{D}(B_1 \\wedge {\\ldots}\\wedge B_m))\\). By D3* and TFC, \\(\\vdash (B_1 \\wedge {\\ldots}\\wedge B_m) \\rightarrow (\\mathbf{D}(A_1\\vee {\\ldots}\\vee An) \\rightarrow (\\mathbf{D}B_1 \\vee {\\ldots}\\vee \\mathbf{D}B_m))\\). By D4* and TFC, \\(\\vdash (B_1 \\wedge {\\ldots}\\wedge B_m) \\rightarrow ((\\mathbf{D}A_1\\wedge {\\ldots}\\wedge \\mathbf{D}An) \\rightarrow (\\mathbf{D}B_1 \\vee {\\ldots}\\vee \\mathbf{D}B_m))\\).\nIt remains only to check the cases \\(m{=}0\\) and \\(n{=}0\\). But for all \\(m\\), RD\\(_{m,0}\\) is a consequence of TFC: if \\((B_1\\wedge {\\ldots}\\wedge B_m)\\rightarrow \\bot\\) is provable then so is any formula with \\(B_1\\wedge {\\ldots}\\wedge B_m\\) as antecedent, including the consequence of RD\\(_{m,0}\\).\nFor the case \\(m=0\\) we will need D4* and D5. Suppose \\(\\vdash \\top \\rightarrow (A_1\\vee {\\ldots}\\vee A_n)\\). Then \\((A_1\\vee {\\ldots}\\vee A_n)\\) is provably equivalent to \\(\\top\\). Since \\(\\vdash \\neg D\\top\\) and our logic is congruential it follows that \\(\\vdash \\neg \\mathbf{D}(A_1\\vee {\\ldots}\\vee A_n)\\). By D4, \\(\\vdash \\neg (\\mathbf{D}A_1\\wedge {\\ldots}\\wedge \\mathbf{D}A_n)\\). By TFC, \\(\\vdash \\top \\rightarrow ( (\\mathbf{D}A_1\\wedge {\\ldots}\\wedge \\mathbf{D}An) \\rightarrow \\bot)\\), and so RD\\(_{0,n}\\) is derivable.\\(\\blacksquare\\)\nOur proof that Humberstone’s logic of coming about has a simple, finite axiomatization is complete and so we turn our attention to the axiomatization of the logic of contingent truth.\n\nTheorem 6. The axioms D0, D1, D3, D4, D5 and rules MP and RD\\(_{1,1}\\) provide a complete axiomatization of the logic of contingently true* under the interpretation CT.*\n\n\nProof. Soundness was observed above so it is sufficient to prove sufficiency. This can be done by constructing a canonical model out of maximally consistent sets in a familiar way. Let \\(M^c=(W^c\\!, R^c\\!,V^c)\\), where \\(W^c\\) is the set of all maximally consistent formulas, \\(V^c(p)=\\{w{\\in}W^c\\!: p{\\in}w\\}\\), and \\(xR^cy\\) iff \\(A{\\in}y\\) whenever both \\(A{\\in}x\\) and \\(\\mathbf{D}A{\\notin}x\\).\n\nLemma 2 (Witness lemma). If \\(\\mathbf{D}A{\\in}x\\) then \\(\\exists y (xR^cy\\) and \\(A{\\in}y)\\).\n\n\nProof. Suppose \\(\\mathbf{D}A {\\in}x\\) and let \\(y^- = \\{B{:}~ B{\\in}x\\) and \\(\\mathbf{D}B{\\in}x\\}\\cup\\{\\neg A\\}\\). Then \\(y^-\\) is consistent. For otherwise either \\(\\vdash A\\) or there are formulas \\(B_1,{\\ldots},B_n\\) such that for \\(1\\le i\\le n, B_i{\\in}x\\) and \\(\\mathbf{D}B_i{\\in}x\\) and \\(\\vdash B_1\\wedge {\\ldots}\\wedge B_n\\rightarrow A\\). In the first case, by D0, \\(\\vdash A{\\leftrightarrow}\\top\\). Since the logic is congruential, \\(\\vdash \\mathbf{D}\\top\\), violating D5. So we may assume that the second case obtains. By RD\\(_{1,1}\\), \\(\\vdash B_1\\wedge {\\ldots}\\wedge B_n\\rightarrow (\\mathbf{D}A\\rightarrow \\mathbf{D}(B_1\\wedge {\\ldots}\\wedge B_n))\\). Each \\(B_i\\) is a member of \\(x\\) by construction and \\(\\mathbf{D}A\\) is a member of \\(x\\) by supposition, so it follows that \\(\\mathbf{D}(B_1\\wedge {\\ldots}\\wedge B_n){\\in}x\\). By D3* this implies \\(\\mathbf{D}B_1\\vee {\\ldots}\\vee \\mathbf{D}B_n \\in x\\). But by construction none of the formulas \\(\\mathbf{D}B_i\\) is a member of \\(x\\), so we have reached a contradiction. Thus \\(y^-\\) is consistent as claimed. By Lindenbaum’s lemma, it can be extended to a maximal consistent set y satisfying the conditions of the lemma. ◻\n\n\nLemma 3 (Truth Lemma). In the canonical model for our logic, \\(\\models _x A\\) iff \\(A{\\in}x\\).\n\n\nProof. By induction on \\(A\\). We consider the case \\(A=\\mathbf{D}B\\). First suppose \\(\\models _x A\\). By the truth definition, \\(\\models _x B\\) and \\(\\exists y (xR^cy\\) and \\({\\nvDash_y}B)\\). By induction hypothesis, \\(B{\\in}x\\) and \\(\\exists y (xR^cy\\) and \\(B{\\notin}y)\\). By the definition of \\(R^c\\), \\(\\mathbf{D}B{\\in}x\\) as required.\nFor the converse, suppose \\(A{\\in}x\\). By the witness lemma, \\(\\exists y (xR^cy\\) and \\(B{\\in}y)\\). By induction hypothesis, \\(\\exists y( xR^cy\\) and \\({\\models_y}B)\\). Furthermore, since \\(A{\\in}x\\), D1 implies that \\(B{\\in}x\\), and,by induction hypothesis, this implies that \\(\\models _x B\\). So, by the truth definition, \\(\\models _x A\\), as required. ◻\n\nTo prove the theorem note that, by Lindenbaum’s lemma, any consistent set in the logic described can be extended to a maximal consistent set, which will be one of the worlds in the canonical model. By the truth lemma, all the members of the set will be true at that world. ◻\n\n\n\nFrench, Rohan. 2010. “Translational Embeddings in Modal Logic.” PhD thesis, Department of Philosophy, Monash University. https://doi.org/10.4225/03/587c09ed83982.\n\n\nHumberstone, Lloyd. 2011. The Connectives. Cambridge, MA: MIT Press.\n\n\n———. 2016. Philsophical Applications of Modal Logic. Milton Keynes: College Publications.\n\n\nKuhn, Steven. 1978. Many-Sorted Modal Logics. Uppsala: Department of Philosophy, Uppsala University.\n\n\nPelletier, Francis Jeffry, and Alasdair Urquhart. 2003. “Synonymous Logics.” Journal of Philosophical Logic 32 (3): 259–85. https://doi.org/10.1023/a:1024248828122.\n\n\nPrior, Arthur N. 1951. “The Ethical Copula.” Australasian Journal of Philosophy 29 (3): 137–54. https://doi.org/10.1080/00048405185200171.\n\n\nStalnaker, Robert. 2006. “On Logics of Knowledge and Belief.” Philosophical Studies 128 (1). https://doi.org/10.1007/s11098-005-4062-y.\n\n\nVoorbraak, Frans. 1991. “The Logic of Objective Knowledge and Rational Belief.” In Logics in AI, edited by J. van Eijck, 499–515. Berlin, Heidelberg: Springer, Berlin.\n\n\nZolin, Evgeni E. 2000. “Embeddings of Propositional Monomodal Logics.” Logic Journal of IGPL 8 (6): 861–82. https://doi.org/10.1093/jigpal/8.6.861.\n\n\nIn the terminology of Humberstone (2011) (page 62), these logics are \\(\\#\\)-classical for every Boolean connective \\(\\#\\). Humberstone (2011) spells out necessary and sufficient conditions that are omitted here. The term classical which might have been preferred over classically based is avoided because classical modal logic is sometimes used for other purposes.↩︎\nA paradigm case is the relation between classical propositional logic formulated with \\(\\neg\\) and \\(\\wedge\\) and that formulated with the Sheffer stroke. The notion has been defined in a number of ways, which are nicely surveyed in chapter 5 of French (2010). The definitions that follow are close to those in Kuhn (1978). Other definitions may diverge when certain Boolean connectives are absent or fail to behave classically, but our emphasis here is on logics for which they coincide.\n\n↩︎\n",
    "preview": "posts/2021-03-12-notes-on-some-ideas-in-lloyd-humberstones-philosophical-applications-of-modal-logic/monash_lib.jpg",
    "last_modified": "2021-03-16T22:08:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-vagueness-as-indeterminacy/",
    "title": "Vagueness as Indeterminacy",
    "description": "Traditionally, we thought vague predicates were predicates with borderline cases. In recent years traditional wisdom has come under attack from several leading theorists. They are motivated by a common idea, that terms with borderline cases, but sharp boundaries around the borderline cases, are not vague. I argue for a return to tradition. Part of the argument is that the alternatives that have been proposed are themselves subject to intuitive counterexample. And part of the argument is that we need a theory of what vagueness is that applies to non-predicates. The traditional picture can be smoothly generalised to non-predicates if we identify vagueness generally with indeterminacy. Modern rivals to tradition do not admit of such smooth generalisation.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2018-04-14",
    "categories": [
      "logic",
      "language",
      "vagueness"
    ],
    "contents": "\n\nContents\nExamples\nIndeterminacy without Vagueness\nNon-Predicate Vagueness\nPhilosophically Interesting Vague Terms\nDiscrete Vague Terms\nVagueness without Boundaries\n\nVagueness as Indeterminacy\nRival Definitions\n\nRecently there has been a flurry of proposals on how to ‘define’ vagueness. These proposals are not meant to amount to theories of vagueness as, for instance, epistemic or supervaluational theories of vagueness are. That is, they are not meant to provide solutions to the raft of puzzles and paradoxes traditionally associated with vagueness. Rather, they are meant to give us a sense of which terms in the language are vague, and to use Matti Eklund’s phrase, in what their vagueness consists. Doing this might be a prelude to a successful theory of vagueness, or it might just be an interesting classificatory question in its own right.\nPublished in Cuts and Clouds: Vagueness, its Nature, & its Logic, edited by Richard Dietz and Sebastiano Moruzzi, Oxford, 77-90.\nPicture via Metropolitan Museum of Art.\nWhen this activity started, most notably with Patrick Greenough’s “A Minimal Theory of Vagueness,” I suspected that it would be a hopeless project. Imagine, I thought, trying to give a definition of what causation is that didn’t amount to a theory of causation. That project seems hopeless, and I didn’t think the prospects for a definition of vagueness were much better. I now think I was wrong, and we can learn a lot from thinking about which terms are vague, independent of our theory of vagueness. (As we’ll get to below, Greenough’s theory isn’t marketed as a definition of vagueness, but rather a ‘minimal theory’ to which all parties can agree. But it has been taken, e.g. by Eklund and Nicholas Smith, to be providing a rival to genuine definitions of vagueness, and I’ll follow Eklund and Smith in this respect.)\nThe point of this exercise is not to give an analysis of how the man on the Clapham omnibus uses ‘vague’ and its cognates. As is widely recognised, ‘vague’ is often used in ordinary language as a predicate that applies to claims like The Grand Canyon is between 2 and 2 trillion years old, i.e. claims that are consistent with a wide range of possible worlds. That’s not the sense of ‘vague’ which philosophers use, nor the sense we are trying to define. But nor should we think we are just trying to analyse philosophical use of ‘vague.’ The philosophers’ usage may be our starting point, but if we find philosophers have traditionally being ignoring theoretically important commonalities, or blurring theoretically important distinction, our best definition may well amount to a revision of philosophical usage.\nThe game, I think, is one of setting goals for what a theory of vagueness should do. It is a legitimate objection to a theory of vagueness that it isn’t comprehensive, that it doesn’t cover the field. If supervaluationism was only a theory of how vague words that started with consonants behaved, for example, that would be a problem for supervaluationism. But to press objections of this form we must have an antecedent answer to the question of which words are in the field, and hence should be covered. That’s the good question which these definitions of vagueness address. Because I take this to be the important issue, I’m going to start this paper with a bunch of examples of apparently vague, and apparently non-vague, terms. We’ll then look at which theories do the best job at systematising intuitions about these cases. I’ll then argue that the best way to systematise our intuitions about these cases while respecting theoretically important commonalities and distinctions is to take vagueness to be indeterminacy, while staying silent for now on whether the indeterminacy is semantic or epistemic. In doing so I’m returning to a traditional view of vagueness, one that is discussed in such classic works as Kit Fine’s statement of supervaluationism (Fine 1975). So I make no claim to originality in my conclusions here, though I hope at least some of the arguments are original.\nExamples\nI’m going to introduce five classes of examples, which will serve as our data in what follows. I’ll give a fairly tendentious description of each class to orient us before starting. Our five classes are (a) words that are indeterminate but not vague, (b) vague words that are not predicates, especially predicate modifiers, (c) vague predicates whose conditions of application are contentious, (d) vague predicates whose application depends on discrete states of the world, and (e) vague predicates that do not determine boundaries.\nIndeterminacy without Vagueness\nMany philosophers, if asked, would say that vague words are those that have borderline cases. As noted above, Fine (1975) takes exactly this view. My preferred view, that vagueness is indeterminacy, is a simple generalisation of this view to non-predicates. But it is a commonplace of the literature on definitions of vagueness that this won’t do because of examples of indeterminacy without vagueness. Two examples are commonly used. One of these is Sainsbury’s example child (Sainsbury 1991). By definition, the extension of child is the set of persons under sixteen years old, and its anti-extension is the set of persons eighteen years old or older. Sixteen and seventeen year olds are borderline cases. The intuition is that even though child has borderline cases it is not vague, because there are sharp boundaries to its borderline.\nA similar case arrives with mass as it is used by a Newtonian physicist. (I’m grateful to Delia Graff Fara for pointing out the connection here.) As Field (1973) showed, mass is indeterminate between two meanings, rest mass and proper mass. But it is intuitively not vague, because it is determinate that it means either rest mass or proper mass. These cases are well discussed in the existing literature, and I won’t say much more about them here, save to note that one of the examples that is usually taken to be very problematic for the vagueness as indeterminacy view, child, is not synonymous with any term in any natural language. This is not a reason that it could not serve as a counterexample, because a definition should cover all terms actual and possible.\nNon-Predicate Vagueness\nNot only predicates are vague. There is an extensive literature on vague singular terms. Arguably many determiners are vague. And, as I’ll stress here, many predicate modifiers are vague.\nWe can make an intuitive distinction between vague and precise predicate modifiers. Compare the following two (obviously artificial) predicate modifiers. (I owe these examples to David Chalmers.) Where F is a predicate such that Fa is true iff for some variable v, v(a) > x and v has a natural zero value (e.g. like height and unlike utility) then we can define doubly F and bigly F. It is true that a is doubly F iff v(a) > 2x and a is bigly F iff v(a)/x is big. Now there’s a good sense in which doubly is a precise modifier, for the modification it makes to its attached predicate can be precisely defined, while bigly is a vague modifier. That’s the sense in which I mean some modifiers are vague and others are precise. Note that even though doubly is precise it can be a constituent of a vague predicate, such as doubly tall. That makes sense; just as a vague sentence need only contain one vague word, so need a vague complex predicate need only contain one vague word.\nNow we might well ask whether natural language modifiers like very are vague or precise. I’m sad to say that I really don’t have an answer to that question, but I think it’s an excellent question. To get a sense of how hard it is, note one awkward feature of very – it is most comfortable attaching to words that are themselves vague. For instance (1a) is a sentence of English while (1b) is not.\nI don’t know whether this is a universal feature of very. My best guess is that it is though in conversation some people have proposed interesting putative counterexamples. (I’m grateful here to Daniel Nolan.) But to avoid that complication, I’ll introduce a new word very. This modifier is defined such that if F is vague then very* F means the same thing as very F, and if F is not vague then very* F is meaningless, like very forty-seven years old. It’s an excellent question whether very is vague, and I think it’s a requirement on a definition of vagueness that it allow this question to be asked. As we’ll see, this is sadly not true of most proposed definitions of vagueness on the market.\nPhilosophically Interesting Vague Terms\nIt’s morally obligatory that someone with my standard of living donate 1% of their income to charity. It’s not morally obligatory that someone with my standard of living donate 100% of their income to charity. What is the largest x such that it’s morally obligatory that someone with my standard of living donate x% of their income to charity? (As a moralistic cheapskate I’d rather like to know.) Arguably this is vague. But perhaps only arguably. On some divine command theories it is precise, because there’s a fact about what God wants me to do, however hard this is to figure out. (It’s even a knowable fact, since God knows it.) But on more standard secular moral theories this may indeed be vague.\nThere are two lessons to draw from this case. First, if two philosophers can debate what the correct theory of morality is while one thinks it is vague and the other thinks is precise, as I think could happen in a dispute between a divine command theorist and a virtue ethicist, then knowing that a vague term is vague is not required for understanding the term. (I assume here the divine command theorist is not so confused that she’s not really talking about goodness.)\nSecond, it is important to remember that for some vague terms competent users of the term need not know in virtue of what they apply. Much of the literature on vagueness focuses on words like tall, thin and bald where all competent users know which kinds of underlying facts are relevant to their application. But not all vague terms are like that, as good illustrates. And this phenomena extends beyond the normative, at least narrowly conceived. If you believe Tom Wolfe (2000) then among the youth of America going out with is vague and many do not know exactly in virtue of what it applies. It’s a familiar point in philosophy of mind that competent users can disagree about what kinds of features a thing must have to satisfy is thinking. And we can multiply instances of this by considering any area of philosophy we like.\nDiscrete Vague Terms\nAn academic with one child has few children for an academic. An academic with five children does not have few children for an academic. (I’ll omit the comparison class ‘for an academic’ from now on.) Where is the borderline between those with few children and those not with few children? (I don’t ask out of personal interest this time.) This question, like the question of how much giving is morally obligatory, feels vague. But note that we cannot generate a compelling Sorites paradox using has few children. Let’s see how badly this Sorites argument fails.\nArguably premise e is plausible because as a material conditional it can be seen to be true via the falsity of the antecedent. And at a pinch I can see d as compellingly true for the same reason. But neither b nor c strike me as at all compelling. If someone presents this argument as a Sorites paradox, I simply deny that the paradox-mongerer can know these premises to be true, or that I have a reason to believe they are true. To be sure, I don’t know which premise is false. (If you think you know b to be false replace academics in the example with a more fertile professional group.) But just because I don’t know where the argument fails doesn’t mean it presents any kind of paradox. When I have no reason to accept two, maybe three, of the premises, the argument falls well short of being paradoxical.\nA small note on terminology. Contemporary scientific theories imply that many familiar vague predicates apply in virtue of facts about the world that are, at some level, discrete. What I’m interested in under this heading are predicates where the differences between salient adjacent cases are easily observable, such as the difference between having two and three children.\nVagueness without Boundaries\nThe letter of Patrick Greenough’s proposal (to be discussed in section three below) suggests that every vague term has only vague boundaries. This is not true. The predicate in one’s early thirties has a sharp boundary at the lower end and a precise boundary at the upper end. But it isn’t too hard to amend his theory to allow for such cases, by saying (in effect) that a vague term is a term with at least one vague boundary. Nicholas Smith makes basically that move in his paper. But such a move won’t work, because some vague predicates don’t have boundaries. Indeed, some predicates can be vague even though they are satisfied by every object in the domain. The examples here are a little more complicated than in the rest of the paper, but I think they are important enough to warrant the complexity.\nFor the next several paragraphs the domain will be adult Australian women, and when I use tall I’ll mean tall for an adult Australian woman. I don’t know enough facts to know where the boundaries are for tall in this context, but I’ll stipulate that a woman shorter than 170cm is determinately not tall, and a woman taller than 180cm is determinately tall. I claim here neither that I know where these boundaries are nor that I could know where they are. But I assume there are boundaries. I’m making these stipulations because it is easier to follow the examples if I use 170 and 180 rather than variables like y and z. It will become obvious that the particular numbers won’t matter, as long as there’s separation between them. It also doesn’t matter whether we use a semantic or epistemic account of determinacy here. It will matter that we use classical logic at various points (e.g. in assuming there are boundaries), but I think that’s perfectly reasonable in this context. (Here I follow the arguments in section 2 of Greenough’s paper.)\nConsider the class of predicates defined by the following schema.\n\ntallx =df tall or shorter than x cm\n\nFor x < 170, tallx has all the same borderline cases as tall, and is presumably vague in anyone’s book. For x > 180, tallx determinately applies to everyone in the domain, and for now we’ll say that makes it not vague. (Though note it need not determinately determinately apply to everyone in the domain, and we’ll see below that might be a reason to group it with the vague predicates.)\nWhen x is between 170 and 180, tallx has some very odd properties. The borderline cases are those women whose height is between x and 180cm. When x is close to 180, this might be a very small border. While we’re assuming classical logic, we can assume that there is a value y such that women taller than y cm are tall and those shorter than y cm are not tall. We need not here assume the value of y is either epistemically or semantically determinate. Consider a value of x, say 179, such that x > y. (Again it’s not a necessary assumption that 179 > y, but it makes the example easier to understand if I use a particular number.) Now tall179 has some interesting properties. It has borderline cases, those women between 179 and 180cm tall. But it is satisfied by every woman, since every woman is either tall or shorter than 179cm. I think the existence of the borderline cases is sufficient to make tall179 vague. Note that these cases are quite different to child, because at the upper boundary there is no sharp jump from borderline cases to clear cases – the two blur together in just the way borderline cases and clear cases of tall blur together, so whatever reasons we had to worry about child being vague are not applicable here. Still the ‘borderline cases’ are mislabelled here for there is no border they fall on. Every woman satisfies the predicate. So no definition of vagueness in terms of having a vague boundary, indeed of having a boundary at all, can work.\nOne might object here that a definition of vagueness is only meant to apply to words not phrases. But just as we can worry about a possible word child, we can worry about a possible atomic word gish that means the same thing as tall179, so that move won’t help here.\nWe now have enough data on the table. In the next section I argue that treating vagueness as being indeterminacy provides a satisfactory treatment of the data. In the third section I argue that none of the live alternatives is so satisfactory. So I conclude, somewhat tentatively, that we should define vagueness as indeterminacy.\nVagueness as Indeterminacy\nBack when I was a supervaluationist, I thought that what it was for a term to be vague was for it to refer to different things on different precisifications. That won’t do as a theory-neutral definition, for it presupposes supervaluationism, which is not only a theory but a false theory. But we can capture the essential idea in slightly less loaded language.\nI will have to make three possibly controversial assumptions. First, I assume a broadly Montagovian perspective, on which we can talk about the referent of an arbitrary term. (See Montague (1970, 1973) for more details.) That referent might be an object, or a truth value, or a function from objects to truth values, or a more complicated function built out of these. Second, I assume we can sensibly use an expanded Lagadonian language where objects can be names for themselves, truth-values can be names for themselves, functions from objects to truth-values can be names for themselves, and so on. (See Lewis 1986 for more on Lagadonian languages.) Third, I assume there is no metaphysical vagueness, so each of these Lagadonian names is not vague.\nThose assumptions let us make a first pass at a definition of vagueness, as follows. A term t is vague iff there is some object, truth-value or function l which can serve as its own name such that the following sentence is neither determinately true nor determinately false.\nThat delivers the intuitively correct account in four of the five cases we discuss above, all except the cases like child*. I’ll say much more about that case below. But it is in one respect slightly too liberal, and we need to make a small adjustment or two to fix this. Consider a predicate F that is defined over a vague domain, but which is determinately satisfied by every object in the domain. Intuitively it is a partial function, which maps every member of its domain to true. And assume for sake of argument that it is determinate that it maps every member of the domain to true. (Say, for example, it means is self-identical when applied to a member of the domain.) Such a predicate is not, I think, vague. But since it is indeterminate which partial function it denotes, the above theory suggests it is vague. We need to make a small adjustment. To state the corrected theory, we will stipulate that every term denotes a function. What were previously thought of as terms denoting constants will be treated as terms denoting constant functions. So instead of a name like Scott Soames denoting Scott Soames, we’ll take it to denote the function that takes anything whatsoever as input, and returning Scott Soames as output. Given that, our second take at a definition of vagueness is as follows.\n\nt is vague iff \\({\\exists}\\)x, y1, y2 such that y1\\({\\neq}\\) y2 and it is indeterminate whether \\({\\exists}\\)l such that t denotes l and l(x) = y1, and it is indeterminate whether \\({\\exists}\\)l such that t denotes l and l(x) = y2.\n\nTo get a sense of the definition, it helps to translate it back into supervaluational talk, and look at the special case where t is a predicate. Then the definition comes to the claim that there is some object that is in the extension of t on one precisification, and in the anti-extension of t on another, which seems like what was intended.\nArguably even that is not enough of a correction. (I’m indebted in the following three paragraphs to Mark Johnston.) Frequently there are debates in semantics over the appropriate type of various terms.1 For instance, a straightforward account would say that in She ran yesterday, yesterday modifies the intransitive verb run, so it denotes a function of type \\(\\langle\\)\\(\\langle\\)e,t\\(\\rangle\\), \\(\\langle\\)e,t\\(\\rangle\\)\\(\\rangle\\). But on a Davidsonian semantics, yesterday denotes a property of the running event being discussed, so its type is simply \\(\\langle\\)e,t\\(\\rangle\\). Now it is at least a philosophical possibility that there should be no fact of the matter which of these theories is correct.\nThere are two things we might say about such a possibility. On the one hand, it doesn’t at all seem right that a word should count as vague because it is indeterminate what its type should be. That suggests the above definition needs modification. On the other hand, the above definition doesn’t imply t is vague whenever there are two distinct functions that could be the denotation of t; it must also be the case that these functions have overlapping domains. The most natural cases of syntactic indeterminacy are cases where the two possible denotations are functions of quite different types. That suggests the above definition needs no modification.\nI think the case for modification is a little stronger. That’s partially because the possibility of type-shifting suggests there’s a possibility, perhaps a distant one but a possibility, that the second suggestion could fail. And partially because even if there are no uncontroversial cases of syntactic indeterminacy that will mistakenly be treated as cases of vagueness by this theory, the mere possibility of classifying a case of syntactic indeterminacy as a case of vagueness should be enough to warrant concern. And there is a way to modify the definition that does not look like it will lead to mistakenly ruling out any cases of vagueness that should be ruled in, as follows.\n\nt is vague iff \\({\\exists}\\)x, y1, y2 such that y1\\({\\neq}\\) y2 and y1 is of the same type as y2, and it is indeterminate whether \\({\\exists}\\)l such that t denotes l and l(x) = y1, and it is indeterminate whether \\({\\exists}\\)l such that t denotes l and l(x) = y2.\n\nThat implies that if yesterday is indeterminate merely because it is indeterminate what type of function it denotes, it won’t count as vague, and that’s all to the good. So this is our final definition of vagueness.\nStill there’s a problem with child. Many people have thought that it should not be considered vague for one reason or another. Sometimes this is just asserted as a raw intuition, as in Smith and Eklund. There’s no arguing with an intuition, so I won’t try arguing with it. Rather I’ll just repeat a point I made at the start. We aren’t here in the business simply of summarising ordinary or philosophical intuitions. Rather we are looking for a definition that captures all the cases that fall into the most theoretically important categories. And intuitions about theoretical importance are less impressive than demonstrations of theoretical importance.\nPatrick Greenough (2003) suggests that the problem with terms like child is that they aren’t vague, but rather that they are simply undefined for the alleged borderline cases. If that’s true, and perhaps for some of the examples people had in mind in this area it is, then our definition agrees that they are not vague. For a term that carves a precise division out of part of the domain, and then stays silent, is precise not vague on my account.\nGreenough also suggests that the problem with child is that it is not higher-order vague.2 But as he says this can hardly be the entirety of the problem. For it does not seem to be definitional that the vague terms are also higher-order vague. True, there is a theoretically important category of terms that are vague and higher-order vague. But it is not a category that we cannot represent. A term t is in this category just in case t is vague, and definitely t is vague, and definitely definitely t is vague, and so on. So we can capture that category, even if we don’t call only members of that category the vague terms. And this doesn’t seem to diminish the theoretical importance of the category of terms I called vague.\nIt might be thought that what is wrong with child is that it cannot be used to generate a Sorites argument. If you think that’s what is centrally important to vague terms, then there’s a theoretical reason to separate child from the genuinely vague. But we should have seen enough by now to show that that can’t be right. It’s hard to know what it is for a predicate modifier to be Sorites-susceptible, and our last two example predicates, has few children and tall179 cannot be used to set up Sorites arguments. So that child does not generate a Sorites paradox is no reason to classify it outside the vague.\nSo I take it there is no compelling reason to classify child and similar terms as precise rather than vague. Admittedly there is an intuition that they are not vague, and perhaps that should be respected. But if the cost of respecting that intuition is that we misclassify several other terms, we should reject the intuition. That’s what I’ll argue in the next section.\nRival Definitions\nI just mentioned the idea that a vague predicate could be defined as one that is susceptible to a Sorites argument. This account is sometimes attributed to Delia Graff Fara (2000), but it seems quite a widespread view. For instance, Terence Horgan (1995) says that it is distinctive of vague predicates that they can be used to generate inconsistency because the Sorites premises attaching to them are true. As I mentioned, such views are vulnerable to a wide variety of counterexamples. Many of these counterexamples also apply to rival definitions of vagueness.\nMatti Eklund (2005) develops a similar kind of definition. He starts with Crispin Wright’s (1975) famous definition of what it is for a predicate F to be tolerant.\n\nWhereas large enough differences in F’s parameter of application sometimes matter to the justice with which it is applied, some small enough difference never thus matters.\n\nEklund’s position then is that F is vague iff it is part of semantic competence with respect to F to be disposed to accept that F is tolerant. Eklund agrees that it is inconsistent to assert that F is indeed tolerant. But as he has argued extensively elsewhere, the falsity of the tolerance principle is compatible with it being part of competence that one is disposed to accept it. (A view in the same family is put forward in Sorensen (2001).) I have no wish to dispute this part of Eklund’s theory. Indeed that meaning principles can be false, even inconsistent, it seems to have been a fairly fruitful idea in a variety of areas of Eklund’s philosophy. But I don’t think it helps with vague terms.\nThree of the problems with this have already been given. It is not clear what a parameter of application for a non-predicate like very even is, so it isn’t clear what it means to say that very is tolerant. It surely is not required of competent users of few children that they are disposed to accept the premises in our earlier Sorites argument. And for some vague predicates, like tall179, the tolerance principle is not plausible to a competent speaker because it is not plausible that a “large enough” difference in the parameter of application (presumably height) matters. These problems all seem to carry over from the problems associated with Sorites based definitions.\nI suspect, though I’m less certain here, that the philosophically interesting cases also pose a problem for Eklund’s view. When we look at philosophically interesting cases, like being good, there are two distinct ways to read Eklund’s claim that competent speakers are disposed to accept the tolerance principle. These are the wide scope and the narrow scope reading. To see the ambiguity, let’s write out Eklund’s principle in full.\n\nCompetent speakers are disposed to accept that whereas large enough differences in F’s parameter of application sometimes matter to the justice with which it is applied, some small enough difference never thus matters.\n\nHere’s the wide scope reading of this.\n\nF’s parameter of application is such that whereas competent speakers are disposed to accept that large enough differences in it sometimes matter to the justice with which F is applied, some small enough difference never thus matters.\n\nAnd here is the narrow scope reading, with a phrase added for emphasis.\n\nCompetent speakers are disposed to accept that whereas large enough differences in F’s parameter of application, whatever it is, sometimes matter to the justice with which it is applied, some small enough difference never thus matters.\n\nTo see the difference between the two cases, assume for the sake of argument that a competent speaker thinks that to be good is to do actions whose consequences have a high enough utility, whereas in reality to be good is to obey enough of God’s commands. In each case being good is vague, because we are using satisficing versions of consequentialism and divine command theory. So the parameter of application for being good is the number of God’s commands you obey. The competent speaker will not accept the wide scope version of tolerance with respect to being good, because they don’t think that large differences with respect to how many of God’s commands you obey matter to the justice with which being good is applied. Such cases can be multiplied endlessly to show that the wide scope version of Eklund’s principle cannot generally be true, because it makes it the case that competent speakers have correct views on contentious philosophical matters the resolution of which goes beyond semantic competence. For these reasons Eklund has said (personal communication) that he intends the narrow scope version.\nBut the narrow scope version also faces some difficulties. The most direct problem is that one can be a competent user of a term like food or dangerous or beautiful without having any thoughts about parameters of application. I suspect I was a competent user of these terms before I even had the concept of a parameter of application. Even bracketing this concern, there is a worry that competence requires knowing of a term whether it is vague or not. But this seems to be a mistake. It is not a requirement of competence with moral terms like good that one know whether they are maximising or satisficing terms. Tom Wolfe and the students he observed while writing I Am Charlotte Simmons seemed to disagree about whether going out with is vague, but they were both competent users, they simply disagreed on something like a normative question. (See Wolfe (2000) for more on his take on matters.) And it seems that two users of language could disagree over whether is thinking is vague without disagreeing over whether either is a competent semanticist. They may well disagree over whether either is a competent philosopher of mind, but such disagreements are neither here nor there with respect to our present purposes. So I don’t think that either disambiguation of Eklund’s principle can properly account for vagueness in philosophically interesting terms.\nNicholas Smith argues for a definition of vagueness that uses some heavier duty assumptions about the foundations of semantics. In particular, he sets out the following definition,\nCloseness\nIf a and b are very similar in F-relevant respects, then ‘Fa’ and ‘Fb’ are very similar in respect of truth.\n\nand goes on to say that vague predicates are those that satisfy non-vacuously satisfy Closeness over some part of their domain. For this to work there must be, as Smith acknowledges, both degrees of truth and something like a distance metric defined on them. (These are separate assumptions; in the theory of Weatherson 2005 the first is true but not the second.) I won’t question those assumptions, but rather focus on the problems the definition has even granting the assumptions.\nAs with the two definitions considered so far, it is hard to see how this could possibly be generalised to cover vagueness in non-predicates. It’s true (given our assumptions) that if a and b are similar in very tall-relevant respects, then ‘a is very tall’ and ‘b is very tall’ will be similar in respect of truth. But that doesn’t show very is vague, for the same condition is satisfied when we replace very with the precise modifier doubly. This isn’t an argument that Smith’s definition couldn’t be extended to cover modifiers, but a claim that it is hard to see how this will work.\nThe definition also has trouble with tall179 for this satisfies Closeness vacuously. Though to be fair given the logical assumptions Smith makes, it is possible that no predicate with the properties I’ve associated with tall179 can be defined.\nMore seriously, there is a problem with predicates like has few children. It just isn’t true that “An academic with two children has few children” is close in truth value to “An academic with one child has few children.” In general Smith’s theory has trouble with, i.e. rules out by definition, vague terms where the underlying ‘relevant respects’ are highly discrete. Note that the problem here extends to some predicates where the underlying facts are continuous. Consider the predicate is very late for the meeting. At least where I come from, a person who is roughly ten minutes late is a borderline case of this predicate. But which side of ten minutes late they are matters. (In what follows I make some wild guesses about how numerical degrees of truth, which aren’t part of my preferred theory, should operate. But I think the guesses are defensible given the empirical data.) If Alice is nine and three-quarters minutes late, and Bob is ten and a quarter minutes late, then the degree of truth of “Alice is very late” will be much smaller than the degree of truth of “Bob is very late.” The later you are the truer “you are very late” gets, but crossing conventionally salient barriers like the ten minutes barrier matter much more to the degree of truth than crossing other barriers like the nine minutes thirty-three seconds barrier. Smith (in conversation) has suggested that he’s prepared to accept that is very late for the meeting is only partially vague if the truth-values ‘jump’ at the ten minute mark as I’m suggesting. But this seems improper, for this is as clear a case of a vague predicate as we have. Still, it’s worth remembering as always that every definition has its costs, and this may be a cost one chooses to live with. Personally I think it is excessive.\nPatrick Greenough did not put forward his theory as a definition of vagueness, but rather as a minimal theory to which all partisans could agree. Like Eklund, Greenough plays off Crispin Wright’s idea of tolerance. Roughly, a vague predicate is one that is epistemically tolerant – it’s one where you can’t know that a small difference makes a difference. Here’s a less rough statement of it, though note this is heavily paraphrased.\nLet \\({\\tau}\\) be a variable that ranges over truth states (e.g. true, determinately true, not determinately determinately not determinately true, etc.) v a function from objects to real numbers such that whether x is F depends only on the value of v(x) (i.e. v is F’s parameter of application) and c a suitably small number. Then F is vague iff the following claim non-vacuously holds.\n\n\\({\\forall}{\\tau}{\\forall}{\\alpha}{\\forall}{\\beta}{\\forall}\\)a\\({\\forall}\\)b, if v(\\({\\alpha}\\)) - v(\\({\\beta}\\)) < c and a names \\({\\alpha}\\) and b names \\({\\beta}\\) and it is knowable that Fa is \\({\\tau}\\) then it is not knowable that Fb is not \\({\\tau}\\).\n\nLess formally, we can’t know where any boundary at any order of definiteness for F lies. (It isn’t clear in Greenough’s presentation exactly what the non-vacuous condition comes to. He only explicitly says that for the special case where \\({\\tau}\\) is is true there must be an a and a b such it is knowable that Fa is \\({\\tau}\\) and Fb is not \\({\\tau}\\), but maybe that should be extended to all \\({\\tau}\\).) Because of cases like in one’s early thirties this cannot do as a general definition, but it is easy enough to repair it by restricting the quantifier attaching to a and b to a range over which F has only vague boundaries. Doing this amounts to weakening Greenough’s claim from the view that vague terms have only vague boundaries to the view that they have some vague boundaries, which seems plausible. But still there are problems.\nMost obviously, tall179 does not non-vacuously satisfy the tolerance requirement. And like all the tolerance-based theories it is far from clear how it should be extended to vagueness in non-predicates. On the other hand, Greenough’s theory might well handle the discrete cases like has few children. I say might rather than does because it is rather hard to work out how the higher-orders of vagueness work for such terms. I’ll simply note that there are some plausible enough epistemic models on which has few children satisfies his requirement.\nThere is a problem which is distinctive to Greenough’s view of his theory as a minimal theory. As Smith notes, Greenough makes it a requirement that vague boundaries are unknown. But this is controverted in some mainstream theories, for example the version of supervaluationism in Dorr (2003). Since Dorr’s theory should not be ruled out by a minimal theory or a definition, this is a weakness in Greenough’s theory.\nThe more philosophically interesting problems concern, appropriately enough, the philosophically interesting terms. Greenough has a proof that his definition is equivalent to a definition in terms of borderline cases. The proof has several assumptions, one of which being that we know what the parameter of application of a vague term is. More precisely, he assumes that we know everyone older than an old person is old, which is unproblematic, but he also assumes that the proof generalises to all vague cases, and this amounts to the assumption that we know parameters of application. As we’ve seen, this isn’t true of philosophically interesting vague terms. This leaves open the possibility that Greenough’s theory, unlike Smith’s and Eklund’s theories, overgenerates. The following is probably not a live possibility in any interesting sense, but it isn’t I think the kind of thing a definition (or minimal theory) should rule out by definition.\nIt is possible that a kind of mysterianism about ethics is true, and we cannot know whether good is vague or precise. For a concrete example, let’s assume it is knowable that some kind of divine command theory is true, but it is unknowable whether to be good one must obey all of God’s commands or merely enough of them, where it is vague what counts as enough of them. In fact morality requires obeying all God’s commands, but this is not knowable – for all we know the satisficing version is the true moral theory. If this is the case then good will be epistemically tolerant, for we cannot know that a small difference in how many of God’s commands you obey makes a difference to whether you are good, or determinately good etc. But in fact good is precise, for it precisely means obeying all of God’s commands. Earlier I objected to Eklund’s theory because semantic competence does not require knowing parameters of application, especially as such. This is the converse objection – I claim that a term’s being precise does not imply that we know, or even could know, that it applies in virtue of a precise condition. All that matters is that it does apply in virtue of a precise condition.\nIt’s a constant danger in philosophy that one infer from the falsity of all extant rivals that one’s preferred theory is correct. I certainly don’t want to argue that because Eklund’s, Smith’s and Greenough’s definitions are incorrect that the traditionalist definition I have offered must be right. But we can make that conclusion more plausible by noting how widely the arguments levelled here generalise. The philosophically interesting cases seem to tell against any definition of vagueness in terms of semantic competence, for they show that competent users can have exactly the same attitude towards vague terms as they have towards precise terms. And our moral example suggests that any definition in terms of epistemic properties will be in trouble for it might not be knowable whether a particular term is vague or precise. Finally, the cases of vague predicate modifiers raise difficulties for any attempt to define the vagueness of a term in terms of properties of sentences in which it is used rather than mentioned. For it seems that as long as very attaches only to vague predicates, then whether very* is vague or precise will make no salient differences to the sentences in which it appears. So we have to look at sentences in which the allegedly vague term is mentioned. And while I don’t have a definitive argument here, I think looking at the range of cases we want to cover, and in particular at the range of cases where tolerance-type principles fail to be non-vacuously satisfied, our best option for completing these sentences is to look whether the term has a determinate or indeterminate denotation. We can then pass the questions of what determinacy consists in, and in particular the question of whether it is an epistemic or semantic feature, to the theorist of vagueness.\n\n\nDorr, Cian. 2003. “Vagueness Without Ignorance.” Philosophical Perspectives 17: 83–113. https://doi.org/10.1111/j.1520-8583.2003.00004.x.\n\n\nEklund, Matti. 2005. “What Vagueness Consists In.” Philosophical Studies 125 (1): 27–60. https://doi.org/10.1007/s11098-005-7773-1.\n\n\nFara, Delia Graff. 2000. “Shifting Sands: An Interest-Relative Theory of Vagueness.” Philosophical Topics 28 (1): 45–81. https://doi.org/10.5840/philtopics20002816.\n\n\nField, Hartry. 1973. “Theory Change and the Indeterminacy of Reference.” Journal of Philosophy 70 (14): 462–81. https://doi.org/10.2307/2025110.\n\n\nFine, Kit. 1975. “Vagueness, Truth and Logic.” Synthese 30 (3-4): 265–300. https://doi.org/10.1007/bf00485047.\n\n\nGreenough, Patrick. 2003. “Vagueness: A Minimal Theory.” Mind 112 (446): 235–81. https://doi.org/10.1093/mind/112.446.235.\n\n\nHorgan, Terrence. 1995. “Transvaluationism: A Dionysian Approach to Vagueness.” Southern Journal of Philosophy 33: Spindel Conference Supplement: 97–125. https://doi.org/10.1111/j.2041-6962.1995.tb00765.x.\n\n\nMontague, Richard. 1970. “Universal Grammar.” Theoria 36 (3): 373–98. https://doi.org/10.1111/j.1755-2567.1970.tb00434.x.\n\n\n———. 1973. “The Proper Treatment of Quantification in Ordinary English.” In Approaches to Ordinary Language, edited by K. J. J. Hintikka, J. M. E. Moravcsik, and P. Suppes, 221–42. Dordrecht: Reidel.\n\n\nSainsbury, Mark. 1991. “Is There Higher-Order Vagueness?” The Philosophical Quarterly 41 (163): 167–82. https://doi.org/10.2307/2219591.\n\n\nSorensen, Roy. 2001. Vagueness and Contradiction. Oxford: Oxford University Press.\n\n\nWolfe, Tom. 2000. Hooking up. Farrar, Strauss; Giroux: New York.\n\n\nWright, Crispin. 1975. “On the Coherence of Vague Predicates.” Synthese 30 (3-4): 325–65. https://doi.org/10.1007/bf00485049.\n\n\nIn what follows I’ll refer to functions of type \\(\\langle\\)X, Y\\(\\rangle\\). These are functions from things of type X to things of type Y, where the basic types are entities, represented by e, and truth values, represented by t. So a function of type \\(\\langle e, t \\rangle\\) is a function from objects to truth values, or, equivalently, the characteristic function of a set. A function of type \\(\\langle\\)\\(\\langle\\)e, t\\(\\rangle\\), \\(\\langle\\)e, t\\(\\rangle\\)\\(\\rangle\\) is a function from characteristic functions of sets to characteristic functions of sets. This is plausibly the semantic value of a predicate modifier like very.↩︎\nWhen I say a term is higher-order vague, I mean that it is subject to higher-order vagueness, not that it is vague whether the term is vague.\n\n↩︎\n",
    "preview": "posts/2021-03-12-vagueness-as-indeterminacy/clouds.jpg",
    "last_modified": "2021-03-13T00:07:46-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-relativism/",
    "title": "Relativism",
    "description": "Relativism is the view that the truth of a sentence is relative both to a context of utterance and to a context of assessment. That the truth of a sentence is relative to a context of utterance is uncontroversial in contemporary semantics. This chapter focuses on three points: whether the version of contextualism is vulnerable to the disagreement and retraction arguments, and if so, whether these problems can be avoided by a more sophisticated contextualist theory. The points include: whether relativism really does avoid the four problems posed for the other theories; and whether there are other theories that also avoid the problems, without running into the problems facing relativism or problems of their own. The chapter concentrates on two families of views that have been called relativist: Relativism about propositional truth; and Relativism about utterance truth.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      },
      {
        "name": "Patrick Shirreff",
        "url": "https://patrickshirreff.wixsite.com/home"
      }
    ],
    "date": "2017-04-24",
    "categories": [
      "language",
      "relativism"
    ],
    "contents": "\n\nContents\nVarieties of Relativism\nIndex, Context and Content\nRetraction and Disagreement\nRetraction\nDisagreement and Agreement\nClarifying the Data\nPresuppositions and Common Context\nRelativism and Disagreement\nProblems with the Data\n\nControl and Syntax\n\nRelativism, in the sense we’re interested in here, is the view that the truth of a sentence is relative both to a context of utterance and a context of assessment. That the truth of a sentence is relative to a context of utterance is uncontroversial in contemporary semantics. If the authors of this paper were to both utter the sentence “I’m Canadian,” then one of us would say something true, and the other something false. And that’s because the truth of the utterance “I’m Canadian” is sensitive to a feature of the context of utterance, namely the identity of the speaker. And that in turn is explained by the fact that the proposition expressed by that sentence is different in different contexts. Relativists deny that this simple story can explain all the ways in which context language.\nPublished in Blackwell Companion to the Philosophy of Language, edited by Bob Hale, Crispin Wright and Alexander Miller, 787-803.\nPicture by Lon&Queta via Creative Commons.\nA core motivation for relativism comes from looking at the problems with other views. So let’s start by thinking about what might be happening when a speaker says “This is tasty.” (Call this utterance, which we’ll come back to a bit, U, and its speaker S.) The demonstrative ‘this’ is context-sensitive, but let’s assume it is clear what is being referred to, and think about what contribution the predicate is making. There are two natural proposals that are simple, easy to fit into familiar frameworks, and most likely wrong.\nFirst, the predicate might pick out an objective property of tastiness, and when S utters U, she ascribes this property to a thing. This objective view has a number of problems. First, there is a metaphysical problem; just what is this objective tastiness? Second, there is an epistemological problem. Typically, a speaker like S could be prepared to utter U after taking one bite of the thing. But for most plausible answers to the first question, it is unclear how they could know that the object had that property. The two problems reinforce each other; The more plausible answers to the metaphysical question make the property sensitive to very abtruse conditions, such as how idealised observers would react to ingesting the substance. But that makes it even more unlikely that typical utterances of U satisfy the epistemological requirement. (The points we’re making here are well known, but our presentation owes a lot to Lasersohn (2005).)\nSecond, the predicate might be context-sensitive. For concreteness, let’s focus on a very simple contextualist theory of ‘tasty.’ An utterance of U is true, relative to a context \\(c\\), iff the speaker in \\(c\\) likes the taste of the referent of ‘this’ in \\(c\\). So “This is tasty” and “I like the taste of this” express something very close to the same proposition. (At least they express propositions that are necessarily materially equivalent, but the relation between the two is probably closer than that.) Again there are two problems, both of them to do with the different dynamics of “This is tasty” and “I like the taste of this.” A hearer H, who does not like the taste of this substance, could more readily disagree with “This is tasty” than with “I like the taste of this.” If H doesn’t like its taste, he’ll nevertheless concede that S’s utterance of “I like the taste of this” is true, but won’t concede that “This is tasty” is true. And a similar phenomenon occurs when S herself changes her mind. If later she dislikes the taste of the substance, she will be disposed to retract her utterance of “This is tasty,” but not of “I like the taste of this.” So both disagreement and retraction data pose problems for the contextualist. (On disagreement, see especially Tamina Stephenson (2007). On retraction, and most everything else we’ll talk about in this paper, see John MacFarlane (2014).)\nIf the objectivist and contextualist solutions fail, the relativist suggests that they have a useful alternative. Say that truth is doubly relativised, both to contexts of utterance and to context of assessment. So the utterance, or perhaps just the proposition expressed by it, is true relative to a context of assessment \\(c_a\\) and context of utterance \\(c_u\\) iff the agent at \\(c_a\\) likes the taste of the denotation of ‘this’ in \\(c_u\\). So now the truth of the utterance is relative both to the context of utterance and the context of assessment. This solves the metaphysical problem; talk about what a person likes is unproblematic. It solves at least a version of the epistemological problem; a speaker knows what they like, so can make utterances that are true in their context. It solves a version of the disagreement problem; if H doesn’t like the taste of the stuff, he can truly say that what S said is not true, since it isn’t true in his context. And it explains the retraction data, since once S changes her taste, what she said is no longer true relative to her new context, and hence should be retracted.\nAs everywhere else in philosophy, arguments and claims can be and are contested. In the quick case for relativism we’ve described so far, there are at least four points where one could readily disagree.\nWhether objectivism is really vulnerable to the combination of the metaphysical and the epistemological arguments.\nWhether this version of contextualism is vulnerable to the disagreement and retraction arguments, and if so, whether these problems can be avoided by a more sophisticated contextualist theory.\nWhether relativism really does avoid the four problems posed for the other theories.\nWhether there are other theories that also avoid the problems, without running into the problems facing relativism or problems of their own.\nIn this entry, we’ll focus on the last three points, since they are more widely discussed in the literature than the first one. And indeed, the last three points are extremely actively debated in recent years. We won’t try to take sides in those debates, though we will note that on no point is the final picture nearly as rosy for the relativist as this initial sketch may suggest.\nWe’ve so far focussed on one particular predicate, ‘tasty,’ though the points we’ve been making generalise to most predicates of personal taste. But this is far from the only area where relativist theories have been proposed. There has been a lot of discussion of relativist theories of epistemic modals. The arguments here are fairly similar to the arguments about predicates of personal taste (though see the discussion below about syntax and control). And MacFarlane has argued for relativist treatments of future contingents, and knowledge ascriptions. We will not spend much time on those constructions explicitly, but the issues they raise are broadly similar to the issues raised by predicates of personal taste, and by epistemic modals.\nWe will largely bypass two quite different areas where relativist theories have been proposed. Mark Richard (2008) has argued for a relativist treatment of comparative adjectives. And Weatherson (2011) arguedfor relativist treatments of areas of certain areas discourse where common assumptions about the area are false, and a relativist treatment might be the most charitable fix. As interesting as we find these proposals, they haven’t occasioned nearly as much discussion as the proposals discussed above, and so we’ll set them aside.\nWhat we will do is first clarify some of the many ways in which the term ‘relativism’ has been used in recent debates, then review some technical material about indices and contexts that is essential for understanding some relativist views, then look at the main line of recent debate concerning relativism, the one centered on issues about retraction and disagreement, and finally look at some syntactic evidence for relativism.\nVarieties of Relativism\nAs we noted in the very first paragraph, it is uncontroversial that sentence truth is sensitive to the context of utterance. It is extremely contentious just how many such sentences are sensitive to the context of utterance. (See, for instance (DeRose 2009; Harman and Thomson 1996; Cappelen and Lepore 2005) for some of the disputes.) But that there is some sensitivity here is uncontroversial. The view that a sentence type’s truth is sensitive to its context of utterance is sometimes called “indexical relativism” (Kölbel 2004; Einheuser 2008). For some purposes this is a useful name. In particular, the view that the content of moral predicates, and hence the truth value of ascriptions of moral predicates, is sensitive to the context of utterance does seem like a form of moral relativism, as that term is standardly understood. It is, for instance, the view that Harman (1975) defends in a self-proclaimed defence of moral relativism. (That this was traditionally known as relativism is stressed in Cappelen and Huvenes (2014), who draw some interesting conclusions from this fact.) But for present purposes we want to clearly exclude those views, and focus on much more radical proposals that have been recently made.\nThere are two (overlapping) families of views that have been called relativist, and which we will be concentrating on for the bulk of this entry. They are:\nRelativism about propositional truth - Whether a proposition is true is not an absolute fact. Propositions can be true relative to some contexts of assessment, and false relative to others.\nRelativism about utterance truth - Whether an utterance is true is not an absolute fact. Utterances can be true relatve to some contexts of assessment, and false relative to others.\nIt isn’t always clear whether a particular author, in defending relativism, is primarily defending the first or second of these claims. But it is natural to interpret most relativists as defending relativism about propositional truth. This is most clearly true for Kölbel (2002) and Egan (2007), but there are few relativists that it is hard to interpret as taking this to be the primary focus. The one big exception, however, is the most prominent defender of relativism in the contemporary literature, John MacFarlane (2014). He takes relativism about utterance truth to be the only genuine form of relativism, though he also endorses relativism about propositional truth. We are not going to engage with the dispute over what is really a relativist view. Instead we’ll just describe a pair of views that are relativist in one of these senses but not the other, to explain how the two senses come apart.\nFirst, let’s think about how to make propositional truth, but not utterance truth, relative. Assume that Suzy actually swims, but that she might not have. Then, on a natural way of thinking about modality, the proposition Suzy swims will be true relative to the actual world, but false relative to any possible world in which she does not. So propositional truth is relative, with the relativity being to possible worlds.\nCappelen and Hawthorne (2009) argue that the reasoning of the previous paragraph misunderstands the nature of truth and modality. The proposition Suzy swims is, they say, simply true. It might have been false. But that doesn’t mean it is merely true relative to some world or other; it just means that it might have had a different property than it actually has. Schaffer (2012) argues that the things we say and think, i.e., propositions, typically have worlds in their content, so what we express by the sentence “Suzy swims” is the proposition *Suzy swims at , and that proposition, like all propositions, has a non-relative truth value.\nEven if all that is wrong, and the contents of sentences like “Suzy swims” are propositions that are true in some worlds, false in others, there is a reason not to call this ‘relativism.’ After all, other worlds don’t exist, or at least don’t exist in the way ours does. So it isn’t true that the proposition expressed by “Suzy swims” is true relative to something and false relative to something else; there isn’t a something else for it to be false relative to.\nTaking the last point on board, a serious relativism about propositional truth should say that there is a proposition p, and two relata a and b such that p is true relative to a, and false relative to b. One way to do that, as seen in for example Kaplan (1989), is to make truth relative to times, and believe that other times exist. (The contrast here is with the eternalist view of propositions defended by Evans (1985).) But the radical view that has been central to many recent versions of relativism has been to say that truth is relative to world, time, individual triples. So there is a proposition, for example, that is true relative to a triple \\(\\langle w, t, a \\rangle\\) iff in \\(w\\), the individual \\(a\\) disapproves of murder at \\(t\\). Perhaps, on a relativist framework, this just is the proposition that murder is wrong. The picture of propositions here owes something to the discussion of de se beliefs in Lewis (1979), since Lewis thought the contents of such beliefs were sets of such triples. But Lewis put this forward as a theory about mental content; the move to extending it to linguistic content is more recent.\nOn this view, the contents of utterances will be propositions that can be true relative to one individual (in a world, at a time) and false relative to another. But the utterances themselves will naturally be thought to have absolute truth value. Think back to the view that propositional truth is just relative to worlds. Then an utterance of “Suzy swims” will, intuitively, be true iff Suzy swims in the world the utterance is made. When we think about a counterfactual utterance of “Suzy swims” in a world where she does not, we don’t think the utterance is true just because its content is actually true. So although this is a form of relativism about propositional truth, it is not yet a form of relativism about utterance truth. MacFarlane, who takes utterance relativism to be central, calls this view ‘non-indexical contextualism,’ with the name meant to highlight that it isn’t, by his lights, genuinely relativist.\nNow turn to relativism about utterance truth. A simple way to implement this is to say that the propositional content of an utterance is relative to an assessor. So consider a very simple, and surely false, view about ‘you’ in English. It holds that the content of an utterance of ‘you,’ relative to a context of assessment, is the agent of that context. And it says that the utterance is true at a context of assessment iff the proposition it expresses relative to that context is (absolutely) true. So an utterance of “You swim” will express a different proposition relative to contexts with different agents. And since some of those propositions will be true, and some false, we have a version of relativism about utterance truth. But this view is consistent with the view that propositional truth is absolute, not relative even to worlds.\nSo the two versions of relativism are dissociable. But it is also possible to hold them simultaneously, as MacFarlane (2014) does. To set up MacFarlane’s view, it is helpful to review the framework against which it was developed.\nIndex, Context and Content\nThe existence of indexical terms, like ‘I,’ means that we can’t give a theory for truth conditions of sentences as such. The sentence-type I am in Ann Arbor now doesn’t have truth conditions; only occurrences of this sentence do. So a natural move is to build up a theory that assigns to each term a function from contexts to contents, and use that to provide a theory of which contexts a sentence is true in. It turns out that there are reasons to endorse more complications than this. In particular, sentences like (1) show the need for relativising truth conditions relative to both a context and an index.\n\n\\(1\\) It might have been the case that my actual parents never met\n\nThat’s true. But think of how we might naturally provide truth conditions for it. First, we define the context-relative truth conditions for “My actual parents never met.” The context will provide a world for ‘actually’ (i.e., the actual world), and an agent for ‘my’ (i.e., the speaker). Then, we say that the modal operator shifts the world of evaluation. Intuitively, Might p is true iff p is true in some possible world. So we need some way to see if the content of “My actual parents never met” is true in some other world. But that’s hard to do, since (given origin essentialism), there will be no context in which “My actual parents never met” can be truly uttered, so its content will be the function that maps every context into FALSE. And that’s the same content as, for example, “Two plus two is five,” and so it can’t be possibly true.\nThere are actually two related problems here. One is that while we need to look around the other worlds to see whether some thing is true, the nature of that thing is fixed by the actual context. So if the sentence is uttered by Sasha Obama in this world, the content is something like It might have been the case that Barack Obama and Michelle Obama never met. We need to fix the values of the contextually sensitive terms (‘my’ and ‘actual’) even when they appear inside operators. The second problem is that something as coarse-grained as a function is very hard to ‘shift’ in just one respect.\nThere is a well-known solution to this, developed primarily by Hans Kamp (1971), then put to important philosophical use by David Lewis (1980) and David Kaplan (1989). Say that our semantic theory will assign a function from both contexts and indices to terms. Ultimately, each sentence will get as its semantic value a function from contexts and indices to truth values. Lewis describes the distinction between contexts and indices thus,\n\nA context is a location – time, place, and possible world – where a sentence is said. It has countless features, determined by the character of the location. An index is an n-tuple of features of context, but not necessarily features that go together in any possible context. Thus an index might consist of a speaker, a time before his birth, a worlds where he never lived at all, and so on.  (Lewis 1980, 79)\n\nindices are structured, and so they can be ‘improper’; it might consist of things that don’t go together. If the index contains just worlds and individuals, the index could be \\(\\langle w,\\) Sasha Obama\\(\\rangle\\), even if Sasha doesn’t exist in \\(w\\). Such an index will play a central role in making (1), as uttered by Sasha Obama, true. A context, on the other hand, will by its nature be proper; it will pick out a world, and a time in that world, and a person existing at that time, and so on.\nIndices are, unlike contexts, ‘shiftable.’ There can be sentential operators that say the truth value of the whole sentence is given by looking at the truth value of the embedded sentence at some different index. Assume, for example, that there is a world parameter in the index, so the index is \\(\\langle w, x\\rangle\\), where \\(x\\) contains everything else in the index. Then Might p will be true relative to context \\(c\\) and index \\(\\langle w, x\\rangle\\) just in case for some \\(w^{\\prime}\\), p is true relative to context \\(c\\) and index \\(\\langle w^{\\prime}, x\\rangle\\). Note that in this definition, we do not shift \\(c\\); so if \\(c\\) determines that the referent of ‘I’ is Sasha Obama, that stays even if we ‘shift’ the index to a world where Sasha does not exist.\nThe separation between context and index was developed to solve some technical problems, but it can do some philosophical work. The theory associates each sentence with a function from contexts and indices to truth values. Equivalently, it associates sentence-context pairs with functions from indices to truth values. So it is natural to say that the content of an utterance is (or is at least intimately connected to) the function associated with the pair consisting of the sentence uttered and the context it was uttered in. If the index is simply a world, then the contents will be functions from worlds to truth-values, as defended by classical contextualists such as Robert Stalnaker (1984). David Lewis (1980) objected that assigning contents at just this stage was arbitrary, why not associate contents with functions from context to truth values instead? François Recanati (2007) responds well to Lewis’s arguments, although his defence requires making indices much more complex. In particular, he includes parameters for times and individuals in the index, leading him to support relativism about propositional truth.\nWith this background, we can more easily describe two distinctive features of MacFarlane’s views. First, he makes indices very complicated indeed. The index might include, among other things, an information set (for interpreting epistemic and deontic modals), a standard of taste (for interpreting predicates of personal taste), relevant alternatives (for interpreting knowledge ascriptions), a time (for handling future contingents), and perhaps many more things. For simplicity, we’ll call these things collectively a perspective, and say that indices are world-perspective pairs. (Though note that our terminology here requires perspectives to be structured entities, and for them to be potentially improper.) If the propositional content of an utterance is (intimately connected to) a function from indices to truth-values, then the one proposition will be true relative to one perspective and false relative to another.\nSo far we only have relativism about propositional truth, or what MacFarlane calls non-indexical contextualism. MacFarlane argues for a second revision to contextualist orthodoxy. Assume that we have a particular utterance \\(U\\) of a sentence \\(S\\) in context of utterance \\(c_u\\), and that utterance is being assessed in context \\(c_a\\). And assume that \\(w(c)\\) is the world of context \\(c\\), and \\(p(c)\\) is the perspective of context \\(c\\). Sentence \\(S\\) is associated with a function \\(f_S\\) from context-index pairs, i.e., context-world-perspective triples, to truth values. The relativist about propositional truth, but not utterance truth, says that \\(U\\) is true iff \\(f_S(c_u, w(c_u), p(c_u)) = TRUE\\). MacFarlane’s relativist1 says that \\(U\\) is true iff \\(f_S(c_u, w(c_u), p(c_a)) = TRUE\\). Crucially, we assess the utterance itself, and not just its content, by the perspective of the assessor. MacFarlane argues that this move allows a better understanding of disagreement and retraction, which were central to the phenomena that motivated relativism. So let’s turn to how well relativism explains the phenomena it was designed to explain.\nRetraction and Disagreement\nRetraction\nContextualists have a difficult time explaining why we retract earlier claims involving predicates of personal taste or epistemically modals assertions when our perspective has changed in the intervening time. Consider this example:\n\nKim (age 8): Lunchables are delicious.\nKim (age 26 being reminded of previous assertion): I take it back/I was wrong/what I said was false. Lunchables aren’t delicious.\n\nWhen Kim retracts her earlier assertion, it is natural to use any of the three forms of retraction listed here. It isn’t natural to say either of the following things:\nLunchables were delicious back then, but they aren’t delicious any more.\nWhen I said that back then, I only meant that they were delicious to me back then.\nThere is a contrast with how clearly context sensitive terms like ‘here’ are used. It isn’t natural to use any of the three forms of retraction we attribute to Kim below.\n\nKim (in a café): It is pleasant here.\nKim (in an oil refinery, reminded of previous assertion) I take it back/I was wrong/What I said was false. It isn’t pleasant here.\n\nOn the other hand, it is natural to say things like:\nIt was pleasant where we were, but it isn’t pleasant where we are now.\nWhen I said that back then, I only meant it was pleasant where we were.\nPredicates of personal taste, like ‘delicious,’ don’t behave like explicitly context-sensitive expressions like ‘pleasant here.’ We see the same pattern with epistemic modals.\n\nHakeem: It might be snowing outside.\nFabian: No/that’s wrong/that’s false, it can’t be snowing. I just looked out the window and there were clear skies and the sun was out.\nHakeem: Really? Then I guess I was mistaken.\n\nIf It might be that p is true iff the speaker’s knowledge is compatible with p, as it does on a simple contextualist theory, then none of this conversation makes any sense. But, argue relativists, it is perfectly natural. A natural move here is to argue that the problem with simple contextualism isn’t the contextualism, but the simplicity. Exploring the moves that can be made here will take us too far afield; MacFarlane (2014) goes through a number of possible alternative contextualist theories and shows how examples like the ones involving Kim, Hakeem and Fabian can be generated to raise problems for each of them. Instead, let’s look at how the relativist handles the cases.\nAccording to MacFarlane, relativists are committed to the following principle about retraction:\n\nThe speaker ought to retract the assertion if she has good grounds for thinking that its content is false (as assessed from the perspective she currently occupies).\n\nGiven that Kim and Hakeem are evaluating their earlier utterances from new perspectives, perspectives where the assertions are now judged to be false, they both should retract that earlier assertion because they now take them to be false. But arguably this principle overgenerates. Consider this example from Fintel and Gillies (2008).\n\nAlex: The keys might be in the drawer.\nBilly: (Looks in the drawer, agitated) They’re not. Why did you say that?\nAlex: Look, I didn’t say that they were in the drawer. I said they might be there – and they might have been. Sheesh.  (Fintel and Gillies 2008, 81)\n\nThe lesson they draw from the example is that retraction is somewhat voluntary. Despite what relativists claim, it is not always true that ‘might’ claims are retracted or rejected in light of new evidence. Instead, what seems to be true is that that solipsistic readings for the modals are virtually always available. They say that the relativist can’t easily explain this data. MacFarlane (2014Ch. 10) responds by saying, in effect, that epistemic modal claims sometimes mean what contextualists say they mean, and this is compatible with relativism.\nDisagreement and Agreement\nThe data about retraction are closely related to a phenomenon that classically was central to debates about relativism. A traditional argument for relativism is that it is necessary to explain ‘faultless disagreement.’ If one person says Vegemite is tasty, and another says that it is not, the alleged datum is that they are disagreeing, but neither is wrong. Objective treatments of taste save the phenomenon of disagreement, but lose the faultlessness. Contextualist, or otherwise subjective, treatments of taste save the faultlessness, but lose the disagreement. Relativism was alleged to keep both. Faultless disagreement plays a big role in Kölbel (2002), but it has dropped out of the recent literature somewhat. But the focus on disagreement remains, with the central claim being that contextualists cannot explain why some conversations are genuinely disagreements. The following is how MacFarlane puts the point using an example from predicates of personal taste:\n\nIf the truth of my claim that a food is “tasty” depends on how it strikes me, while the truth of your claim that the same food is “not tasty” depends on how it strikes you, then our claims are compatible, and we do not disagree in making them. But it seems that we do disagree–even if we are aware that the source of our disagreement is our differing tastes.  (MacFarlane 2014, 8)\n\nThis is no argument against any kind of objectivist treatment of predicates of personal taste. The issue is solely whether relativism or contextualism does a better job of explaining the facts about disagreement. We’ll look at a couple of reasons for thinking that the problem is less pressing for contextualists than it might first seem, then an interesting attempt to resolve the problems that remain for contextualism, then at some reasons for doubting that relativism helps solve the remaining problems.\nClarifying the Data\nThe puzzle for contextualism is supposed to be that there is a big difference between the felicity of Mark’s reply in the following two cases.\n\nSally: This chilli is tasty.\nMark: I disagree. It’s too hot.\n\n\nSally: I’m from Barcelona.\nMark: I disagree. #I’m from Oslo.\n\nAssume Sally and Mark are both being sincere. So Sally does like the chilli and is from Barcelona, and Mark doesn’t like it and is from Oslo. In neither case can Mark sincerely repeat the words that Sally uttered. Indeed, he can sincerely utter the negations of the sentences Sally uttered. But in the first case, this seems to amount to a disagreement, and in the second case it does not. If the context-sensitivity of ‘tasty’ and ‘I’ is explained the same way, this is mysterious.\nBut note that there are other examples that do not seem to be amenable to a relativist treatment where Mark can express disagreement with Sally.\n\nSally: I like this chilli.\nMark: I disagree. It is too hot.\n\nWhat seems to be going on there is that Mark is disagreeing with an attitude that Sally has, but not with any proposition she expressed. After all, Mark presumably agrees with the proposition that Sally likes the chilli. That’s why he knows he is disagreeing with her. And that’s the content of what he uttered. So some disagreements that are triggered by assertions are not with the content of what is asserted. This might be the germ of an idea for how contextualists can explain the data about disagreement, one nicely developed by Torfinn Huvenes (2012). He argues that in a lot of cases of disagreements involving taste, what we see is not a disagreement about any content, but a disagreement of attitudes.\n\nTwo parties disagree just in case there is something towards which they have conflicting attitudes. This sometimes means that there is a content that one party accepts and the other rejects, but that does not always have to be the case. Just as two parties may have conflicting beliefs, they may also have conflicting desires or preferences.  (Huvenes 2012, 178)\n\nIf Huvenes is right, then we can’t draw any conclusions for semantics from the facts about disagreement.\nThere is a further problem with using facts about disagreement to argue against contextualism. Consider the following dialogue.\n\nSally: Joe might be in Boston.\nMark: I disagree; he’s definitely in China.\n\nIf Joe might be in Boston just means For all I know, Joe is in Boston, then it is prima facie unclear why Mark is disagreeing. After all, it is consistent with Joe’s being in China that for all Sally knows, Joe is in Boston. But, say some contextualists, Mark need not disagree with the whole content of Sally’s utterance. He may just be disagreeing with the prejacent of the modal, i.e., that Joe is in Boston.\nMost of the terms of disagreement that have been used in examples motivating anti-contextlism can be seen to target something other than the entire proposition  (Fintel and Gillies 2008, 83). This is a particular problem for arguments for relativism involving epistemic modals. It’s possible to say “I disagree,” “that’s false,” “no,” or “you’re mistaken” and disagree with the prejacent of someone’s modal claim, not necessarily the whole claim.\nOne potential avenue for the relativist to get around this worry is to limit the range of disagreement markers that count as expressing the right kind of disagreement. For example, Tamina Stephenson (2007) limits the terms of disagreement in the examples she uses to “no” and “nuh-uh.” And John MacFarlane (2014, 11) discusses disagreements that more explicitly target the entire asserted proposition such as “the proposition you expressed is false” and “what you asserted is false.” The issue with the two former disagreement markers is that they can explicitly target the embedded clause of an expression. Indeed, Stephenson herself provides a clear example of this phenomenon.\n\nMary: How’s the cake?\nSam: I think it’s tasty.\nSue: Nuh-uh, it isn’t tasty at all!  (Stephenson 2007, 512)\n\nThe issue with the disagreement markers that MacFarlane uses is that they have become too technical to do the type of work they need to do. Relativism is meant to be an empirical thesis that relied on natural language data to back it up. We have moved well outside of the realm of natural language and the types of natural language intuitions about the acceptability of sentences that we can get when we move to “the proposition you expressed is false” and “what you asserted is false.”\nNote that the two contextualist responses we’ve described here are rather complementary. The point Huvenes makes, that disagreements can involve attitudes other than belief, seems best served to defuse arguments from disagreements concerning predicates of personal taste. And the point that von Fintel and Gillies make, that disagreements can target prejacents, seems best served to defuse arguments from disagreements concerning epistemic modals. It is possible there are replies to this last point; Weatherson and Egan (2011) for example suggest that examples involving agreements are invulnerable to the response that von Fintel and Gillies make. But as it stands, the dominant trend in the literature seems to be in the direction of thinking the contextualist has the resources to answer these relativist arguments.\nPresuppositions and Common Context\nThere is a more radical, and perhaps more concessive, response to the disagreement arguments available to the contextualist. Dan López de Sa (2008) develops a contextualist theory that explains the disagreement data by positing that for many context-sensitive terms, there is a presupposition that users of the term are in the same context. (Note that López de Sa calls his theory an “indexical relativist” theory, but it is a kind of contextualist theory in the way the terms are being used here.)\nIt isn’t true in general that there is a presupposition of commonality of context. If two speakers both say “I am happy,” they should be interpreted as putting forward different propositions. And that is because, in the relevant sense, they are in different contexts. But, perhaps, many terms are not like this. In particular, cases where there appears to be a problem with explaining the phenomena involving disagreement, perhaps this is not so. So consider the stock example López de Sa uses, a variant on one that may seem familiar by now.\n\nHannah: Homer Simpson is funny.\nSarah: I disagree. Homer is not funny.\n\nIf ‘funny’ denotes a different property when Hannah and Sarah use it, there is no disagreement about propositional content here. And the contextualist account of predicates of personal taste would predict that it could, and perhaps often will, denote a different propertty on different occasions of usage. But it seems that there is a disagreement here, and arguably even one about propositional content. The solution López de Sa offers is that in any conversation, there is a presupposition that we are applying the same aesthetic standards. The model he uses is a suggestion made by David Lewis (1989) in defending a contextualist treatment of claims about value.\n\nWouldn’t you hear them saying ‘value for me and my mates’ or value for the likes of you’? Wouldn’t you think they’d stop arguing after one speaker says X is a value and the other says it isn’t? – Not necessarily. They might always presuppose, with more or less confidence (well-founded or otherwise), that whatever relativity there is won’t matter in this conversation.  (Lewis 1989, 84)\n\nHere is how López de Sa develops the point.\n\nAccording to the approach, ‘is funny’ triggers a presupposition of commonality to the effect that both Hannah and Sarah are similar with respect to humor. Thus, in any non-defective conservation where Hannah uttered ‘Homer is funny’ and Sarah replied ‘No, it is not,’ it would indeed be common ground that Hannah and Sarah are relevantly alike, and thus that they are contradicting each other. After all, provided they are alike, either both Hannah and Sarah are amused by Homer or they are not.  (López de Sa 2008, 305)\n\nThis is an ingenious idea, but there are a few hurdles to be cleared before it could be declared a full solution to the problem. First, it needs a way to deal with eavesdroppers. If Hannah writes “Homer Simpson is funny” on a scrap of paper, and later Sarah chances upon that paper, she can still say that she disagrees. But it is very odd to think there is a presupposition of commonality of taste with anyone who chances upon one’s writings. Second, it needs to account for cases where the presupposition is expressly cancelled. The Hannah/Sarah dialogue feels natural even in cases where it has been made explicit that Hannah and Sarah have completely different tastes, and they are displaying their differences for the amusement of their friends  (MacFarlane 2014, 131–32). Third, it doesn’t quite capture the idea that Hannah and Sarah are contradicting each other. When Sarah disagrees, it shows that either there is a proposition one accepts and the other rejects, or that the context is defective. If we thought the data was that there was a contradiction between what they say, López de Sa’s approach can’t explain that. And finally, it isn’t obvious how to extend this theory to other terms for which relativism seems promising. It is one thing to say that conversations about humor presuppose a common standard for humor. It is much less plausible to say that conversations about what might be the case presuppose that the parties to the conversation know the same things. None of these hurdles seem impossible to clear, but they do raise doubts about whether presuppositions can solve all the contextualists’ problems with disagreement.\nRelativism and Disagreement\nIf, after all that, we conclude that the contextualist still has a problem with disagreement, it’s fair to ask whether the relativist does any better. Let’s think again about López de Sa’s example of Hannah and Sarah.\n\nHannah: Homer Simpson is funny.\nSarah: I disagree. Homer is not funny.\n\nA simple relativist theory assigns truth conditions to Hannah’s utterance relative to a context of utterance, her own, and an index that consists of a world-perspective pair. (Remember that we’re using ‘perspective’ to cover everything other than a world that a relativist may want to put into an index, and that it will be a structured entity.) The content of Hannah’s utterance will be set by her context. So it will be (or at least will determine) a function from indices, i.e., world-perspective pairs, to truth-values. Call the world Hannah and Sarah occupy \\(w\\), and their perspectives \\(p_H\\) and \\(p_S\\). Then the proposition that Homer Simpson is funny will be true relative to \\(\\langle w, p_H\\rangle\\) and false relative to \\(\\langle w, p_S\\rangle\\). So there is a proposition Hannah accepts and Sarah rejects, and so they disagree. Doesn’t think mean that the relativist can explain the sense in which Hannah and Sarah disagree?\nNot so fast. Consider another case involving One, who lives in \\(w_1\\) where Mars has one moon, and Two, who lives in \\(w_2\\) where Mars has two moons. They make the following utterances:\n\nOne: Mars has one moon.\nTwo: Mars has two moons.\n\nNow most theorists would say that One and Two have expressed propositions that cannot be true together. (As noted earlier, Schaffer (2012) disagrees, though not in a way that helps relativism.) But they don’t disagree. Among other things, they both think that the other speaks truly.\nHannah doesn’t just think that the proposition that Homer Simpson is funny is true relative to \\(\\langle w, p_H\\rangle\\); she thinks it is simply true. That is because she occupies (for want of a better word) the index \\(\\langle w, p_H\\rangle\\). And one doesn’t just think that the proposition that Mars has one moon is true relative to \\(\\langle w_1, p_1\\rangle\\), she thinks it is simply true. But in doing so, she need not be in disagreement with someone who occupies a different index, such as Two, and who thinks it is false. It isn’t easy to read off the existence of disagreement from the endorsement of conflicting propositions when the parties occupy different indices. And that raises doubts about whether the relativist has really explained the disagreement. (The Mars example is from MacFarlane (2014, 128). Both Dreier (2009) and Francén (2010) raise doubts about whether the relativist can explain disagreement.)\nPart of MacFarlane’s response to this is to insist that the different elements of an index are treated differently. If we are relativists about propositional truth, but not utterance truth, it is natural to treat the two elements the same. A speaker, we’ll then say, speaks truly iff the proposition they express is true relative to the context they occupy. But that isn’t MacFarlane’s view. He holds that an utterance is true, i.e., that a speaker speaks truly, relative to a context of assessment iff the proposition they express is true relative to the world they occupy, and the perspective of the context of assessment. This gives him a way to distinguish the Hannah/Sarah case from the One/Two case. There is still a lot more work to do to turn this into a full theory of disagreement, and chapter 5 of MacFarlane’s book has a very careful study of the varieties of disagreement that are possible on a relativst theory, and how they can be used to explain the data. We’re not going to attempt to evaluate the success of these responses, but rather conclude by noting that even if the contextualist has work to do to explain the phenomena involving disagreement, so does the relativist.\nProblems with the Data\nMost of the arguments in the literature have started with intuitions about disagreeement. We don’t think there is anything wrong with this in principle; indeed, it is what we’ve done so far. But when the intuitions get a little shaky, as they are in a few cases we’ve described so far, it is worth checking them more carefully, against a broader range of informants. And when that is done, it isn’t clear that the data help the relativists as much as the relativists have claimed. Knobe and Yalcin (2014) provide evidence that the following claim, which we’ll call (J), isn’t as empirically justified as the relativists have made it out to be.\n\n\\(J\\) Competent speaker/hearers tend to judge a present-tense bare epistemic possibility claim (BEP) true only if the prejacent is compatible with their information (whether or not they are the producer of that utterance); otherwise the BEP is judged false.\n\nThey argue that many relativists, in particular Egan and MacFarlane, are committed to this claim. Although Egan and MacFarlane differ on several points (Egan takes relativism about propositional truth to be primary, MacFarlane relativism about utterance truth), it does seem true that (J) is important to both of them. As Knobe and Yalcin put it,\n\nEgan and MacFarlane are both clearly animated by the thought that “people tend to assess epistemic modal claims for truth in light of what they (the assessors) know, even if they realize that they know more than the speaker (or relevant group) did at the time of utterance” (MacFarlane 2011: 160; see also Egan 2007: 2–5, the section entitled “Motivation for relativism: eavesdroppers”).  (Knobe and Yalcin 2014, 3–4)\n\nThis isn’t what their data showed. The subjects were shown speakers whose evidence strongly, but falsely, suggested that Fat Tony was dead. They overwhelmingly said that an utterance of “Fat Tony is dead” is false, but most said an utterance of “Fat Tony might be dead” was true. (Though it is worth noting that the responses displayed a considerably ambivalence; the answers weren’t in line with what either a contextualist or a relativist would straightforwardly predict.) The subjects did say that it would be correct for the speaker who said “Fat Tony might be dead” to retract that utterance once it was clear Fat Tony was alive. But this typically wasn’t because they thought the earlier utterance was false.\nThe point of this study was not to directly target intuitions about disagreement, but rather inter-contextual judgments and felicity of retraction. But the issues are closely related. If subjects who know Fat Tony is alive don’t judge that an utterance of “Fat Tony might be dead” is false, then either they don’t disagree with such an utterer, or the disagreement is, as Huvenes suggests, not the kind that motivates altering our semantics in the direction relativists suggest.\nAs Knobe and Yalcin are careful to note, even if relativists were completely wrong about inter-contextual evaluation of utterances, about disagreement, and about retraction, there are still other arguments for relativism. We’ll end with one other such argument, due primarily to Tamina Stephenson (2007).\nControl and Syntax\nThere is a striking semantic/syntactic phenomenon that epistemic modals and predicates of personal taste share. It’s easiest to describe the phenomenon if we assume a contextualist semantics, though we’ll eventually use the puzzle to cast doubt on that semantics. So assume, for now, that It must be that p is true iff p is guaranteed to be true by the (possibly idealised) knowledge of X, where X is an individual or group supplied by context. (And perhaps the amount of idealisation is context-sensitive too.) And assume that F is tasty is true iff F tastes good to (the possibly idealised version of) X, where X is again supplied by context.\nWhen ‘might’ or ‘tasty’ are not embedded, then X seems like it has to include the speaker, and perhaps not much more than that. Even making some other taster or knower salient does not suffice to change the value of X. For instance, if one utters “Joe is a great cook and connoisseur of fine food. The meals he prepares are always tasty,” the last sentence is not naturally construed as saying that Joe always likes the taste of what Joe cooks, but that the speaker and her hearer do (or will). Or if, to borrow an example from  (Weatherson 2011), we say “Jones must know who the killer is,” the relevant X for interpreting the ‘must’ consists of the speaker and her hearers, not Jones. There is something strange about this; it normally isn’t that hard to make others relevant in a way that makes them the value of a contextually filled variable.\nOr, perhaps, it isn’t normally that hard with one key exception. Some context-sensitive terms have, as part of their meaning, that their extension includes the speaker. Such terms include ‘I’ and ‘we.’ Perhaps ‘must’ and ‘tasty’ are like ‘I’ and ‘we’ in this respect. Except, and here is Stephenson’s key insight, there is a big difference between ‘I’/‘we’ and ‘must’/‘tasty.’ The former still pick out the speaker (and perhaps those near her) under the sceopt of an attitude verb. The latter do not. Consider the natural interpretations of these sentences.\nJoe thinks my gumbo is tasty.\nJoe thinks we must have stolen the candy.\nIn each case, the personal pronouns (‘my,’ ‘we’) have their customary denotations. It is the speaker’s gumbo that is being praised, and the speaker and her friends who are being suspected of theft. But ‘tasty’ and ‘must’ do not behave like that. It isn’t that Joe thinks the speaker likes the taste of her gumbo, but that Joe himself does. And it isn’t that Joe thinks the speaker’s evidence entails her guilt, but that Joe’s evidence does.\nStephenson points out that we get even more dramatic results with more complex sentences. Consider these two sentences.\nMary thinks that Sam thinks it must be raining.\nMary thinks that Sam must think it is raining.  (Stephenson 2007, 490)\nThe value of X for the first ‘must’ has to be Sam, and for the second ‘must’ it has to be Mary. In neither case is it the speaker, and in neither case is it even particularly optional how to interpret the ‘must.’ Compare Mary thinks that Jane likes her house, which is naturally read as three-way ambiguous. (The house might be Jane’s, or Mary’s, or a contextually supplied third party’s.) The general point here is that the X values that the contextualist posits behave rather unlike other implicit or explicit context-sensitive terms.\nThe relativist explanation of this is that epistemic modals, and predicates of personal taste are, to use Stephenson’s phrase, “inherently judge-dependent.” That is, they are inherently dependent on some perspective for their truth. In the terms we’ve been using so far, we need a perpective in the index, and not just in the context, to explain the truth of these claims. When this is combined with a natural view that attitude predicates obligatorily shift the perspective (or judge) parameter, then it just naturally falls out that epistemic modals must take on the perspective of the immediate subjective of the attitude verb. Such a semantics is defended by Stephenson, and by Peter Lasersohn (2005).\nIt falls out of this semantics that there cannot be “exocentric” uses of bare epistemic modals. These are uses of bare epistemic modals that take on the perspective of someone other than the speaker. (A similar point applies to predicates of personal taste.) This is a nice explanation of the fact that it is very hard to generate these exocentric uses. But it arguably overgenerates; there are cases where it seems we do get the exocentric reading. Here is one case from Egan, Hawthorne, and Weatherson (2005).\n\nAnn is planning a surprise party for Bill. Unfortunately, Chris has discovered the surprise and told Bill all about it. Now Bill and Chris are having fun watching Ann try to set up the party without being discovered. Currently Ann is walking past Chris’s apartment carrying a large supply of party hats. She sees a bus on which Bill frequently rides home, so she jumps into some nearby bushes to avoid being spotted. Bill, watching from Chris’s window, is quite amused, but Chris is puzzled and asks Bill why Ann is hiding in the bushes. Bill says, “I might be on that bus”  (Egan, Hawthorne, and Weatherson 2005, 140)\n\nThe natural reading of what Bill says is that for all Ann knows, Bill is on the bus. And this is predicted to be impossible on the type of relativist view that gets us the correct results in the obligatory control cases. Stephenson suggests that there is an ellipsis in Bill’s sentence. It should really be understood as:\nAnn is hiding in the bushes because I might be on that bus.\nSo in a sense, it isn’t a bare epistemic modal; it is in a ‘because’-clause. Moreover, suggests Stephenson, we should take ‘because’-clauses to express something like a person’s conscious reasoning or rationale. So in this case ‘because’ acts like an attitude verb and shift the perspective (or judge) of the epistemic modal. (A similar move, in response to a similar objection, is defended by John MacFarlane (2014, 272ff).)\nThe relativist needs two premises here for the defence to work. The first is that all these exocentric uses are either in the scope of an attitude verb, or are in an explanatory context. The second is that it is fair to treat ‘because’ as sufficiently like an attitude verb for these purposes. A contextualist could well object to either assumption. But even if they grant both assumptions, a contextualist may want to simply resist the whole line of reasoning. At most what these arguments about control show is that ‘might,’ and ‘tasty,’ behave rather differently to other context-sensitive terms. There is nothing inconsistent about just accepting that as a surprising fact. And given how radical a thesis relativism seems to many, accepting relativism as an explanation for these facts about control could well be an excessive reaction. The issues here have not been worked out in nearly as much detail as the issues concerning disagreement and retraction, and we are a long way from having a full accounting of the costs and benefits of the possible dialectical moves.\n\n\nCappelen, Herman, and John Hawthorne. 2009. Relativism and Monadic Truth. Oxford: Oxford University Press.\n\n\nCappelen, Herman, and Torfinn Tomesen Huvenes. 2014. “Relative Truth.” In The Oxford Handbook of Truth, edited by Michael Glanzberg. Oxford: Oxford University Press.\n\n\nCappelen, Herman, and Ernest Lepore. 2005. Insensitive Semantics: A Defence of Semantic Minimalism and Speech Act Pluralism. Oxford: Blackwell.\n\n\nDeRose, Keith. 2009. The Case for Contextualism: Knowledge, Skepticism and Context. Oxford: Oxford.\n\n\nDreier, James. 2009. “Relativism (and Expressivism) and the Problem of Disagreement.” Philosophical Perspectives 23: 79–110. https://doi.org/10.1111/j.1520-8583.2009.00162.x.\n\n\nEgan, Andy. 2007. “Epistemic Modals, Relativism and Assertion.” Philosophical Studies 133 (1): 1–22. https://doi.org/10.1007/s11098-006-9003-x.\n\n\nEgan, Andy, John Hawthorne, and Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nEinheuser, Iris. 2008. “Three Forms of Truth-Relativism.” In Relativising Utterance Truth, edited by Manuel Garcia-Carpintero and Max Kölbel, 187–203. Oxford: Oxford University Press.\n\n\nEvans, Gareth. 1985. “Does Tense Logic Rest on a Mistake?” In Collected Papers, 343–63. Oxford: Clarendon Press.\n\n\nFintel, Kai Fintelvon, and Anthony S. Gillies. 2008. “CIA Leaks.” Philosophical Review 117 (1): 77–98. https://doi.org/10.1215/00318108-2007-025.\n\n\nFrancén, Ragnar. 2010. “No Deep Disagreement for New Relativists.” Philosohical Studies 151 (1): 19–37. https://doi.org/10.1007/s11098-009-9414-6.\n\n\nHarman, Gilbert. 1975. “Moral Relativism Defended.” Philosophical Review 84 (1): 3–22. https://doi.org/10.2307/2184078.\n\n\nHarman, Gilbert, and Judith Jarvis Thomson. 1996. Moral Relativism and Moral Objectivity. Cambridge, MA: Blackwell.\n\n\nHuvenes, Torfinn Thomesen. 2012. “Varieties of Disagreement and Predicates of Taste.” Australasian Journal of Philosophy 90 (1): 167–81. https://doi.org/10.1080/00048402.2010.550305.\n\n\nKamp, Hans. 1971. “Formal Properties of ‘Now’.” Theoria 37: 227–74.\n\n\nKaplan, David. 1989. “Demonstratives.” In Themes from Kaplan, edited by Joseph Almog, John Perry, and Howard Wettstein, 481–563. Oxford: Oxford University Press.\n\n\nKnobe, Joshua, and Seth Yalcin. 2014. “Epistemic Modals and Context: Experimental Data.” Semantics and Pragmatics 7 (10): 1–21. https://doi.org/10.3765/sp.7.10.\n\n\nKölbel, Max. 2002. Truth Without Objectivity. London: Routledge.\n\n\n———. 2004. “Indexical Relativism Vs Genuine Relativism.” International Journal of Philosophical Studies 12: 297–313. https://doi.org/Indexical Relativism vs Genuine Relativism.\n\n\nLasersohn, Peter. 2005. “Context Dependence, Disagreement and Predicates of Personal Taste.” Linguistics and Philosophy 28 (6): 643–86. https://doi.org/10.1007/s10988-005-0596-x.\n\n\nLewis, David. 1979. “Attitudes de Dicto and de Se.” Philosophical Review 88 (4): 513–43. https://doi.org/10.2307/2184646.\n\n\n———. 1980. “Index, Context, and Content.” In Philosophy and Grammar, edited by Stig Kanger and Sven Öhman, 79–100. Dordrecht: Reidel.\n\n\n———. 1989. “Dispositional Theories of Value.” Aristotelian Society Supplementary Volume 63 (1): 113–37. https://doi.org/10.1093/aristoteliansupp/63.1.89.\n\n\nLópez de Sa, Dan. 2008. “Presuppositions of Commonality.” In Relativising Utterance Truth, edited by Manuel Garcia-Carpintero and Max Kölbel, 297–310. Oxford University Press.\n\n\nMacFarlane, John. 2014. Assessment Sensitivity. Oxford: Oxford University Press.\n\n\nRecanati, François. 2007. Perspectival Thought: A Plea for (Moderate) Relativism. Oxford: Oxford University Press.\n\n\nRichard, Mark. 2008. When Truth Gives Out. Oxford: Oxford University Press.\n\n\nSchaffer, Jonathan. 2012. “Necessitarian Propositions.” Synthese 189 (1): 119–62. https://doi.org/10.1007/s11229-012-0097-8.\n\n\nStalnaker, Robert. 1984. Inquiry. Cambridge, MA: MIT Press.\n\n\nStephenson, Tamina. 2007. “Judge Dependence, Epistemic Modals, and Predicates of Personal Taste.” Linguistics and Philosophy 30 (4): 487–525. https://doi.org/10.1007/s10988-008-9023-4.\n\n\nWeatherson, Brian. 2011. “No Royal Road to Relativism.” Analysis 71 (1): 133–43. https://doi.org/10.1093/analys/anq060.\n\n\nWeatherson, Brian, and Andy Egan. 2011. “Epistemic Modals and Epistemic Modality.” In Epistemic Modality, edited by Andy Egan and Brian Weatherson, 1–18. Oxford: Oxford University Press.\n\n\nStrictly speaking, to get MacFarlane’s exact view which kinds of views are relativist we need to complicate things even more. MacFarlane explicitly leaves open the possibility that we have a non-indexical contextualist view about some terms, and a relativist view about others, and expressly says that such a view is a form of relativism. So what we should say is that perspectives are sub-divided into those features where the context of utterance is relevant for utterance truth, and those where the context of assessment is relevant to utterance truth. If the first set of features of \\(p_u\\), and the second set is \\(p_a\\), then the utterance \\(U\\) of sentence \\(S\\) in context \\(c_u\\) will be true, relative to \\(c_a\\), iff \\(f_S(c_u, w(c_u), p_u(c_u), p_a(c_a)) = TRUE\\). MacFarlane’s own view seems to be that \\(p_u\\) will be empty, so the simplified view we’ve given in the text is a fair representation of how he thinks actual natural languages work. But there are possible languages where the complications noted here are relevant.\n\n↩︎\n",
    "preview": "posts/2021-03-12-relativism/tasty.jpg",
    "last_modified": "2021-03-12T14:19:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-intellectual-skill-and-the-rylean-regress/",
    "title": "Intellectual Skill and the Rylean Regress",
    "description": "Intelligent activity requires the use of various intellectual skills. While these skills are connected to knowledge, they should not be identified with knowledge. There are realistic examples where the skills in question come apart from knowledge. That is, there are realistic cases of knowledge without skill, and of skill without knowledge. Whether a person is intelligent depends, in part, on whether they have these skills. Whether a particular action is intelligent depends, in part, on whether it was produced by an exercise of skill. These claims promote a picture of intelligence that is in tension with a strongly intellectualist picture, though they are not in tension with a number of prominent claims recently made by intellectualists.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2017-04-02",
    "categories": [
      "epistemology",
      "skill"
    ],
    "contents": "\n\nContents\nThe Volitional Regress\nPicturing Intelligent Action\nFour Objections\n\nIn recent work about know how, Rylean regress arguments have largely dropped out of focus. They play little role in the anti-intellectualist arguments of various kinds in the papers collected in Bengson and Moffett (2011). They are used as something like target practice by intellectualists like Jason Stanley (2011), who uses the first chapter of his book to dispose of them before getting onto the real business. And even Yuri Cath, who in other work has launched sharp critiques of intellectualism, has argued that the regress arguments for anti-intellectualism don’t work (Cath 2011, 2013). The majority view seems to be that Carl Ginet (1975) basically showed these arguments didn’t work, and it’s time to move onto other considerations for or against intellectualism.\nPublished in Philosophical Quarterly, 67 (2017): 370-386.\nImage from Wikipedia. It is Rex Whistler’s portrait of Ryle.\nI think this isn’t exactly right. In particular, I think regress arguments can be used to show a few different things. For one, they can be used to refute a precisification of this thesis, which plays a key role in some intellectualist arguments.\nOnly volitional actions are normatively assessable.\nOnce we have seen that thesis is false, we need a new picture of how action can be at once intelligent and non-volitional. Some considerations similar to those adduced by Ryle (1949) concerning agents who either concentrate on irrelevant considerations, or ignore relevant ones, show there is a role for intellectual skill that cannot be identified with any piece of knowledge that. And some further considerations, similar to those adduced by Cath (2011), suggest that this intellectual skill can’t even be constituted by a piece of knowledge that. So regress arguments, I’ll argue, can do quite a lot to motivate the thought that there was a lot wrong with the intellectual picture Ryle tried to attack.\nThe position I’m going to be defending is a long way from the strongest kinds of Rylean position that contemporary intellectualists such as Stanley are focussed on arguing against. My focus is primarily on intellectual skill. This has some relevance for debates about know how, though less relevance for debates about the semantics of know how ascriptions. This focus on skill rather than know how ascriptions is hardly novel; it is continuing a trend that we see exemplified in recent work by, inter alia, Carlotta Pavese (2013), Ellen Fridland (2014) and Cheng-Hung Tsai (2014). And in fact that conclusions I’ll draw here are, I think, similar to the ones that Fridland draws.\nOnce we move towards thinking about skill, we can get varieties of anti-intellectualism that are very different from those that were the focus of most philosophical discussion until very recently. For example, the anti-intellectualist view I’m defending is consistent with the following four theses.\nInstances of intellectual skill are usually, and perhaps always, not happily reported using know how ascriptions.\nKnow how ascriptions are rarely, if ever, reports of intellectual skill, and are frequently reports of propositional knowledge.\nIntellectual skill is guided by, and dependent on, propositional knowledge.\nPropositional knowledge is not behaviourally inert.\nNot just is the view consistent with these four, I’m fairly confident that the last three at least are true. But that’s all consistent with the view that intellectual skill is not itself propositional knowledge. And it’s all consistent with the view that we can learn philosophically significant conclusions from Ryle’s regress arguments.\nOne disclaimer before I start. Although this paper is heavily influenced by Ryle (1945, 1949), and sympathetic interpreters of Ryle such as Jennifer Hornsby (2011), I make no attempt at Ryle exegesis here. I think there’s a decent case to be made that Ryle was sympathetic to the position defended here, but I’m going to leave that debate for another day.\nThe Volitional Regress\nDefine a volitional action to be one that is preceeded by a volition to perform that very action. And say an action is normatively assessable if it can properly be assessed using terms like praiseworthy, blameworthy, intelligent or stupid. Note that I’m ruling out assessments as good or bad as versions of normative assessment, in the relevant sense. Someone who has a good digestive system is not, thereby, normatively assessable in the stipulative sense I’m using. Both of these definitions are to an extent stipulative; the terms ‘volitional’ and ‘normative’ can sensibly receive many other definitions. Still, I will stick to these definitions here. In light of those stipulations, consider the following set of propositions.\nOnly volitional actions are normatively assessable.\nThe action of forming a volition is normatively assessable.\nSome public actions, such as making a move in a chess game, are normatively assessable.\nIt should be obvious that this leads to a regress. Whether the kind of regress in question is impossible, or even impractical, is a tricky question. (See Robert K. Meyer (1987) for some of the complications that arise when trying to reason about regresses.) But it is commonly assumed in this literature that the kind of regress that these three premises lead to is problematic.\nSince the third premise is obviously true, the issue is whether the first or second is false. But it seems that second is true as well. Just as we can assess a person’s actions as praise or blameworthy, intelligent or stupid, we can assess the process by which she decided to perform those actions in the same way. Consider two people who make the same, as it turns out great, chess move in the same situation. The first notices an initially appealing counter to her move, and sees after careful thought that it won’t work. The second simply doesn’t notice the counter, and is stumped when her opponent makes it. It seems the first has engaged in a more intelligent practice of volition formation than the second. Or imagine a third player, whose initial analysis of the move starts by considering a recipe for arroz con leche. Unless there turns out to be an unnoticed connection here, this looks even less intelligent than the second player.\nOn the other hand, the first premise is rather unintuitive. To borrow an example from Angela Smith (2005), it is blameworthy to forget a friend’s birthday, although forgettings are rarely volitional. So we must reject 1 or 2, and while 1 is subject to independent counterexample, 2 seems independently plausible. So 1 must be false.1\nThat’s already a substantial conclusion. Something like 1 is behind William Alston’s famous, and influential, arguments against deontological approaches to epistemology (Alston 1988). But the negation of 1 is not a novel claim; I’m saying nothing here that Smith didn’t say in her rejection of the “volitional view of responsibility” (Smith 2005, 238). And similar views have been put forward by other critics of Alston such as Sharon Ryan (2003) and Matthias Steup (2008).\nBut still, the fact that 1 is false seems not to have been sufficiently appreciated in the recent literature on intellectualism. To see one place where it is relevant, consider this set of propositions, which also seem to trigger a regress.\nIntelligent action requires the triggering of a prior representation of knowledge relevant to the action.\nThe triggering of a representation, when done well, is an intelligent action.\nSome public actions, such as making a move in a chess game, are intelligent.\nAgain, these propositions obviously trigger a regress, and that seems like good evidence to take one of them to be false. This is very similar to one of the regresses Jason Stanley considers in chapter 2 of his (2011). And Stanley thinks the false proposition is 5. He writes “Triggering a representation can be done poorly or well. But this does not show it can be done intelligently or stupidly.” (Stanley 2011, 16) Indeed, he writes that since “triggering representations is something we do automatically” (Stanley 2011, 16) a statement like 5 is a “manifest implausibility” (Stanley 2011, 16). But the argument here relies on 1. If you think things done non-volitionally can be intelligent or stupid, it isn’t too much of a stretch to think that things done automatically can be intelligent or stupid. Indeed, Smith’s birthday example is already enough to undermine Stanley’s point; forgetting a friend’s birthday seems automatic in the sense he has in mind, but is also stupid.\nMore generally, it seems very intuitive to describe everyday cases in such a way that 5 must be true. For example, Billy asks Suzy whether she thinks Jill’s party will be a success. There are a lot of things that are common knowledge between the two of them. One is that Jill is a proficient party host. Another is that Jill has invited all of their colleagues, including Jack. Another is that parties which Jack attends are rarely successes. But Suzy thinks for a minute, remembers that Jack is away in Ohio, and says that it will be a success.\nIt was smart of Suzy to think about Jack’s whereabouts. It wasn’t, perhaps, necessary. If she’d just reasoned from Jill’s general proficiency to the success of the party, she would have got to the right conclusion. But it was better to note a possible complication, and check that it wouldn’t actually get in the way.\nIt would have been stupid to perform the same activity for many other kinds of possible complications. If Suzy had thought to herself, “The party will be a disaster if there’s an alien invasion in the middle of it, but there’s no reason to think the aliens will invade just now, so I’ll keep on thinking it will be a success,” that would have been stupid. Other possible complications are not stupid to consider, but they are intellectual mistakes. The party won’t be a success if there’s a police raid in the middle, based on a mistaken view the police have about where a particular drug dealer lives. Police do make mistakes, so even if Jill isn’t a drug dealer, this could be a genuine concern, depending on how nearby the mistakes are. But if the nearest mistake was a botched raid in a neighbouring state in the previous year, it’s wrong for Suzy to worry about this before answering Billy’s question.\nStanley’s view has to be that I’ve been misusing adjectives systematically through the last two paragraphs. I shouldn’t have said that it was smart of Suzy to consider Jack’s whereabouts, or that it would have been stupid to consider the alien invasion. Rather, it was just her cognitive system working well when she considered Jack, and would have been working poorly had she considered the aliens, and sub-optimally had she considered the police. This doesn’t seem at all the natural way to describe the case to me, in part because I’m not sure I see the difference Stanley is hinting at. Intelligence just is the good operation of the cognitive system, and stupidity its poor operation.\nSo these two regresses lead to two interesting conclusions. First, some non-volitional actions are normatively assessable. Second, intelligent action does not always require the prior triggering of a representation of relevant knowledge. Both of these are interesting. Both of these are negations of part of what you might consider “the intellectualist picture.” (Cath (2013) notes that Ryle often refers to the regresses as arguments against this picture, not against any particular thesis.) But neither of them get us very close to a distinction between know how and know that, or between intellectual skill and know that. The next section addresses some ways we might move closer to arguments against more central intellectualist claims.\nPicturing Intelligent Action\nAs noted in the introduction, my plan is not to offer an argument with regress like premises, and the conclusion that intellectual skill is distinct from propositional knowledge, or that know how is distinct from propositional knowledge. What I do want to do is sketch a picture of human intelligence (at a very high level of generality) that presupposes that intellectual skill is not identical to propositional knowledge, and suggest some considerations to the effect that no similarly plausible picture exists in which intellectual skill and propositional knowledge are identified. The thought here is not that the only way out of the regress involves distinguishing skill and knowledge – and perhaps distinguishing know how from know that – but rather that the best way out does.\nStart with a well known, if not obviously authentic, exchange.2\n\nOscar Wilde: I wish I’d said that.\nJames McNeill Whistler: You will Oscar; you will.\n\nAssuming this really happened, that’s a clever response. It’s an occupational hazard of philosophers to think that the ability to come up with quick, clever responses is somehow central to intelligence. But we can reject that wildly implausible view without thinking that it’s wrong to think of these quips as a manifestation of a kind of intelligence.\nNow let’s think of how someone could have come up with this response. Even before we start researching the neural patterns behind quips like this, we can be pretty sure the following is not what happened in Whistler’s brain. He first made an exhaustive list of all possible responses, from “Green ideas sleep furiously” to what he actually said, then figured out which would be best, then produced the best one. On this wildly implausible model, the reply would be intelligent because it would reflect the speaker’s ability to properly evaluate this list of responses. That’s implausible because the list is simply too big. Indeed, it is in principle infinitely large. The list is too big to survey not just consciously, but subconsciously.\nComing up with a response like this requires first coming up with a narrower list of possible responses, and then evaluating which is best from that list.3 There’s a romantic model of intellect where the list in question consists of just the reply actually issued. On this model the perfect reply appears fully and perfectly formed in the mind of the intelligent person. Now such a model may often fit the phenomenology, but I don’t think we should give that much credit. It’s an empirical question how many possible replies are represented in the mind in a situation like this, before the chosen reply is issued. What’s not an empirical issue is whether the list of possible replies that is represented in the mind is finite or infinite. It simply must be finite, which means that there must be better and worse lists to consider. And that suggests that there is some skill involved in coming up with the list.\nOne could reject this last conclusion. One could try saying that the coming up with a list of possible replies is no manifestation of skill, but the skill is only involved in the evaluation and selection of replies. But this seems to generate a bizarre explanation about why the less skilled interlocutor comes up with worse replies. The model, presumably, is that the lack of skill does not explain having the wrong list of replies to choose between. Rather, what explains their less skilled reply must simply be that they misevaluated the possible replies. But that doesn’t fit the observed data. It’s much easier to see of someone else’s reply that it was clever than it is to come up with a clever reply.\nIt could also be objected that the model I’ve suggested is much too simple. It isn’t just that the mind issues a list of options, then evaluates them, and then selects the best. A more plausible model involves more recursive steps. The mind first generates a list of options, selects the best, then generates a list of refinements of that best option, selects the best of those, and so on. Perhaps when we consider superficial forms of intelligence, such as quips, it makes sense to consider a ‘one-step’ model, where a list is generated and evaluated, followed by a speech. But when one is choosing one’s words carefully, as in say Wilde’s writing, the simple model I’ve described feels much too simple.\nBut although the simple model is too simple for considered writing, the general structure must be right. Even a writer working at a leisurely pace, such as Joyce taking decades to write Finnegans Wake, doesn’t have time to consider, even subconsciously, all possible constructions. There are still too many. And nor is it true that the difference between Joyce’s skill and ours is that he realises the value of the sentences we all represent. The rest of us didn’t simply misjudge the value of “Nobirdy aviar soar anywing to eagle it” (Joyce 1939/2012, 505); we simply didn’t token it. The ability to token mental representations like that is part of what Joyce’s genius consists in.\nI’ve focussed so far on cases where it is a priori implausible that human thinkers start by surveying the range of possible things they could do. It is also interesting to look at cases where this is in principle possible, but doesn’t seem to happen in practice. There have been, traditionally, major differences in the style of play between human and computer chess players. (Since so many young players learn from machines these days, Kasparov (2010) suggests these differences are diminishing.4) This isn’t necessarily because humans can’t consider all options on the chess board. Usually there will be fewer than a hundred available moves, and a human could consider each. But that isn’t, it seems, how humans think. They don’t allocate equal resources to working through each of the possible options. As a result, computers often come up with surprising kinds of moves. Now computers are actually very good at chess, so these pre-deliberative allocations of cognitive resources may not have been optimal. Perhaps it would have been better for traditional chess players to spend more time thinking through unlikely progressions of the game. But it is evidence that even when we could use an unintelligent method for beginning inquiry, namely recursively generating the possible options, we prefer to use intelligent methods.\nSo intelligent action, at least in humans in the kinds of situations humans normally find themselves in, consists in part of making intelligent choices about where to start inquiry. Given that intelligent action need not be volitional, as we established above, it isn’t surprising that being intelligent consists in part in starting in the right places. But perhaps this intelligence is just itself a kind of knowledge. It is, we might suspect, just the knowledge of what a good starting point will be. Or, since we will want to start with all and only the considerations relevant to a given inquiry, it is just knowledge of what is relevant.\nThe resulting picture is both perfectly intellectualist, and immune to the regresses considered above. The intelligent person knows what is relevant to what inquiry. Her choice of starting points is guided by this knowledge. (The ‘guidance’ metaphor recurs frequently in Stanley’s work.) This isn’t because it leads to a volition to start just here. Such a volition would be self-defeating, since in the relevant sense of ‘start,’ by the time this volition is formed, one has already started, and indeed started elsewhere. Nor can she be guided by even a triggered representation of this knowledge of relevance. Again, if that happens, she is in the relevant sense starting elsewhere. But perhaps propositional knowledge can guide directly; not by generating volitions, and without even being represented anew.\nNow I don’t think this picture is right. But it isn’t incoherent either, and it takes work to see why it isn’t right.\nOne bad argument against this picture starts with the idea that skills are active, while knowledge is passive. The thought is that the person who knows a lot is like the Tortoise in Louis Carroll’s dialogue (Carroll 1895), only able to add more premises but never to reach a conclusion. It is only with skill that we can get to the conclusion. Stanley rightly objects to this argument on the grounds that it just isn’t true that knowledge is passive in the relevant sense. We should not, as Stanley puts it, “over-intellectualize knowing that.” (Stanley 2012, 773). (A similar point is made in Stalnaker (2012).) Knowing that p is not just a matter of having p written in a knowledge box somewhere in the brain; it can in part be constituted by active dispositions.\nA better argument looks at the very different modal profiles of intellectual skill and knowledge of relevance. Someone can know that something is irrelevant and yet lack the skill to ignore it; or they can know that something is relevant and yet lack the skill to consider it in a timely manner. Examples from the other direction, where there is skill without knowledge, are a little more contentious, but we’ll look at some possible cases of those too. But first we’ll run through two examples to show how easy it is to have knowledge without skill.\nAlice has spent a lot of money on video-conferencing equipment. But it isn’t working at all well, and she now has to decide whether to try and patch it into something better, or buy a whole new system. She knows the sunk cost fallacy is a fallacy; that buying a new system would make the previous purchases a waste is no reason to not buy a new system, especially if doing so is good value compared to the cost of buying a ‘patch.’ But she can’t bring herself to ignore this fact when deliberating. Even though she eventually makes the right decision and buys new equipment, she takes much longer about this than she would have if, say, the existing equipment was old enough that she could easily conceptualise it as obsolete.\nBob is trying to solve a puzzle about the properties of functions from rationals to rationals. He knows that it is often helpful, when solving such puzzles, to transform the puzzle into one about functions from ordered pairs of integers to ordered pairs of integers. He knows that in the sense that if you asked him whether it could be useful to consider that transformation of the puzzle, he would immediately say yes, and this answer would come with the phenemenology of recollection, not of new insight. But no one does ask him that question, and the transformation in question simply never occurs to Bob. Since the untransformed puzzle is very hard, while the transformed puzzle is manageable, Bob never solves the problem.\nIt seems to me that what’s happened in both cases is that the agent has some knowledge, but is incapable of using it. What they lack is a skill. In particular, they lack what Fridland (2014, 2746) calls ‘selective, top-down, automatic attention.’ Alice keeps attending to something she should not, even though she knows she should not. Bob fails to attend to something he should, although in some sense he knows that is what he should attend to. Bob’s case is one of the reasons I find the picture of skill presented by Stanley and Williamson (2017) unhelpful. They say skill is a disposition to form knowledge. But Bob has the important knowledge. The disposition he lacks is the disposition to activate that knowledge, and let it guide deliberation. That’s what constitutes his lack of skill.\nIt’s true that knowledge isn’t completely passive. If Alice never appealed to the fact that the sunk cost fallacy is a fallacy in her reasoning, we wouldn’t say that she knows it. If none of Bob’s answers were guided by the existence of natural and useful transformations between rational numbers and ordered pairs of integers, we wouldn’t say he knows such transformations are natural and useful. I’m here agreeing with Stanley and Stalnaker that knowledge is itself a kind of disposition. And intellectual skill is a kind of disposition too. But they are very different dispositions. In particular, they have very different triggering conditions. Bob lacks some skill because he does not call to mind this fact about rational numbers right now. He has the salient knowledge about rational numbers because he is disposed to use the facts in question often enough.\nSo intellectual skill and knowledge of relevance have different manifestation conditions, and so they are not identical. But we can say something stronger than that. The cases of Alice and Bob are not in any way unusual. Examples where we forget the salience of some consideration, or can’t get an irrelevant point out of our heads, are frequent. In principle, one could respond to the arguments I’ve made so far by saying that while knowledge of relevance is not identical to skill, nevertheless the two are as closely linked as, say, a material object and the matter that constitutes it. And if I had to resort to bizarre cases of the kind we torture introductory students with to make my point, I’d say that would be the right response. But given how normal Alice and Bob’s cases are, this seems like the wrong move. Skill and knowledge don’t just come apart in theory, they come apart in practice, frequently.\nFour Objections\nSo far I’ve defended three theses that are in tension with some forms of intellectualism. They are:\nSome non-volitional actions are normatively assessable.\nNot all intelligent action is preceded by the triggering of representations of relevant knowledge.\nIntellectual skill, in particular the intellectual skill associated with starting inquiry in the right place, is not identical to any piece of propositional knowledge.\nWhile this doesn’t show that, for instance, know how and know that are distinct, and is completely silent on what we should say about know how ascriptions, it does undermine some intellectualist programs. I’ll conclude with some objections either to the arguments I’ve put forward, or to their significance.\nObjection: Even if all of this is true, there may still be a sense in which intellectualism is true. After all, it could still be that knowledge guides action in a suitable way. (Compare (Stanley 2011, 2).)\nReply: This could be true. Whether it is a win for intellectualism depends a bit on the boring question of how we settle the term ‘intellectualism,’ and a bit on more interesting questions about priority. Let’s start by distinguishing five theories we might call intellectualist.\nIdentity Intellectualism\n- The possession of an intellectual skill just is the possession of a piece of knowledge.\n\nConstitution Intellectualism\n- The possession of an intellectual skill is, always, constituted by a piece of knowledge.\n\nWeak Constitution Intellectualism\n- The possession of an intellectual skill is, often, constituted by a piece of knowledge.\n\nCausal Intellectualism\n- The possession of an intellectual skill is, always, caused by the possession of a piece of knowledge.\n\nWeak Causal Intellectualism\n- The possession of an intellectual skill is, often, caused by the possession of a piece of knowledge.\n\nThis paper has been arguing against Identity Intellectualism. I think the falsity of this is as much as we could reasonably hope to prove using regress arguments. (I think I’m here agreeing with Wiggins (2009) and Hornsby (2011).) And the considerations behind the regress argument do, I think, show it to be false. If someone wants to insist that by intellectualism, they mean something weaker than this, I’m not going to quarrel over terminology. I’ll just note that Identity Intellectualism is an interesting, and false, thesis.\nThe arguments here are clearly not arguments against either form of Weak Intellectualism. Indeed, they are naturally understood as the kind of cases that confirm Weak Intellectualism. Mathematics students, like Bob, train by learning a lot of mathematical facts. And it’s hard to see how they could develop the relevant skills without knowing some important facts. This is, I suspect, the general case. Skillfully bringing the right considerations to bear on a problem requires, and is probably the causal consequence of, knowing a lot of relevant facts. (Tsai (2014) makes clear how one can simultaneously hold that skills are in part constituted by knowledge of facts without having an intellectualist picture of skill.)\nBut what of the other two intellectualist theories? Do we have reason to think that there are some skills that are not constituted by, or not caused by, the possession of factual knowledge? One way to quickly show that would be to show that there can be skills without the related knowledge. Perhaps that’s not just sufficient for rejecting Constitutive/Causal Intellectualism, but necessary. If knowledge without skills is possible, as in Alice and Bob’s cases, and skills without knowledge were impossible, that asymmetry would call out for explanation. And something in the vicinity of Constitutive or Causal Intellectualism would be a very good candidate explanation.\nThere are (at least) two promising routes to showing that there can be skills without knowledge. One is due to Imogen Dickie (2012). She argues that since there are so many different routes to skill than there are to knowledge, we should expect that there will be cases of skill that are causally prior to knowledge. Jason Stanley (2012) replies that Dickie’s argument assumes an overly narrow conception of propositional knowledge. This is a fascinating debate, but I don’t have anything useful to add to it, so I’ll just note the existence of this route, and move on.\nThe other route is due to Yuri Cath (2011). He suggests that facts in virtue of which a person might lose propositional knowledge do not always bring about a loss of knowledge that. I’m going to sketch a Cath-style argument that we can have intellectual skills without knowledge. I think the argument has some force, though there are more ways to resist it than there are to resist the argument against Identity Intellectualism.\nRoss and Rachel are economics students taking an exam. They are given a hard question asking about the likely effects of an exogenous shock, say an earthquake affecting an area the supplies crucial raw materials, on some related markets. The question is hard, with the relevant causal pathways being interconnected and often opposing. The only plausible way forward is to use a model and search for equilibrium points in the model. That’s what Ross and Rachel have both been taught to do. And in fact both of them quickly select the right kind of model, with just the right amount of complexity in it to answer the question without being overburdened, and set out on the difficult algebra involved in solving the question.\nSo far it looks like both Ross and Rachel have shown intellectual skill. Now it turns out Ross and Rachel have very different views about the role of models in economic thinking. (My own thinking about models has been heavily influenced by (Strevens 2008 ch. 8) and Davey (2011), and I rely on their insights in what follows.) These models involve, as all models do, some serious idealisation. Most notably, they assume that all the relevant actors are perfectly rational utility maximisers. Rachel hasn’t given much thought to this assumption, though she knows it to be literally false. But if pressed, she would say some reasonably sensible things about why she was using the model. For one thing, the familiar failures of human rationality aren’t obviously relevant to the puzzle being presented. For another, they’ve been taught that using these models is a good way to solve problems, and that testimonial evidence carries some weight. And for another, it’s an exam, and it is likely that questions have been selected to test how well students can use the models they have been taught. If those are her background, implicit, views, I think it is plausible to say that Rachel knows that the model is relevant to the exam question, even if she couldn’t produce a theory of idealisations in economics of the standards of the best philosophers.\nRoss’s views about models are rather different. He thinks the familiar models in economics work, when they do, because the background assumptions are strictly and literally true. He thinks economic agents are utility maximisers, and the apparent evidence to the contrary is due to sloppy experimental design. He thinks markets are always in general equilibrium. And so he thinks that the only sources of error in predictions we can make about markets are from errors about things like the costs of extracting raw materials after the earthquake. This perspective is, of course, grossly mistaken. Moreover, Ross thinks that if the assumptions were not correct, there would be no point in using the models. This too is a mistake, though perhaps not as dramatic as his other mistakes.\nNow even if Ross and Rachel aren’t thinking about these philosophical views about the nature of models, I think they are relevant to whether each of them know that the models are relevant to the puzzle. In particular, I think Rachel does know that the models are relevant, while Ross’s belief that they are relevant is more like a lucky guess than a piece of knowledge. Still, I think we should say that Ross showed skill in using this model rather than a more or less complex model, or a different kind of model, or no model at all. So he is a case of intellectual skill without knowledge of relevance.\nI don’t think this case is conclusive. I can think of at least four ways someone might reasonably object to the case.\nIt might be argued that despite his false views about why the models are relevant, he really does know that they are relevant. In other words, we would have another counterexample, to be added to those discussed by Warfield (2005) and Luzzi (2010), to the theory that false beliefs cannot generate knowledge.\nIt might be argued that Ross is not really skilled, since it is a matter of luck that the falsity of his beliefs does not lead him to false conclusions here.\nIt might be argued that although Ross doesn’t know that this model is relevant, his skill is constituted by, or caused by, some other knowledge he has.\nIt might be argued that the broad picture of the role of idealisations in scientific reasoning that I’m adopting from Strevens and Davey is mistaken, and this fatally undermines my use of the case to argue against intellectualism.\nI don’t think these arguments are going to ultimately work. But it’s clear we are a long way from Rylean regress arguments here. And that’s where I think the debate about regress arguments should end. We have a good argument against Identity Intellectualism. And we have some suggestive considerations that seem to tell against Constitutive and Causal Intellectualism, but whether these arguments ultimately work will depend on considerations independent of the regress.\nObjection: Stanley and Williamson (2017) have recently defended the idea that skill is a disposition to form knowledge. And they back this up with empirical analysis of intelligent motor skills, especially drawing on the survey by Yarrow, Brown, and Krakauer (2009). Is this kind of intellectualism subject to the regress worries?\nReply: Once we are taking the dispositions themselves to be the skills, not the underlying knowledge, it feels that we are a long way from traditional intellectualism. But the view is independently interesting, and it is a useful segue to thinking about the relationship between intellectual skills, as conceived of in this paper, and motor skills.\nI’ve already mentioned that the Bob example does not seem to fit well with Stanley and Williamson’s paradigm. And there is something suspicious about a theory of physical skill that divorces it so strongly from the physical. To be a skilled batsman requires more than dispositions to get knowledge, one might suspect. Stanley and Williamson have a reply to this suspicion. They write,\n\nConsider the difference between someone who can bench-press a maximum of 100 pounds and someone who can bench press 150 pounds. We may suppose that both employ the same technique; only brute strength makes the difference between them. Both are equally skilled ...Any view of skill must account for such cases. In particular, it must explain why strength, speed, and stamina are not themselves skills.(Stanley and Williamson 2017, 9, page references to preprint)\n\nBut even if strength is not a skill, it might be a prerequisite for a skill. A batsman whose degenerative back condition means he lacks the flexibility to deploy his trademark pull shot has lost a skill, even if he hasn’t lost any dispositions to form knowledge. There is a puzzle as to why qualitative physical differences matter so much to skill attributions why quantitative ones do not. If you can’t turn to pull the ball, you’ve lost a skill, but if a muscle strength decline reduces the power of your pull shot, your skills haven’t declined. But that difference doesn’t justify making skills entirely cognitive.\nStill, there is a cognitive angle. One central point of this paper is completely consistent with Stanley and Williamson’s picture; motor skills often require forming the right knowledge. The skilled batsman doesn’t just pick up many characteristics of the bowler’s delivery, they pick up the ones that are most relevant to the trajectory of the ball. As the Bob example shows, they also have to activate that knowledge for it really to be a skill, but that’s not a new objection.\nThere is one other cognitive aspect of motor skill that Yarrow, Brown, and Krakauer (2009) draw attention to, and which fits very nicely with the theme of this paper. It’s a specific instance of a much more wide-ranging skill. Sometimes an agent knows that in some time some evidence, drawn from a large space, will come in. She will shortly thereafter have to act in response to the evidence. She has some time to plan now. What should she do? In many such cases, backwards induction is impossible; there are too many possible pieces of evidence that could come in, and planning for each of them is a waste of resources. On the other hand, not planning at all is also a waste of the time she now has, and will lack once the evidence comes in. The solution is to do some planning. And there is a real skill involved in getting the resource allocation right, and neither wasting effort planning for unlikely scenarios, nor wasting the ability to be prepared before one needs to act.\nYarrow, Brown, and Krakauer (2009, 590–91) suggest the same thing happens at a very low level. Highly skilled athletes are making many places in advance of knowing exactly how they will act. Part of the skill involved is allocating the right resources to each of these planning activities. Many of them will ultimately be wasteful, since they are plans for eventualities that do not arise. And one failure condition is that a single plan is not selected, and the agent performs some combination of multiple plans that are worse than either one plan. That failure state is part of the evidence that there is this low-level planning going on before actions. But it is a real skill, and part of the skill is focussing on just the right things.\nSo motor skills often have as a constituent part intellectual skills. Some of those skills are closely tied to knowledge; for instance, having priors that track frequencies. Sometimes the skill involved is in focussing on the evidence that the posterior probability is maximally sensitive to, and reacting to that evidence. Sometimes the skill is not attending to evidence that is just going to be unhelpful noise in the activity in question (Yarrow, Brown, and Krakauer 2009, 589). And sometimes it is in allocating just the right resources to forward planning. All of these seem like intellectual skills, and parts of motor skills. We could try to squeeze all of them into a framework of being dispositions to form knowledge, but it seems more perspicuous to just present the plurality of ways in which the intellect and the body interact, rather than trying to find a single framework.\nObjection: Appeal to skill does not stop the regress. If we need to posit something, say a skill, that comes between the possession of knowledge and the use of knowledge in reasoning or action, then we also to posit something that comes between the possession of a skill, and the use of that skill in reasoning or action. (Compare (Stanley 2011, 26)).\nReply: What I’m going to say here is similar to what Jeremy Fantl (2011) said in a response to an earlier version of Stanley’s argument, so I’ll be brief. Skills are dispositions. We don’t need to posit anything that comes between the disposition and its triggering. If a string is disposed to produce a middle C when struck, and it is struck, we don’t need to posit an extra intermediary between the striking and the note. Dispositions stop regresses.\nBut, you might insist, couldn’t the same be true of knowledge? After all, on a broadly functionalist construal of the mental, knowledge is a kind of disposition. My reply is in theory knowledge could stop such a regress, but in practice it is unlikely. An agent could be facing a problem where the possible considerations and options can be enumerated without using any particular skill, and the options are few enough that they can be each considered in turn. That is the situation an agent playing a relatively simple game might face. But it isn’t the general human condition. In practice, we face problems every moment where it requires skill to bring the right considerations to bear, at least given the processing capacities we have available.\nObjection: There are semantic arguments that attributions of know how are attributions of propositional knowledge. This shows that Ryle was wrong to draw a broad distinction between know how and know that.\nReply: I’m not making any claims about either know how or about ‘know how.’ I am making some claims about skill, and those imply some claims about ‘skill.’ But I’m sympathetic to the idea that reports of know how are often reports of some kind of practical propositional knowledge. I certainly haven’t offered any arguments, nor I think any considerations in the direction of an argument, against this view.\nIndeed, there are a lot of intellectualst positions that I’m not arguing against here. Anti-intellectualism is often tied up with the view that there is an important distinction between theoretical and practical fields. The arguments I’ve developed here suggest that if there is such a distinction, then proving mathematical theorems is on the ‘practical’ side. I think that’s a strange enough conclusion that it is time to change our terminology. That’s why I’ve talked about the distinction between intellectual skills and knowledge, not the distinction (if such there is) between know how and know that, or between praxis and theory.\n\n\n\nAdams, Robert Merrihew. 1985. “Involuntary Sins.” Philosophical Review 94 (1): 3–31. https://doi.org/10.2307/2184713.\n\n\nAlston, William. 1988. “The Deontological Conception of Epistemic Justification.” Philosophical Perspectives 2: 257–99. https://doi.org/10.2307/2214077.\n\n\nBengson, John, and Marc Moffett, eds. 2011. Knowing How. Oxford: Oxford University Press.\n\n\nCarroll, Lewis. 1895. “What the Tortoise Said to Achilles.” Mind 4 (14): 278–80. https://doi.org/10.1093/mind/iv.14.278.\n\n\nCath, Yuri. 2011. “Knowing How Without Knowing That.” In Knowing How, edited by John Bengson and Marc Moffett, 113–35. Oxford: Oxford University Press.\n\n\n———. 2013. “Regarding a Regress.” Pacific Philosophical Quarterly 94 (3): 358–88. https://doi.org/10.1111/papq.12004.\n\n\nDavey, Kevin. 2011. “Idealizations and Contextualism in Physics.” Philosophy of Science 78 (1): 16–38. https://doi.org/10.1086/658093.\n\n\nDempsey, Luke, ed. 2012. Monty Python’s Flying Circus: Complete and Annotated...all the Bits. New York: Black Dog & Leventhal Publishers.\n\n\nDickie, Imogen. 2012. “Skill Before Knowledge.” Philosophy and Phenomenological Research 85 (3): 737–45. https://doi.org/10.1111/j.1933-1592.2012.00638.x.\n\n\nFantl, Jeremy. 2011. “Ryle’s Regress Defended.” Philosophical Studies 156 (1): 121–30. https://doi.org/10.1007/s11098-011-9800-8.\n\n\nFridland, Ellen. 2014. “They’ve Lost Control: Reflections on Skill.” Synthese 191 (12): 2729–50. https://doi.org/10.1007/s11229-014-0411-8.\n\n\nGinet, Carl. 1975. Knowledge, Perception and Memory. Dordrecht: Riedel.\n\n\nHadley, Frank A. 1903. “Whistler, the Man, as Told in Anecdote.” Brush & Pencil 12 (5): 334–59. https://doi.org/10.2307/25505918.\n\n\nHornsby, Jennifer. 2011. “Ryle’s Knowing How, and Knowing How to Act.” In Knowing How, edited by John Bengson and Marc Moffett, 80–98. Oxford: Oxford University Press.\n\n\nJoyce, James. 1939/2012. Finnegans Wake. Oxford World’s Classics. Oxford: Oxford University Press.\n\n\nKasparov, Garry. 2010. “The Chess Master and the Computer.” The New York Review of Books. The New York Review of Books. http://www.nybooks.com/articles/archives/2010/feb/11/the-chess-master-and-the-computer/.\n\n\nLuzzi, Federico. 2010. “Counter-Closure.” Australasian Journal of Philosophy 88 (4): 673–83. https://doi.org/10.1080/00048400903341770.\n\n\nMeyer, Robert K. 1987. “God Exists!” Noûs 21 (3): 345–61. https://doi.org/10.2307/2215186.\n\n\nPavese, Carlotta. 2013. “The Unity and Scope of Knowledge.” PhD thesis, Rutgers University, New Brunswick.\n\n\nRyan, Sharon. 2003. “Doxastic Compatibilism and the Ethics of Belief.” Philosophical Studies 114 (1-2): 47–79. https://doi.org/10.1023/A:1024409201289.\n\n\nRyle, Gilbert. 1945. “Knowing How and Knowing That.” Proceedings of the Aristotelian Society 46 (1): 1–16. https://doi.org/10.1093/aristotelian/46.1.1.\n\n\n———. 1949. The Concept of Mind. New York: Barnes; Noble.\n\n\nSmith, Angela M. 2005. “Responsibility for Attitudes: Activity and Passivity in Mental Life.” Ethics 115 (2): 236–71. https://doi.org/10.1086/426957.\n\n\nStalnaker, Robert. 2012. “Intellectualism and the Objects of Knowledge.” Philosophy and Phenomenological Research 85 (3): 754–61. https://doi.org/10.1111/j.1933-1592.2012.00640.x.\n\n\nStanley, Jason. 2011. Know How. Oxford: Oxford University Press.\n\n\n———. 2012. “Replies to Dickie, Schroeder and Stalnaker.” Philosophy and Phenomenological Research 85 (3): 762–78. https://doi.org/10.1111/j.1933-1592.2012.00641.x.\n\n\nStanley, Jason, and Timothy Williamson. 2017. “Skill.” Noûs 51 (4): 713–26. https://doi.org/10.1111/nous.12144.\n\n\nSteup, Matthias. 2008. “Doxastic Freedom.” Synthese 161 (3): 375–92. https://doi.org/10.1007/s11229-006-9090-4.\n\n\nStrevens, Michael. 2008. Depth: An Account of Scientific Explanations. Cambridge, MA: Harvard University Press.\n\n\nTsai, Cheng-hung. 2014. “The Structure of Practical Expertise.” Philosophia 42 (2): 539–54. https://doi.org/10.1007/s11406-013-9513-7.\n\n\nWarfield, Ted A. 2005. “Knowledge from Falsehood.” Philosophical Perspectives 19: 405–16. https://doi.org/10.1111/j.1520-8583.2005.00067.x.\n\n\nWiggins, David. 2009. “Knowing How to and Knowing That.” In Wittgenstein and Analytic Philosophy: Essays for p. M. S. Hacker, edited by Hans-Johann Glock and John Hyman, 263–77. Oxford: Oxford University Press.\n\n\nYarrow, Kielan, Peter Brown, and John W. Krakauer. 2009. “Inside the Brain of an Elite Athlete: The Neural Processes That Support High Achievement in Sports.” Nature Reviews Neuroscience 10 (8): 585–96. https://doi.org/10.1038/nrn2672.\n\n\nThis argument is obviously rather quick, and I doubt will persuade someone already convinced of 1. For much more extensive arguments against 1, see the Smith, Ryan and Steup articles cited in the text, plus Adams (1985).↩︎\nI thought this example was purely fictional, coming from the Monty Python sketch reproduced in Dempsey (2012, 741). But Ben Wolfson pointed out to me that it’s recorded as a true story in Hadley (1903, 255).\nIt’s actually striking how few really good off-the-cuff quips there are in recorded history. The famous one attributed to Wilde, “I have nothing to declare but my genius,” is probably apocryphal, and in any case sounds prepared. Lists of famous come-backs and ripostes are usually crowded with written responses. Word play is hard.↩︎\nOr, perhaps even more plausibly, coming up with a short list of possible openings, choosing the best, and doing what one can to figure out how to complete the response while uttering the start of it. Thanks here to Ben Wolfson.↩︎\nThanks to Bernard Kobes and John Collins for helpful discussions about the chess examples.↩︎\n",
    "preview": "posts/2021-01-03-intellectual-skill-and-the-rylean-regress/ryle.jpg",
    "last_modified": "2021-02-04T15:24:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-02-interest-relative-invariantism/",
    "title": "Interest-Relative Invariantism",
    "description": "An opinionated survey of the state of the literature on interest-relative invariantism.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2017-03-17",
    "categories": [
      "epistemology",
      "interest-relativity"
    ],
    "contents": "\n\nContents\nIntroduction\nMotivations\nOdds and Stakes\nWhat Kind of Interests?\nGlobal or Partial\nBelief, Justification and Interest\nDebunking Objections\nDirect Objections\n\nIntroduction\nOne of the initial motivations for epistemological contextualism was that the appropriateness of self-ascriptions of knowledge seemed to depend, in some circumstances, on factors that were traditionally thought to be epistemologically irrelevant. So whether our hero S was prepared to say “I know that p” would depend not just on how strong S’s evidence for p was, or how strongly they believed it, but on factors such as how much it mattered whether p was true, or what alternatives to p were salient in their thought or talk.\n\nPublished in Routledge Handbook of Epistemic Contextualism, edited by Jonathan Jenkins Ichikawa, 2017, 240-253.\nIt was immediately noted that this data point, even if accepted, is consistent with a number of theories of the truth of knoweldge ascriptions. It might be that things like stakes and salient alternatives affect the assertability conditions of knowledge ascriptions, but not their truth conditions  (Rysiew 2017). But let’s assume that we’ve convinced ourselves that this isn’t right, and that whether S can truly (and not just appropriately) say “I know that p” depends on things like the stakes or salient alternatives.\nIt still doesn’t follow that contextualism is true. It might be that in all contexts, whether an utterance of “S knows that p” is true depends on the stakes for S, or on the salient alternatives for S. That would be true, the idea is, whether S is talking about herself, or someone else is talking about her. The stakes, or salient alternatives, would affect the truth conditions of S’s utterance not because she is the one doing the talking, but the one being talked about. The practical and theoretical situation of the ascribee of the knowledge ascription may be relevant, even if the practical and theoretical situation of the ascribor need not be.\nThis line of thought leads to the idea that knowledge itself is interest-relative. Whether an utterance here and now of “S knows that p” is true, i.e., whether S knows that p, depends on how much it matters to S that p is true, or on which alternative are salient to S. The thesis that knowledge is interest-relative is consistent with contextualism. It could be that whether a knowledge ascription is true depends on the interests of both the ascriber, and the ascribee. In this entry, however, I’m going to largely focus on the view that knowledge is interest-relative, but contextualism is false. On this view, the interests of the ascribee do matter to the truth of a knowledge ascription, but the interests of the ascribee do not.\nThis view is naturally called interest-relative invariantism, since it makes knowledge interest-relative, but it is a form of anti-contextualism, i.e., invariantism. The view is sometimes called subject-sensitive invariantism, since it makes knowledge relevant to the stakes and salient alternatives to the subject. But this is a bad name; of course whether a knowledge ascription is true is sensitive to who the subject of the ascription is. I know what I had for breakfast and you (probably) don’t. What is distinctive is which features of the subject’s situation that interest-relative invariantism says are relevant, and the name interest-relative invariantism makes it clear that it is the subject’s interests. There is one potential downside to this name; it suggests that the practical interests of the subject are relevant to what they know. I intend to use the predicate ‘interest-relative’ to pick out a class of theories, including the theory floated by John Hawthorne (2004), where the options that are salient to the subject matter to what the subject knows. If forced to defend the name, I’d argue that salience is relevant to the theoretical interests of the subject, if not necessarily to their practical interests. But the name is still potentially misleading; my main reason for using it is that ‘subject-sensitive’ is even more misleading. (I’ll shorten ‘interest-relative invariantism’ to IRI in what follows. I’ll return to the question of practical and theoretical interests in section 4.)\nThere are a number of ways to motivate and precisify IRI. I’ll spend most of this entry going over the choice points, starting with the points where I think there is a clearly preferably option, and ending with the choices where I think it’s unclear which way to go. Then I’ll discuss some general objections to IRI, and say how they might be answered.\nMotivations\nThere are two primary motivations for IRI. One comes from intuitions about cases, the other from a pair of principles. It turns out the two are connected, but it helps to start seeing them separately.\nJason Stanley (2005) starts with some versions of the ‘bank cases’ due originally to Keith DeRose (1992). These turn on idiosyncratic, archaic details of the US payments system, and I find it hard to have clear intuitions about them. A cleaner pair of examples is provided by Angel Pinillos (2012); here are slightly modified versions of his examples.\n\nAnkita and Bojan each have an essay due. They have, surprisingly, written word for word identical papers, and are now checking the paper for typos. The papers have no typos, and each student has checked their paper twice, with the same dictionary, and not found any typos. They are, in general, equally good at finding typos, and have true beliefs about their proficiency at typo-spotting.\nThe only difference between them concerns the consequence of a typo remaining. If the paper is a borderline A/A- paper, a typo might mean Ankita gets an A- rather than an A. But the grade doesn’t matter to her; she’s already been accepted into a good graduate program next year so long as she gets above a C. But Bojan’s instructor is a stickler for spelling. Any typo and he gets a C on the paper. And he has a very lucrative scholarship that he loses if he doesn’t get at least a B on this paper. (Compare the Typo-Low and Typo-High examples in Pinillos (2012, 199).)\n\nThe intuition that helps IRI is that Ankita knows she has no typos in her paper, and should turn it in, while Bojan does not know this, and should do a third (and perhaps fourth or fifth) check. Contextualists have a hard time explaining this; in this very context I can say “Ankita knows her paper has no typos, but Bojan does not know his paper has no typos.” If the intuition is right, it seems to support interest-relativity, since the difference in practical situation between Ankita and Bojan seems best placed to explain their epistemic difference. Alternatively, if there is a single context within which one can truly say \"Ankita knows her paper has no typos’‘, and’‘Bojan does not know his paper has no typos’’, that’s again something an interest-invariant contextualism can’t explain. Either way, we have an argument from cases for a form of interest-relativity.\nThe argument from principles takes off from the idea that knowledge plays an important role in good deliberation, and that knowledge does not require maximal confidence. It is easiest to introduce with an example, though note that we aren’t going to rely on epistemic intuitions about the example. Chika looked at the baseball scores last night before going to bed and saw that the Red Sox won. She remembers this when she wakes up, though she knows that she does sometimes misremember baseball scores. She is then faced with the following choice: take the red ticket, which she knows pays $1 if the Red Sox won last night, and nothing otherwise or the blue ticket, which she knows pays $1 iff 2+2=4, and nothing otherwise. Now consider the following principle, named by Jessica Brown (2014):\nK-Suff\nIf S knows that p, then S can rationally take p as given in practical deliberation.\n\nThe following trio seems to be inconsistent:\nChika knows the Red Sox won last night.\nChika is rationally required to take the blue ticket.\nK-Suff is true.\nBy 1 and 3, Chika can take for granted that the Red Sox won last night. So the value of the red ticket, for her, is equal to its value conditional on the Red Sox winning. And that is $1. So it is at least as valuable as the blue ticket. So she can’t be rationally required to take the blue ticket. Hence the three propositions are inconsistent.\nThis is worrying for two reasons. For one thing, it is intuitive that Chika knows that the Red Sox won. For another thing, it seems this form of argument generalises. For almost any proposition at all, if Chika knows the red ticket pays out iff that proposition is true, she should prefer the blue ticket. So she knows very little.\nHow could this argument be resisted? One move, which we’ll return to frequently, is to deny K-Suff. Maybe Chika’s knowledge that the Red Sox won is insufficient; she needs to be certain, or to have some higher order knowledge. But denying K-Suff alone will not explain why Chika should take the blue ticket. After all, if K-Suff is false, the fact that Chika knows the payout terms of the tickets is not in itself a reason for her to choose the blue ticket.\nSo perhaps we could deny that she is rationally required to choose the blue ticket. This does seem extremely unintuitive to me. Intuitions around here do not seem maximally reliable, but this is a strong enough intuition to make it worthwhile to explore other options.\nAnd IRI provides a clever way out of the dilemma. Chika does not know the Red Sox won last night. But she did know that, before the choice was offered. Once she has that choice, her knowledge changes, and now she does not know. The intuition that she knows is explained by the fact that relative to a more normal choice set, she can take the fact that the Red Sox won as a given. And scepticism is averted because Chika does normally know a lot; it’s just in the context of strange choices that she loses knowledge.\nThe plotline here, that principles connecting knowledge and action run up against anti-sceptical principles in contrived choice situations, and that IRI provides a way out of the tangle, is familiar. It is, simplifying greatly, the argumentative structure put forward by Hawthorne (2004), and by Fantl and McGrath (2002, 2009), and by Weatherson (2012). It does rely on intuitions, but they are intuitions about choices (such as that Chika should choose the blue ticket), not about knowledge directly.\nSome discussions of IRI, especially that in Hawthorne and Stanley (2008) use a converse principle. Again following the naming convention suggested by Jessica Brown (2014), we’ll call this K-Nec.\nK-Nec\nAn agent can properly use p as a reason for action only if she knows that p.\n\nI’ll mostly set the discussion of K-Nec aside here, since my preferred argument for IRI, the argument from Chika’s case, merely relies on K-Suff. But it is interesting to work through how K-Nec helps plug a gap in the argument by cases for IRI.\nBuckwalter and Schaffer (2015) argue that the intuitions behind Pinillos’s examples are not as solid as we might like. It’s true that experimental subjects do say that Bojan has to check the paper more times than Ankita does before he knows that the paper contains no typos. But those subjects also say he has to check more times before he believes that the paper has no typos. And, surprisingly, they say that he has to check more time before he guesses the paper has no typos. They suggest that there might be interest-relativity in the modal ‘has’ as much as in the verb ‘knows.’ To say someone ‘has’ to X before they Y, typically means that it is improper, in some way, to Y without doing X first. That won’t be a problem for the proponent of IRI as long as at least in some of the cases Pinillos studies, the relevant senses of propriety are connected to knowledge. And that’s plausible for belief; Bojan has to know the paper is typo free before he (properly) believes it. At least, that’s a plausible move given K-Nec.1\nThere is one other problem for argument from cases for IRI. Imagine that after two checks of the paper, we tell Bojan that Ankita’s paper is a duplicate of hers, and she has checked her paper in just the same way he has checked his. And we tell him that Ankita does not overly care whether her paper is typo-free, but is confident that it is. We then ask him, does Ankita know her paper is typo free? Many philosophers think Bojan should answer “No” here. And that isn’t something IRI can explain. According to IRI, he should say, “I don’t know.” He can’t say Ankita does know, since he doesn’t know their common paper has no typos. But it’s hard to see why he should deny knowledge. Keith DeRose (2009, 185) thinks this case is particularly hard for IRI to explain, while Brian Kim (2016) offers some possible explanations. This objection doesn’t tell against the claim that knowledge is interest-relative, but it does threaten the invariantism. An interest-relative contextualist should say that everyone should deny Bojan knows his paper is typo free, and Bojan should deny Ankita knows her paper is typo-free.\nOdds and Stakes\nInterest-relative invariantism says that the interests of the subject matter to what she knows. This is a fairly vague statement though; there are a number of ways to make it precise. Right now I have interests in practical questions (such as whether I should keep writing or go to lunch) and in theoretical questions (such as whether IRI is true). Do both kinds of interests matter? We’ll return to that question in the next section. For now we want to ask a prior question: when do practical interests matter for whether a subject knows that p? There are two main answers to this question in the literature.\nStakes\nWhen the agent has a possible bet on p that involves large potential losses, it is harder to know that p.\n\nOdds\nWhen the agent has a possible bet on p that involves long odds, it is harder to know that p.\n\nThe difference between these two options becomes clear in a simple class of cases. Assume the agent is faced with a choice with the following structure:\nThere is a safe option, with payout S.\nAnd there is a risky option, with good payout G if p is true, and bad payout B if p is false.\nThese choices need not involve anything like a ‘bet,’ in the ordinary folk sense. But they are situations where the agent has to make a choice between a path where the payouts are p-dependent, and one where they are independent of p. And those are quite common situations.\nThe Stakes option says that the relevant number here is the magnitude \\(S-B\\). If that is large, then the agent is in a high-stakes situation, and knowledge is hard. If it is low, then the agent is in a low stakes situation, and knowledge is relatively easy. (Perhaps the magnitude of \\(G-S\\) is relevant as well, though the focus in the literature has been on examples where \\(S-B\\) is high.)\nThe Odds option says that the relevant number is is the ratio:\n\\[\\frac{S-B}{G-S}\\] If that number is high, the agent faces a long odds bet, and knowledge is hard. If that number is low, the agent faces a short odds bet, and knowledge is relatively easy.\nIf our motivation for IRI came from cases, then it is natural to believe Stakes. Both Bojan and Chika face bets on p at long odds, but intuition is more worried about whether Bojan knows that p than whether Chika does. (At least my intuition is worried about whether Bojan knows, and I’ve seen little evidence that Chika’s case is intuitively a case of non-knowledge.)\nBut if our motivation for IRI came from principles, then it is natural to believe Odds. One way to think of the argument from principles for IRI is that it is a way to make all four of the following intuitive claims true:\nAgents should maximise evidential expected utility; i.e., they should choose the option whose expected utility is highest if the utilities are the agent’s own, and the probabilities are the evidential probabilities given the agent’s evidence.\nIf an agent knows that p, they can ignore possibilities where p is false; i.e., they can make whatever choice is the rational choice given p.\nChika cannot ignore possibilities where the Red Sox lost; she should consider those possibilities because it is in virtue of them that the evidential expected utility of taking the red ticket is higher.\nAgents with Chika’s evidence, background and dispositions typically know that the Red Sox won.\nThe first three principles imply that Chika does not know the Red Sox won. The only way to square that with the anti-sceptical fourth principle is to say that Chika is in some way atypical. And the only way she has been said to be atypical is in the practical choices she faces. But note it is not because she faces a high-stakes choice: precisely one dollar is at stake. It is because she faces a long (indeed infinitely long) odds bet.\nIn the general case we discussed above, agents maximise expected utility by taking the risky choice iff:\n\\[\\frac{S-B}{G-S} < \\frac{Pr(p)}{1-Pr(p)}\\] where \\(Pr(p)\\) is the probability of p given the agent’s evidence. The actual magnitudes at play don’t matter to what choice maximses expected utility, just the odds the agent faces. So if one’s motivation to keep IRI is to square expected utility maxmisation with natural principles about knowledge and action, it seems the relevant feature of practical situations should be the stakes agents face.\nWhy could it seem stakes matter then? I think it is because in high stakes situations, the odds an agent faces are typically long ones. It is much easier to lose large amounts of utility than to gain large amounts of utility. Bojan stands to lose a lot from a typo in his paper; he doesn’t stand to lose much by taking the time to check it over. So a high stakes situation will, at least typically, be a long odds situation. So if we say the odds the agent faces are relevant to what they know, we can explain any intuition that the stakes at play are relevant.\nJessica Brown (2008, 176) also notes that cases where the agent faces long odds but low stakes raise problems for the stakes-based version of IRI.\nWhat Kind of Interests?\nLet’s return to the question of whether theoretical interests are relevant to knowledge, or only practical interests. There is some precedent for the more restrictive answer. Stanley’s book on IRI is called Knowledge and Practical Interests. And he defends a theory on which what an agent knows depends on the practical questions they face. But there are strong reasons to think that theoretical reasons matter as well.\nIn the previous section, I suggested that agents know that p only if they would maximise expected utility by choosing the choice that would be rational given p. That is, agents know that p only if the answer to the question “What choice maximises expected utility?” is the same unconditionally as it is conditional on p. My preferred version of interest-relative invariantism generalises this approach. An agent knows that p only if the rational answer to a question she faces is the same unconditionally as it is conditional on p. What it is for an agent to face a question is dependent on the agent’s interests. If that’s how one thinks of IRI, the question of this section becomes, should we restrict questions the agent faces to just being questions about what choice to make? Or should they include questions that turn on her thoeretical interests, but which are irrelevant to choices before her. There are two primary motivations for allowing theoretical interests as well as practical interests to matter.\nThe first comes from the arguments for what Jeremy Fantl and Matthew McGrath call the Unity Thesis  (Fantl and McGrath 2009, 73–76). They are interested in the thesis that whether or not p is a reason for an agent is independent of whether the agent is engaged in practical or theoretical deliberation. But we don’t have to be so invested in the ideology of reasons to appreciate their argument. Note that if only practical interests matter, then the agent should come up with different answers to the question “What to do in situation S” depending on whether the agent is actually in S, or they are merely musing about how one would deal with that situation. And it is unintuitive that this should matter.\nLet’s make that a little less abstract. Imagine Chika is not actually faced with the choice between the red and blue tickets. In fact, she has no practical decision to make that turns on whether the Red Sox won. But she is idly musing over what she would do if she were offered the red ticket and the blue ticket. If she knows the Red Sox won, then she should be indifferent between the tickets. After all, she knows they will both return $1. But intuitively she should think the red ticket is preferable, even in the abstract setting. And this seems to be the totally general case.\nThe general lesson is that if whether one can take p for granted is relevant to the choice between A and B, it is similarly relevant to the theoretical question of whether one would choose A or B, given a choice. And since those questions should receive the same answer, if p can’t be known while making the practical deliberation between A and B, it can’t be known while musing on whether A or B is more choiceworthy.\nIn Weatherson (2012) I suggest another reason for including theoretical interests in what’s relevant to knowledge. There is something odd about the following reasoning: The probability of p is precisely x, therefore p, in any case where \\(x < 1\\). It is a little hard to say, though, why this is problematic, since we often take ourselves to know things on what we would admit, if pushed, are purely probabilistic grounds. The version of IRI that includes theoretical interests allows for this. If we are consciously thinking about whether the probability of p is x, then that’s a relevant question to us. Conditional on p, the answer to that question is clearly no, since conditional on p, the probability of p is 1. So anyone who is thinking about the precise probability of p, and not thinking it is 1, is not in a position to know p. And that’s why it is wrong, when thinking about p’s probability, to infer p from its high probability.\nPutting the ideas so far together, we get the following picture of how interests matter. An agent knows that p only if the evidential probability of p is close enough to certainty for all the purposes that are relevant, given the agent’s theoretical and practical interests. Assuming the background theory of knowledge is non-sceptical, this will entail that interests matter.\nGlobal or Partial\nSo far I’ve described three ways to refine the defence of IRI.\nThe motivation could come from cases or principles.\nThe relevant feature that makes it hard to have knowledge could be that the agent faces a high-stakes choice, or a long-odds choice.\nOnly practical interests may be relevant to knowledge, or theoretical interests may matter as well.\nFor better or worse, the version of IRI I’ve defended has fairly clear commitments on all three; in each case, I prefer the latter option. From here on, I’m much less sure of the right way to refine IRI.\nIRI, like contextualism, was introduced as a thesis about knowledge. But it need not be restricted that way. It could be generalised to a number of other epistemically interesting notion. At the extreme, we could argue that every epistemologially interesting notion is interest-relative. Doing so gives us a global version of IRI.\nJason Stanley (2005) comes close to defending a global version. He notes that if one has both IRI, and a ‘knowledge first’ epistemology  (Williamson 2000), then one is a long way to towards globalism. Even if one doesn’t accept the whole knowledge first package, but just accepts the thesis that evidence is all and only what one knows, then one is a long way towards globalism. After all, if evidence is interest-relative, then probability, justification, rationality, and evidential support are interest-relative too.\nKatherine Rubin (2015) objects to globalist versions of IRI. But the objections she gives turn, as she notes, on taking stakes not odds to be relevant.\nIf a non-global version of IRI could be made to work, it would have some theoretical advantages. It’s nice to be able to say that Chika should take the blue ticket because the evidential probability of the Red Sox winning is lower than the evidential probability of two plus two being four. But that won’t be a non-circular explanation if we also say that something is part of Chika’s evidence in virtue of being known.\nOn the other hand, the motivations for interest-relativity of knowledge seem to generalise to all other non-gradable states. In ordinary cases, Chika could use the fact that the Red Sox won as a given in practical or theoretical reasoning. That is, she could properly treat it as evidence. But she can’t treat it as evidence when deciding which ticket to take. So at least what she can properly treat as evidence seems to be interest-relative, and from there it isn’t obvious how to deny that evidence itself is interest-relative too.\nThere remains a question of whether gradable notions, like epistemic probabilities, are also interest-relative. One of the aims of my first paper on IRI  (Weatherson 2005) was to argue that probabilistic notions are interest-invariant while binary notions are interest-relative. But if propositions that are part of one’s evidence have maximal probability (in the relevant sense of probability), and evidence is interest-relative, that combination won’t be sustainable.\nIn short, while the non-global version of IRI allows for some nice reductive explanations of why interests matter, the global version is supported by the very intuitions that motivated IRI. There is a danger here that whatever way the IRI theorist goes, they will run into insuperable difficulties. Ichikawa, Jarvis, and Rubin (2012) argue strongly that this danger is real; there is no plausible way to fill out IRI. I’m not convinced that the prospects are quite so grim, but I think this is one of the more pressing worries for IRI.\nBelief, Justification and Interest\nIf we decide that not everything in epistemology is interest-relative, then we face a series of questions about which things are, and are not, interest relative. One of these concerns belief. Should we say that what an agent believes is sensitive to what her interests are?\nNote that the question here concerns whether belief is constitutively related to interests. It is extremely plausible that belief is causally related to interests. As Jennifer Nagel (2008) has shown, many agents will react to being in a high-stakes situation by lowering their confidence in relevant propositions. In this way, being in a high-stakes situation may cause an agent to lose beliefs. This is not the kind of constitutive interest-relativity that’s at issue here, though the fact this happens makes it harder to tell whether there is such a thing as constitutive interest-relativity of belief.\nI find it useful to distinguish three classes of views about beliefs and interests.\nBeliefs are not interest-relative. If knowledge is interest-relative, the interest-relativity is in the conditions a belief must satisfy in order to count as knowledge.\nBeliefs are interest-relative, and the interest-relativity of belief fully explains why knowledge is interest-relative.\nBeliefs are interest-relative, but the interest-relativity of belief does not fully explain why knowledge is interest-relative.\nIn Weatherson (2005), I suggested an argument for option 2. I now think that argument fails, for reasons given by Jason Stanley (2005). I originally thought option 2 provided the best explanation of cases like Chika’s. Assume Chika does the rational thing, and takes the blue ticket. She believes it is better to take the blue ticket. But that would be incoherent if she believed the Red Sox won. So she doesn’t believe the Red Sox won. But she did believe the Red Sox won before she was offered the bet, and she hasn’t received any new evidence that they did not. So, assuming we can understand an interest-invariant notion of confidence, she is no less confident that the Red Sox won, but she no longer believes it. That’s because belief is interest-relative. And if all cases of interest-relativity are like Chika’s, then they will all be cases where the interest-relativity of belief is what is ultimately explanatory.\nThe problem, as Stanley had in effect already pointed out, is that not all cases are like Chika’s. If agents are mistaken about the choice they face, the explanation I offered for Chika’s case won’t go through. This is especially clear in cases where the mistake is due to irrationality. Let’s look at an example of this. Assume Dian faces the same choice as Chika, and this is clear, but he irrationally believes that the red ticket pays out $2. So he prefers the red ticket to the blue ticket, and there is no reason to deny he believes the Red Sox won. Yet taking the red ticket is irrational; he wouldn’t do it were he rational. Yet it would be rational if he knew the Red Sox won. So Dian doesn’t know the Red Sox won, in virtue of his interests, while believing they did.\nNote this isn’t an argument for option 1. Everything I said about Dian is consistent with the Chika-based argument for thinking that belief is interest-relative. It’s just that there are cases where the interest-relativity of knowledge can’t be explained by the interest-relativity of belief. So I now think option 3 is correct.\nWe can ask similiar questions about whether justified belief is interest-relative, and whether if so this explains the interest-relativity of knowledge. I won’t go into as much detail here, save to note that on my preferred version of IRI, Dian’s belief that the Red Sox won is both justified and rational. (Roughly, this is because I think his belief that the Red Sox won just is his high credence that the Red Sox won, and his high credence the Red Sox won is justified and rational. I defend this picture at more length in \\[Weatherson2005;\\]. And while that paper makes some mistaken suggestions about knowledge, I still think what it says about belief and justification is broadly correct.) That is, Dian has a justified true belief that the Red Sox won, but does not know it. This is, to put it mildly, not the most intuitive of verdicts. I suspect the alternative verdicts lead to worse problems elsewhere. But rather than delving deeper into the details of IRI to confirm whether that’s true, let’s turn to some objections to the view.\nDebunking Objections\nMany arguments against IRI are, in effect, debunking arguments. The objector’s immediate conclusion is not that IRI is false, but that it is unsupported by the arguments given for it.\nArguments that people do not have the intuition that, for exaple, Bojan lacks knowledge that his paper is typo-free, do not immediately show thtat IRI is false. That’s because the truth of IRI can be made compatible with that intuition in two ways. For one thing, it is possible that people think Bojan knows because they think Bojan betting that his paper is typo free is, in the circumstances, a good bet.2 For another thing, intuitions around here might be unreliable. Remember that one of the original motivations for IRI was that it was the lowest cost solution to the preface paradox and lottery paradox. We shouldn’t expect intuitions to be reliable in the presence of serious paradox. That consideration cuts both ways; it makes debunking objections to arguments for IRI from intuitions about cases look very promising. And I think those objections are promising; but they don’t show IRI is false.\nSimilarly, objections to the premises of the argument from principles don’t strictly entail that IRI is false. After all, IRI is an existential thesis; it says sometimes interests matter. The principles used to defend it are universal claims; they say (for example) it is always permissible to act on knowledge. Weaker versions of these principles might still be consistent with, or even supporting of, IRI. But this feels a little desperate. If the premises of these arguments fail, then IRI looks implausible.\nBut there are still two methodological points worth remembering. Sometimes it seems that critics of principles like K-Suff reason that K-Suff entails IRI, and IRI is antecedently implausible, so we should start out suspicious of K-Suff. Now why might IRI be antecedently implausible?\nI think to some extent it is because it is thought to be so revolutionary. The denial of interest-relativity is often taken to be a “traditional” view. This phrasing appears, for example, in Boyd (2016), and in Ichikawa, Jarvis, and Rubin (2012), and even in the title of Buckwalter (2014). And if this were correct, that would be a mark against interest-relativity. The “inherited experience and acumen of many generations of men”  (Austin 1956, 11) should not be lightly forsaken. The problem is that it isn’t true that IRI is revolutionary. Indeed, in historical terms there is nothing particularly novel about contemporary IRI. As Stephen R. Grimm (2015) points out, you can see a version of the view in Locke, and in Clifford. What’s really radical, as Descartes acknowledged, is to think the perspective of the Cartesian meditator is the right one for epistemology.\nPerhaps what is unintuitive about IRI is that it makes knowledge depend on factors that are not ‘truth-directed,’ or ‘truth-conducive.’ There is a stronger and weaker version of the principle that might be being appealed to here. The stronger version is that IRI makes practical matters into one of the factors on which knowledge depends, and this is implausible. But IRI doesn’t do this. It is consistent with IRI to say that only truth-conducive features of beliefs are relevant to whether they amount to knowledge, but how much of each feature one needs depends on practical matters. The weaker principle is that IRI makes knowledge counterfactually sensitive to features irrelevant to the truth, justification or reliability of the belief. This is true, but it isn’t an objection to IRI. Any theory that allows defeaters to knowledge, and defeaters to those defeaters, will make knowledge counterfactually sensitive to non-truth-conducive features in just the same way. And it is independently plausible that there are defeaters to knowledge, and they can be defeated.3\nThese are all reasons to think that IRI is not antecedently implausible. There is one reason to think it is antecedently plausible. On a functionalist theory of mind, belief is a practical notion. And it is plausible that knowledge is a kind of success condition for belief. Now it’s possible to have non-practical success conditions for a state our concept of which is practical. But I don’t find that a natural starting assumption. It’s mucn more intuitive, to me at least, that the norms of belief and the metaphysics of belief would be tightly integrated. And that suggests that IRI is, if anything, a natural default.\nThat’s not an argument for IRI, or of course for K-Suff. And there are important direct objections to K-Suff. Jessica Brown (2008) and Jennifer Lackey (2010) have examples of people in high stakes situations who they say are intuitively described as knowing something, but not being in a position to act on it. I’m sympathetic to the two-part reply that Masashi Kasaki (2014) makes to these examples. The first thing to note is that these are hard cases, in areas where several paradoxes (e.g., lottery, preface, sceptical) are lurking. Intuitions are less reliable than usual around here. But another thing to notice is that it is very hard to say what actions are justified by taking p for granted in various settings. Brown and Lackey both describe cases where doctors have lots of evidence for p, and given p a certain action would maximise patient-welfare, but where intuitively it would be wrong for the doctor to act that way. As it stands, that’s a problem for IRI only if doctors should maximise epistemic expected patient-welfare, and that principle isn’t true. Kasaki argues that there isn’t a way to fill out Lackey’s example to get around this problem, and I suspect the same is true for Brown’s example.\nFinally, note that K-Suff is an extensional claim. Kenneth Boyd (2016) and Baron Reed (2014) object to a principle much stronger than K-Suff: the principle that what an agent knows should explain why some choices are rational for them. Both of them say that if IRI is inconsistent with the stronger principle, that is a serious problem for IRI. (In Boyd’s case this is part of an argument that IRI is unmotivated; in Reed’s case he takes it to be a direct objection to IRI.) Now I think IRI is inconsistent with this principle. Chika doesn’t know the Red Sox won because she can’t rationally choose the red ticket, not the other way around. But I don’t see why the principle is so plausible. It seems plausible to me that something else (e.g., evidence) explains both rational choice and knowledge, and the way it explains both things makes IRI true.\nDirect Objections\nLet’s close with direct arguments against IRI. There are two kinds of arguments that I won’t address here. One of these is the argument, developed in Ichikawa, Jarvis, and Rubin (2012) that there isn’t a good way to say how far interest-relativity should extend. As I noted above, I agree this is a deep problem, and don’t think there is a good answer to it in the existing literature. The other kind are objections that only apply to the Stakes version of IRI, not the Odds version. One instance of this kind is the Dutch Book argument deployed by Baron Reed (2014). I think several instances of that kind of argument are successful. But the theory they succeed against is not IRI, but a sub-optimal version of IRI. So I’ll stick to objections that apply to the Odds version.\nIRI does allow knowledge to depend on some unexpected factors. But so do most contemporary theories of knowledge. Most contemporary theories allow for knowledge to be defeated in certain ways, such as by available but unaccessed evidence  (Harman 1973, 75), or by nearby possibilities of error  (Goldman 1976), or by mistakes in the background reasoning. The last category of cases aren’t really contemporary; they trace back at least to Dharmottara  (Nagel 2014, 58). And contemporary theories of knowledge also allow for defeaters to be defeated. Once we work through the details of what can defeat a defeater, it turns out many surprising things can affect knowledge.\nIndeed, for just about any kind of defeater, it is possible to imagine something that in some ways makes the agent’s epistemic position worse, while simultaneously defeating the defeater.4 If interests matter to knowledge because they matter to defeaters, as is true on my version of IRI, we should expect strange events to correlate with gaining knowledge. For example, it isn’t surprising that one can gain knowledge that p at exactly the moment one’s evidential support for p falls. This consequence of IRI is taken to be obviously unacceptable by Eaton and Pickavance (2015), but it’s just a consequence of how defeaters generally work.\nIRI has been criticised for making knowledge depend on agents not allowing agents to get knowledge by not caring, as in these vivid quotes:\n\nNot giving a damn, however enviable in other respects, should not be knowledge-making.  (Russell and Doris 2009, 433)\n\n\nIf you don’t now whether penguins eat fish, but want to know, you might think … you have to gather evidence. \\[But if IRI\\] were correct, though, you have another option: You could take a drink or shoot heroin.  (Cappelen and Lepore 2006, 1044–45)\n\nLet’s walk through Cappelen and Lepore’s case. IRI says that there are people who both have high confidence that penguins eat fish, and they have this confidence for reasons that are appropriately connected to the fact that penguins eat fish. But one of them really worries about sceptical doubts, and so won’t regard the question of what penguins eat as settled. The other brushes off excessive sceptical doubts, and rightly so; they are, after all, excessive. IRI says that the latter knows and the former does not. If the former were to care a little less, in particular if they cared a little less about evil demons and the like, they’d know. Perhaps they could get themselves to care a little less by having a drink. That doesn’t sound like a bad plan; if a sceptical doubt is destroying knowledge, and there is no gain from holding on to it, then just let it go. From this perspective, Cappelen and Lepore’s conclusion does not seem like a reductio. Excessive doubt can destroy knowledge, so people with strong, non-misleading evidence can gain knowledge by setting aside doubts. And drink can set aside doubt. So drink can lead to knowledge.5\nBut note that the drink doesn’t generate the knowledge. It blocks, or defeats, something that threatens to block knowledge. We should say the same thing to Russell and Doris’s objection. Not giving a damn, about scepticism for example, is not knowledge-making, but it is knowledge-causing. In general, things that cause by double prevention do not make things happen, although later things are counterfactually dependent on them  (Lewis 2004). And the same is true of not caring.\nFinally, it has been argued that IRI makes knowledge unstable in a certain kind of way  (Lutz 2014; Anderson 2015). Practical circumstances can change quickly; something can become a live choice and cease being one at a moment’s notice. If knowledge is sensitive to what choices are live, then knowledge can change this quickly too. But, say the objectors, it is counterintuitive that knowledge changes this quickly.\nNow I’m not sure this is counterintuitive. I think that part of what it takes to know p is to treat the question of whether p as closed. It sounds incoherent to say, “I know a is the F, but the question of who is the F is still ope.” And whether a question is treated as open or closed does, I think, change quite rapidly. One can treat a question as closed, get some new reason to open it (perhaps new evidence, perhaps an interlocutor who treats it as open), and then quickly dismiss that reason. So I’m not sure this is even a problem.\nBut to the extent that it is, it is only a problem for a somewhat half-hearted version of IRI. The puzzles the objectors raise turn on cases where the relevant practical options change quickly. But even once a practical option has ceased to be available, it can be hard in practice to dismiss it from one’s mind. One may often still think about what to do if it becomes available again, or about exactly how unfortunate it is that the option went away. As long as theoretical as well as practical interests matter to knowledge, it will be unlikely that knowledge will be unstable in just this way. Practical interests may change quickly; theoretical ones typically do not.\n\n\nAnderson, Charity. 2015. “On the Intimate Relationship of Knowledge and Action.” Episteme 12 (3): 343–53. https://doi.org/10.1017/epi.2015.16.\n\n\nAustin, J. L. 1956. “A Plea for Excuses.” Proceedings of the Aristotelian Society 57 (1): 1–30. https://doi.org/10.1093/aristotelian/57.1.1.\n\n\nBoyd, Kenneth. 2016. “Pragmatic Encroachment and Epistemically Responsible Action.” Synthese 193 (9): 2721–45. https://doi.org/10.1007/s11229-015-0878-y.\n\n\nBrown, Jessica. 2008. “Subject-Sensitive Invariantism and the Knowledge Norm for Practical Reasoning.” Noûs 42 (2): 167–89. https://doi.org/10.1111/j.1468-0068.2008.00677.x.\n\n\n———. 2014. “Impurism, Practical Reasoning and the Threshold Problem.” Noûs 48 (1): 179–92. https://doi.org/10.1111/nous.12008.\n\n\nBuckwalter, Wesley. 2014. “Non-Traditional Factors in Judgments about Knowledge.” Philosophy Compass 7 (4): 278–89. https://doi.org/10.1111/j.1747-9991.2011.00466.x.\n\n\nBuckwalter, Wesley, and Jonathan Schaffer. 2015. “Knowledge, Stakes and Mistakes.” Noûs 49 (2): 201–34. https://doi.org/10.1111/nous.12017.\n\n\nCappelen, Herman, and Ernest Lepore. 2006. “Shared Content.” In The Oxford Handbook of Philosophy of Language, edited by Ernest Lepore and Barry C. Smith, 1020–55. Oxford: Oxford University Press.\n\n\nDeRose, Keith. 1992. “Contextualism and Knowledge Attributions.” Philosophy and Phenomenological Research 52 (4): 913–29. https://doi.org/10.2307/2107917.\n\n\n———. 2009. The Case for Contextualism: Knowledge, Skepticism and Context. Oxford: Oxford.\n\n\nEaton, Daniel, and Timothy Pickavance. 2015. “Evidence Against Pragmatic Encroachment.” Philosophical Studies 172: 3135–43. https://doi.org/10.1007/s11098-015-0461-x.\n\n\nFantl, Jeremy, and Matthew McGrath. 2002. “Evidence, Pragmatics, and Justification.” Philosophical Review 111 (1): 67–94. https://doi.org/10.2307/3182570.\n\n\n———. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nFeltz, Adam, and Chris Zarpentine. 2010. “Do You Know More When It Matters Less?” Philosophical Psychology 23 (5): 683–706. https://doi.org/10.1080/09515089.2010.514572.\n\n\nGoldman, Alvin I. 1976. “Discrimination and Perceptual Knowledge.” The Journal of Philosophy 73 (20): 771–91. https://doi.org/10.2307/2025679.\n\n\nGrimm, Stephen R. 2015. “Knowledge, Practical Interests and Rising Tides.” In Epistemic Evaluation: Purposeful Epistemology, edited by David K. Henderson and John Greco, 117–37. Oxford: Oxford University Press.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHawthorne, John, and Jason Stanley. 2008. “Knowledge and Action.” Journal of Philosophy 105 (10): 571–90. https://doi.org/10.5840/jphil20081051022.\n\n\nIchikawa, Jonathan Jenkins, Benjamin Jarvis, and Katherine Rubin. 2012. “Pragmatic Encroachment and Belief-Desire Psychology.” Analytic Philosophy 53 (4): 327–43. https://doi.org/10.1111/j.2153-960X.2012.00564.x.\n\n\nKasaki, Masashi. 2014. “Subject-Sensitive Invariantism and Isolated Secondhand Knowledge.” Acta Analytica 29: 83–98. https://doi.org/10.1007/s12136-013-0215-3.\n\n\nKim, Brian. 2016. “In Defense of Subject-Sensitive Invariantism.” Episteme 13 (2): 233–51. https://doi.org/10.1017/epi.2015.40.\n\n\nLackey, Jennifer. 2010. “Acting on Knowledge.” Philosophical Perspectives 24: 361–82. https://doi.org/10.1111/j.1520-8583.2010.00196.x.\n\n\nLasonen-Aarnio, Maria. 2014a. “Higher-Order Evidence and the Limits of Defeat.” Philosophy and Phenomenological Research 88 (2): 314–45. https://doi.org/10.1111/phpr.12090.\n\n\n———. 2014b. “The Dogmatism Puzzle.” Australasian Journal of Philosophy 92 (3): 417–32. https://doi.org/10.1080/00048402.2013.834949.\n\n\nLewis, David. 2004. “Causation as Influence.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 75–106. Cambridge: MIT Press.\n\n\nLutz, Matt. 2014. “The Pragmatics of Pragmatic Encroachment.” Synthese 191 (8): 1717–40. https://doi.org/10.1007/s11229-013-0361-6.\n\n\nNagel, Jennifer. 2008. “Knowledge Ascriptions and the Psychological Consequences of Changing Stakes.” Australasian Journal of Philosophy 86 (2): 279–94. https://doi.org/10.1080/00048400801886397.\n\n\n———. 2014. Knowledge: A Very Short Introduction. Oxford: Oxford University Press.\n\n\nPinillos, Ángel. 2012. “Knowledge, Experiments and Practical Interests.” In Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 192–219. Oxford: Oxford University Press.\n\n\nReed, Baron. 2014. “Practical Matters Do Not Affect Whether You Know.” In Contemporary Debates in Epistemology, edited by Matthias Steup, John Turri, and Ernest Sosa, 2nd ed., 95–106. Chicester: Wiley-Blackwell.\n\n\nRubin, Katherine. 2015. “Total Pragmatic Encroachment and Epistemic Permissiveness.” Pacific Philosophical Quarterly 96: 12–38. https://doi.org/10.1111/papq.12060.\n\n\nRussell, Gillian, and John M. Doris. 2009. “Knowledge by Indifference.” Australasian Journal of Philosophy 86 (3): 429–37. https://doi.org/10.1080/00048400802001996.\n\n\nRysiew, Patrick. 2017. “Warranted Assertability Maneuvers.” In Routledge Handbook of Epistemic Contextualism, edited by Jonathan Jenkins Ichikawa, n/a–a. London: Routledge.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\n———. 2011. “Defending Interest-Relative Invariantism.” Logos & Episteme 2 (4): 591–609. https://doi.org/10.5840/logos-episteme2011248.\n\n\n———. 2012. “Knowledge, Bets and Interests.” In Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.\n\n\n———. 2014. “Probability and Scepticism.” In Scepticism and Perceptual Justification, edited by Dylan Dodd and Elia Zardini, 71–86. Oxford: Oxford University Press.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nWorsnip, Alex. 2017. “Knowledge Norms.” In Routledge Handbook of Epistemic Contextualism, edited by Jonathan Jenkins Ichikawa, n/a–a. London: Routledge.\n\n\nWright, Crispin. 2004. “Warrant for Nothing (and Foundations for Free)?” Proceedings of the Aristotelian Society, Supplementary Volume 78 (1): 167–212. https://doi.org/10.1111/j.0309-7013.2004.00121.x.\n\n\nI’m suggesting here that in some sense, knowledge is a norm of belief. For more on the normative role of knowledge, see Worsnip (2017).↩︎\nCompare the response to Feltz and Zarpentine (2010) that I make in Weatherson (2011, sec. 1), or the response to Lackey (2010) by Masashi Kasaki (2014, sec. 5).↩︎\nThe argument of the last two sentences is expanded on greatly in Weatherson (2014, sec. 3). The idea that knowledge allows for defeaters is criticised by Maria Lasonen-Aarnio (2014b). Eaton and Pickavance (2015) make an objection to IRI that does not take this point into account.↩︎\nThe argument of the last two sentences is expanded on greatly in Weatherson (2014, sec. 3), where it is credited to Martin Smith. The idea that knowledge allows for defeaters is criticised by Maria Lasonen-Aarnio (2014a).↩︎\nWright (2004) notes that there often is not value in holding on to sceptical doubts, and the considerations of this paragraph are somewhat inspired by his views. That’s not to endorse the idea that using alcohol or heroin is preferable to being gripped by sceptical doubts, especially heroin, but I do endorse the general idea that those doubts are not cost-free.\n\n↩︎\n",
    "preview": "posts/2021-02-02-interest-relative-invariantism/survey.jpg",
    "last_modified": "2021-02-05T20:43:43-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-analytic-synthetic-and-a-priori-a-posteriori/",
    "title": "Analytic-Synthetic and A Priori-A Posteriori",
    "description": "This article focuses on the distinction between analytic truths and synthetic truths (i.e. every truth that isn’t analytic), and between a priori truths and a posteriori truths (i.e. every truth that isn’t a priori) in philosophy, beginning with a brief historical survey of work on the two distinctions, their relationship to each other, and to the necessary/contingent distinction. Four important stops in the history are considered: two involving Kant and W. V. O. Quine, and two relating to logical positivism and semantic externalism. The article then examines questions that have been raised about the analytic–synthetic and a priori–a posteriori distinctions, such as whether all distinctively philosophical truths fall on one side of the line and whether the distinction is relevant to philosophy. It also discusses the argument that there is a lot more a priori knowledge than we ever thought, and concludes by describing epistemological accounts of analyticity.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2016-05-01",
    "categories": [
      "epistemology",
      "language",
      "history of analytic"
    ],
    "contents": "\n\nContents\nHistory\nFive Questions\nThe Traditional Notion of the A Priori\nA Priori Knowledge and Practical Skills\nInnate Knowledge\nSubstantive A Priori\nThe A Priori In Philosophy\nMetaphysical Accounts of Analyticity\nEpistemological Accounts of Analyticity\n\nHistory\nIt’s easy to give a rough gloss of the notions of analyticity and a priority.\nSomething is an analytic truth iff it is true in virtue of its meaning.\nSomething is an a priori truth iff it is knowably true without justification by experience.\nAnd this yields us two distinctions, between analytic truths and synthetic truths (i.e., every truth that isn’t analytic), and between a priori truths and a posteriori truths (i.e., every truth that isn’t a priori). But fleshing out these distinctions takes some work, as we’ll see. Let’s start by a quick historical survey of work on the two distinctions, their relationship to each other, and their relationship to the necessary/contingent distinction. There are four important stops in the history.\nPublished in Oxford Handbook of Philosophical Methodlology, edited by Herman Cappelen, Tamar Szabó Gendler, and John Hawthorne, 231-248.\nPicture via Tambako the Jaguar via Creative Commons.\nThe distinction between “analytic” and “synthetic” traces back to Kant. He thought that both distinctions in our title (analytic/synthetic and a priori/a posteriori) were real, and that they were not the same distinction. In particular, he held that most interesting philosophical and mathematical claims were synthetic a priori. This is because he (at least most of the time) worked with a fairly narrow notion of analyticity. A subject-predicate sentence A is B is analytic if “the predicate B belongs to the subject A as something that is (covertly) contained in this concept A”  (Kant 1781/1787/1999, 6). But there can be plenty of a priori truths that do not fall under this narrow category.\nLet’s look at one example of current importance. Consider the claim Whatever is known is true. It is at least plausible that that is a priori; that we don’t need to look into the world to know that knowledge implies truth. But is it analytic for Kant? It is iff there is an analysis of knowledge, and the analysis is of the form S knows that p iff p is true, and X, for some value of X. Most epistemologists nowadays would reject the idea that there is an analysis of knowledge. But even among those who hold out hope for such an analysis, an argument by Linda Zagzebski (1994) has convinced most people that the analysis cannot be of this form. In particular, Zagzebski argued that if there is such an analysis, X must entail that p is true, eliminating the need for this clause. So Whatever is known is true will turn out, by Kantian standards, to be synthetic, even if it is a priori.\nAlthough Kant did not think the two distinctions we are focussing on are equivalent, most scholars take him to have thought all and only necessary truths are a priori knowable. (Though see Strang (2011) for a dissent.) This will be a common theme throughout much of the history.\nOur second stop on the history is logical positivism, most clearly represented in English by Ayer (1936). The positivists thought that all three distinctions were in a fairly deep sense equivalent. In particular, they thought all three were very close to the distinction between theorems and non-theorems of logic. The positivists were, self-consciously, building on the tradition of British empiricism. But unlike some empiricists, they didn’t want to insist on an empirical basis for logical and mathematical knowledge.1 The solution was to build on the logicism about mathematics developed by Frege and Russell.\nBorrowing a term from Boghossian (1996), let’s say that a sentence is Frege-analytic iff it can be converted to a logical truth by the substitutions of synonyms. The positivists thought that all a priori, necessary and analytic truths were the Frege-analytic truths. Without logicism, this would be wildly implausible, since mathematical truths would be an exception. But Frege and Russell had done enough to make that possibility less worrying.\nThe positivists’ view has some epistemological attractiveness. How can we know things without having empirical input? And how can we know that some things are true not just in this world, but in all worlds? Well, say the positivists, by knowing the language (which we learn empirically) we learn what sentences are related by the substitution of synonyms. And then the puzzles about knowledge of analytic or necessary truths just reduce to puzzles about the epistemology of logic.\nOur third major stop then is with Quine, who questioned both steps of this attempted explanation. First, Quine (1936) noted that the story still needs an epistemology of logic. The obvious expansion of the story told so far won’t work. It can’t just be by learning the meanings of the logical connectives that we come to learn which the logical truths are. That’s because we need to be able to derive the consequences of those meanings, and for that we need logic.\nBut second, Quine (1951) argued that we have no independent way to make sense of the notion of synonymy that is at the heart of Frege-analyticity. This is the most famous part of Quine’s attack on the empiricists epistemology of logic and mathematics, but it isn’t the strongest part of it. Indeed, the argument in “Two Dogmas” is both strange and self-undermining.2 Quine’s primary complaint in that paper about the notions of analyticity, synonymy and meaning is that the only way we have of understanding these notions is in terms of the others. But that would only be a problem if we thought we needed to understand them in terms of something else. Arguably we need not; the notions could be theoretical primitives. And especially if one is a confirmation holist (and part of the point of “Two Dogmas” is to defend holism) we shouldn’t worry about circularity cropping up near the core of our epistemology. Relatedly, a naturalist like Quine shouldn’t care about whether we can give a definition of terms like ‘meaning,’ but rather about whether it is a useful concept in a science like linguistics or cognitive science.\nTo avoid attributing an incoherent position to Quine, we should interpret Quine the argument of “Two Dogmas” as part of his larger argument against the appeal to meanings and analyticities.3 Quine’s larger point, as defended in  (Quine 1960), was that meanings were unnecessary scientific postulates. He thinks that we simply don’t need them to explain all the facts about cognition and communication that need explaining. Now it isn’t clear how many people will share Quine’s view that meanings are unnecessary for these sciences, since without his behaviourism the attempt to do without meanings looks unsuccessful. But the larger point is that Quine isn’t simply relying on an argument from the irreducibility of analyticity to a dismissal of the analytic/synthetic distinction.\nThe last stop on our history tour is semantic externalism. The externalists complicated the above story in two overlapping ways. First, they developed convincing arguments that necessity and a priority were dissociable. The most compelling of these arguments were the examples of necessary a posteriori truths, such as Water contains oxygen. No matter what surface characteristics or functional roles a substance might play, if it does not contain oxygen, it could not be water.\nSecond, they showed that the pre-theoretical notion of meaning, which had seemed good enough for much prior philosophical theorising, contained a number of distinct ideas. Here is how Gillian Russell (whose writings I’ve leaned heavily on in this introduction) puts it,\n\nIn three astonishingly influential pieces of philosophical writing, Hilary Putnam (1973) argued that meaning couldn’t be both what a speaker grasped and what determined extension, Kaplan (1989) argued that what determines extension (character) and what got contributed to what a sentence said (content) came apart in the cases of indexicals and demonstratives, and Kripke (1980) argued that what determined the extension of a name or natural kind term need not be known in order for a speaker to understand the expression, nor was it what was contributed to the proposition expressed by a sentence containing one. Each was suggesting that the roles attributed to a single thing-the expression’s meaning-in the [pre-theoretical] picture, can be played by distinct things. (Russell 2008 x)\n\nFrom this point on, when we talk about truth in virtue of meaning, we have to clarify which aspect of ‘meaning’ we mean. With that in mind, let’s turn to the questions that have been raised about the distinctions.\nFive Questions\nTo focus our discussion, let’s start with five questions we could ask about either the purported distinction between analytic and synthetic, or between a priori and a posteriori.\nIs there a sensible distinction here?\nAre there truths on either side of the line?\nDoes the distinction track something of independent significance?\nDo all distinctively philosophical truths fall on one side of the line?\nIs the distinction relevant to philosophy?\nThe questions are obviously not independent; a negative answer to the first suggests that we better not offer a positive answer to any of the rest, for example. But there are more degrees of freedom here than might immediately be apparent.\nA negative answer to the second question, for instance, need not imply a negative answer to the first. If one held, with Phillip Kitcher (1980) that a priori warrant is by its nature indefeasible, and as a matter of fact no warrants are indefeasible, then one would think the a priori/a posteriori distinction is sensible, but in fact everything falls on one side of it.\nWith respect to the a priori/a posteriori distinction, I’ll argue below that while the answer to question 4 is clearly negative, the answer to question 5 is positive. The a priori/a posteriori distinction may be relevant to philosophy even if it isn’t relevant to, for example, demarcating philosophy from non-philosophy. Alternatively, a positive answer to question 5 may follow from a negative answer to one of the earlier questions. (Williamson (2013) suggests, but ultimately I think does not endorse, the view suggested in the following sentences.) If we learned that all knowledge was a posteriori, i.e., that all knowledge depended in an epistemologically significant way on experience, that would be epistemologically interesting. So the distinction might have a valuable role in articulating and perhaps defending a key philosophical insight, even if all the actual cases fall on one side of the distinction.\nThe point of raising these questions at the start is to ward off a possible confusion that can easily arise when discussing distinctions. It is common to hear about ‘attacks’ on a distinction, or ‘scepticism’ about a distinction, but a moment’s reflection shows that it isn’t clear what this comes to. I think that most of the ‘attacks’ on either of our two distinctions are arguments for a negative answer to one of these five questions. (We’ll see some instances of this as we go through the entry.) But different attackers may argue for different negative answers, and different defenders defend different positive answers. So it is, I think, helpful to have these distinct questions in mind before we begin.\nThe Traditional Notion of the A Priori\nThe traditional notion of the a priori makes the best sense, I think, if you start with the following three assumptions.\nThere is a notion of justification that is distinct from, but a constituent of, knowledge.\nWhether a belief is justified, in this sense, depends just on the evidence the believer has.\nEvidence about the external world consists solely of perceptual experiences.\nFrom 2 and 3 we get the idea that there could be some beliefs whose justification does not depend on any perceptual experience, i.e., beliefs that are justified by a null set of perceptual experiences. These are the beliefs that are justified first, i.e., a priori. Then by 1 we can say that these beliefs satisfy a part, possibly a large part, of the conditions for being knowledge. And this is the a priori knowledge.\nThe problem, as will probably be clear to most readers, is that all three of the assumptions I started with are contentious. As noted in the introduction, Linda Zagzebski (1994) has shown that there cannot be any non-factive notion of justification that is a constituent of knowledge. Timothy Williamson (2000Ch. 8) has argued convincingly against the phenomenal account of evidence; our evidence consists of facts about the world, not just facts about our experience. Point 2 is less clearly mistaken, but is still far from obvious. (See Conee and Feldman (2004) for a long defence of point 2, as well as discussion of several problems with it.)\nOnce we drop the three ideas though, or even just the first and third, what could be left to say about the a priori? A natural first move is to think about what explains a person’s knowledge, rather than what constitutes it. On the classical picture I just sketched, Bob’s knowledge that there are tigers nearby might be constituted by his experience of hearing tiger-like growls. On that picture, having that experience is (partially) constitutive of being justified in believing that there are tigers nearby, and that justification is (partially) constitutive of his knowing there are tigers nearby, and it is these constitutive connections that make his knowledge a posteriori. We don’t need to make assumptions that are nearly so strong to conclude that the experience partially explains his knowledge. The experience could (partially) explain why he is justified, without being any part of the justification, and the justification could (partially) explain why he knows, without being any part of the knowledge.\nBut there’s a problem with this move too. I know that all tigers are tigers. On a standard view about the a priori, this will be a piece of a priori knowledge. But to explain why I have that knowledge, you have to appeal to some experiences I have had. After all, with no experiences, I would not be able to think about tigers. So maybe nothing will end up a priori.\nThere’s another usual response here. The experiences I have enable me to think about tigers, without doing anything to justify my belief that all tigers are tigers. So maybe a priori knowledge is that knowledge where experiences do not play any justificatory role, although they may play an enabling role.\nThat distinction between justifying and enabling will do a lot of work in what follows, so it is worth pausing over it. Perhaps we can say a bit more precisely what it means. An experience is a mere enabler if it explains why a person knows that p, but not in virtue of explaining how it is they can believe that p. I think something like that is plausibly true, but it still makes a rather large epistemological assumption, namely that justification is explanatorily prior to knowledge. That’s something that will be rejected by those who accept the ‘knowledge first’ epistemology of Williamson (2000).\nIf you don’t accept that justification is explanatorily prior to knowledge, this route at least to articulating the difference between experiences that enable, and experiences that justify, is closed off. And perhaps the enabling/justifying distinction is too obscure to do much work. That’s what Williamson has recently argued, and in the next section we’ll look at his argument.\nA Priori Knowledge and Practical Skills\nThe a priori/a posteriori distinction, on the best way of freeing it from outdated epistemological assumptions, relies on the idea we can make sense of the idea that some experiences are necessary for knowledge because they enable that knowledge, rather than that they justify that knowledge. Timothy Williamson (2013) has argued that this distinction is too unclear to be useful, and as a result the a priori/a posteriori distinction cannot do the work epistemologists need.\nI find the example Williamson uses, involving Norman and Who’s Who  (Williamson 2013, 295), rather unintuitive, so I’ll substitute a different example that I think makes the same point. Diane is a great basketball player. One of her great skills is being able to anticipate the moves a defence will make, and responding with a move that will maximise her team’s chance of scoring. This is a skill she’s honed through years of practice and competition. And her most common manifestation of it comes in game situations, when she sees an opposing defender and realizes what move will maximise her team’s expected points. But she can also manifest this skill ‘off-line,’ when she considers conditional questions of the form If the opposing team were to do this, what should I do?.\nWilliamson notes that in some such cases, these questions will be solved through the use of imagination, which is surely right at least for some sense of ‘imagination.’ And in these cases, there won’t be any particular experience that justifies the answer. Yet Diane can acquire knowledge by these acts of the imagination. Is the knowledge she gets a priori or a posteriori? Williamson thinks there is no good answer to this question, since Diane’s experiences play a role in honing her skills that goes beyond the enabling role, but this role very different to the role experiences play in classical examples where we can point to a particular experience that justifies the answer.\nNow it might seem that there’s a simple move to make here. Diane’s knowledge is obviously a posteriori because it is explained by her years of experience playing basketball. It is a case of (massive) overdetermination, but that doesn’t mean the experiences collectively are not an essential part of the explanation. Williamson’s response is that if we go down this route, some paradigmatic instances of a priori reasoning will turn out to be a posteriori. For instance, our ability to engage in logical reasoning might turn out to be dependent on our ability to track identity of objects (or even just terms) across time. In general, this kind of response threatens to drive the a priori out of philosophy altogether.4\nThis is not, I hasten to note, Williamson’s conclusion. Williamson thinks that some of our logical knowledge is a priori, and Diane’s knowledge is a posteriori, but the salient explanations of how those pieces of knowledge are obtained and sustained are similar in epistemologically salient respects. So he concludes the a priori/a posteriori distinction does not track anything of epistemological significance.\nInnate Knowledge\nIn the previous section we considered an argument that there is much less a priori knowledge than we usually assume. In this section we’ll look at an argument, tracing back to work by John Hawthorne (2007) that there is a lot more a priori knowledge than we ever thought, in principle a lot lot more.\nRecall that we’ve argued, on pain of losing all a priori knowledge, that we must understand a priori knowledge as knowledge that is in some sense prior to experience, not knowledge that is independent of experience. So now consider beliefs that really are prior to experience, namely innate beliefs. There is a lot of evidence that neonates have differential reaction to (right way up) human faces than they do to other objects. (See Chien (2011) and Heron-Delaney, Wirth, and Pascalis (2011) for some recent studies on this and citations to many more.) It is natural to explain this by positing an internal representation in the neonate of the structure of human faces; i.e., a belief about how human faces are structured. Since these beliefs are true, and are in a good sense held because they are true, they seem to amount to knowledge. Yet they are clearly not grounded in, or explained by, the experiences of the neonate. So they look like a priori knowledge.\nThis is obviously very different to the standard conception of what is a priori knowledge. As we noted at the top, and will expand on in the next section, there is a lot of interest in the possibility of a priori knowledge of contingent truths. But even the most enthusiastic supporters of the a priori don’t think we have a priori knowledge of facial structure of conspecifics.\nThe problem is actually worse than this. We don’t normally focus on what is actually known a priori, but what is a priori knowable. The reason for this is fairly simple. Most of us cannot know complicated enough multiplications without the aid of empirical evidence.5 But this doesn’t compromise the idea that mathematical truths are in a deep sense a priori. That’s because one could, in principle, know them a priori, even if creatures with small brains or limited skills need assistance from their perceptions.\nBut if that’s right, then we can imagine creatures with all sorts of different innate beliefs. Indeed, for any law about the world, we can imagine a creature who innately believes that law to hold, and whose belief has the right kind of evolutionary explanation for it to count as knowledge. (Why the restriction to laws? Well, beyond that there might be issues about whether the innate beliefs are accidentally true. In any case, I’m not claiming that only laws could be known a priori this way.)\nThere isn’t any obvious way out here. I think the best thing to do is to say that when we say that something is a priori knowable, we have to mean that it is a priori knowable for creatures like us. That rules out the possibility of having all the laws be a priori, but at the cost of making some arithmetic truths a posteriori.\nThe arguments by Williamson and Hawthorne I’ve discussed in the last two sections challenge the utility of the traditional notion of the a priori. But for the rest of this entry I’ll set them aside, and discuss what ways we might modify, or use, the traditional notion should we find responses to these challenges.\nSubstantive A Priori\nAs we noted in the introduction, a common thread through much of the history of this topic was a belief in a close relationship between a priority and necessity. Most writers take Kant to have treated them as co-extensive notions, the positivists thought they were identical, and Quine took them to suffer from similar defects. It is only with the externalists that we see a gap appearing between the two.\nEven once the externalists appear, the gap is not as wide as it may be for two interlocking reasons. The first is that the argument from externalism to the existence of the necessary a posteriori is clearer than the argument from externalism to the existence of the contingent a priori. The second is that externalism may only give a ‘shallow’ distinction between necessity and a priority. Let’s take these in turn.\nAssuming externalism, we can identify examples of the necessary a posteriori by using familiar natural kind terms. To take a famous example, it is necessary and a posteriori that water is H\\(_2\\)O. It is harder to even identify the contingent a priori. The rough idea is clear enough. We take the characteristics by which ordinary language users identify water, and say it is a priori that water has those characteristics. But what are those characteristics? Water is the stuff which falls from the sky, fills the rivers, lakes and oceans, and so on. Is any one of these a priori? Not really. It could turn out that nothing had all these properties. (Is it really water in the oceans anyway, or is salt water a different substance?) So we could introduce a new term, Chalmers (1996) suggests ‘watery,’ for the long disjunction of conjunctions of properties that a substance must have if we are to identify it as water. Perhaps we come up with a list such that Water is watery will be a priori, though since H\\(_2\\)O need not have been watery, it will be contingent. Note we will, at the least, have to introduce new vocabulary to identify this kind of contingent a priority.\nThe other worry is that the gap opened up here is ‘shallow’ in the sense of Gareth Evans (1979). Given the way things turned out, water must be H\\(_2\\)O. But in some intuitive sense, things could have turned out differently. (I’m taking the helpful locution ‘could have turned out’ from Yablo (2002).) It could have turned out that the stuff in the rivers, oceans etc was XYZ. So while it is necessary that water is H\\(_2\\)O, it could have turned out that this not only wasn’t necessary, it wasn’t even true. If that all sounds plausible to you, you may well think that the a priori truths are all and only those truths which couldn’t have turned out to be false. This way of thinking is behind the important two-dimensionalist approach. Important works in this tradition, as well as Evans (1979), include Davies and Humberstone (1980), Chalmers (1996) and Jackson (1998).\nNow there are significant challenges facing two-dimensionalists, several of which are set out in Block and Stalnaker (1999). But my sense is that several of these challenges are very similar to the challenges facing anyone trying to get an argument from semantic externalism to the contingent a priori. If we could say more clearly what it is for something to be watery, it would be easier to say whether a particular world is one where water turned out to be XYZ. So I suspect if externalism gives us a reason to believe in the contingent a priori, it will be a fairly shallow distinction. (This doesn’t extend to the argument from externalism to the necessary a posteriori; we don’t need to shore up two-dimensionalism to say that Water contains oxygen is necessary a posteriori.)\nBut that’s not the only way that a priority might outrun necessity. In recent years there has been a surge of interest in the idea that we can know a priori various anti-sceptical propositions. This idea was advanced in detail by John Hawthorne (2002), and then suggested as a way out of sceptical problems by Roger White (2006) and Brian Weatherson (2005).\nRecently, Stewart Cohen (2010) and Sinan Dogramaci (2010) have suggested that (assuming inductive scepticism is false), we can use ampliative inferential steps in suppositional reasoning. That is, if it possible to inductively infer \\(B\\) when we know \\(A\\), it is possible to infer \\(B\\) on the supposition that \\(A\\), and go on to infer the material conditional \\(A \\supset B\\). That conditional will be contingent if the inference was ampliative, but since we’ve discharged the only supposition we used, it could be a priori in a good sense. I have doubts about this route to the contingent a priori  (Weatherson 2012), but I think the general idea is plausible.\nTo make things more concrete, consider ‘bubble worlds.’ A bubble world consists of a person, and the space immediately around them. If you think that evidence supervenes on sensory irritation, then you have a duplicate in a bubble world who has the same evidence as you.6 But you’re not in a bubble world, and you know it. There’s a well known probabilistic argument that your evidence can’t be grounds for ruling out possibilities that entail you have just that evidence.7 So your evidence doesn’t rule out that you’re in a bubble world. But you know you’re not. Hence that knowledge is a priori. So I’m not in a bubble world might be contingent a priori.8 Moreover, it’s not a ‘shallow’ contingency. It could have turned out that you were in a bubble world. Indeed, with some more evidence you might even know this.\nI’m not going to defend here the claim that it’s contingent a priori that you’re not in a bubble world. Indeed, I don’t even believe the probabilistic argument for that conclusion that I just referenced. But it is worth noting this trend towards taking seriously the possibility of substantive a priori knowledge.\nThe A Priori In Philosophy\nI’ve argued so far that the best sense we can make of the a priori allows for a lot of a priori knowledge. Once we realise that a priori knowledge, like any other kind of knowledge, is defeasible, and fallible, it seems possible that an agent could have a lot of foundational knowledge of contingent matters. And that foundational knowledge does seem to be a priori. Of course, such an agent would not be very much like us; so there is still a question of what agents like us could know a priori. And it might seem that the class of such pieces of knowledge might be relatively small and interesting.\nIn particular, one might think that philosophical knowledge, or at least some interesting part of philosophical knowledge, might be a priori. Herman Cappelen (2012) notes that a wide range of philosophers, with very different commitments, end up with the view that the a priori has a distinctive role to play in philosophy. (See, especially, chapters 1 and 6 of that book.) But, as Cappelen also shows, these philosophers are mistaken; outside perhaps of philosophical logic, the a priori doesn’t have a particularly special role to play in philosophical inquiry. We can see this, I think, by working through one recent debate.\nTimothy Williamson (2007) noted that philosophical thought experiments are almost always incomplete. The text of an example doesn’t guarantee that conclusions that are usually drawn from it. To use his example, to guarantee that the subject in one of Gettier’s examples has justified beliefs (that don’t amount to knowledge), we have to suppose that there are no defeaters in the vicinity, but that isn’t stated in the example. Williamson’s solution to this is that we should read the example as a certain kind of counterfactual. What we know, Williamson argues, is that in the nearest world where the example was instantiated, the subject would have justified true belief without knowledge. (I’m ignoring here some complications involving names, and donkey anaphora, that are not relevant to this debate.)\nJonathan Ichikawa and Benjamin Jarvis (2009) object that this makes philosophical knowledge a posteriori. We have to know what the world is like to know what the nearest world in which the Gettier case is instantiated is like. Ichikawa and Jarvis reject this because they want to defend a “traditional” conception of thought experiments on which they provide a priori knowledge. I’m not convinced that this really is part of philosophical tradition; it seems to me the thought experiments in Hobbes, Hume, Mill and many others in the canon rely on empirical knowledge. But I won’t press that point here. If one does want to avoid empirical knowledge coming in via the route Williamson suggests, Ichikawa and Jarvis develop a nice way of doing so.\nThey say that thought experiments are little fictions. We need some empirical knowledge to interpret the fiction. But, they insist, once we are given the fiction, it is a priori that the fiction is possible, and that in it the subject has a justified true belief without knowledge. And, note this point, it is a priori that these two facts entail that it is not necessary that all justified true beliefs amount to knowledge. On the last point, they agree with Williamson. It is also a priori that if the Gettier case could happen, and if it were to happen then there would be justified true belief without knowledge, then it is not necessary that all justified true beliefs are knowledge.\nBut let’s try and generalise this to other thought experiments. Start, for example, with the famous violinist described by Thomson (1971). That violinist plays a key role in an argument whose conclusion is that abortion is often morally permissible. And one premise of the argument is something about an imagined violinist. Williamson will say that premise is an a posteriori counterfactual proposition. Ichikawa and Jarvis will say it is an a priori proposition about what’s true in a fiction. Perhaps there’s another premise about the possibility of the example, and maybe that’s a priori too. But those premises don’t come close to supporting Thomson’s conclusion. We need another premise about the analogy between the violinist and a woman contemplating having an abortion to get Thomson’s conclusion. Any such premise will not be a priori, unless some detailed facts about human biology are a priori. It’s a little tricky, but it isn’t obvious the soundness of the abductive inference from those premises to Thomson’s conclusion is a priori either. (See Pargetter and Bigelow (1997) for some discussion of this point.)\nI think Thomson’s example is more typical of philosophical reasoning than Gettier’s. We don’t just use thought experiments to dismiss theories, like the JTB theory of knowledge. We also use them to defend philosophical conclusions, such as the permissibility of abortion. And in general inferences from a thought experiment to the truth of a theory will involve some a posteriori steps. So even if Ichikawa and Jarvis are right that we can know a lot about thought experiments a priori, it won’t follow that in general philosophical knowledge derived from thought experiments is a priori. Get away from special cases where the facts about the thought experiment entail the philosophically interesting result, and this should be reasonably clear.\nThat doesn’t mean that there’s no use for the a priori in philosophy. It might be a very helpful concept to use in argument, even if it isn’t true that our conclusions are generally a priori. I’ll illustrate with one example from my own work. One way to support the sceptical intuition that we don’t know we aren’t brains in vats is to ask, how could one possibly know that? Rhetorical questions are not arguments, the received wisdom of undergraduates notwithstanding, so the sceptic needs to find some way to extract argumentative force from that question. An attractive option is argue that these are all the ways to know something, and you can’t know you’re not a brain it a vat any of these ways. Such an argument typically runs into problems at the first step; arguing that one has exhausted all possible ways of getting knowledge is not easy.\nHume (1739/1978) had the best idea for how to overcome this step. Don’t list the ways someone can know something; use some property of knowledge gathering methods to partition the methods. Then argue that in no cell of the partition can one find a method that allows knowledge of the undesired kind. If the partition just consists of the presence or absence of some property, you’re guaranteed at least to have covered the field. I’ve argued Weatherson (2007) that you get an interesting argument by letting the property in question be is an a priori method. By ‘interesting’ I certainly don’t mean sound. (And nor do I insist that Hume equated interesting sceptical arguments with sound ones.) But I think you get epistemological insight by thinking about whether knowledge that we’re not brains in vats could be a priori, or could be a posteriori. You don’t have to think that philosophical conclusions themselves are a priori to think this could be a useful philosophical approach. That last point is probably obvious. We could have developed the sceptical argument by asking whether knowledge of nonenvattedness is innate or acquired. But suggesting that’s an interesting argument wouldn’t imply the very traditional view that philosophical knowledge is typically innate.\nMetaphysical Accounts of Analyticity\nLet’s turn now from the a priori to the analytic. As we noted at the start, the traditional notion is that a sentence is analytic iff it is true in virtue of meaning. And, as we saw at the end of the introduction, this notion is complicated by the fact that traditional theories of meaning conflated several things that should be kept separate.\nPaul Boghossian (1996) makes a distinction that has been highly influential between metaphysical and epistemological conceptions of analyticity, and I will follow many contemporary writers in splitting the topic up in this way. The metaphysical conception is the one most continuous with the traditional notion of analyticity, and also the one least popular with contemporary theorists, so we will start with that. It is the notion that some sentences are true merely in virtue of their meaning.\nBoghossian, following Quine, argues that this notion is either nonsensical or trivial. Consider a simple example of a putatively analytic truth, say Everything is self-identical. Why is this true? In part, because it means that everything is self-identical. But that can’t be what we mean to say that it is analytic. Paris is beautiful is true in part because it means that Paris is beautiful, but that doesn’t make the sentence an analytic truth. What we need is that this is the only thing needed for the sentence to be true. And that isn’t the case for either sentence. Everything is self-identical is true because of what it means and the fact that everything is indeed self-identical, and Paris is beautiful is true because of what it means and the fact that Paris is indeed beautiful. We haven’t yet found a difference between the two.\nIt might be easy to see a response here. Start with a less discriminating treatment of truth makers than I supposed in the previous paragraph. Say that a sentence is true in virtue of what it means, and the way the world is. So both of our examples are true in virtue of their meaning and the way this world is. But for Everything is self-identical, it doesn’t matter how the world actually is, any way it could have been would have made the sentence true. The contribution of the world is like the contribution of the 5 in What is 0 times 5? You need a second number there, or the question doesn’t make sense, but it doesn’t matter which. In some good sense, the 0 does all the work. (This example, and most of the discussion in the rest of this section, lean heavily on chapter 2 of  (Russell 2008).)\nBut this won’t do as a conception of analyticity either, because of the examples of the necessary a posteriori. Consider the example Gold has atomic number 79. It is true in virtue of what it means, thay gold has atomic number 79, and how the world is. But it doesn’t matter which world we choose; in any world gold has atomic number 79. Yet it is not, intuitively, analytic.\nRussell suggests a solution to this problem that draws on the developments in externalist theories of meaning that we discussed in the introduction. Start with the following three way distinction. (These definitions are a quote from page x of  (Russell 2008).)\nCharacter: The thing speakers must know (perhaps tacitly) to count as understanding an expression.\nContent: What the word contributes to what a sentence containing it says (the proposition it expresses).\nReference Determiner: A condition which an object must meet in order to be the reference of, or fall in the extension of, an expression.\nThese can all come apart. In the case of pure indexicals like I, the content comes apart from the character and reference determiner in familiar ways. But it is tempting in those cases to equate character and reference determiner. What makes it the case that a token of I picks out me is that I use it, and that relation between usage and content is what someone must know to understand the term. But that’s an all too special case. I can be a competent user of the name ‘Alex’ as a name for my friend Alex without knowing whether she got that name at birth in the normal way, or knowing whether she acquired it later. Competence may require that I know the reference of Alex was somehow determined to be her, but I need not know what that reference determiner was.\nMoreover, the character and reference determiner relate to contexts in different ways. It is a familiar point that a sentence like If you were speaking, I would have been speaking may be false. That’s because when we evaluate the I in the consequent, we don’t look to who the speaker is in the context of evaluation, i.e., the world where you are speaking, but to the world of utterance, i.e., the context of my utterance. Just like this familiar distinction between contexts of utterance and contexts of evaluation, Russell requires us to think about contexts of introduction. An example helps bring this out.\nSay I, on Monday, introduce the name ‘Inigo’ for the shortest sword fighter. When the name is used on Tuesday, it need not pick out the shortest sword fighter even in the context of utterance. Inigo might have grown, or a shorter person may have taken up sword fighting. Of course, when I use the name in counterfactuals, it might pick out someone who was never a sword fighter. So we to distinguish the context the term was introduced in, in this case Monday, from the context it is uttered in, in this case Tuesday.\nWith these distinctions in mind, we can give Russell’s first pass at a definition of analyticity.\n\nA sentence S is true in virtue of meaning just in case for all pairs of context of introduction and context of utterance, the proposition expressed by S with respect to those contexts is true in the context of evaluation. (Russell 2008, 56)\n\nThis will, says Russell, solve the problem about gold. It is true that when someone now utters Gold has atomic number 79, they express a necessary truth. But we could have introduced the terms in the very same way, and had the world not cooperated, this sentence would have been false. Indeed, Russell splits analyticity from necessity twice over. She thinks that I am here now will be analytic in this sense though it expresses a contingent proposition.\nThis does rely on understanding what it is for terms in different worlds to have the same reference determiner. Perhaps one could object that had we been pointing at something else when we introduced the term gold, we would have been using a crucially different reference determiner. But the issues here about the metaphysics of words and demonstrations, are subtle, and Russell’s view that the same reference determiner could determine different contents in different worlds seems plausible.\nRussell says that this is a perfectly good notion of truth in virtue of meaning. Of course, as she says, it is really a kind of truth in virtue of what determines meaning, not meaning itself. Reference determiners are part of meta-semantics, not semantics. But that seems continuous enough with the tradition. And there is no reason to think that analytic sentences, so understood, will be epistemologically distinctive. In this respect we may end up agreeing with the primary conclusion of the discussion of metaphysical analyticity in chapter 3 of Williamson (2007), namely that it isn’t directly relevant to philosophical methodology. But it could be an interesting notion in its own right, and as discussed in the previous section, it could be philosophically useful without playing its traditional role.\nI have considerably simplified the presentation of Russell’s view, however. The definition so far implies that some theorems of geometry, and perhaps fundamental laws of ethics, will be analytic. Like Kant, Russell wants these to be synthetic. Her solution is to say that analytic truths must not just satisfy the constraint given above, but that they must do so because the reference determiners of their parts stand in the right kind of containment relations. But spelling this part of her view out will take too much space, so instead I’ll close with a discussion of epistemological analyticity.\nEpistemological Accounts of Analyticity\nIn a series of influential articles, Paul (Boghossian 1996, 1997, 2003) argued that we should accept that Quine’s argument against metaphysical versions of the analytic/synthetic distinction, but that Quine’s arguments left untouched an epistemic understanding of the distinction. On this way of understanding the distinction, a sentence is analytic iff it is knowably true merely in virtue of understanding it. Consider, for instance, this sentence.\n: If frogs bark and ducks howl, then frogs bark.\nNow make the following four assumptions.\nUnderstanding the non-logical terms in (E) suffices to see it is of the form \\((A \\wedge B) \\rightarrow A\\).\nFor logical terms like \\(\\wedge\\) and \\(\\rightarrow\\), understanding involves accepting, perhaps implicitly in one’s inferential practices, the basic introduction and elimination rules they license.\nFor \\(\\wedge\\), the basic rules are the familiar introduction and elimination rules.\nFor \\(\\rightarrow\\), the basic rules are modus ponens and conditional proof.\nThen anyone who understands (E) is in a position to prove it to be true by a trivial three line proof. Generalizing this example, we can get an argument that all logically true sentences are analytic. Generalizing a bit further, we may be able to argue that the propositions they express are a priori knowable, but this requires resolving many of the issues we have already discussed in the discussion of the a priori, and I will set it aside for the remainder of this entry.9\nThe problem we will focus on is that assumptions 2 and 4, and hence presumably 3, are not clearly true. Actually 4 as stated is almost surely false. If the basic rules for \\(\\rightarrow\\) are modus ponens and conditional proof, then \\(\\rightarrow\\) is material implication. But \\(\\rightarrow\\) was meant to be our symbol for natural language ‘if,’ which is not material implication. So the rule must be something else. It isn’t clear what this rule could be. It is plausible that we can use a restricted version of conditional proof when reasoning about ‘if,’ such as a version which requires that there be no undischarged assumptions when we apply conditional proof. That will make the proof of (E) go through, but it is unlikely to be a basic rule in the relevant sense, since it does not combine with an elimination rule (i.e., modus ponens) to pick out a unique meaning for ‘if.’\nDisagreement about the introduction rule for ‘if’ is endemic to the literature on conditionals. But there is almost a consensus that the elimination rule is modus ponens. Almost, but not quite - Vann McGee (1985) is a notable dissenter. Timothy Williamson (2007) uses the existence of notable dissenters like McGee to mount a sustained assault on Boghossian’s position. It is a consequence of the assumptions we have made, and which Boghossian needs, that anyone who doesn’t accept modus ponens does not understand ‘if.’ But that seems implausible. By any familiar standard, McGee understands conditionals quite well. Indeed, he is an expert on them.\nThis point generalizes, as Williamson stresses. On the inferentialist view about the meaning of logical terms, in any debate about the correctness of fundamental logical principles, either one party doesn’t understand the key terms, or the parties are speaking at cross purposes. The intuitionist mathematician endorses the sentence “All functions are continuous,” and the classical mathematician rejects it. But it isn’t plausible that one party fails to understand ‘all,’ ‘functions’ or ‘continuous,’ or that they are speaking at cross purposes in that they are assigning different meanings to one of these terms. (I’m assuming the context makes it clear that both parties are speaking of functions whose domain is the reals, and whose range is subset of the reals.) I’ve used an example from real analysis, but we could make the same point less pithily using Pierce’s Law if we wanted to stick to propositional logic.\nI’ll close with two replies on behalf of the defending of epistemic analyticity, and some reasons for being dissatisfied with each. The discussion will follow somewhat the recent exchange between Boghossian (2011) and Williamson (2011).\nThe first response says that we shouldn’t have said if a sentence is epistemically analytic, then understanding it is sufficient for knowing that a sentence is true. Rather, we should have said that knowing the meaning is sufficient for knowing the sentence is true. Notably, Boghossian does not make this defence in his exchange with Williamson, so it seems he accepts that Williamson was right to take epistemic analyticity to involve a connection between understanding and knowability. And this seems to be right. Consider again Water contains oxygen. In one sense of meaning, the meaning of ‘water’ is H\\(_2\\)O. So anyone who knows what ‘water’ means in that sense knows that Water contains oxygen is true. But it doesn’t feel like this claim is analytic, especially not in the epistemic sense that interests Boghossian. It is possible that there is some other sense of meaning that will be more useful for Boghossian’s project, but it isn’t clear that knowing the meaning in this other sense will differ particularly from understanding.\nThe second response, and one that Boghossian has used on several occasions, is that the only plausible theory of meaning for the logical connectives is inferentialist, and on an inferentialist theory of meaning it will be true that anyone who understands a connective is disposed to reason correctly with it. That last sentence is deliberately sloppy, much more so than any statement of the response in Boghossian’s own work. But the sloppiness is there because it makes a potential equivocation more easily visible.\nConsider a theory that says the meaning of a logical connective is either constituted by, or at least constitutively connected to, its appropriate inferential rules. But to understand the term is not to grasp the meaning in this sense, any more than to understand the term ‘water’ one has to know it is H\\(_2\\)O. Rather, understanding involves participating in the right kind of way in a social practice, and it is that social practice (plus perhaps some facts about the nature of logic, if such facts there be) that determines the appropriate inferential rules for the connective.\nIs the theory in the previous paragraph inferentialist? If not, then it is false that no theory other than inferentialism is plausible as an account of the meaning of the logical connectives. For this kind of socialised theory of meaning is, it seems to me, highly plausible. (Williamson (2011) notes that a socialised theory of meaning for the connectives is plausible, though I don’t think he would sign up for the view that the result of such socialization is a theory in terms of inferential rules.) If, on the other hand, the theory is inferentialist, then it doesn’t follow that understanding requires a disposition to use the rules. Perhaps understanding requires being part of a community many members of which have the appropriate dispositions, but it does not require that any one member have these dispositions. So it won’t be true that mere understanding puts one in a position to know. At best, understanding a logical truth means one is in a community in which some people are in a position to the sentence is true. But that doesn’t do much to rescue the notion of epistemic analyticity.\n\n\n\nAyer, Alfred. 1936. Language, Truth and Logic. London: Gollantz.\n\n\nBlock, Ned, and Robert Stalnaker. 1999. “Conceptual Analysis, Dualism, and the Explanatory Gap.” Philosophical Review 108 (1): 1–46. https://doi.org/10.2307/2998259.\n\n\nBoghossian, Paul. 1996. “Analyticity Reconsidered.” Noûs 30 (3): 360–91. https://doi.org/10.2307/2216275.\n\n\n———. 1997. “Analyticity.” In A Companion to the Philosophy of Language, edited by Bob Hale and Crispin Wright, 331–68. Oxford: Blackwell.\n\n\n———. 2003. “Epistemic Analyticity: A Defense.” Grazer Philosophische Studien 66 (1): 15–35. https://doi.org/10.1163/18756735-90000810.\n\n\n———. 2011. “Williamson on the a Priori and the Analytic.” Philosophy and Phenomenological Research 82 (2): 488–97. https://doi.org/10.1111/j.1933-1592.2010.00395.x.\n\n\nCappelen, Herman. 2012. Philosophy Without Intuitions. Oxford: Oxford University Press.\n\n\nChalmers, David. 1996. The Conscious Mind. Oxford: Oxford University Press.\n\n\nChien, Sarina Hui-Lin. 2011. “No More Top-Heavy Bias: Infants and Adults Prefer Upright Faces but Not Top-Heavy Geometric or Face-Like Patterns.” Journal of Vision 11 (13): 1–14. https://doi.org/10.1167/11.6.13.\n\n\nCohen, Stewart. 2010. “Bootstrapping, Defeasible Reasoning and a Priori Justification.” Philosophical Perspectives 24 (1): 141–59. https://doi.org/10.1111/j.1520-8583.2010.00188.x.\n\n\nConee, Earl, and Richard Feldman. 2004. Evidentialism: Essays in Epistemology. Oxford: Oxford University Press.\n\n\nDavies, Martin, and I. L. Humberstone. 1980. “Two Notions of Necessity.” Philosophical Studies 38 (1): 1–30. https://doi.org/10.1007/bf00354523.\n\n\nDogramaci, Sinan. 2010. “Knowledge of Validity.” Noûs 44 (3): 403–32. https://doi.org/0.1111/j.1468-0068.2010.00746.x.\n\n\nEbert, Philip. 2005. “Transmission of Warrant Failure and the Notion of Epistemic Analyticity.” Australasian Journal of Philosophy 83 (4): 505–22. https://doi.org/10.1080/00048400500338724.\n\n\nEvans, Gareth. 1979. “Reference and Contingency.” Monist 62: 161–89.\n\n\nHawthorne, John. 2002. “Deeply Contingent a Priori Knowledge.” Philosophy and Phenomenological Research 65 (2): 247–69. https://doi.org/10.1111/j.1933-1592.2002.tb00201.x.\n\n\n———. 2007. “Craziness and Metasemantics.” Philosophical Review 116 (3): 427–40. https://doi.org/10.1215/00318108-2007-004.\n\n\nHeron-Delaney, Michelle, Sylvia Wirth, and Olivier Pascalis. 2011. “Infants’ Knowledge of Their Own Species.” Philosophical Transactions of the Royal Society B 366 (1571): 1753–63. https://doi.org/10.1098/rstb.2010.0371.\n\n\nHume, David. 1739/1978. A Treatise on Human Nature. Edited by L. A. Selby-Bigge and P. H. Nidditch. Second. Oxford: Clarendon Press.\n\n\nIchikawa, Jonathan, and Benjamin Jarvis. 2009. “Thought-Experiment Intuitions and Truth in Fiction.” Philosophical Studies 142 (2): 221–46. https://doi.org/10.1007/s11098-007-9184-y.\n\n\nJackson, Frank. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nJehle, David, and Brian Weatherson. 2012. “Dogmatism, Probability and Logical Uncertainty.” In New Waves in Philosophical Logic, edited by Greg Restall and Gillian Russell, 95–111. Bassingstoke: Palgrave Macmillan.\n\n\nJenkins, C. S. 2008. “Boghossian and Epistemic Analyticity.” Croatian Journal of Philosophy 8 (22): 113–27.\n\n\nKant, Immanuel. 1781/1787/1999. Critique of Pure Reason. Edited by Paul Guyer and Allen Wood. Cambridge: Cambridge University Press.\n\n\nKaplan, David. 1989. “Demonstratives.” In Themes from Kaplan, edited by Joseph Almog, John Perry, and Howard Wettstein, 481–563. Oxford: Oxford University Press.\n\n\nKitcher, Philip. 1980. “A Priori Knowledge.” Philosophical Review 89 (1): 3–23. https://doi.org/10.2307/2184861.\n\n\nKripke, Saul. 1980. Naming and Necessity. Cambridge: Harvard University Press.\n\n\nMcGee, Vann. 1985. “A Counterexample to Modus Ponens.” Journal of Philosophy 82 (9): 462–71. https://doi.org/10.2307/2026276.\n\n\nPargetter, Robert, and John Bigelow. 1997. “The Validation of Induction.” Australasian Journal of Philosophy 75 (1): 62–76. https://doi.org/10.1080/00048409712347671.\n\n\nPutnam, Hilary. 1973. “Meaning and Reference.” Journal of Philosophy 70 (19): 699–711. https://doi.org/10.2307/2025079.\n\n\nPutnam, Hillary. 1981. Reason, Truth and History. Cambridge: Cambridge University Press.\n\n\nQuine, W. V. O. 1936. “Truth by Convention.” In Philosophical Essays for a. N. Whitehead, edited by O. H. Lee, 90–124. New York: Longmans.\n\n\n———. 1951. “Two Dogmas of Empiricism.” Philosophical Review 60 (1): 20–43. https://doi.org/10.2307/2181906.\n\n\n———. 1960. Word and Object. Cambridge, MA.: MIT Press.\n\n\nRussell, Gillian. 2008. Truth in Virtue of Meaning: A Defence of the Analytic/Synthetic Distinction. Oxford: Oxford University Press.\n\n\nSober, Elliot. 2000. “Quine’s Two Dogmas.” Aristotelian Society Supplementary Volume 74 (1): 237–80. https://doi.org/10.1111/1467-8349.00071.\n\n\nStrang, Nicholas F. 2011. “Did Kant Conflate the Necessary and the a Priori.” Noûs 45 (3): 443–71. https://doi.org/10.1111/j.1468-0068.2010.00809.x.\n\n\nThomson, Judith Jarvis. 1971. “A Defense of Abortion.” Philosophy and Public Affairs 1 (1): 47–66.\n\n\nWeatherson, Brian. 2005. “Scepticism, Rationalism and Externalism.” Oxford Studies in Epistemology 1: 311–31.\n\n\n———. 2007. “The Bayesian and the Dogmatist.” Proceedings of the Aristotelian Society 107: 169–85. https://doi.org/10.1111/j.1467-9264.2007.00217.x.\n\n\n———. 2012. “Induction and Supposition.” The Reasoner 6 (6): 78–80.\n\n\nWhite, Roger. 2006. “Problems for Dogmatism.” Philosophical Studies 131 (3): 525–57. https://doi.org/10.1007/s11098-004-7487-9.\n\n\nWhitmore, Charles E. 1945. “Mill and Mathematics: An Historical Note.” Journal of the History of Ideas 6 (1): 109–12. https://doi.org/10.2307/2707061.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\n———. 2007. The Philosophy of Philosophy. Blackwell.\n\n\n———. 2011. “Reply to Boghossian.” Philosophy and Phenomenological Research 82 (2): 498–506. https://doi.org/10.1111/j.1933-1592.2010.00400.x.\n\n\n———. 2013. “How Deep Is the Distinction Between a Priori and a Posteriori Knowledge.” In The a Priori in Philosophy, edited by Albert Casullo and Joshua C. Thurow, 291–312. Oxford: Oxford University Press.\n\n\nYablo, Stephen. 2002. “Coulda, Woulda, Shoulda.” In Conceivability and Possibility, edited by Tamar Szabó Gendler and John Hawthorne, 441–92. Oxford: Oxford University Press.\n\n\nZagzebski, Linda. 1994. “The Inescapability of Gettier Problems.” The Philosophical Quarterly 44 (174): 65–73. https://doi.org/10.2307/2220147.\n\n\nActually, the history of what pre-positivist empiricists believed about mathematics is a little more complicated than the standard story. See Whitmore (1945) for some details.↩︎\nThe next few sentences follow the arguments of Sober (2000) fairly closely.↩︎\nThis paragraph follows closely the discussion of Quine in Russell (2008).↩︎\nNote this argument is distinct from the argument Williamson (2007) makes about the role of knowledge of counterfactuals in philosophical reasoning, and its implications for the a priori status of philosophical knowledge. We’ll return to that argument, and the response by Ichikawa and Jarvis (2009) below. The key point is that this argument only turns on the idea that philosophical reasoning might rest on empirically acquired and honed skills.↩︎\nI think that when one carries out multiplication by hand, using the techniques taught at school, the marks on the paper play a justifying and not an enabling role. But arguing for that would be beyond the scope of this entry. It should be less controversial that multiplications carried out by machine give us a posteriori knowledge of the answer.↩︎\nIf you prefer a wider conception of evidence, so that for instance two people who are looking at distinct duplicates have distinct evidence, just make the bubble a little bigger, and this argument will still go through.↩︎\nSee White (2006). David Jehle and I have argued that this argument uses distinctively classical logical principles in a way that might be problematic  (Jehle and Weatherson 2012). And I’ve argued that even slight weakenings of the assumptions about how to update credences make the argument fail.  (Weatherson 2007).↩︎\nI’m assuming here that the semantic response to scepticism, as defended by  (Hillary Putnam 1981), doesn’t work for ruling out bubble worlds. Defending this assumption would take us too far from the current topic.↩︎\nThere is an interesting worry around here that the three-line argument for (E) is circular, and so cannot justify (E), and this fact undermines Boghossian’s argument that it is a priori. See Ebert (2005) and Jenkins (2008) for two ways of developing this worry.↩︎\n",
    "preview": "posts/2021-03-12-analytic-synthetic-and-a-priori-a-posteriori/vixen.jpg",
    "last_modified": "2021-03-12T14:19:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-for-bayesians-rational-modesty-requires-imprecision/",
    "title": "For Bayesians, Rational Modesty Requires Imprecision",
    "description": "Gordon Belot has recently developed a novel argument against Bayesianism. He shows that there is an interesting class of problems that, intuitively, no rational belief forming method is likely to get right. But a Bayesian agent's credence, before the problem starts, that she will get the problem right has to be 1. This is an implausible kind of immodesty on the part of Bayesians. My aim is to show that while this is a good argument against traditional, precise Bayesians, the argument doesn't neatly extend to imprecise Bayesians. As such, Belot's argument is a reason to prefer imprecise Bayesianism to precise Bayesianism.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2016-03-11",
    "categories": [
      "epistemology",
      "games and decisions",
      "imprecise probability"
    ],
    "contents": "\n\nContents\nThe Puzzle\nMaking the Puzzle Less Precise\nMeeting the Challenge, Imprecisely\nObjections and Replies\n\nGordon Belot (2013) has recently developed a novel argument against Bayesianism. He shows that there is an interesting class of problems that, intuitively, no rational belief forming method is likely to get right. But a Bayesian agent’s credence, before the problem starts, that she will get the problem right has to be 1. This is an implausible kind of immodesty on the part of Bayesians.1 My aim is to show that while this is a good argument against traditional, precise Bayesians, the argument doesn’t neatly extend to imprecise Bayesians. As such, Belot’s argument is a reason to prefer imprecise Bayesianism to precise Bayesianism.\n\nPublished in Ergo 2: 20.\nFor present purposes, the precise Bayesian agent has just two defining characteristics. First, their credences in all propositions are given by a particular countably additive probability function. Second, those credences are updated by conditionalisation as new information comes in. These commitments are quite strong in some respects. They say that there is a single probability function that supplies the agent’s credences no matter which question is being investigated, and no matter how little evidence the agent has before the investigation is started. The everyday statistician, even one who is sympathetic to Bayesian approaches, may feel no need to sign up for anything this strong. But many philosophers seem to be interested in varieties of Bayesianism that are just this strong. For instance, there has been extensive discussion in recent epistemology of whether various epistemological approaches, such as dogmatism, can be modeled within the Bayesian framework, with the background assumption being that it counts against those approaches if they cannot.2 In these debates, the issue is not whether the Bayesian approach works in the context of a well-defined question and a substantial evidential background, but whether it does so for all questions in all contexts. Indeed, the assumption is that it does, and epistemological theories inconsistent with it are false. So the precise Bayesian is a figure of some interest, at least in epistemology.\nThe imprecise Bayesian doesn’t have a single probability function for their credences. Rather, they have a representor consisting of a set of probability functions. The agent is more confident in \\(p\\) than \\(q\\) just in case \\(\\Pr(p) > \\Pr(q)\\) for every \\(\\Pr\\) in this representor.3 Just like the precise Bayesian, the imprecise Bayesian updates by conditionalisation; their new representor after an update is the result of conditionalising every member of the old representor with the new information. The added flexibility in imprecise Bayesianism will allow us to develop a suitably modest response to Belot’s puzzle.\nThe Puzzle\nThe set up Belot uses is this. An agent, A, will receive a data stream of 0s and 1s. The data stream will go on indefinitely. I will use \\(\\boldsymbol{x}\\) for the (infinite) sequence of data she would (eventually) get, \\(x_k\\) for the \\(k\\)th element of this sequence, and \\(\\boldsymbol{x}_k\\) for the sequence consisting of the first \\(k\\) elements of the stream. These variables are, as usual, rigid designators. I’ll also use the capitalised \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{X}_k\\) as random variables for the sequence itself, and for the first \\(k\\) elements of the sequence, respectively. So \\(\\boldsymbol{X}= \\boldsymbol{x}\\) is the substantive and true claim that the sequence that will be received is actually \\(\\boldsymbol{x}\\). And \\(\\boldsymbol{X}_k = \\boldsymbol{x}_k\\) is the substantive and true claim that the first \\(k\\) elements of that sequence are \\(\\boldsymbol{x}_k\\). Propositions of this form will play a major role below, since they summarise the evidence the agent has after \\(k\\) elements have been revealed. I’ll use \\(+\\) as a sequence concatenation operator, so \\(\\boldsymbol{y}+ \\boldsymbol{z}\\) is the sequence consisting of all of \\(\\boldsymbol{y}\\), followed by all of \\(\\boldsymbol{z}\\).\nBelot is interested in a quite general puzzle, but I’ll focus for most of the paper on a very specific instance of the puzzle. (We’ll return to the more general puzzle in the last section.) We’re going to look at the agent’s evolving credence that \\(\\boldsymbol{X}\\) is periodic. Let \\(p\\) be the proposition that \\(\\boldsymbol{X}\\) is periodic, since we’ll be returning to that proposition a lot. And let’s start by assuming the agent is a precise Bayesian, to see the challenge Belot develops.\nSay that the agent succeeds just in case her credence in \\(p\\) eventually gets on the correct side of \\(\\frac{1}{2}\\), and stays there. (The correct side is obviously the high side if \\(p\\) is true, and the low side otherwise.) That is, if \\(v\\) is the truth value function, it succeeds just in case this is true.4 \\[\\exists n \\forall m \\geq n: |v(p) - \\textit{Cr}(p | \\boldsymbol{X}_m = \\boldsymbol{x}_m)| < \\frac{1}{2}\\] The agent fails otherwise. Given the assumption that the agent is a classical Bayesian, we can step back from evaluating the agent and evaluate her prior probability function directly. So a prior \\(\\Pr\\) succeeds relative to \\(\\boldsymbol{x}\\) just in case this is true. \\[\\exists n \\forall m \\geq n: |v(p) - \\Pr(p | \\boldsymbol{X}_m = \\boldsymbol{x}_m)| < \\frac{1}{2}\\] This is reasonably intuitive; the agent is going to get a lot of data about \\(\\boldsymbol{X}\\), and it is interesting to ask whether that data eventually lets her credence in \\(p\\) get to the right side of \\(\\frac{1}{2}\\).\nGiven these notions of success and failure, we can naturally define the success set of a prior (or agent) as the set of sequences it succeeds on, and the failure set as the set of sequences it fails on.\nAbusing notation a little, say that \\(\\boldsymbol{x}_i \\supset \\boldsymbol{x}_k\\) iff \\(\\boldsymbol{x}_i\\) is a sequence that has \\(\\boldsymbol{x}_k\\) as its first \\(k\\) entries. Then we can state the first of Belot’s conditions on a good Bayesian agent/prior. A prior is open-minded just in case this condition holds: \\[\\forall \\boldsymbol{x}_k \\exists \\boldsymbol{x}_i \\supset \\boldsymbol{x}_k, \\boldsymbol{x}_j \\supset \\boldsymbol{x}_k: \\Pr(p | \\boldsymbol{X}_i = \\boldsymbol{x}_i) < \\frac{1}{2} \\wedge \\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) > \\frac{1}{2}\\] That is, no matter what happens, it is possible that the probability of \\(p\\) will fall below , and possible it will rise above . To motivate the first, consider any situation where the sequence to date has looked periodic. (If it had not looked periodic to date, presumably the probability of \\(p\\) should already be low.) Now extend that sequence with a large of random noise. At the end of this, it should no longer be probable that the sequence is periodic. On the other hand, assume the sequence has not looked periodic to date. Extend it by repeating \\(\\boldsymbol{x}_k\\) more than \\(k\\) times. At the end of this, it should look probable that the sequence is periodic (at least for large enough \\(k\\)). So open-mindedness looks like a good condition to impose.\nThe second condition we might impose, though not one Belot names, is modesty. Any function might fail. One natural way it might fail is that it might get, to use a term Belot does use, flummoxed. It could change its mind infinitely often about whether the sequence is periodic. By definition, open-mindedness entails the possibility of being flummoxed. Given the definitions of success and failure, \\(\\Pr\\) will fail relative to any \\(\\boldsymbol{x}\\) that flummoxes it. So success is not a priori guaranteed. Now for any function we can work out the set of sequences relative to which it fails. It turns out this will be a rather large set. Indeed, the set of sequences on which any open-minded function succeeds is meagre.5 Say a function is modest if the initial probability it gives to \\(\\boldsymbol{X}\\) being in its success set is less than 1. Given how large the failure set is, modesty also seems like a good requirement.6\nThe argument for modesty is not that it is an immediate consequence of regularity. It does follow from regularity, but in the case we’re considering, regularity is quite implausible. Some sets, even some quite large sets in some sense, will have to be given probability 0. The surprising thing is that a residual set (i.e., the complement of a meagre set) gets probability 0.\nIt might be thought that modesty here is problematic for the same reason that epistemic modesty is often problematic: it validates Moore-paradoxical thoughts. It’s bad to say p, but there is a probability that not p. It’s even bad, though as Briggs (2009) points out, not quite bad for the same reasons, to say Whether I believe p is true or false tomorrow, there will be a probability I’m false. Perhaps modesty is a requirement that someone say something like that, and hence is an improper requirement.\nBut in fact the requirement of modesty is disanalogous to the ‘requirement,’ suggested in the previous paragraph, that agents endorse Moore-paradoxical principles. There isn’t anything wrong with saying Whichever side of one half my credence in p is tomorrow, there is a probability that the truth will be the other side of one half. That’s not Moore-paradoxical. Indeed, unless one is sure that one’s credence in \\(p\\) tomorrow will be 0 or 1, it is something one should endorse.\nOr consider a different example. There will be a sequence of 0s and 1s, but this time there will only be three elements, and the agent will only be shown the first of them tomorrow. Let \\(q\\) be the proposition that there are more 1s than 0s in the three-element sequence. Say the agent succeeds iff tomorrow, after seeing just one element, her credence in \\(q\\) is the same side of one-half as the truth. And say the agent is modest iff, right now, her credence that she succeeds tomorrow is less than one. There is nothing incoherent about being modest. If her credal distribution today is completely flat, giving \\(\\frac{1}{8}\\) credence to each of the eight possible sequences, she will be modest, for example.\nNow this case is somewhat different to the one Belot started with in a couple of respects. On the one hand, we’re asking about modesty at a particular point, i.e., tomorrow, rather than over a long sequence. On the other hand, we’re asking about whether the agent’s credences will be on the right side of one-half after having seen one-third of the data, rather than, as in the original case, after seeing measure zero of the sequence. The first difference makes it easier to be modest, the second difference makes it harder. So the cases are not perfect analogies, but they are similar enough in respect of modesty to make it plausible that if modesty is coherent in this case, as we’ve shown it is, then it should be coherent in Belot’s case as well.\nSo that’s the argument that open-mindedness and modesty are good conditions for priors to satisfy. Here’s the worrying result that Belot proves. There are no open-minded modest priors. If \\(A\\) is a classical Bayesian, she will either have to be closed minded or immodest. Neither seems rational, so it seems that being a classical Bayesian is incompatible with being rational. That is, we can’t be precise Bayesians if we accept the following two constraints.\nOpen-Mindedness: For any initial sequence, there is a continuation after which it seems probable that \\(\\boldsymbol{X}\\) is periodic, and a continuation after which it seems probable that \\(\\boldsymbol{X}\\) is not periodic.\nModesty: The initial probability that the agent will succeed, i.e., that their credence in \\(p\\) will eventually get to the right side of \\(\\frac{1}{2}\\) and stay there, is less than 1.\nSince both open-mindedness and modesty are very plausible constraints, it follows that there is no good way to be a precise Bayesian in the face of this puzzle.\nMaking the Puzzle Less Precise\nWhat happens, though, if the agent is an imprecise Bayesian? Is there a parallel version of Belot’s argument that shows this kind of imprecise Bayesian is necessarily irrational? I’m going to argue that the answer is no.\nThe first thing we have to do is work out how to redefine the key terms in Belot’s argument once we drop the assumption that the agent is a classical Bayesian. There are several ways of formulating our definitions which are equivalent given that assumption, but not equivalent given that the agent is an imprecise Bayesian. There are three major choice points here.\nWhat is success?\nWhat is open-mindedness?\nWhat is modesty?\nAssume our agent’s credal state is represented by set \\(S\\) of probability functions. Then there are two natural ways to think about success. \\[\\begin{aligned}\n\\forall \\Pr \\in S:  \\exists n \\forall m \\geq n: |v(p) - \\Pr(p | \\boldsymbol{X}_m= \\boldsymbol{x}_m)| < \\frac{1}{2} \\\\\n\\exists n \\forall \\Pr \\in S: \\forall m \\geq n: |v(p) - \\Pr(p | \\boldsymbol{X}_m= \\boldsymbol{x}_m)| < \\frac{1}{2}\\end{aligned}\\] The second is obviously stronger than the first, since it involves moving an existential quantifier out in front of a universal quantifier. And there are some natural cases where an agent could succeed on the first definition, and fail on the second. Here’s one such case.\nLet \\(\\Pr_0\\) be the fair-coin measure. Acccording to the fair coin measure, if \\(\\boldsymbol{y}\\) is any \\(k\\) length sequence of 0s and 1s we have \\(\\Pr_0(\\boldsymbol{x}_k = \\boldsymbol{y}) = 2^{-k}\\). Intuitively, it thinks the 0s and 1s are generated by flips of a fair coin, and it won’t change its mind about that no matter what happens.\nSay a probability function \\(\\Pr\\) is regular periodic iff it satisfies these two conditions.\n\\(\\Pr(p) = 1\\).\nFor any periodic sequence \\(\\boldsymbol{y}\\), \\(\\Pr(\\boldsymbol{X}= \\boldsymbol{y}) > 0\\).\nIntuitively, these functions are certain that \\(X\\) is periodic, and assign positive probability to each possible periodic sequence. Now consider the family of functions we get by taking equal weighted mixtures of \\(\\Pr_0\\) with each regular periodic function. Let that family represent the agent’s credence. And assume for now that \\(\\boldsymbol{X}\\) is the sequence \\(\\langle 0, 0, 0, \\dots \\rangle\\). Does the agent succeed?\nWell, each \\(\\Pr\\) in her representor succeeds. To prove this, it will be helpful to prove a lemma that we’ll again have use for below. For this lemma, let \\(\\Pr_0\\) be the fair-coin measure (as already noted), \\(\\Pr_1\\) be any measure such that \\(\\Pr_1(p) = 1\\), and \\(\\Pr_2\\) be the equal mixture of \\(\\Pr_0\\) and \\(\\Pr_1\\).\nLemma 1: \\(\\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\frac{1}{2}\\) iff \\(\\Pr_1(\\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\Pr_0(\\boldsymbol{X}_k = \\boldsymbol{y}_k).\\)\n\nProof. Let \\(\\Pr_i(\\boldsymbol{X}_k = \\boldsymbol{y}_k) = a_i\\) for \\(i \\in {0, 1}\\). Recall that \\(\\Pr_0(p) = 0\\) and \\(\\Pr_1(p) = 1\\). Then we can quickly get that \\(\\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) = \\frac{a_1}{a_0 + a_1}\\), from which the lemma immediately follows. ◻\n\nFor any \\(\\Pr\\) in the agent’s representor, there is some \\(k\\) such that \\(\\Pr(\\boldsymbol{X}= \\langle 0, 0, 0, \\dots \\rangle)\\) \\(> 2^{-k}\\). So after at most \\(k\\) 0s have appeared, \\(\\Pr(p)\\) will be above \\(\\frac{1}{2}\\), and it isn’t coming back. That means it succeeds. And since \\(\\Pr\\) was arbitrary, it follows that all \\(\\Pr\\) succeed.\nBut the agent in a good sense doesn’t succeed. No matter how much data she gets, there will be \\(\\Pr\\) in her representor according to which \\(\\Pr(p) < \\frac{1}{2}\\). After all, for any \\(k\\), there are regular periodic \\(\\Pr\\) such that the probability of \\(\\boldsymbol{x}_k\\) being \\(k\\) 0s is below \\(\\frac{1}{2^k}\\). So if we mix that function with \\(\\Pr_0\\), we get a function where the most probable continuations of this initial sequence are the random sequences provided by the fair coin measure.\nIn terms of our definitions of success above, the agent satisfies the first, but not the second. Every function in her representor eventually has the probability of \\(p\\) go above \\(\\frac{1}{2}\\). But at any time, there are functions in her representor according to which the probability of \\(p\\) is arbitrarily low.\nHere I think we have to make a distinction between different ways of understanding the formalism of imprecise probabilities. (What follows is indebted to Bradley (2014), especially his section 3.1, but I’m disagreeing somewhat with his conclusions, and following more closely the conclusions of Joyce (2010) and Schoenfield (2012).)\nOne way of thinking about imprecise credences is that each probability function in the representor is something like an advisor, and the agent who is imprecise simply hasn’t settled on which advisor to trust. Call this the pluralist interpretation of the formalism. On this interpretation, it is natural to think that what is true of every function is true of the agent.\nAnother way is to think of the agent’s mind as constituted by, but distinct from, the representors. An analogy to keep in mind here is the way that a parliament is constituted by, but distinct from, its members. Keeping with this analogy, call this the corporate interpretation of the formalism. Note that corporate bodies will typically have their own rules for how the views of the members will be translated into being views of the whole. Even if every member of the parliament believes that the national cricket team will win its upcoming game, it doesn’t follow that the parliament believes that; the parliament only believes what it resolves it has believed.\nNow I only want to defend the imprecise Bayesian model on the corporate interpretation.7 The pluralist interpretation, it seems to me, faces grave difficulties. For one thing, it has a hard time explaining what’s wrong with the existential claim “There is a precise number \\(x\\) such that \\(x\\) is the probability of \\(p\\).” Every advisor believes that, so on the pluralist model the agent does too. (Compare the criticisms of “fanatical supervaluationism” in Lewis (1993).) More relevant to the discussion here, I am following Belot in thinking we have an argument that each precise Bayesian is unreasonably proud. On the pluralist interpretation, the agent is undecided which of these unreasonable advisors she will follow. But such a state is itself unreasonable; she should have decided not to follow any of them, since they are all provably unreasonable!\nA surprising fact about corporate bodies is that they can be immune to problems that beset each of their members. It would be illegitimate for any one parliamentarian to have law-making power; it is (or at least can be) legitimate for them all to have such power. Indeed, it would be unreasonable for any of them to think that they individually should have law-making powers; that would be unreasonably proud. But it is not unreasonable for them to collectively think that they should collectively have law-making powers. If they are a well-constituted parliament, this is a perfectly reasonable thought. Similarly here, the agent, the corporate body, could avoid being unreasonably proud even though each of the representors is over-confident in its own powers.\nNow going back to success and modesty, it seems to me that the first definition of success is appropriate on the pluralist interpretation of the imprecise framework, and the second is appropriate on the corporate interpretation. The first interpretation says that the agent succeeds iff every member succeeds. And the second says that the agent succeeds iff the body of functions, collectively, succeed. Since I’m defending the use of the imprecise framework on the corporate interpretation, it is the second definition of success that is appropriate, and that’s what I will use here.\nThis understanding isn’t without costs. Bradley (2014) argues, in effect, that the best responses to dilation-based arguments against imprecise probabilities (as in White (2010)), are only available on the pluralist interpretation. I’m not going to try to solve those problems here, but I will note that the interpretative choice I’m making generates some extra philosophical work elsewhere. Against that, the corporate interpretation has some benefits. It lets us agree with Peter Walley (1991) that there are rational agents who are represented by sets of merely finitely additive probability functions, though no merely finitely additive probability function on its own could represent a rational agent. So the issues between the two interpretations are extensive. For now, I’ll simply note that I’m interested in defending the imprecise Bayesian from Belot’s argument on the corporate interpretation. And with that I’ll return to translating Belot’s puzzle into the imprecise framework, with the second, corporate-friendly, interpretation of success on board.\nThere are also two natural ways to generalise Belot’s notion of open-mindedness to the imprecise case. We could require that the agent satisfies either the first or second of these conditions. \\[\\begin{aligned}\n\\forall \\boldsymbol{x}_k \\exists \\boldsymbol{x}_i \\supset \\boldsymbol{x}_k, \\boldsymbol{x}_j \\supset \\boldsymbol{x}_k: \\neg(\\Pr(p | \\boldsymbol{X}_i = \\boldsymbol{x}_i) \\geq \\frac{1}{2}) \\wedge \\neg(\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) < \\frac{1}{2}) \\\\\n\\forall \\boldsymbol{x}_k \\exists \\boldsymbol{x}_i \\supset \\boldsymbol{x}_k, \\boldsymbol{x}_j \\supset \\boldsymbol{x}_k: \\Pr(p | \\boldsymbol{X}_i = \\boldsymbol{x}_i) < \\frac{1}{2} \\wedge \\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) \\geq \\frac{1}{2}\\end{aligned}\\] The second is just the same symbols as in Belot’s, and it is what I’ll end up arguing is the right constraint to put on the imprecise Bayesian agent. And it is a considerably more demanding constraint than the first. But the first is perhaps the more natural understanding of open-mindedness. It says that no matter what the initial evidence is, the agent is not guaranteed to settle her credence in \\(p\\) on one side of \\(\\frac{1}{2}\\). That’s a way of being open-minded.\nBut if the agent satisfies that constraint, she may be open-minded, but she won’t necessarily be responsive to the evidence. Here’s how I’m using the terms ‘open-minded’ and ‘evidence-responsive.’ In both clauses, the quantification is intended to be over a salient class of propositions. (The relevant class in the application we’re most interested in is just \\(\\{X\\) is periodic, \\(X\\) is not periodic\\(\\}\\).) And I’ll say an agent is ‘confident’ in a proposition iff her credence in it is above \\(\\frac{1}{2}\\).\nOpen-Minded\nAny time an agent is confidence in a proposition, there is some evidence she could get that would make her lose confidence in it.\n\nEvidence-Responsive\nFor any proposition, there is some evidence the agent could get that would make her confident in it.\n\nOnce we allow imprecise credences, these two notions can come apart. Consider the agent we described above, whose representor consists of equal mixtures of the fair-coin measure and regular periodic functions. They are open-minded; they can always lose confidence that \\(X\\) is periodic or not. But they aren’t evidence-responsive; no matter what the evidence, their credence that \\(X\\) is periodic will never rise above \\(\\frac{1}{2}\\). In fact, their credence that \\(X\\) is periodic will never rise above any positive number.\nThat suggests open-mindedness is too weak a constraint. If the evidence the agent gets is a string of several hundred 0s, she shouldn’t just lose any initial confidence in \\(\\neg p\\), she should become confident in \\(p\\). And arguably (though I could imagine a dissent here), if the initial sequence is a seemingly random sequence, the credence in \\(p\\) should drop well below \\(\\frac{1}{2}\\). (The imagined dissent here is from someone who thinks that the noisier the data, the more imprecise credences should get. That’s an interesting view, but perhaps orthogonal to the issues we’re debating here.)\nAnd when we look back at Belot’s motivations for open-mindedness, we see that they are really motivations for being evidence-responsive. One of the distinctive (and I would say problematic) features of precise Bayesianism is that it doesn’t really have a good way of representing a state of indecisiveness or open-mindedness. In the terms we’ve been using here, there’s no difference for the precise Bayesian between being evidence responsive and open minded. The imprecise Bayesian can distinguish these. And in Belot’s puzzle, we should require that the imprecise Bayesian agent is evidence responsive. So we should impose the second, stronger, condition.\nThe final condition to discuss is modesty. There are three natural candidates here. We could merely require that the agent’s prior probability that \\(\\boldsymbol{x}\\) is in her success set is not equal to 1. Or we could require that it be less than 1. Or, even more strongly, we could require that it be less than some number that is less than 1. If her credence that \\(\\boldsymbol{x}\\) is in her success set is imprecise over some interval \\([k, 1]\\), she satisfies the first condition, but not the second or third. If it is imprecise over some interval \\((k, 1)\\), or \\([k, 1)\\), she satisfies the first and second conditions, but not the third. In the interests of setting the imprecise Bayesian the hardest possible challenge, though, let’s say that modesty requires the third criteria. Her ex ante credence in success should not just be less than 1, it should be less than some number less than 1.\nThe aim of the next section is to describe a representor that satisfies open-mindedness and modesty with respect to the question of whether the sequence is periodic. The representor will not represent a state that it is rational for a person to be in; we’ll come back in the last section to the significance of this. My aim is just to show that for the imprecise Bayesian, unlike the precise Bayesian, open-mindedness and modesty are compatible. And the proof of this will be constructive; I’ll build a representor that is, while flawed in some other ways, open-minded and modest.\nMeeting the Challenge, Imprecisely\nRecall that \\(\\Pr_0\\) is the fair-coin measure, according to which, if \\(\\boldsymbol{y}\\) is any \\(k\\) length sequence of 0s and 1s we have \\(\\Pr_0(\\boldsymbol{X}_k = \\boldsymbol{y}) = 2^{-k}\\).\nSay a finite sequence \\(\\boldsymbol{y}_k\\) of length \\(k\\) is repeating iff for some \\(n > 1\\), \\(\\boldsymbol{y}_k\\) consists of \\(n\\) repetitions of a sequence of length \\(k/n\\). For any non-repeating sequence \\(\\boldsymbol{y}_k\\) (of length \\(k\\)) let \\(\\boldsymbol{s}_{\\boldsymbol{y}_k}\\) be the sequence consisting of \\(\\boldsymbol{y}_k\\) repeated infinitely often. Let \\(\\Pr_1\\) be the function such that, \\[\\Pr{}_1(\\boldsymbol{X}= \\boldsymbol{s}_{\\boldsymbol{y}_k}) = \\frac{1}{2^{2k}-1}\\] Intuitively, we can think of \\(\\Pr_1\\) as follows. Consider a measure over representations of periodic sequences. Any periodic sequence can be represented just as a finite sequence, plus the instruction repeat infinitely often, so this is really just a measure over finite sequences. One natural such measure assigns measure \\(\\frac{1}{2^{2k}}\\) to each sequence of length \\(k\\). Of course, several of these representations will be representations of the same sequence. For instance, \\(\\langle 0, 1 \\rangle\\), \\(\\langle 0, 1, 0, 1 \\rangle\\) and \\(\\langle 0, 1, 0, 1, 0, 1 \\rangle\\) repeated infinitely produce the same sequence. Now the probability of a sequence, according to \\(\\Pr_1\\) is just the measure, so defined, of the class of representations of that measure. (It’s a little easier to confirm that the measures sum to 1 than that the probabilities do, which is why I’ve included this little explanation.)\nNow define \\(\\Pr_2\\) as the equal weight mixture of \\(\\Pr_0\\) and \\(\\Pr_1\\), i.e., \\(\\Pr_2(q) = (\\Pr_0(q) + \\Pr_1(q))/2\\). Since \\(\\Pr_0(p) = 0\\), and \\(\\Pr_1(p) = 1\\), \\(\\Pr_2(p) = \\frac{1}{2}\\). There will be several facts about \\(\\Pr_2\\) that are useful to have in place for future reference. (Recall I’m using \\(\\boldsymbol{X}\\) as a random variable for the sequence the agent will see, \\(\\boldsymbol{x}\\) as a rigid designator of that sequence, \\(\\boldsymbol{y}\\) and \\(\\boldsymbol{z}\\) are variables for arbitrary sequences, and the \\(k\\) subscript to restrict sequences to length \\(k\\).) The first of these was proven as Lemma 1.\nLemma 1.\n\\(\\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\frac{1}{2} \\text{ iff } \\Pr_1(\\boldsymbol{X}_k = \\boldsymbol{y}_k) > \\Pr_0(\\boldsymbol{X}_k = \\boldsymbol{y}_k).\\)\n\nDefine a new predicate \\(N\\) of finite sequences \\(\\boldsymbol{y}_k\\), to hold just in case \\(\\boldsymbol{y}_k\\) could be the initial segment of an infinite sequence of period at most \\(\\frac{k}{2}\\). So \\(\\boldsymbol{y}_k\\) must consist of some sequence repeated twice, and anything else in \\(\\boldsymbol{y}_k\\) must be consistent with that sequence repeating again (and if necessary again, and again, …). Then we get,\nLemma 2.\nFor \\(k \\geq 2\\), \\(\\Pr_2(p | \\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) > \\frac{1}{2}\\) iff \\(N\\boldsymbol{y}_{2k}\\).\n\n\nProof. By Lemma 1, this reduces to the question of the relationship \\(\\Pr_1(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) > \\Pr_0(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k})\\). Moreover, we know that \\(\\Pr_0(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) = 2^{-2k}\\). So the question is whether \\(\\Pr_1(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) > 2^{-2k}\\).\nIf \\(N\\boldsymbol{y}_{2k}\\), then it is consistent with \\(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}\\) that \\(\\boldsymbol{x}\\) is a particular periodic sequence with period at most \\(k\\). Since the probability, according to \\(\\Pr_1\\) of any such sequence is greater than \\(2^{-2k}\\), the right-to-left direction follows.\nIf \\(\\neg N\\boldsymbol{y}_{2k}\\), then the possibilities that get positive probability according to \\(\\Pr_1\\) are at most among the following: \\(\\boldsymbol{X}\\) consists of the first \\(k + 1\\) digits of \\(\\boldsymbol{y}_{2k}\\) repeated endlessly; \\(\\boldsymbol{X}\\) consists of the first \\(k + 2\\) digits of \\(\\boldsymbol{y}_{2k}\\) repeated endlessly; …; \\(\\boldsymbol{x}\\) consists of the first \\(2k\\) digits of \\(\\boldsymbol{y}_{2k}\\) repeated endlessly; \\(\\boldsymbol{X}\\) is one of the two sequences of period \\(2k + 1\\) starting with \\(\\boldsymbol{y}_{2k}\\), or one of the four sequences of period \\(2k+2\\) starting with \\(\\boldsymbol{y}_{2k}\\) or …. So we get the following, starting with the probabilities of each of the possibilities listed in the previous sentence, \\[\\begin{aligned}\n\\Pr{}_1(\\boldsymbol{X}_{2k} = \\boldsymbol{y}_{2k}) \n    &\\leq \\frac{1}{2^{2k+2}-1} \n    &+ &\\frac{1}{2^{2k+4}-1} \n    &+ \\dots \n    &+ &\\frac{1}{2^{4k}-1} \n    &+ &\\frac{2}{2^{4k+2}-1} \n    &+ \\dots \\\\\n%\n    &< \\frac{1}{2^{2k+1}} \n    &+ &\\frac{1}{2^{2k+3}} \n    &+ \\dots\n    &+ &\\frac{1}{2^{4k-1}}\n    &+ &\\frac{1}{2^{4k}}\n    &+ \\dots \\\\\n%\n    &< \\frac{1}{2^{2k}}\\end{aligned}\\] And from that the left-to-right direction follows. ◻\n\nLemma 3\n\\(\\Pr_2\\) is open-minded.\n\n\nProof. Since any initial sequence \\(\\boldsymbol{y}_k\\) that is not \\(N\\) can be easily extended into one that is \\(N\\) (by, e.g., repeating \\(\\boldsymbol{y}_k\\)), and one is that is \\(N\\) can be extended into one that is not (by, e.g., having the repeating sequence stop at the very next step), this follows immediately from Lemma 2. ◻\n\nDefine \\(f\\) to be a function from sequences of length \\(k \\geq 2\\) to sequences of length \\(k+1\\) such that \\[f(\\boldsymbol{y}_k) = \\boldsymbol{y}_k + \n    \\begin{cases} \n        \\langle 0 \\rangle &\\text{if } N\\boldsymbol{y}_k \\leftrightarrow \\Pr{}_1(x_{k+1} = 0 | \\boldsymbol{X}_k = \\boldsymbol{y}_k) \\leq \\frac{1}{2} \\\\\n        \\langle 1 \\rangle &\\text{otherwise}\n    \\end{cases}\\] In the normal way, define \\(f^n(\\boldsymbol{y}_k)\\) to be the result of applying \\(f\\) \\(n\\) times to \\(\\boldsymbol{y}_k\\). And define \\(f^\\infty(\\boldsymbol{y}_k)\\) to be the infinite sequence we get by doing this infintely often.\nIntuitively, the way \\(f\\) works is that if \\(\\boldsymbol{y}_k\\) is already somewhat sequential, then we include the less likely digit, and if it isn’t, then we include the more likely digit. (With ties resolved in favour of including 0 rather than 1.) If we define \\(p(\\boldsymbol{y}_k)\\) to be the smallest \\(n\\) such that \\(\\boldsymbol{y}_k\\) could be the initial segment of a periodic sequence of length \\(n\\), then we’ll get that \\(p(f(\\boldsymbol{y}_k)) > p(\\boldsymbol{y}_k) \\leftrightarrow N\\boldsymbol{y}_k\\) in all cases, except for the case where \\(\\Pr{}_1(\\boldsymbol{x}_k = 0 | \\boldsymbol{X}_k = \\boldsymbol{y}_k) = \\frac{1}{2}\\). That is, if \\(N\\boldsymbol{y}_k\\), then extending \\(\\boldsymbol{y}_k\\) in this way will wipe out the possibility of that smallest sequence being extended indefinitely, while if \\(\\neg N\\boldsymbol{y}_k\\), then that possibility will still be on the table.\nFrom this, it follows that \\(f^{\\infty}(\\boldsymbol{y}_k)\\) will flummox \\(\\Pr_2\\), no matter which \\(\\boldsymbol{y}_k\\) we start with.\nWe need one last classification of finite sequences, and then we are done. Say that \\(O\\boldsymbol{y}_k\\) just in case some initial segment of \\(\\boldsymbol{y}_k\\) of length \\(r\\) could be the initial segment of an infinite period sequence of period less than \\(\\frac{r}{2}\\). This contrasts with \\(N\\) in two ways. First, it requires a sequence that repeats twice, and then starts a third repetition. Second, it does not require that the sequence be ‘live’; there might be subsequent parts of \\(\\boldsymbol{y}_k\\) that are not compatible with the sequence repeating. So the sequence \\(\\langle 0, 0, 1, 0, 0, 1\\rangle\\) satisfies \\(N\\) but not \\(O\\), while the sequence \\(\\langle 0, 1, 0, 1, 0, 0\\rangle\\) satisfies \\(O\\) but not \\(N\\).\nThere are a countable infinity of finite sequences \\(\\boldsymbol{y}_k\\) such that \\(\\neg O \\boldsymbol{y}_k\\). Produce some ordering of them, then define \\(\\Pr_i\\), for \\(i \\geq 3\\), to be the probability function such that \\(\\Pr_i(\\boldsymbol{X}= f^\\infty(\\boldsymbol{y}_k)) = 1\\), where \\(\\boldsymbol{y}_k\\) is the \\(i-2\\)’th sequence in this order.\nNow, consider the set \\(R\\) of all probability functions of the form: \\[\\Pr = \\sum_{i = 2}^\\infty a_i\\Pr{}_i\\] where each of the \\(\\Pr_i\\) are defined as above, each \\(a_i\\) is non-negative, \\(a_2\\) is , and the sum of the \\(a_i\\) from 3 to \\(\\infty\\) is also \\(\\frac{1}{2}\\). Intuitively, each function starts by halving the probability \\(\\Pr_2\\) gives to each initial (or completed) sequence, and distributing the remaining probability over the countable infinity of flummoxing sequences of the form \\(f^\\infty(\\boldsymbol{y}_k)\\), where \\(\\neg O\\boldsymbol{y}_k\\).\nI’ll now prove that \\(R\\) is open minded.\nLemma 4\nIf \\(\\neg O \\boldsymbol{y}_k\\), then \\(\\neg O f(\\boldsymbol{y}_k)\\).\n\n\nProof. Since \\(\\neg O \\boldsymbol{y}_k\\), the only way that \\(O f(\\boldsymbol{y}_k)\\) could be true is if \\(k = 2r +1\\), and \\(f(\\boldsymbol{y}_k)\\) consists of some sequence of length \\(r\\) repeated twice, plus the first digit repeated a third time. But that means that \\(N\\boldsymbol{y}_k\\). And if that’s the case, then the extra digit that is added by \\(f(\\boldsymbol{y}_k)\\) will not be the necessary digit to repeat this sequence. So it is impossible that \\(O f(\\boldsymbol{y}_k)\\). ◻\n\nLemma 5\nIf \\(\\neg O \\boldsymbol{y}_k\\), then \\(\\neg O f^\\infty(\\boldsymbol{y}_k)\\).\n\n\nProof. This follows trivially from Lemma 4. ◻\n\nTheorem 6\n\\(R\\) is open-minded.\n\n\nProof. Any initial sequence can be extended to a sequence satisfying \\(O\\). For example, the initial sequence can be repeated in full twice. An immediate consequence of Lemma 5 is that for all \\(i \\geq 3, O\\boldsymbol{y}_k \\rightarrow \\Pr_i(\\boldsymbol{X}_k = \\boldsymbol{y}_k) = 0\\). That means that if \\(O\\boldsymbol{y}_k\\), then for any \\(\\Pr \\in R,{ } \\Pr(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k) = \\Pr_2(p | \\boldsymbol{X}_k = \\boldsymbol{y}_k)\\). And now the theorem is an immediate consequence of Lemma 3. ◻\n\nLet \\(F\\) be the set of all sequences \\(f^\\infty(\\boldsymbol{y}_k)\\), where \\(\\neg O \\boldsymbol{y}_k\\).\nLemma 7\nIf \\(\\boldsymbol{x}\\in F\\), then \\(R\\) fails.\n\n\nProof. Assume \\(\\boldsymbol{x}\\in F\\), so \\(\\boldsymbol{x}\\) is not periodic. Then proving the lemma requires showing that for any \\(i\\), there is a \\(j \\geq i\\) such that, according to \\(R\\), the probability of \\(p\\) given \\(\\boldsymbol{X}_j =\\boldsymbol{x}_j\\) is not less than \\(\\frac{1}{2}\\). And that requires showing that there is a \\(\\Pr \\in R\\) such that \\(\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) \\geq \\frac{1}{2}\\). This is easy to do. Consider any sequence \\(\\boldsymbol{y}_i\\) of length \\(i\\) not identical to \\(\\boldsymbol{x}_i\\) such that \\(\\neg O \\boldsymbol{y}_i\\). Consider the probability function \\(\\Pr_k \\in R\\) such that \\(\\Pr_k(\\boldsymbol{X}= f^\\infty(\\boldsymbol{y}_i)) = \\frac{1}{2}\\). Once we conditionalise on \\(\\boldsymbol{X}_i = \\boldsymbol{x}_i\\), that function will behave just like \\(\\Pr_2\\). And since \\(\\boldsymbol{X}\\) flummoxes \\(\\Pr_2\\), that means there is a \\(\\boldsymbol{x}_j\\) such that \\(\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) > \\frac{1}{2}\\), and hence \\(\\Pr(p | \\boldsymbol{X}_j = \\boldsymbol{x}_j) \\geq \\frac{1}{2}\\). ◻\n\nLemma 8\nFor each \\(\\Pr \\in R, \\Pr(\\boldsymbol{x}\\in F) = \\frac{1}{2}.\\)\n\n\nProof. It helps to think of each of the \\(\\Pr \\in R\\) as mixtures of \\(\\Pr_0\\) and \\(\\Pr_1\\), plus a mixture of the \\(\\Pr_i\\) for \\(i \\geq 3\\). Now \\(\\Pr_0(\\boldsymbol{x}\\in F) = 0\\), since for any countable set, \\(\\Pr_0\\) says the probability that \\(\\boldsymbol{x}\\) is in that set is 0. And \\(\\Pr_1(\\boldsymbol{x}\\in F) = 0\\), since \\(\\Pr_1\\) says that the probability of \\(\\boldsymbol{x}\\) being periodic is 1, and none of the members of \\(F\\) are periodic. But for each \\(\\Pr_i\\) for \\(i \\geq 3\\), \\(\\Pr_i(\\boldsymbol{x}\\in F) = 1\\). Indeed, for each such function, there is a particular sequence in \\(F\\) such that the probability that \\(\\boldsymbol{x}\\) is that sequence is 1. So for each \\(\\Pr \\in R, \\Pr(\\boldsymbol{x}\\in F) = \\frac{1}{4} \\times 0 + \\frac{1}{4} \\times 0 + \\frac{1}{2} \\times 1 = \\frac{1}{2}\\). ◻\n\nTheorem 9\nAccording to \\(R\\), the probability of an agent whose representor is \\(R\\) failing is at least \\(\\frac{1}{2}\\).\n\n\nProof. Immediate from Lemma 7 and Lemma 8. ◻\n\nSo if an agent’s credences are represented by a non-singleton set of probability functions, not a single probability function, it is possible for them to be open-minded and modest. On the other hand, if an agent is represented by a single probability function, as the precise Bayesian desires, then it is impossible to be open-minded and modest. Since being open-minded and modest is desirable, this is a reason to prefer the imprecise Bayesian picture.\nObjections and Replies\nI’m going to reply to three objections, but since my replies overlap, I’ll group the objections together.\nObjection 1\nThe model here only gives you conditional modesty. Once the initial sequence is \\(O\\), the representor becomes the singleton of an open-minded probability function, and Belot showed that to be immodest. Ideally, the agent would have a prior that is in some way resiliently modest, whereas this prior is fragilely modest.\n\nObjection 2\nThis representor is open-minded and modest towards one particular problem, namely whether \\(\\boldsymbol{X}\\) is periodic. But Belot was interested in a wider range of problems, indeed in all problems of the form: does \\(\\boldsymbol{x}\\) fall into some set that is measurable, dense, and has a dense complement. Ideally, we’d have a prior which is widely open-minded and modest, in the sense that it had an open-minded and modest attitude towards many problems. But this prior is narrowly modest, in the sense that it is open-minded and modest about only one problem.\n\nObjection 3\nThe representor described here is clearly not a representation of a credal state of anyone rational. Look what it does if the data is a 1 followed by thousands of 0s, or is the first few thousand digits of the binary expansion of \\(\\pi\\), or has a frequency of 0s of 0.2 over all large sub-intervals. No one could adopt this prior, so it doesn’t show anything about the advantages of imprecise Bayesianism.\n\n\nReply. My responses are going to be (1) that we should want more resilient modesty, and though this is a hard technical challenge, it’s possible to see a way forward on it, (2) that we should want somewhat wider open-minded modesty, though how much wider is a hard question, and (3) that the third objection should simply be rejected. Let’s go through those in reverse order, since it’s the response to the third that explains part of what I’m doing in response to the other two.\nWhat we have in section three is a consistency proof. For the imprecise Bayesian, unlike the precise Bayesian, being open-minded is consistent with being modest. That’s good, since it shows that we can’t rule out a rational response to problems like Belot’s. It’s obviously true that the prior in question isn’t rational, but that’s not needed for a consistency proof.\nMoreover, we don’t just have a consistency proof, we have a constructive consistency proof - the prior is described in detail. It’s just not going to be possible to do a constructive proof that open-mindedness, modesty and full rationality are consistent. And that’s because to do that would essentially be to solve all of the problems of epistemology ever. Demonstrating a fully rational prior, even for the range of questions Belot considers, is too much to ask.\nIf there’s a reasonable looking argument that imprecise Bayesians are unlikely to be able to satisfy some set of plausible constraints, then the defender of imprecise Bayesianism is, I think, obliged to show how those constraints can be satisfied. But to ask for a demonstration of how all reasonable constraints can be satisfied at once, in the absence of a decent argument that they cannot be, would clearly be asking too much.\nSo I don’t care that the prior I described is irrational; it serves its purpose in proving consistency. Now what would be nice is to show that some slightly stronger constraints can be simultaneously satisfied. But we have to be sure that those constraints are in fact reasonable constraints. Here’s one constraint that I think isn’t reasonable: be open-minded towards any proposition of the form \\({\\boldsymbol{X}\\in S}\\), where \\(S\\) is a dense set of sequences. Let \\(S\\), for example, be the set consisting of all sequences of the form \\(\\boldsymbol{y}_k + \\boldsymbol{z}\\), where \\(\\boldsymbol{y}_k\\) ranges over all finite sequeneces, and \\(\\boldsymbol{z}\\) is a particular arbitrary sequence that lacks finite definition in our current language. That set is dense, and indeed measurable. But there’s no evidence that could make it reasonable to take \\(\\boldsymbol{X}\\in S\\) to be probable. So a prior that wasn’t open-minded towards \\(\\boldsymbol{X}\\in S\\) could still be perfectly reasonable.\nThat said, the prior I demonstrated is closed-minded towards several propositions that should be taken seriously. It will never have positive credence that \\(\\boldsymbol{X}\\) is eventually periodic without being periodic, or that \\(\\boldsymbol{X}\\) is generated by a chance process that gives each data point chance \\(c \\neq \\frac{1}{2}\\) of being 0. It would be good to have a prior whose open-minded modesty was wider. But before we do that technical work, I think there’s a need to figure out which propositions we should be open-minded about.\nI am more worried by the fragility of the modesty of this prior. There’s a reasonable sense in which the prior is open-minded only in virtue of the fact that it has parts which are immodest. At any point where the agent has credence above \\(\\frac{1}{2}\\) that \\(p\\), she has credence 1 that she will succeed.\nWe could try to complicate the prior a bit more to avoid that. Here’s a sketch of how it could go, with application to one particular initial sequence of data. Consider what happens to \\(R\\) if the initial input is \\(\\langle 0, 1, 0, 0, 1, 0, 0, 1\\rangle\\), hereafter \\(\\boldsymbol{y}\\). According to \\(\\Pr_0\\), that initial sequence has probability \\(\\frac{1}{256}\\). According to \\(\\Pr_1\\), it has probability \\(\\frac{1}{63} + \\frac{1}{4095} + \\frac{1}{65535} \\approx \\frac{1}{62}\\). So given that initial sequence, \\(\\Pr_2\\) says the probability of \\(p\\) is about \\(\\frac{4}{5}\\). And since the sequence is \\(O\\), it could be the start of the the sequence \\(\\langle 0, 1, 0\\rangle\\) repeated indefinitely, its probability according to \\(\\Pr_i\\) is 0, for \\(i \\geq 3\\). Now consider the set of all probability functions of the form \\(a\\Pr_{R} + b\\Pr_{New}\\), where \\(a + b = 1\\), \\(b \\in (0, \\frac{1}{256}), \\Pr_{R} \\in R\\) and \\(\\Pr_{New}\\) is the function which gives probability 1 to \\(\\boldsymbol{X}\\) being \\(O(\\boldsymbol{y})\\). That prior is open-minded, and even after conditionalising on \\(\\boldsymbol{y}\\) satisfies the intermediate of the three modesty conditions described on page - the probability of failure is less than one, though it isn’t less than some number less than one. And this trick could be generalised to satisfy more modesty conditions, and even (though it would take some time to prove this) be unconditionally modest.\nBut I’m not going to go through those steps here. That’s mostly because I think we already have shown enough to show that imprecise Bayesianism has an advantage over precise Bayesianism. The imprecise Bayesian can, and the precise Bayesian can’t, have an open-minded modest attitude. It would be good to press home that advantage and show that there are other things the imprecise Bayesian can do that the precise Bayesian can’t do, such as having a widely open-minded and resiliently modest prior. But even before such a demonstration takes place, the advantage has been established. ◻\n\n\n\nBelot, Gordon. 2013. “Bayesian Orgulity.” Philosophy of Science 80 (4): 483–503. https://doi.org/10.1086/673249.\n\n\nBradley, Seamus. 2014. “Imprecise Probabilities.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Winter 2014. http://plato.stanford.edu/archives/win2014/entries/imprecise-probabilities/; Metaphysics Research Lab, Stanford University.\n\n\nBriggs, Rachael. 2009. “Distorted Reflection.” Philosophical Review 118 (1): 59–85. https://doi.org/10.1215/00318108-2008-029.\n\n\nElga, Adam. 2010. “How to Disagree about How to Disagree.” In Disagreement, edited by Ted Warfield and Richard Feldman, 175–87. Oxford: Oxford University Press.\n\n\nJoyce, James M. 2010. “A Defence of Imprecise Credences in Inference and Decision Making.” Philosophical Perspectives 24 (1): 281–323. https://doi.org/10.1111/j.1520-8583.2010.00194.x.\n\n\nLasonen-Aarnio, Maria. 2015. “New Rational Reflection and Internalism about Rationality.” Oxford Studies in Epistemology 5: 145–71. https://doi.org/10.1093/acprof:oso/9780198722762.003.0005.\n\n\nLewis, David. 1971. “Immodest Inductive Methods.” Philosophy of Science 38 (1): 54–63. https://doi.org/10.1086/288339.\n\n\n———. 1993. “Many, but Almost One.” In Ontology, Causality, and Mind: Essays on the Philosophy of D. M. Armstrong, edited by Keith Campbell, John Bacon, and Lloyd Reinhardt, 23–38. Cambridge: Cambridge University Press. https://doi.org/10.1017/CBO9780511625343.010.\n\n\nPryor, James. 2000. “The Sceptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\nSchoenfield, Miriam. 2012. “Chilling Out on Epistemic Rationality: A Defense of Imprecise Credences (and Other Imprecise Doxastic Attitudes).” Philosophical Studies 158 (2): 197–219. https://doi.org/10.1007/s11098-012-9886-7.\n\n\n———. 2015. “A Dilemma for Calibrationism.” Philosophy and Phenomenological Research 91 (2): 425–55. https://doi.org/10.1111/phpr.12125.\n\n\nWalley, Peter. 1991. Statisical Reasoning with Imprecise Probabilities. London: Chapman & Hall.\n\n\nWhite, Roger. 2006. “Problems for Dogmatism.” Philosophical Studies 131 (3): 525–57. https://doi.org/10.1007/s11098-004-7487-9.\n\n\n———. 2010. “Evidential Symmetry and Mushy Credence.” Oxford Studies in Epistemology 3: 161–89.\n\n\nThere is another sense of immodesty that is often discussed in the literature, going back to Lewis (1971). This is the idea that some agents think their attitudes are optimal by some standards; these are the immodest ones. And often, it is held that not being self-endorsing in this way is a coherence failure Elga (2010). I don’t think this kind of immodesty is rationally required, for reasons set out by Miriam Schoenfield (2015) and Maria Lasonen-Aarnio (2015), but in any case that’s not the kind of modesty that’s at issue in Belot’s argument.↩︎\nFor dogmatism, see Pryor (2000). The canonical argument that it is inconsistent with Bayesianism is White (2006).↩︎\nNote that this formulation leaves it open which side of the biconditional is explanatorily prior. I’m going to defend a view on which the left hand side, i.e., the comparative confidences, are more explanatorily basic than the facts about what is in the agent’s representor. I say a little more about why I take this stand in footnote 7. For much more detail on varieties of imprecise Bayesianism, see Walley (1991), from whom I take the view that the representor and its members are much less explanatorily important than the comparative judgments the agent makes.↩︎\nBelot lets an agent succeed if \\(\\boldsymbol{X}\\) is periodic, and the credence in \\(p\\) never drops below \\(\\frac{1}{2}\\), but I think it’s neater to say that the agent is undecided in this case.↩︎\nA meagre subset of a space is any set built up as a countable union of nowhere dense sets.↩︎\nBelot goes into much more detail about why modesty is a good requirement to put on a rational prior, but I’m omitting those details since I have very little to add to what Belot says.↩︎\nI have an independent metaphysical reason for preferring the corporate interpretation. I think that comparative confidences, things like being at least as confident in \\(p\\) as in \\(q\\), are metaphysically prior to numerical credences, or even sets of numerical credences. On such a metaphysics, what it is for \\(\\Pr\\) to be in the representor just is for every \\(p, q, r, s\\), if the agent is at least as confident in \\(p\\) given \\(q\\) as in \\(r\\) given \\(s\\), then \\(\\Pr(p | q) \\geq \\Pr(r | s)\\). And it seems, though I won’t defend this claim here, that the corporate interpetation fits more naturally with the idea that comparative confidences are primitive.\n\n↩︎\n",
    "preview": "posts/2021-02-03-for-bayesians-rational-modesty-requires-imprecision/mingwing.jpg",
    "last_modified": "2021-02-05T20:44:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-reply-to-eaton-and-pickavance/",
    "title": "Reply to Eaton and Pickavance",
    "description": "David Eaton and Timothy Pickavance argued that interest-relative invariantism has a surprising and interesting consequence. They take this consequence to be so implausible that it refutes interest-relative invariantism. But in fact it is a consequence that any theory of knowledge that has the resources to explain familiar puzzles (such as Gettier cases) must have.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2016-03-11",
    "categories": [
      "epistemology",
      "interest-relativity",
      "notes"
    ],
    "contents": "\nDaniel Eaton and Timothy Pickavance note an interesting consequence of some popular versions of interest-relative invariantism (IRI). They take this consequence to be a reductio of (those versions of) IRI. I’ll agree it is an interesting consequence, but argue that it cannot be a reductio. They take as a fixed point the following principle:\n\nPublished in Philosophical Studies 173: 3231–3233.\n\nWe take it to be rather obvious that one ought not be able to go from not knowing that \\(p\\) to knowing that \\(p\\) by getting evidence against \\(p\\), nor should one be able to go from knowing that \\(p\\) to not knowing that \\(p\\) by getting evidence for \\(p\\).  (Eaton and Pickavance 2015, 3142, emphasis in original)\n\nThe interesting consequence they note is that some versions of IRI violate this principle. It is possible, they argue, to come to know \\(p\\) solely by getting evidence against it, at least on common versions of IRI.\nBut this is not a distinctive, or problematic, feature of IRI. It is true of any theory of knowledge that allows for Gettier cases.1 Assume that our protagonist S has evidence that makes q incredibly likely, and on that basis the agent believes q. Assume further that q entails p, that q is false, and p is true. At least in typical such cases, S will not know that p.2 Now imagine that S gets new evidence that makes q very unlikely, but makes r, which also entails p, likely enough. If r is true, then getting this new evidence could both decrease the evidential probability of p, and make it the case that S knows that p. The converse is possible too. If the agent originally believes p on the basis of true r, then gets evidence that undermines r, but makes p more likely by making q, which is false and entails p, very likely, they will get evidence for p, and in virtue of that lose knowledge that p.\nThis point, that in Dharmottara/Gettier cases evidence can both create knowledge and be evidence against the proposition now known, is not novel. Brian  (Weatherson 2014) discusses some such cases, attributing them to Martin Smith. Weatherson argues that any theory that allows for defeaters will have this consequence, since it will always be possible for evidence to defeat a defeater, while ever so slightly lowering the evidential probability of p. The same will be true of any theory that puts a safety condition on knowledge, since it will always be possible that a new experience will make an agent’s belief safer, while ever so slightly lowering its evidential probability.\nHere’s the particular way I think IRI should be implemented so that it has the consequence Eaton and Pickavance reveal. But note that the broader response I’m making to Eaton and Pickavance doesn’t turn on any of the details here; it turns on interests being relevant to something that grounds the difference between knowledge and justified true belief.\nI think interests matter to knowledge because they determine whether or not practical considerations generate defeaters. Someone who believes p, but has different preferences over live issues conditional on p to what they have unconditionally, is in a way incoherent. This incoherence defeats any claim to knowledge of p. This kind of defeater should be interest-relative, for if it were not, we’d say that only things one is completely certain of can be known. If p is not certain, one prefers $1 to a bet that pays $1 iff p. But conditional on p, one is indifferent between these bets. The interest-relativity becomes relevant here; this change in preference conditional on p doesn’t matter because one isn’t really faced with such a choice.\nIf interest-relativity matters to knowledge because it matters to defeaters, or to safety, or to whatever explains Dharmottara/Gettier cases, then we should expect IRI to have just the consequence Eaton and Pickavance uncover. In short, any IRI theorist who thinks interests are relevant to knowledge in ways that go beyond how they are relevant to justified belief should view Eaton and Pickavance’s results as an interesting discovery, not any kind of reductio.\n\n\nEaton, Daniel, and Timothy Pickavance. 2015. “Evidence Against Pragmatic Encroachment.” Philosophical Studies 172: 3135–43. https://doi.org/10.1007/s11098-015-0461-x.\n\n\nLuzzi, Federico. 2010. “Counter-Closure.” Australasian Journal of Philosophy 88 (4): 673–83. https://doi.org/10.1080/00048400903341770.\n\n\nNagel, Jennifer. 2014. Knowledge: A Very Short Introduction. Oxford: Oxford University Press.\n\n\nWeatherson, Brian. 2014. “Probability and Scepticism.” In Scepticism and Perceptual Justification, edited by Dylan Dodd and Elia Zardini, 71–86. Oxford: Oxford University Press.\n\n\nGettier cases should perhaps be called Dharmottara cases, since Dharmottara’s 8th Century examples somewhat predate Gettier’s, and are in some ways a little cleaner. Jennifer Nagel (2014, 57) has more discussion of Dharmottara.↩︎\nIs this restriction to ‘typical’ cases necessary? Perhaps – see recent work by Federico Luzzi (2010) for a good discussion of the point.\n\n↩︎\n",
    "preview": "posts/2021-02-03-reply-to-eaton-and-pickavance/fire.jpg",
    "last_modified": "2021-02-05T20:44:30-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-02-games-beliefs-and-credences/",
    "title": "Games, Beliefs and Credences",
    "description": "In previous work I've defended an interest-relative theory of belief. This paper continues the defence. I have four aims. First, to offer a new kind of reason for being unsatisfied with the simple Lockean reduction of belief to credence. Second, to defend the legitimacy of appealing to credences in a theory of belief. Third, to illustrate the importance of theoretical, as well as practical, interests in an interest-relative account of belief. And finally, to have another try at extending my basic account of belief to cover propositions that are practically and theoretically irrelevant to the agent.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2016-03-01",
    "categories": [
      "epistemology",
      "interest-relativity",
      "games and decisions"
    ],
    "contents": "\n\nContents\nPlaying Games with a Lockean\nHolton on Credence\nThe Power of Theoretical Interests\nConclusion\n\nIn previous work (Weatherson 2005, 2011, 2012b) I’ve defended an interest-relative theory of belief. This paper continues the defence. I have four aims.\n\nPublished in Philosophy and Phenomenological Research 92: 209-236.\nTo offer a new kind of reason for being unsatisfied with the simple Lockean reduction of belief to credence.\nTo defend the legitimacy of appealing to credences in a theory of belief.\nTo illustrate the importance of theoretical, as well as practical, interests in an interest-relative account of belief.\nTo have another try at extending my basic account of belief to cover propositions that are practically and theoretically irrelevant to the agent.\nYou’re probably familiar with the following dialectic. We want there to be some systematic connection between credences and beliefs. At first blush, saying that a person believes \\(p\\) and has a very low credence in \\(p\\) isn’t just an accusation of irrationality, it is literally incoherent. The simplest such connection would be a reduction of beliefs to credences. But the simplest reductions don’t work.\n\nImage via Meagan via Creative Commons.\nIf we identify beliefs with credence 1, and take credences to support betting dispositions, then a rational agent will have very few beliefs. There are lots of things that an agent, we would normally say, believes even though she wouldn’t bet on them at absurd odds. Note that this argument doesn’t rely on reducing credences to betting dispositions; as long as credences support the betting dispositions, the argument goes through.\nA simple retreat is to the so-called Lockean thesis, which holds that to believe that \\(p\\) is to have credence in \\(p\\) greater than some threshold \\(t\\), where \\(t < 1\\). Just how the threshold is determined could be a matter of some discretion. Perhaps it is a function of the agent’s situation, or of the person ascribing beliefs to the agent, or to the person evaluating that ascription. Never mind these complexities; assuming all such things are held fixed, the Lockean thesis says that there is a threshold \\(t\\) such that everything with credence above \\(t\\) is believed.\nThere’s a simple objection to the Lockean thesis. Given some very weak assumptions about the world, it implies that there are plenty of quadruples \\(\\langle S, A, B, A \\wedge B \\rangle\\) such that\n\\(S\\) is a rational agent.\n\\(A, B\\) and \\(A \\wedge B\\) are propositions.\n\\(S\\) believes \\(A\\) and believes \\(B\\).\n\\(S\\) does not believe \\(A \\wedge B\\).\n\\(S\\) knows that she has all these states, and consciously reflectively endorses them.\nNow one might think, indeed I do think, that such quadruples do not exist at all. But set that objection aside. If the Lockean is correct, these quadruples should be everywhere. That’s because for any \\(t \\in (0, 1)\\) you care to pick, quadruples of the form \\(\\langle S, C, D, C \\wedge D \\rangle\\) are very very common.\n\\(S\\) is a rational agent.\n\\(C, D\\) and \\(C \\wedge D\\) are propositions.\n\\(S\\)’s credence in \\(C\\) is greater than \\(t\\), and her credence in \\(D\\) is greater than \\(t\\).\n\\(S\\)’s credence in \\(C \\wedge D\\) is less than \\(t\\).\n\\(S\\) knows that she has all these states, and reflectively endorses them.\nThe best arguments for the existence of quadruples \\(\\langle S, A, B, A \\wedge B \\rangle\\) are non-constructive existence proofs. David Christensen (2005) for instance, argues from the existence of the preface paradox to the existence of these quadruples. I’ve expressed some reservations about that argument in the past (Weatherson 2005). But what I want to stress here is that even if these existence proofs work, they don’t really prove what the Lockean needs. They don’t show that quadruples satisfying the constraints we associated with \\(\\langle S, A, B, A \\wedge B \\rangle\\) are just as common as quadruples satisfying the constraints we associated with \\(\\langle S, C, D, C \\wedge D \\rangle\\), for any \\(t\\). But if the Lockean were correct, they should be exactly as common.\nThis kind of consideration pushes some of us, well me in any case, towards an interest-relative account of belief. But I’m going to set that move aside to start by investigating a different objection. This objection holds that the Lockean thesis could not be true, because credence 1 is not sufficient for belief. That is, the Lockean is committed to the thesis known as regularity; that everything left open by belief gets a positive credence. I think regularity is false. That’s hardly news, there are plenty of good arguments against it, though most of these involve cases with some idealisations. Timothy Williamson (2007a) has a compelling argument against regularity turning on reflections about a case involving infinite coin flips.1 I’m going to offer a ‘finite’ argument against regularity, which I hope is of independent interest, and from that conclude the Lockean is mistaken. There is a worry that my argument against the Lockean also undermines my preferred positive view, and I’ll suggest an independently motivated patch. I’ll then turn to Richard Holton’s attack on the very notion of credence, which obviously would have repercussions for attempts to understand beliefs in terms of credences were it to succeed. I think it doesn’t succeed, but it does show there are important and underappreciated constraints on a theory of belief. I’ll conclude with a comparison between my preferred interest-relative account of belief, and a recent account suggested by Jacob Ross and Mark Schroeder. The short version of the comparison is that I think there’s less difference between the views than Ross and Schroeder think, though naturally I think what differences there are favour my view.\nPlaying Games with a Lockean\nI’m going to raise problems for Lockeans, and for defenders of regularity in general, by discussing a simple game. The game itself is a nice illustration of how a number of distinct solution concepts in game theory come apart. (Indeed, the use I’ll make of it isn’t a million miles from the use that Kohlberg and Mertens (1986) make of it.) To set the problem up, I need to say a few words about how I think of game theory. This won’t be at all original - most of what I say is taken from important works by Robert Stalnaker (1994, 1996, 1998, 1999). But it is different to what I used to think, and perhaps to what some other people think too, so I’ll set it out slowly.2\nStart with a simple decision problem, where the agent has a choice between two acts \\(A_1\\) and \\(A_2\\), and there are two possible states of the world, \\(S_1\\) and \\(S_2\\), and the agent knows the payouts for each act-state pair are given by the following able.\n\n\n\\(S_1\\)\n\\(S_2\\)\n\\(A_1\\)\n4\n0\n\\(A_2\\)\n1\n1\n\nWhat to do? I hope you share the intuition that it is radically underdetermined by the information I’ve given you so far. If \\(S_2\\) is much more probable than \\(S_1\\), then \\(A_2\\) should be chosen; otherwise \\(A_1\\) should be chosen. But I haven’t said anything about the relative probability of those two states. Now compare that to a simple game. Row has two choices, which I’ll call \\(A_1\\) and \\(A_2\\). Column also has two choices, which I’ll call \\(S_1\\) and \\(S_2\\). It is common knowledge that each player is rational, and that the payouts for the pairs of choices are given in the following table. (As always, Row’s payouts are given first.)\n\n\n\\(S_1\\)\n\\(S_2\\)\n\\(A_1\\)\n4, 0\n0, 1\n\\(A_2\\)\n1, 0\n1, 1\n\nWhat should Row do? This one is easy. Column gets 1 for sure if she plays \\(S_2\\), and 0 for sure if she plays \\(S_1\\). So she’ll play \\(S_2\\). And given that she’s playing \\(S_2\\), it is best for Row to play \\(A_2\\).\nYou probably noticed that the game is just a version of the decision problem that we discussed a couple of paragraphs ago. The relevant states of the world are choices of Column. But that’s fine; we didn’t say in setting out the decision problem what constituted the states \\(S_1\\) and \\(S_2\\). And note that we solved the problem without explicitly saying anything about probabilities. What we added was some information about Column’s payouts, and the fact that Column is rational. From there we deduced something about Column’s play, namely that she would play \\(S_2\\). And from that we concluded what Row should do.\nThere’s something quite general about this example. What’s distinctive about game theory isn’t that it involves any special kinds of decision making. Once we get the probabilities of each move by the other player, what’s left is (mostly) expected utility maximisation. (We’ll come back to whether the ‘mostly’ qualification is needed below.) The distinctive thing about game theory is that the probabilities aren’t specified in the setup of the game; rather, they are solved for. Apart from special cases, such as where one option strictly dominates another, we can’t say much about a decision problem with unspecified probabilities. But we can and do say a lot about games where the setup of the game doesn’t specify the probabilities, because we can solve for them given the other information we have.\nThis way of thinking about games makes the description of game theory as ‘interactive epistemology’ (Aumann 1999) rather apt. The theorist’s work is to solve for what a rational agent should think other rational agents in the game should do. From this perspective, it isn’t surprising that game theory will make heavy use of equilibrium concepts. In solving a game, we must deploy a theory of rationality, and attribute that theory to rational actors in the game itself. In effect, we are treating rationality as something of an unknown, but one that occurs in every equation we have to work with. Not surprisingly, there are going to be multiple solutions to the puzzles we face.\nThis way of thinking lends itself to an epistemological interpretation of one of the most puzzling concepts in game theory, the mixed strategy. Arguably the core solution concept in game theory is the Nash equilibrium. As you probably know, a set of moves is a Nash equilibrium if no player can improve their outcome by deviating from the equilibrium, conditional on no other player deviating. In many simple games, the only Nash equilibria involve mixed strategies. Here’s one simple example.\n\n\n\\(S_1\\)\n\\(S_2\\)\n\\(A_1\\)\n0, 1\n10, 0\n\\(A_2\\)\n9, 0\n-1, 1\n\nThis game is reminiscent of some puzzles that have been much discussed in the decision theory literature, namely asymmetric Death in Damascus puzzles. Here Column wants herself and Row to make the ‘same’ choice, i.e., \\(A_1\\) and \\(S_1\\) or \\(A_2\\) and \\(S_2\\). She gets 1 if they do, 0 otherwise. And Row wants them to make different choices, and gets 10 if they do. Row also dislikes playing \\(A_2\\), and this costs her 1 whatever else happens. It isn’t too hard to prove that the only Nash equilibrium for this game is that Row plays a mixed strategy playing both \\(A_1\\) and \\(A_2\\) with probability , while Column plays the mixed strategy that gives \\(S_1\\) probability , and \\(S_2\\) with probability .\nNow what is a mixed strategy? It is easy enough to take away form the standard game theory textbooks a metaphysical interpretation of what a mixed strategy is. Here, for instance, is the paragraph introducing mixed strategies in Dixit and Skeath’s Games of Strategy.\n\nWhen players choose to act unsystematically, they pick from among their pure strategies in some random way …We call a random mixture between these two pure strategies a mixed strategy. (Dixit and Skeath 2004, 186)\n\nDixit and Skeath are saying that it is definitive of a mixed strategy that players use some kind of randomisation device to pick their plays on any particular run of a game. That is, the probabilities in a mixed strategy must be in the world; they must go into the players’ choice of play. That’s one way, the paradigm way really, that we can think of mixed strategies metaphysically.\nBut the understanding of game theory as interactive epistemology naturally suggests an epistemological interpretation of mixed strategies.\n\nOne could easily …[model players] …turning the choice over to a randomizing device, but while it might be harmless to permit this, players satisfying the cognitive idealizations that game theory and decision theory make could have no motive for playing a mixed strategy. So how are we to understand Nash equilibrium in model theoretic terms as a solution concept? We should follow the suggestion of Bayesian game theorists, interpreting mixed strategy profiles as representations, not of players’ choices, but of their beliefs. (Stalnaker 1994, 57–58)\n\nOne nice advantage of the epistemological interpretation, as noted by Binmore (2007, 185) is that we don’t require players to have \\(n\\)-sided dice in their satchels, for every \\(n\\), every time they play a game.3 But another advantage is that it lets us make sense of the difference between playing a pure strategy and playing a mixed strategy where one of the ‘parts’ of the mixture is played with probability one.\nWith that in mind, consider the below game, which I’ll call Red-Green. I’ve said something different about this game in earlier work (Weatherson 2012a). But I now think that to understand what’s going on, we need to think about mixed strategies where one element of the mixture has probability one.\nInformally, in this game \\(A\\) and \\(B\\) must each play either a green or red card. I will capitalise \\(A\\)’s moves, i.e., \\(A\\) can play GREEN or RED, and italicise \\(B\\)’s moves, i.e., \\(B\\) can play green or red. If two green cards, or one green card and one red card are played, each player gets $1. If two red cards are played, each gets nothing. Each cares just about their own wealth, so getting $1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with \\(A\\) on the row and \\(B\\) on the column.\n\n\ngreen\nred\nGREEN\n1, 1\n1, 1\nRED\n1, 1\n0, 0\n\nWhen I write game tables like this, and I think this is the usual way game tables are to be interpreted (Weatherson 2012b), I mean that the players know that these are the payouts, that the players know the other players to be rational, and these pieces of knowledge are common knowledge to at least as many iterations as needed to solve the game. With that in mind, let’s think about how the agents should approach this game.\nI’m going to make one big simplifying assumption at first. We’ll relax this later, but it will help the discussion to start with this assumption. This assumption is that the doctrine of Uniqueness applies here; there is precisely one rational credence to have in any salient proposition about how the game will play. Some philosophers think that Uniqueness always holds (White 2005). I join with those such as North (2010) and Schoenfield (2013) who don’t. But it does seem like Uniqueness might often hold; there might often be a right answer to a particular problem. Anyway, I’m going to start by assuming that it does hold here.\nThe first thing to note about the game is that it is symmetric. So the probability of \\(A\\) playing GREEN should be the same as the probability of \\(B\\) playing green, since \\(A\\) and \\(B\\) face exactly the same problem. Call this common probability \\(x\\). If \\(x < 1\\), we get a quick contradiction. The expected value, to Row, of GREEN, is 1. Indeed, the known value of GREEN is 1. If the probability of green is \\(x\\), then the expected value of RED is \\(x\\). So if \\(x < 1\\), and Row is rational, she’ll definitely play GREEN. But that’s inconsistent with the claim that \\(x < 1\\), since that means that it isn’t definite that Row will play GREEN.\nSo we can conclude that \\(x = 1\\). Does that mean we can know that Row will play GREEN? No. Assume we could conclude that. Whatever reason we would have for concluding that would be a reason for any rational person to conclude that Column will play green. Since any rational person can conclude this, Row can conclude it. So Row knows that she’ll get 1 whether she plays GREEN or RED. But then she should be indifferent between playing GREEN and RED. And if we know she’s indifferent between playing GREEN and RED, and our only evidence for what she’ll play is that she’s a rational player who’ll maximise her returns, then we can’t be in a position to know she’ll play GREEN.\nI think the arguments of the last two paragraphs are sound. We’ll turn to an objection presently, but let’s note how bizarre is the conclusion we’ve reached. One argument has shown that it could not be more probable that Row will play GREEN. A second argument has shown that we can’t know that Row will play GREEN. It reminds me of examples involving blindspots (Sorensen 1988). Consider this case:\nBrian does not know (B).\nThat’s true, right? Assume it’s false, so I do know (B). Knowledge is factive, so (B) is true. But that contradicts the assumption that it’s false. So it’s true. But I obviously don’t know that it’s true; that’s what this very true proposition says.4\nNow I’m not going to rest anything on this case, because there are so many tricky things one can say about blindspots, and about the paradoxes generally. It does suggest that there are other finite cases where one can properly have maximal credence in a true proposition without knowledge.5 And, assuming that we shouldn’t believe things we know we don’t know, that means we can have maximal credence in things we don’t believe. All I want to point out is that this phenomena of maximal credence without knowledge, and presumably without full belief, isn’t a quirky feature of self-reference, or of games, or of puzzles about infinity; it comes up in a wide range of cases.\nFor the rest of this section I want to reply to one objection, and weaken an assumption I made earlier. The objection is that I’m wrong to assume that agents will only maximise expected utility. They may have tie-breaker rules, and those rules might undermine the arguments I gave above. The assumption is that there’s a uniquely rational credence to have in any given situation.\nI argued that if we knew that \\(A\\) would play GREEN, we could show that \\(A\\) had no reason to play GREEN. But actually what we showed was that the expected utility of playing GREEN would be the same as playing RED. Perhaps \\(A\\) has a reason to play GREEN, namely that GREEN weakly dominates RED. After all, there’s one possibility on the table where GREEN does better than RED, and none where RED does better. And perhaps that’s a reason, even if it isn’t a reason that expected utility considerations are sensitive to.\nNow I don’t want to insist on expected utility maximisation as the only rule for rational decision making. Sometimes, I think some kind of tie-breaker procedure is part of rationality. In the papers by Stalnaker I mentioned above, he often appeals to this kind of weak dominance reasoning to resolve various hard cases. But I don’t think weak dominance provides a reason to play GREEN in this particular case. When Stalnaker says that agents should use weak dominance reasoning, it is always in the context of games where the agents’ attitude towards the game matrix is different to their attitude towards each other. One case that Stalnaker discusses in detail is where the game table is common knowledge, but there is merely common (justified, true) belief in common rationality. Given such a difference in attitudes, it does seem there’s a good sense in which the most salient departure from equilibrium will be one in which the players end up somewhere else on the table. And given that, weak dominance reasoning seems appropriate.\nBut that’s not what we’ve got here. Assuming that rationality requires playing GREEN/green, the players know we’ll end up in the top left corner of the table. There’s no chance that we’ll end up elsewhere. Or, perhaps better, there is just as much chance we’ll end up ‘off the table,’ as that we’ll end up in a non-equilibrium point on the table. To make this more vivid, consider the ‘possibility’ that \\(B\\) will play blue, and if \\(B\\) plays blue, \\(A\\) will receive 2 if she plays RED, and -1 if she plays GREEN. Well hold on, you might think, didn’t I say that green and red were the only options, and this was common knowledge? Well, yes, I did, but if the exercise is to consider what would happen if something the agent knows to be true doesn’t obtain, then the possibility that one agent will play blue certainly seems like one worth considering. It is, after all, a metaphysical possibility. And if we take it seriously, then it isn’t true that under any possible play of the game, GREEN does better than RED.\nWe can put this as a dilemma. Assume, for reductio, that GREEN/green is the only rational play. Then if we restrict our attention to possibilities that are epistemically open to \\(A\\), then GREEN does just as well as RED; they both get 1 in every possibility. If we allow possibilities that are epistemically closed to \\(A\\), then the possibility where \\(B\\) plays blue is just as relevant as the possibility that \\(B\\) is irrational. After all, we stipulated that this is a case where rationality is common knowledge. In neither case does the weak dominance reasoning get any purchase.\nWith that in mind, we can see why we don’t need the assumption of Uniqueness. Let’s play through how a failure of Uniqueness could undermine the argument. Assume, again for reductio, that we have credence \\(\\varepsilon > 0\\) that \\(A\\) will play RED. Since \\(A\\) maximises expected utility, that means \\(A\\) must have credence 1 that \\(B\\) will play green. But this is already odd. Even if you think people can have different reactions to the same evidence, it is odd to think that one rational agent could regard a possibility as infinitely less likely than another, given isomorphic evidence. And that’s not all of the problems. Even if \\(A\\) has credence 1 that \\(B\\) will play green, it isn’t obvious that playing RED is rational. After all, relative to the space of epistemic possibilities, GREEN weakly dominates RED. Remember that we’re no longer assuming that it can be known what \\(A\\) or \\(B\\) will play. So even without Uniqueness, there are two reasons to think that it is wrong to have credence \\(\\varepsilon > 0\\) that \\(A\\) will play RED. So we’ve still shown that credence 1 doesn’t imply knowledge, and since the proof is known to us, and full belief is incompatible with knowing that you can’t know, this is a case where credence 1 doesn’t imply full belief. So whether \\(A\\) plays GREEN, like whether the coin will ever land tails, is a case the Lockean cannot get right. No matter where they set the threshold for belief our credence is above that threshold, but we don’t believe.\nSo I think this case is a real problem for a Lockean view about the relationship between credence and belief. If A is rational, she can have credence 1 that B will play green, but won’t believe that B will play green. But now you might worry that my own account of the relationship between belief and credence is in just as much trouble. After all, I said that to believe \\(p\\) is, roughly, to have the same attitudes towards all salient questions as you have conditional on \\(p\\). And it’s hard to identify a question that rational A would answer differently upon conditionalising on the proposition that B plays green.\nI think what went wrong in my earlier view was that I’d too quickly equated updating with conditionalisation. The two can come apart. Here’s an example from Gillies (2010) that makes the point well.6\n\nI have lost my marbles. I know that just one of them – Red or Yellow – is in the box. But I don’t know which. I find myself saying things like …“If Yellow isn’t in the box, the Red must be.” (4:13)\n\nAs Gillies goes on to point out, this isn’t really a problem for the Ramsey test view of conditionals.\n\nThe Ramsey test – the schoolyard version, anyway – is a test for when an indicative conditional is acceptable given your beliefs. It says that (if p)(q) is acceptable in belief state B iff q is acceptable in the derived or subordinate state B-plus-the-information-that-p. (4:27)\n\nAnd he notes that this can explain what goes on with the marbles conditional. Add the information that Yellow isn’t in the box, and it isn’t just true, but must be true, that Red is in the box.\nNote though that while we can explain this conditional using the Ramsey test, we can’t explain it using any version of the idea that probabilities of conditionals are conditional probabilities. The probability that Red must be in the box is 0. The probability that Yellow isn’t in the box is not 0. So conditional on Yellow not being in the box, the probability that Red must be in the box is still 0. Yet the conditional is perfectly assertable.\nThere is, and this is Gillies’s key point, something about the behaviour of modals in the consequents of conditionals that we can’t capture using conditional probabilities, or indeed many other standard tools. And what goes for consequents of conditionals goes for updated beliefs too. Learn that Yellow isn’t in the box, and you’ll conclude that Red must be. But that learning can’t go via conditionalisation; just conditionalise on the new information and the probability that Red must be in the box goes from 0 to 0.\nNow it’s a hard problem to say exactly how this alternative to updating by conditionalisation should work. But very roughly, the idea is that at least some of the time, we update by eliminating worlds from the space of possibilities. This affects dramatically the probability of propositions whose truth is sensitive to which worlds are in the space of possibiilties.\nFor example, in the game I’ve been discussing, we should believe that rational B might play red. Indeed, the probability of that is, I think, 1. And whether or not B might play red is highly salient; it matters to the probability of whether A will play GREEN or RED. Conditionalising on something that has probability 1, such as that B will play green, can hardly change that probability. But updating on the proposition that B will play green can make a difference. We can see that by simply noting that the conditional If B plays green, she might play red is incoherent.\nSo I conclude that a theory of belief like mine can handle the puzzle this game poses, as long as it distinguishes between conditionalising and updating, in just the way Gillies suggests. To believe that p is to be disposed to not change any attitude towards a salient question on updating that p. (Plus some bells and whistles to deal with propositions that are not relevant to salient questions. We’ll return to them below.) Updating often goes by conditionalisation, so we can often say that belief means having attitudes that match unconditionally and conditionally on p. But not all updating works that way, and the theory of belief needs to acknowledge this.\nHolton on Credence\nWhile I don’t agree with the Lockeans, I do endorse a lot of similar theses to them about the relationship between belief and credence. These theses include that both beliefs and credences exist and that the two are constitutively (as opposed to merely causally) connected. I differ from the Lockeans in holding that both belief and credence have important explanatory roles, and that the connection between the two goes via the interests of the agent. As with most work in this area, my views start off from considerations of cases much like DeRose’s famous bank cases.7 Here’s another contribution to the genre. I know it’s an overcrowded field, but I wanted a case that (a) is pretty realistic, and (b) doesn’t involve the attribution (either to oneself or others) of a propositional attitude. In the example, X and Y are parents of a child, Z.\n\nY: This salad you bought is very good. Does it have nuts in it?\nX: No. The nuttiness you’re tasting is probably from the beans.\nY: Oh, so we could pack it for Z’s lunch tomorrow.\nX: Hang on, I better check about the nuts. Z’s pre-school is very fussy about nuts. One of the children there might have an allergy, and it would be awful to get in trouble over her lunch.\n\nHere’s what I think is going on in that exchange.8 At \\(t_2\\) (I’ll use \\(t_i\\) for the time of the \\(i\\)’th utterance in the exchange), X believes that the salad has no nuts in it. Indeed, the one word sentence “No” expresses that belief. But by \\(t_4\\), X has lost that belief. It would be fine to pack the salad for lunch if it has no nuts, but X isn’t willing to do this for the simple reason that X no longer believes that it has no nuts. Moreover, this change of belief was, or at least could have been for all we’ve said so far, rational on X’s part.\nThere’s something a little puzzling about that. Jacob Ross and Mark Schroeder (2014) voice a common intuition when they say that beliefs should only change when new evidence comes in. Indeed, they use this intuition as a key argument against my view of belief. But X doesn’t get any evidence that bears on the nuttiness of the salad. Yet X rationally changes beliefs. So I just conclude that sometimes we can change beliefs without new evidence coming in; sometimes our interests, broadly construed, change, and that is enough to change beliefs.\nWe’ll come back to Ross and Schroeder’s arguments in the next section, because first I want to concede something to the view that only evidence changes beliefs. That view is false, but there might be a true view in the area. And that’s the view that only change in evidence can change credences. But that view only makes sense if there are such things as credences, and that’s something that Richard Holton (2014) has recently launched an intriguing argument against.\nHolton’s broader project is a much more sweeping attack on the Lockean thesis than I have proposed. Actually, it is a pair of more sweeping attacks. One of the pair is that the Lockeans identify something that exists, namely belief, with something that doesn’t, namely high credence. I would not, could not, sign up for that critique. But I am much more sympathetic to the other attack in the pair, namely that credences and beliefs have very different dynamics.\nCredences are, by their nature, exceedingly unstable. Whether an agent’s credence that \\(p\\) is above or below any number x is liable to change according to any number of possible changes in evidence. But, at least if the agent is rational, beliefs are not so susceptible to change. Holton thinks that rational agents, or at least rational humans, frequently instantiate the following pattern. They form a belief that \\(p\\), on excellent grounds. They later get some evidence that \\(\\neg p\\). The evidence is strong enough that, had they had it to begin with, they would have remained uncertain about \\(p\\). But they do not decide to reopen the investigation into whether \\(p\\). They hold on to their belief that \\(p\\), the matter having been previously decided.\nSuch an attitude might look like unprincipled dogmatism. But it need not be, I think, as long as four criteria are met. (I think Holton agrees with these criteria.) One is that the agent’s willingness to reopen the question of whether \\(p\\) must increase. She must be more willing, in the light of yet more evidence against \\(p\\), to consider whether \\(p\\) is really true. A second is that, should the agent (irrationally) reopen the question of whether \\(p\\), she should not use the fact that she previously closed that question as evidence. Once the genie is out of the box, only reasoning about \\(p\\) can get it back in. A third is that the costs of the inquiry must be high enough to warrant putting it off. If simply turning one’s head fifteen degrees to the left will lead to acquiring evidence that definitively settles whether \\(p\\), it is a little dogmatic to refuse to do so in the face of evidence against one’s previously formed opinion that \\(p\\). And finally, the costs of being wrong about \\(p\\) must not be too high. X, in our little dialogue above, would be terribly dogmatic if they didn’t reopen the question of whether the salad had nuts in it, on being informed that this information was being used in a high stakes inquiry.\nSo beliefs should have a kind of resilience. Credences, if they exist, should not have this kind of resilience. So this suggests that a simple reduction of belief to credence, as the Lockeans suggest, cannot be right. You might worry that things are worse, that no reduction of belief to credence can be compatible with the difference in resilience between belief and credence. We’ll return to that point, because first I want to look at Holton’s stronger claim: that there are no such things as credences.\nHolton acknowledges, as of course he must, that we have probabilistic truth-directed attitudes. We can imagine a person, call her Paula, who thinks it’s likely that Richard III murdered his nephews, for instance. But Holton offers several reasons for thinking that in these probabilistic truth-directed attitudes, the probability goes in the content, not in the attitude. That is, we should interpret Paula as believing the probabilistic claim, Richard III probably murdered his nephews, and not as having some graded attitude towards the simple proposition Richard III murdered his nephews. More precisely, Holton thinks we should understand Paula’s explicit attitudes that way, and that independent of having reason to think that agents explicitly have probabilistic attitudes, there’s no good way to make sense of the claim that they implicitly have probabilistic attitudes. So there’s no such thing as credences, as usually understood. Or, at least, there’s no good sense to be made of the claim that there are credences.\nIn response, I want to make six points.\nHolton is right about cases like Paula’s, and the possibility of iterating terms like probably provides independent support for this view.\nBeliefs like the one Paula has are odd; they seem to have very strange truth conditions.\nOur theory of mind needs some mechanism for explaining the relationship between confidence and action.\nThe ‘explanatory gap’ here could be filled by positing a binary attitude is more confident that.\nThis binary attitude can do all the work that graded attitudes were posited to do, and in a (historically sensitive) way saves the credences story.\nCredences (or at least confidences) can have a key role within a Holton-like story about graded belief. They can both explain why agents reconsider some beliefs, and provide a standard of correctness for decisions to reconsider.\nLet’s take those in order.\nI’m not going to rehearse Holton’s argument for the ‘content view’: that in cases like Paula’s the content of her attitude, and not the attitude itself, is probabilistic. But I do want to offer one extra consideration in its favour. (I’m indebted here to work in progress by my colleague Sarah Moss (2015), though I’m not sure she’d approve of this conclusion.) As well as Paula, we can imagine a person Pip who isn’t sure that Paula is right, but thinks she’s probably right. That is, Pip thinks that Richard III probably probably murdered his nephews. It’s easy to make sense of Pip on the content view. Modalities in propositions iterate smoothly; that’s what they are designed to do. But it’s much harder to iterate attitudes. The possibility of cases like Pip suggests Holton must be right about Paula’s case.\nBut Paula’s case is odd. Beliefs have truth conditions. What are the truth conditions for Paula’s belief? On the one hand, it seems they must be sensitive to her evidence. If she later acquires conclusive evidence that Richard III was framed, she won’t think her earlier self had a false belief. But if we put the evidence into the content of the belief, we get the strange result that her belief can’t be preserved by uttering the same words to herself over again. That is, if the content of Paula’s belief is Given the evidence I have now, Richard III likely murdered his nephews, she can’t have the very same belief tomorrow by retaining the thought Richard III likely murdered his nephews. And she can’t have a belief with the same content as anyone else by the two of them both thinking Richard III likely murdered his nephews. Those seem like unhappy conclusions, especially in the midst of a project that wants to emphasise the resiliency of belief. So perhaps we should say, following Stephenson (2007) or MacFarlane (2011), that the truth conditions of the belief are agent-relative. Or, if we’re unhappy with the MacFarlane story, we might be pushed towards a kind of expressivism (perhaps a la Yalcin (2011)), which isn’t quite like either the content view or the attitude view that Holton discusses. I’m personally partial to the relativist view, but I don’t want to argue for that here, just note that the content view raises some interesting problems, and that natural solutions to them could in a way blur the boundaries between the content and attitude views.\nAs Holton notes in his discussion of Brutus, when our confidence in a proposition changes, our actions will change. Paula gets a little evidence that Richard III was framed, and her actions may change. Of course, not much of what we do in everyday life is sensitive to facts about English royal history, but there may be some effects. Maybe she’ll be less inclined to speak up if the topic of the princes’ murder comes up, or she’ll take a slightly more jaundiced view of Shakespeare’s play (compare Friend (2003).) Holton says that these falling confidences need not have all the precise structure of credences. In particular, they may not have the topology of the interval \\([0, 1]\\). But lots of credence lovers think that’s too demanding. There’s a long tradition of thinking that credences need not all be comparable.9 What’s important is that the relative confidences exist, and that they have a robust relationship to action.\nThere’s an old fashioned way of doing this. The idea is implicit in Ramsey (1926), and made central in DeFinetti (1964). Take the binary attitude is more confident that p than q as primitive. As Holton notes, surface structure of our attitude reports suggest that this attitude, unlike the graded attitude of credence, is part of folk psychology. Lay down some constraints on this attitude. To get enough constraints that the binary relation determines a unique probability function, the constraints will have to be very tight. In particular, you’ll need some kind of Archimedean principle, and a principle of universal comparability. Those aren’t very plausible, especially the second. But even weaker constraints will get you something interesting. In particular, it isn’t hard to lay down enough constraints that there is a unique set \\(S\\) of probability functions such that the agent is more confident that \\(p\\) than \\(q\\) just in case \\(\\Pr( p) > \\Pr(q)\\) for all \\(\\Pr \\in S\\). (For much more detail, see for instance Walley (1991).)\nIn that way, we can derive credences from the relative confidences of a reasonably coherent agent. But we can do with even less coherence than that I think. A throwaway remark from Ramsey (1929/1990) provides a key clue. What is it to have credence \\(\\frac{2}{3}\\) in \\(p\\)? Don’t say it’s a betting disposition; mental states and behavioural dispositions aren’t that tightly linked. Here’s Ramsey’s idea. To have credence \\(\\frac{2}{3}\\) in \\(p\\) is to be exactly as confident in \\(p\\) as in \\(q \\vee r\\), where \\(q, r\\) and \\(s\\) are taken to be exclusive and exhaustive, and one has equal confidence in all three. It’s easy to see how to extend that to a definition of credence \\(\\frac{m}{n}\\) for any integer \\(m, n\\). It’s a little trickier to say precisely what, say, credence \\(\\frac{1}{\\pi}\\) is, but rational credences are probably credences enough to explain action. And just like that, we have a way of talking about credences, i.e., graded attitudes, without positing anything more than a binary attitude more confident than.\nPerhaps Holton could argue that we only have unary attitudes, not binary attitudes like more confident than. If Maury is more confident that Oswald shot JFK than that Richard III murdered his nephews, that means he really believes the proposition It is more likely that Oswald shot JFK than that Richard III murdered his nephews. But such a view seems forced at best, and isn’t motivated by Holton’s other arguments for the ‘content view.’ This attitude of more confident than isn’t iterable. It isn’t subject to the particular kind of reasoning errors that Holton takes to be evidence for the content view in the probabilistic case. It is an attitude we ordinarily report as a binary attitude in normal speech. In short, it looks like a genuine binary attitude.\nGiven that the binary attitude exists, and that we can define numerical (at least rational) credences in terms of it, I’d say that’s enough to say that credences exist. In a sense, credences will be epiphenomenal. What does the explanatory work is the binary relation more confident that. Maury might stay away from a showing of Richard III because he is less confident that it is historically accurate than he used to be. We can work out from Maury’s other relative confidences what his credence in Richard III’s guilt is and was. Or, at least, we can work out bounds on these. But those numbers aren’t in a fundamental sense explanatory, and neither are the complicated sets of relative confidences that constitute the numbers. What’s really explanatory are relative confidences. But it’s a harmless enough mode of speech to talk as if credences are explanatory; they are easier to talk about than the underlying relative confidences.\nThe Power of Theoretical Interests\nSo I think we should accept that credences exist. And we can just about reduce beliefs to credences. In previous work I argued that we could do such a reduction. I’m not altogether sure whether the amendments to that view I’m proposing here means it no longer should count as a reductive view; we’ll come back to that question in the conclusion.\nThe view I defended in previous work is that the reduction comes through the relationship between conditional and unconditional attitudes. Very roughly, to believe that p is simply to have the same attitudes, towards all salient questions, unconditionally as you have conditional on p. In a syrupy slogan, belief means never having to say you’ve conditionalised. For reasons I mentioned in section 1, I now think that was inaccurate; I should have said that belief means never having to say you’ve updated, or at least that you’ve updated your view on any salient question.\nThe restriction to salient questions is important. Consider any p that I normally take for granted, but such that I wouldn’t bet on it at insane odds. I prefer declining such a bet to taking it. But conditional on p, I prefer taking the bet. So that means I don’t believe any such p. But just about any p satisfies that description, for at least some ‘insane’ odds. So I believe almost nothing. That would be a reductio of the position. I respond by saying that the choice of whether to take an insane bet is not normally salient.\nBut now there’s a worry that I’ve let in too much. For many p, there is no salient decision that they even bear on. What I would do conditional on p, conditional on \\(\\neg p\\), and unconditionally is exactly the same, over the space of salient choices. (And this isn’t a case where updating and conditionalising come apart; I’ll leave this proviso mostly implicit from now on.) So with the restriction in place, I believe p and \\(\\neg p\\). That seems like a reductio of the view too. I probably do have inconsistent beliefs, but not in virtue of p being irrelevant to me right now. I’ve changed my mind a little about what the right way to avoid this problem is, in part because of some arguments by Jacob Ross and Mark Schroeder.\nThey have what looks like, on the surface, a rather different view to mine. They say that to believe p is to have a default reasoning disposition to use p in reasoning. Here’s how they describe their own view.\n\nWhat we should expect, therefore, is that for some propositions we would have a defeasible or default disposition to treat them as true in our reasoning–a disposition that can be overridden under circumstances where the cost of mistakenly acting as if these propositions are true is particularly salient. And this expectation is confirmed by our experience. We do indeed seem to treat some uncertain propositions as true in our reasoning; we do indeed seem to treat them as true automatically, without first weighing the costs and benefits of so treating them; and yet in contexts such as High where the costs of mistakenly treating them as true is salient, our natural tendency to treat these propositions as true often seems to be overridden, and instead we treat them as merely probable.\nBut if we concede that we have such defeasible dispositions to treat particular propositions as true in our reasoning, then a hypothesis naturally arises, namely, that beliefs consist in or involve such dispositions. More precisely, at least part of the functional role of belief is that believing that p defeasibly disposes the believer to treat p as true in her reasoning. Let us call this hypothesis the reasoning disposition account of belief. (Ross and Schroeder 2014, 9–10)\n\nThere are, relative to what I’m interested in, three striking characteristics of Ross and Schroeder’s view.\nWhether you believe p is sensitive to how you reason; that is, your theoretical interests matter.\nHow you would reason about some questions that are not live is relevant to whether you believe p.\nDispositions can be masked, so you can believe p even though you don’t actually use p in reasoning now.\nI think they take all three of these points to be reasons to favour their view over mine. As I see it, we agree on point 1 (and I always had the resources to agree with them), I can accommodate point 2 with a modification to my theory, and point 3 is a cost of their theory, not a benefit. Let’s take those points in order.\nThere are lots of reasons to dislike what Ross and Schroeder call Pragmatic Credal Reductionism (PCR). This is, more or less, the view that the salient questions, in the sense relevant above, are just those which are practically relevant to the agent. So to believe \\(p\\) just is to have the same attitude towards all practically relevant questions unconditionally as conditional on \\(p\\). There are at least three reasons to resist this view.\nOne reason comes from the discussions of Ned Block’s example Blockhead  (Block 1978). As Braddon-Mitchell and Jackson point out, the lesson to take from that example is that beliefs are constituted in part by their relations to other mental states  (Braddon-Mitchell and Jackson 2007, 114ff). There’s a quick attempted refutation of PCR via the Blockhead case which doesn’t quite work. We might worry that if all that matters to belief given PCR is how it relates to action, PCR will have the implausible consequence that Blockhead has a rich set of beliefs. That isn’t right; PCR is compatible with the view that Blockhead doesn’t have credences, and hence doesn’t have credences that constitute beliefs. But the Blockhead example’s value isn’t exhausted by its use in quick refutations.10 The lesson is that beliefs are, by their nature, interactive. It seems to me that PCR doesn’t really appreciate that lesson.\nAnother reason comes from recent work by Jessica Brown (2014). Compare these two situations.\nS is in circumstances C, and has to decide whether to do X.\nS is in completely different circumstances to C, but is seriously engaged in planning for future contingencies. She’s currently trying to decide whether in circumstances C to do X.\nIntuitively, S can bring exactly the same evidence, knowledge and beliefs to bear on the two problems. If C is a particularly high stakes situation, say it is a situation where one has to decide what to feed someone with a severe peanut allergy, then a lot of things that can ordinarily be taken for granted cannot, in this case, be taken for granted. And that’s true whether S is actually in C, or she is just planning for the possibility that she finds herself in C.\nSo I conclude that both practical and theoretical interests matter for what we can take for granted in inquiry. The things we can take for granted into a theoretical inquiry into what to do in high stakes contexts as restricted, just as they are when we are in a high stakes context, and must make a practical decision. Since the latter restriction on what we can take for granted is explained by (and possibly constituted by) a restriction on what we actually believe in those contexts, we should similarly conclude that agents simply believe less when they are reasoning about high stakes contexts, whatever their actual context.\nA third reason to dislike PCR comes from the ‘Renzi’ example in Ross and Schroeder’s paper. I’ll run through a somewhat more abstract version of the case, because I don’t think the details are particularly important. Start with a standard decision problem. The agent knows that X is better to do if p, and Y is better to do if \\(\\neg p\\). The agent should then go through calculating the relative gains to doing X or Y in the situations they are better, and the probability of p. But the agent imagined doesn’t do that. Rather, the agent divides the possibility space in four, taking the salient possibilities to be \\(p \\wedge q, p \\wedge \\neg q, \\neg p \\wedge q\\) and \\(\\neg p \\wedge \\neg q\\), and then calculates the expected utility of X and Y accordingly. This is a bad bit of reasoning on the agent’s part. In the cases we are interested in, q is exceedingly likely. Moreover, the expected utility of each act doesn’t change a lot depending on q’s truth value. So it is fairly obvious that we’ll end up making the same decision whether we take the ‘small worlds’ in our decision model to be just the world where p, and the world where \\(\\neg p\\), or the four worlds this agent uses. But the agent does use these four, and the question is what to say about them.\nRoss and Schroeder say that such an agent should not be counted as believing that q. If they are consciously calculating the probability that q, and taking \\(\\neg q\\) possibilities into account when calculating expected utilities, they regard q as an open question. And regarding q as open in this way is incompatible with believing it. I agree with all this.\nThey also think that PCR implies that the agent does believe q. The reason is that conditionalising on q doesn’t change the agent’s beliefs about any practical question. I think that’s right too, at least on a natural understanding of what ‘practical’ is.\nMy response to all these worries is to say that whether someone believes that p depends not just on how conditionalising (or more generally updating) on p would affect someone’s action, but on how it would affect their reasoning. That is, just as we learned from the Blockhead example, to believe that p requires having a mental state that is connected to the rest of one’s cognitive life in roughly the way a belief that p should be connected. Let’s go through both the last two cases to see how this works on my theory.\nOne of the things that happens when the stakes go up is that conditionalising on very probable things can change the outcome of interesting decisions. Make the probability that some nice food is peanut-free be high, but short of one. Conditional on it being peanut-free, it’s a good thing to give to a peanut-allergic guest. But unconditionally, it’s a bad thing to give to such a guest, because the niceness of the food doesn’t outweigh the risk of killing them. And that’s true whether the guest is actually there, or you’re just thinking about what to do should such a guest arrive in the future. In general, the same questions will be relevant whether you’re in C trying to decide whether to do X, or simply trying to decide whether to X in C. In one case they will be practically relevant questions, in the other they will be theoretically relevant questions. But this feels a lot like a distinction without a difference, since the agent should have similar beliefs in the two cases.\nThe same response works for Ross and Schroeder’s case. The agent was trying to work out the expected utility of X and Y by working out the utility of each action in each of four ‘small worlds,’ then working out the probability of each of these. Conditional on q, the probability of two of them (\\(p \\wedge \\neg q, \\neg p \\wedge \\neg q\\)), will be 0. Unconditionally, this probability won’t be 0. So the agent has a different view on some question they have taken an interest in unconditionally to their view conditional on q. So they don’t believe q. The agent shouldn’t care about that question, and conditional on each question they should care about, they have the same attitude unconditionally and conditional on q. But they do care about these probabilistic questions, so they don’t believe q.\nSo I think that Ross and Schroeder and I agree on point 1; something beyond practical interests is relevant to belief.\nThey have another case that I think does suggest a needed revision to my theory. I’m going to modify their case a little to change the focus a little, and to avoid puzzles about vagueness. (What follows is a version of their example about Dalı́’s moustache, purged of any worries about vagueness, and without the focus on consistency. I don’t think the problem they true to press on me, that my theory allows excessive inconsistency of belief among rational agents, really sticks. Everyone will have to make qualifications to consistency to deal with the preface paradox, and for reasons I went over in  (Weatherson 2005), I think the qualifications I make are the best ones to make.)\nLet D be the proposition that the number of games the Detroit Tigers won in 1976 (in the MLB regular season) is not a multiple of 3. At most times, D is completely irrelevant to anything I care about, either practically or theoretically. My attitudes towards any relevant question are the same unconditionally as conditional on D. So there’s a worry that I’ll count as believing D, and believing \\(\\neg D\\), by default.\nIn earlier work, I added a clause meant to help with cases like this. I said that for determining whether an agent believes that p, we should treat the question of whether p’s probability is above or below 0.5 as salient, even if the agent doesn’t care about it. Obviously this won’t help with this particular case. The probability of D is around , and is certainly above 0.5. My ‘fix’ avoids the consequence that I implausibly count as believing \\(\\neg D\\). But I still count, almost as implausibly, as believing D. This needs to be fixed.\nHere’s my proposed change. For an agent to count as believing p, it must be possible for p to do some work for them in reasoning. Here’s what I mean by work. Consider a very abstract set up of a decision problem, as follows.\n\n\np\nq\nX\n4\n1\nY\n3\n2\n\nThat table encodes a lot of information. It encodes that \\(p \\vee q\\) is true; otherwise there are some columns missing. It encodes that the only live choices are X or Y; otherwise there are rows missing. It encodes that doing X is better than doing Y if p, and worse if q.\nFor any agent, and any decision problem, there is a table like this that they would be disposed to use to resolve that problem. Or, perhaps, there are a series of tables and there is no fact about which of them they would be most disposed to use.\nGiven all that terminology, here’s my extra constraint on belief. To believe that p, there must be some decision problem such that some table the agent would be disposed to use to solve it encodes that p. If there is no such problem, the agent does not believe that p. For anything that I intuitively believe, this is an easy condition to satisfy. Let the problem be whether to take a bet that pays 1 if p, and loses 1 otherwise. Here’s the table I’d be disposed to use to solve the problem.\n\n\np\nTake bet\n1\nDecline bet\n0\n\nThis table encodes that p, so it is sufficient to count as believing that p. And it doesn’t matter that this bet isn’t on the table. I’m disposed to use this table, so that’s all that matters.\nBut might there be problems in the other direction. What about an agent who, if offered such a bet on D, would use such a simple table? I simply say that they believe that D. I would not use any such table. I’d use this table.\n\n\nD\n\\(\\neg D\\)\nTake bet\n1\n–1\nDecline bet\n0\n0\n\nNow given the probability of D, I’d still end up taking the bet; it has an expected return of . (Well, actually I’d probably decline the bet because being offered the bet would change the probability of D for reasons made clear in  Runyon (1992, 14–15). But that hardly undermines the point I’m making.) But this isn’t some analytic fact about me, or even I think some respect in which I’m obeying the dictates of rationality. It’s simply a fact that I wouldn’t take D for granted in any inquiry. And that’s what my non-belief that D consists in.\nThis way of responding to the Tigers example helps respond to a nice observation that Ross and Schroeder make about correctness. A belief that p is, in some sense, incorrect if \\(\\neg p\\). It isn’t altogether clear how to capture this sense given a simple reduction of beliefs to credences. I propose to capture it using this idea that decision tables encode propositions. A table is incorrect if it encodes something that’s false. To believe something is, inter alia, to be disposed to use a table that encodes it. So if it is false, it involves a disposition to do something incorrect.\nIt also helps capture Holton’s observation that beliefs should be resilient. If someone is disposed to use decision tables that encode that p, that disposition should be fairly resilient. And to the extent that it is resilient, they will satisfy all the other clauses in my preferred account of belief. So anyone who believes p should have a resilient belief that p.\nThe last point is where I think my biggest disagreement with Ross and Schroeder lies. They think it is very important that a theory of belief vindicate a principle they call Stability.\n\nStability: A fully rational agent does not change her beliefs purely in virtue of an evidentially irrelevant change in her credences or preferences. (20)\n\nHere’s the kind of case that is meant to motivate Stability, and show that views like mine are in tension with it.\n\nSuppose Stella is extremely confident that steel is stronger than Styrofoam, but she’s not so confident that she’d bet her life on this proposition for the prospect of winning a penny. PCR implies, implausibly, that if Stella were offered such a bet, she’d cease to believe that steel is stronger than Styrofoam, since her credence would cease to rationalize acting as if this proposition is true. (22)\n\nRoss and Schroeder’s own view is that if Stella has a defeasible disposition to treat as true the proposition that steel is stronger than Styrofoam, that’s enough for her to believe it. And that can be true if the disposition is not only defeasible, but actually defeated in the circumstances Stella is in. This all strikes me as just as implausible as the failure of Stability. Let’s go over its costs.\nThe following propositions are clearly not mutually consistent, so one of them must be given up. We’re assuming that Stella is facing, and knows she is facing, a bet that pays a penny if steel is stronger than Styrofoam, and costs her life if steel is not stronger than Styrofoam.\nStella believes that steel is stronger than Styrofoam.\nStella believes that if steel is stronger than Styrofoam, she’ll win a penny and lose nothing by taking the bet.\nIf 1 and 2 are true, and Stella considers the question of whether she’ll win a penny and lose nothing by taking the bet, she’ll believe that she’ll win a penny and lose nothing by taking the bet.\nStella prefers winning a penny and losing nothing to getting nothing.\nIf Stella believes that she’ll win a penny and lose nothing by taking the bet, and prefers winning a penny and losing nothing to getting nothing, she’ll take the bet.\nStella won’t take the bet.\nIt’s part of the setup of the problem that 2 and 4 are true. And it’s common ground that 6 is true, at least assuming that Stella is rational. So we’re left with 1, 3 and 5 as the possible candidates for falsehood.\nRoss and Schroeder say that it’s implausible to reject 1. After all, Stella believed it a few minutes ago, and hasn’t received any evidence to the contrary. And I guess rejecting 1 isn’t the most intuitive philosophical conclusion I’ve ever drawn. But compare the alternatives!\nIf we reject 3, we must say that Stella will simply refuse to infer r from p, q and \\((p \\wedge q) \\rightarrow r\\). Now it is notoriously hard to come up with a general principle for closure of beliefs. But it is hard to see why this particular instance would fail. And in any case, it’s hard to see why Stella wouldn’t have a general, defeasible, disposition to conclude r in this case, so by Ross and Schroeder’s own lights, it seems 3 should be acceptable.\nThat leaves 5. It seems on Ross and Schroeder’s view, Stella simply must violate a very basic principle of means-end reasoning. She desires something, she believes that taking the bet will get that thing, and come with no added costs. Yet, she refuses to take the bet. And she’s rational to do so! At this stage, I think I’ve lost what’s meant to be belief-like about their notion of belief. I certainly think attributing this kind of practical incoherence to Stella is much less plausible than attributing a failure of Stability to her.\nPut another way, I don’t think presenting Stability on its own as a desideratum of a theory is exactly playing fair. The salient question isn’t whether we should accept or reject Stability. The salient question is whether giving up Stability is a fair price to pay for saving basic tenets of means-end rationality. And I think that it is. Perhaps there will be some way of understanding cases like Stella’s so that we don’t have to choose between theories of belief that violate Stability constraints, and theories of belief that violate coherence constraints. But I don’t see one on offer, and I’m not sure what such a theory could look like.\nI have one more argument against Stability, but it does rest on somewhat contentious premises. There’s often a difference between the best methodology in an area, and the correct epistemology of that area. When that happens, it’s possible that there is a good methodological rule saying that if such-and-such happens, re-open a certain inquiry. But that rule need not be epistemologically significant. That is, it need not be the case that the happening of such-and-such provides evidence against the conclusion of the inquiry. It just provides a reason that a good researcher will re-open the inquiry. And, as we’ve stated above, an open inquiry is incompatible with belief.\nHere’s one way that might happen. Like other non-conciliationists about disagreement, e.g.,  Kelly (2010), I hold that disagreement by peers with the same evidence as you doesn’t provide evidence that you are wrong. But it might provide an excellent reason to re-open an inquiry. We shouldn’t draw conclusions about the methodological significance of disagreement from the epistemology of disagreement. So learning that your peers all disagree with a conclusion might be a reason to re-open inquiry into that conclusion, and hence lose belief in the conclusion, without providing evidence that the conclusion is false. This example rests on a very contentious claim about the epistemology of disagreement. But any gap that opens up between methodology and epistemology will allow such an example to be constructed, and hence provide an independent reason to reject Stability.\nConclusion\nYou might well worry that the view here is too complex to really be a theory of belief. Belief is a simple state; why all the epicycles? This is a good question, and I’m not sure I have a sufficiently good answer to it.\nAt heart, the theory I’ve offered here is simple. To believe p is to take p for granted, to take it as given, to take it as a settled question. But one doesn’t take a question as settled in a vacuum. I will take some questions as settled in some circumstances and not others. It’s here that the complexities enter in.\nTo believe p, it isn’t necessary that we take it as settled in all contexts. That would mean that anything one believes one would bet on at any odds. But it isn’t sufficient to take it as settled in some context or other. If I’m facing a tricky bet on p, the fact that I’d take p as settled in some other context doesn’t mean that I believe p. After all, I might even decline the bet, although I desire the reward for winning the bet, and believe that if p I will win. And we can’t just focus on the actual circumstances. Five minutes ago, I neither took it as settled or as open that the Cubs haven’t won the World Series for quite a while. I simply wasn’t thinking about that proposition, and didn’t really take it to be one thing or another.\nThis is why things get so complex. To believe p is to hold a fairly simple attitude towards p in some relevant circumstances. But which circumstances? That’s what’s hard to say, and it’s why the theory is so messy. And I think we have an argument that it must be a little hard to say, namely an argument by exhaustion of all the possible simple things to say. The previous paragraph starts such an argument.\nI’d be a little surprised if the account here is the best or last word on the matter though. It does feel a little disjunctive, as if there is a simpler reduction to be had. But I think it’s better than what came before, so I’m putting it forward.\nThe previous version of the theory I put forward was clearly reductive; beliefs were reduced to credences and preferences. This version is not quite as clearly reductive. Which decision tables the agent is disposed to use, and which propositions those tables encode, are not obviously facts about credences and preferences. So it feels like I’ve given up on the reductive project.\nI’m not altogether happy about this; reduction is a good aim to have. But if reduction of belief to other states fails, I’d think this kind of reason is why it is going to fail. Facts about how an agent conceptualises a problem, how she sets up the decision table, are distinct from facts about which values she writes into the table. This is the deepest reason why the Lockean theory is false. Belief is not the difference between one column in the decision table getting probability 0.98 rather than 0.97; it is the difference between one column being excluded rather than included. If that difference can’t be accounted for in terms of actual credences and preferences, the reductionist project will fail.\n\n\nAumann, Robert J. 1999. “Interactive Epistemology i: Knowledge.” International Journal of Game Theory 28 (3): 263–300. https://doi.org/10.1007/s001820050111.\n\n\nBinmore, Ken. 2007. Playing for Real: A Text on Game Theory. Oxford: Oxford University Press.\n\n\nBlock, Ned. 1978. “Troubles with Functionalism.” Minnesota Studies in the Philosophy of Science 9: 261–325.\n\n\nBraddon-Mitchell, David, and Frank Jackson. 2007. The Philosophy of Mind and Cognition, Second Edition. Malden, MA: Blackwell.\n\n\nBrown, Jessica. 2014. “Impurism, Practical Reasoning and the Threshold Problem.” Noûs 48 (1): 179–92. https://doi.org/10.1111/nous.12008.\n\n\nChristensen, David. 2005. Putting Logic in Its Place. Oxford: Oxford University Press.\n\n\nCohen, Stewart. 1999. “Contextualism, Skepticism, and the Structure of Reasons.” Philosophical Perspectives 13: 57–89. https://doi.org/10.1111/0029-4624.33.s13.3.\n\n\nDeFinetti, Bruno. 1964. “Foresight: Its Logical Laws, Its Subjective Sources.” In Studies in Subjective Probability, edited by Henry E. Kyburg and Howard E. Smokler, 93–156. New York: Wiley.\n\n\nDeRose, Keith. 1992. “Contextualism and Knowledge Attributions.” Philosophy and Phenomenological Research 52 (4): 913–29. https://doi.org/10.2307/2107917.\n\n\nDixit, Avinash K., and Susan Skeath. 2004. Games of Strategy. Second. New York: W. W. Norton & Company.\n\n\nFantl, Jeremy, and Matthew McGrath. 2002. “Evidence, Pragmatics, and Justification.” Philosophical Review 111: 67–94. https://doi.org/10.2307/3182570.\n\n\nFraassen, Bas Fraassenvan. 1989. Laws and Symmetry. Oxford: Clarendon Press.\n\n\nFriend, Stacie. 2003. “How i Really Feel about JFK.” In Imagination, Philosophy and the Arts, edited by Matthew Kieran and Dominic McIver Lopes, 35–53. London. Routledge.\n\n\nGillies, Anthony S. 2010. “Iffiness.” Semantics and Pragmatics 3 (4): 1–42. https://doi.org/10.3765/sp.3.4.\n\n\nHeal, Jane. 1994. “Moore’s Paradox: A Wittgensteinian Approach.” Mind 103 (409): 5–24. https://doi.org/10.1093/mind/103.409.5.\n\n\nHolton, Richard. 2014. “Intention as a Model for Belief.” In Rational and Social Agency: Essays on the Philosophy of Michael Bratman, edited by Manuel Vargas and Gideon Yaffe, 12–37. Oxford: Oxford University Press.\n\n\nIchikawa, Jonathan, and Benjamin Jarvis. 2009. “Thought-Experiment Intuitions and Truth in Fiction.” Philosophical Studies 142 (2): 221–46. https://doi.org/10.1007/s11098-007-9184-y.\n\n\nJeffrey, Richard. 1983. “Bayesianism with a Human Face.” In Testing Scientific Theories, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.\n\n\nKelly, Thomas. 2010. “Peer Disagreement and Higher Order Evidence.” In Disagreement, edited by Ted Warfield and Richard Feldman, 111–74. Oxford: Oxford University Press.\n\n\nKohlberg, Elon, and Jean-Francois Mertens. 1986. “On the Strategic Stability of Equilibria.” Econometrica 54 (5): 1003–37. https://doi.org/10.2307/1912320.\n\n\nKratzer, Angelika. 2012. Modals and Conditionals. Oxford: Oxford University Press.\n\n\nLevi, Isaac. 1974. “On Indeterminate Probabilities.” Journal of Philosophy 71 (13): 391–418. https://doi.org/10.2307/2025161.\n\n\nMacFarlane, John. 2011. “Epistemic Modals Are Assessment-Sensitive.” In Epistemic Modality, edited by Andy Egan and Brian Weatherson, 144–78. Oxford: Oxford University Press.\n\n\nMaitra, Ishani, and Brian Weatherson. 2010. “Assertion, Knowledge and Action.” Philosophical Studies 149 (1): 99–118. https://doi.org/10.1007/s11098-010-9542-z.\n\n\nMalmgren, Anna-Sara. 2011. “Rationalism and the Content of Intuitive Judgements.” Mind 120 (478): 263–327. https://doi.org/10.1093/mind/fzr039.\n\n\nMoss, Sarah. 2015. “On the Semantics and Pragmatics of Epistemic Vocabulary.” Semantics and Pragmatics 8: 1–81. https://doi.org/10.3765/sp.8.5.\n\n\nNagel, Jennifer. 2008. “Knowledge Ascriptions and the Psychological Consequences of Changing Stakes.” Australasian Journal of Philosophy 86 (2): 279–94. https://doi.org/10.1080/00048400801886397.\n\n\n———. 2013. “Motivating Williamson’s Model Gettier Cases.” Inquiry 56 (1): 54–62. https://doi.org/10.1080/0020174X.2013.775014.\n\n\nNorth, Jill. 2010. “An Empirical Approach to Symmetry and Probability.” Studies In History and Philosophy of Science Part B: Studies In History and Philosophy of Modern Physics 41 (1): 27–40. https://doi.org/10.1016/j.shpsb.2009.08.008.\n\n\nRamsey, Frank. 1929/1990. “Probability and Partial Belief.” In Philosophical Papers, edited by D. H. Mellor, 95–96. Cambridge University Press.\n\n\n———. 1926. “Truth and Probability.” In Philosophical Papers, edited by D. H. Mellor, 52–94. Cambridge: Cambridge University Press.\n\n\nRoss, Jacob, and Mark Schroeder. 2014. “Belief, Credence, and Pragmatic Encroachment.” Philosophy and Phenomenological Research 88 (2): 259–88. https://doi.org/10.1111/j.1933-1592.2011.00552.x.\n\n\nRunyon, Damon. 1992. Guys & Dolls: The Stories of Damon Runyon. New York: Penguin.\n\n\nSchoenfield, Miriam. 2013. “Permission to Believe: Why Permissivism Is True and What It Tells Us about Irrelevant Influences on Belief.” Noûs 47 (1): 193–218. https://doi.org/10.1111/nous.12006.\n\n\nSorensen, Roy A. 1988. Blindspots. Oxford: Clarendon Press.\n\n\nStalnaker, Robert. 1994. “On the Evaluation of Solution Concepts.” Theory and Decision 37 (1): 49–73. https://doi.org/10.1007/BF01079205.\n\n\n———. 1996. “Knowledge, Belief and Counterfactual Reasoning in Games.” Economics and Philosophy 12: 133–63. https://doi.org/10.1017/S0266267100004132.\n\n\n———. 1998. “Belief Revision in Games: Forward and Backward Induction.” Mathematical Social Sciences 36 (1): 31–56. https://doi.org/10.1016/S0165-4896(98)00007-9.\n\n\n———. 1999. “Extensive and Strategic Forms: Games and Models for Games.” Research in Economics 53 (3): 293–319. https://doi.org/10.1006/reec.1999.0200.\n\n\nStephenson, Tamina. 2007. “Judge Dependence, Epistemic Modals, and Predicates of Personal Taste.” Linguistics and Philosophy 30 (4): 487–525. https://doi.org/10.1007/s10988-008-9023-4.\n\n\nWalley, Peter. 1991. Statisical Reasoning with Imprecise Probabilities. London: Chapman & Hall.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\n———. 2011. “Defending Interest-Relative Invariantism.” Logos & Episteme 2 (4): 591–609. https://doi.org/10.5840/logos-episteme2011248.\n\n\n———. 2012a. “Games and the Reason-Knowledge Principle.” The Reasoner 6 (1): 6–8.\n\n\n———. 2012b. “Knowledge, Bets and Interests.” In Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.\n\n\nWeintraub, Ruth. 2008. “How Probable Is an Infinite Sequence of Heads? A Reply to Williamson.” Analysis 68 (3): 247–50. https://doi.org/10.1093/analys/68.3.247.\n\n\nWhite, Roger. 2005. “Epistemic Permissiveness.” Philosophical Perspectives 19: 445–59. https://doi.org/10.1111/j.1520-8583.2005.00069.x.\n\n\nWilliamson, Timothy. 1996. “Knowing and Asserting.” Philosophical Review 105 (4): 489–523. https://doi.org/10.2307/2998423.\n\n\n———. 2007a. “How Probable Is an Infinite Sequence of Heads?” Analysis 67 (295): 173–80. https://doi.org/10.1111/j.1467-8284.2007.00671.x.\n\n\n———. 2007b. The Philosophy of Philosophy. Blackwell.\n\n\n———. 2013. “Gettier Cases in Epistemic Logic.” Inquiry 56 (1): 1–14. https://doi.org/10.1080/0020174X.2013.775010.\n\n\nYalcin, Seth. 2011. “Nonfactualism about Epistemic Modality.” In Epistemic Modality, edited by Andy Egan and Brian Weatherson, 295–332. Oxford: Oxford University Press.\n\n\nIf there’s any gap in Williamson’s argument, it is I think at the point where he concludes that any two infinite sequences of coin flips have the same probability of landing all heads. I think that the defender of non-numerical, comparative approaches to probability can deny that with some plausibility. Perhaps the two sequences of coin flips have incomparable probabilities of landing all heads. But this leads us into complications that are irrlevant to this paper, especially since I think it turns out there is a sound Williamsonian argument against the Lockean who lets different sequences have incomparable probabilities. For a more pessimistic take on Williamson’s argument, see Weintraub (2008).↩︎\nI’m grateful to the participants in a game theory seminar at Arché in 2011, especially Josh Dever and Levi Spectre, for very helpful discussions that helped me see through my previous confusions.↩︎\nActually, I guess it is worse than if some games have the only equilibria involving mixed strategies with irrational probabilities. And it might be noted that Binmore’s introduction of mixed strategies, on page 44 of his (2007), sounds much more like the metaphysical interpretation. But I think the later discussion is meant to indicate that this is just a heuristic introduction; the epistemological interpretation is the correct one.↩︎\nIt’s received wisdom in philosophy that one can never properly say something of the form p, but I don’t know that p. This is used as a data point in views as far removed from each other as those defended in Heal (1994) and Williamson (1996). But I don’t feel the force of this alleged datum at all, and (B) is just one reason. For a different kind of case that makes the same point, see Maitra and Weatherson (2010).↩︎\nAs an aside, the existence of these cases is why I get so irritated when epistemologists try to theorise about ‘Gettier Cases’ as a class. What does (B) have in common with inferences from a justified false belief, or with otherwise sound reasoning that is ever so close to issuing in a false conclusion due to relatively bad luck? As far as I can tell, the class of justified true beliefs that aren’t knowledge is a disjunctive mess, and this should matter for thinking about the nature of knowledge. For further examples, see Williamson (2013) and Nagel (2013).↩︎\nA similar example is in Kratzer (2012, 94).↩︎\nThe idea of using allergies to illustrate the kind of case we’re interested in is due to Ross and Schroeder (2014), and I’m grateful for the idea. It makes the intuitions much more vivid. The kind of cases we’re considering play a big role in, inter alia, DeRose (1992; Cohen 1999) and Fantl and McGrath (2002).↩︎\nWhat I say here obviously has some similarities to a view put forward by Jennifer Nagel (2008), but I ultimately end up drawing rather different conclusions to the ones she draws.↩︎\nNotable members of the tradition include Levi (1974), Jeffrey (1983) and Fraassen (1989).↩︎\nThe point I’m making here is relevant I think to recent debates about the proper way to formalise counterexamples in philosophy, as in  (Williamson 2007b; Ichikawa and Jarvis 2009; Malmgren 2011). I worry that too much of that debate is focussed on the role that examples play in one-step refutations. But there’s more, much more, to a good example than that.\n\n↩︎\n",
    "preview": "posts/2021-02-02-games-beliefs-and-credences/dalicat.jpg",
    "last_modified": "2021-02-05T20:43:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-reply-to-blackson/",
    "title": "Reply to Blackson",
    "description": "Thomas Blackson argues that interest-relative epistemologies cannot explain the irrationality of certain choices when the agent has three possible options. I argue that his examples only refute a subclass of interest-relative theories. In particular, they are good objections to theories that say that what an agent knows depends on the stakes involved in the gambles that she faces. But they are not good objections to theories that say that what an agent knows depends on the odds involved in the gambles that she faces. Indeed, the latter class of theories does a better job than interest-invariant epistemologies of explaining the phenomena he describes.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2016-01-01",
    "categories": [
      "epistemology",
      "interest-relativity",
      "notes"
    ],
    "contents": "\nIt isn’t true that all interest-invariant epistemologies are alike, but it is certainly true that every interest-relative theory is interest-relative in its own idiosyncratic way. In fact, there are at least four dimensions along which a theory can be interest-relative.\n\nPublished in Journal of Philosophical Research 46: 73-75.\nI used to think  (Weatherson 2005) that interest-relativity in knowledge was to be explained by interest-relativity in belief, but I came to think that’s not true  (Weatherson 2012). Some prominent defenders of interest-relativity in epistemology focus on practical interests – it’s even there in the title of the book by Jason Stanley (2005) – but others of us think that theoretical interests matter too. At times Stanley writes as if interest-relativity means that there is an extra clause in the theory of knowledge for interests, but one need not think that. It could, for example, be that there is an interest-sensitive domain restriction on a quantifier in one of the clauses.\nBut for present purposes, the key divide among interest-relative epistemologists is between those who think that stakes are relevant, and those who think odds are relevant. I think, following Mark Schroeder (2012), that it is odds that matter. The key examples here are ones where there is little cost to gambling and getting it wrong, but even less to gain by gambling and getting it right.\nSo imagine that Ankita is walking to a restaurant she hasn’t been to for a few months. She is stopped at the lights, reading baseball scores on her phone. She is almost, but not completely, certain that she should turn left at the next block, which indeed she should. If she had been wrong, she would have gone two blocks out of her way. She could avoid this risk by flipping from her baseball app to the map on her phone and checking the address, all of which she could do before the lights change. I say that in this circumstance, she doesn’t know where the restaurant is. She should look up where it is, that’s what maximises expected utility, but she needn’t look up restaurants she knows the location of. So she doesn’t know whether the restaurant is to her left or her right. Ankita’s case is not a high-stakes one. Even on a cold Michigan Fall evening, the downside of walking two extra blocks is not that high. But unless she really really cares about those baseball scores she’s browsing through, deciding not to flip over to the map is a gamble on the correctness of her plans at incredibly long odds. That’s not because the stakes are high, but because the gain from gambling is low.\nThis is a genuine case of interest-relativity though. The argument I just gave wouldn’t go through if Ankita would have no disutility whatsoever from walking two blocks out of her way. (Maybe the exercise gain would completely outweigh the frustration.) In that case, perhaps she does know. But if the two block walk has many times the disutility of no longer browsing baseball scores, as it would in most realistic cases, her interests defeat her knowledge of the restaurant location.\nThe same thing I think is going on in the example Thomas Blackson (2016) gives. The agent has a three-way choice, between taking drug A, taking drug B and doing nothing. The upsides of both drugs are the same; they alleviate a minor medical condition. The downsides are the same too; they lead to death in rare cases. But this is much rarer still in the case of B than A. So, says Blackson, the person should take drug B. And I agree. But, says Blackson, this is a problem, because I’m committed to the following argument.\nThe agent knows that drug A and drug B won’t kill them.\nAny option an agent knows not to obtain can be left off a decision table.\nSo, from 1 and 2, the decision table the agent faces has only the upsides, and not the downsides.\nSo, from 3, the decision tables for the two drugs are the same.\nSo, from 4, the agent can be indifferent between the drugs.\nSince 5 is false, one of the premises must be false. Blackson says that the false premise is 2. I say that it is 1; the patient doesn’t know drug A won’t kill them. Blackson anticipates this, and says that the move won’t work because of a related case. If drug B didn’t exist, the interest-relative theorist might say that the agent has enough evidence to know the drug won’t kill them. Maybe that’s true, but it isn’t clear why it is relevant. The existence of drug B changes the gamble involved in taking drug A. It must, since taking drug A is irrational in the original case, but rational in the version where drug B doesn’t exist. So if what the agent knows is sensitive to what gambles they face, then it isn’t surprising that the presence of extra options changes what the agent knows.\nAnd the interest-relative theory has a nice explanation of one feature of Blackson’s case. Imagine the agent learns that drug A is only ever fatal with people with blood type A2B negative, and that’s not their blood type. There’s no similarly known marking for when drug B is fatal. Now the agent can say, “Now that I know drug A won’t kill me, I’m going to take it, not drug B.” That seems like just the right thing to say, but on Blackson’s telling of the case, they can’t say it. After all, they knew all along that drug A wouldn’t kill them. Instead he has to say that the agent shouldn’t take drug A before they learn this, because of the risk it would kill them, even though the agent knows the drug won’t kill them. That doesn’t sound at all right. Much better, I think, to say they shouldn’t take drug A before learning who it endangers, because it exposes them to a needless risk of death, but once they know it won’t kill them, it is good to take it.\nAnd that’s the general strategy for defending interest-relative treatments of cases like Blackson’s, and the others he describes. It’s a strategy that I don’t think is threatened by any example to date. There’s some action that all parties agree would be irrational, or at least not rationally mandatory. But there’s some evidence we can imagine the agent getting that would improve the rational status of the action. (E.g., change it from being rationally impermissible to rationally permissible, or from not rationally required to rationally required.) If we asked the rational agent why they changed their plans after getting that evidence, it seems to make sense for them to say that they now know the action would have a good outcome. That is, it makes sense for them to cite their knowledge as a reason for doing something different after the evidence-gathering event. And only the interest-relative epistemologist can explain why that is a sensible answer for them to give.\n\n\n\nBlackson, Thomas. 2016. “Against Weatherson on How to Frame a Decision Problem.” Journal of Philosophical Research 41: 69–72. https://doi.org/10.5840/jpr201662868.\n\n\nSchroeder, Mark. 2012. “Stakes, Withholding and Pragmatic Encroachment on Knowledge.” Philosophical Studies 160 (2): 265–85. https://doi.org/10.1007/s11098-011-9718-1.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\n———. 2012. “Knowledge, Bets and Interests.” In Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.\n\n\n\n\n",
    "preview": "posts/2021-02-03-reply-to-blackson/vials.jpg",
    "last_modified": "2021-02-05T20:44:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-memory-belief-and-time/",
    "title": "Memory, Belief and Time",
    "description": "I argue that what evidence an agent has does not supervene on how she currently is. Agents do not always have to infer what the past was like from how things currently seem; sometimes the facts about the past are retained pieces of evidence that can be the start of reasoning. The main argument is a variant on Frank Arntzenius’s Shangri La example, an example that is often used to motivate the thought that evidence does supervene on current features.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2015-12-01",
    "categories": [
      "epistemology",
      "games and decisions",
      "imprecise probability"
    ],
    "contents": "\n\nContents\nEvidentialism\nMemory and Testimony\nShangri La\nIterated Shangri La\nAgainst Indirect Theory\nArgument from A Priority\nArgument from Laundering\nConclusion and Future Research\nPreservation\nInitial Evidence and Over-Riding\nExternalism\n\n\nI know a lot about the past. I know, for instance, that the Chicago White Sox won the 2005 baseball World Series. I remember that’s true. I don’t remember the event. I was in Australia, and it wasn’t on television. I don’t even remember the event of learning that the White Sox won. But I remember that they won. And to remember something is to, inter alia, know it is true. And to know something is to, inter alia, have a rational belief that it is true.\n\nPublished in Canadian Journal of Philosophy 45: 692-715\nSo I have a rational belief that the White Sox won the 2005 baseball World Series. In virtue of what is this belief of mine rational? That’s too big a question to answer here, so let’s start narrowing it down. Is this belief rational in virtue of facts about how I now am, or historical facts about me? Call the former view a temporally local theory of rationality, and the latter a temporally extended view. Which of those is correct?\nI’m going to defend the temporally extended view. In this respect I’m following recent work by David James Barnett (2015), though being a philosopher I’ll quibble about his argument, put forward alternate reasons, and so on. But I’m agreeing with his big conclusion.\nAt least, I’m going to agree with a version of that conclusion. I’m an evidentialist about rationality, in a sense that I’ll try to make clearer as we progress through the paper. So it’s natural to convert the core question into a question about evidence, and about evidence acquisition. What is my evidence that the White Sox won the 2005 World Series, and in virtue of what do I have that evidence? Is it the currect fact that it mnemonically seems to me that the White Sox won, perhaps supplemented with some knowledge I have about the reliabiity of my mnemonic seemings? Or is it something more temporally extended? I’m going to argue that it is the latter.\nMy positive view, inspired to some extent by the evidence is knowledge view defended by Timothy Williamson (2000), is that the fact the White Sox won became part of my evidence some time in 2005, and has stayed in my evidence ever since. At the time this belief, and this knowledge, was grounded in further evidence, presumably perceptual evidence of what some computer screen looked like. But I came to know the White Sox won, and this became part of my evidence. An alternative view is that the visual seemings from 2005 are part of my evidence still. That’s what the view of David Lewis (1996) implies. And yet another view is that the content of those perceptions, perhaps that ESPN is telling me the White Sox won, is still in my evidence. I don’t like either of these latter views, but I’m not going to argue about them here. Rather, the focus is on whether evidence is contemporary or historical, and I want to argue for the class of historical theories over the class of contemporary theories.\nEvidentialism\nI’m interested in memory because it raises challenges for the evidentialist theory I’d like to defend. Evidentialism, as we’ll start construing it, says that the doxastic attitudes it is rational to have depend entirely on the evidence one has. This is a version of evidentialism. I’m taking this to be a thesis both about partial beliefs, what are commonly called credences in the philosophical literature, and full beliefs. I have a lot to say elsewhere about the relationship between full and partial belief  (Weatherson 2012), but I won’t be relying on those views here.\nI am construing the ‘dependence’ in the statement of evidentialism rather weakly. It is just a claim that the evidence one has, and the attitudes it may be rational to hold, co-vary. Put another way, the rationality of doxastic attitudes supervenes on one’s evidence, at least throughout worlds similar enough to this one. I am not defending the stronger claim that facts about what evidence one has are always explanatorily prior to facts about what doxastic attitudes it is rational to hold.\nIt is easy enough to imagine epistemologies that aim for this more ambitious, priority, thesis. David Lewis (1996), for instance, suggests we should understand evidence in terms of phenomenal states; two agents with the same phenomenology over time have the same evidence. It’s arguable that facts about phenomenology are metaphysically prior to facts about rationality. So, if one was an evidentialist with Lewis’s theory of evidence, it would be natural to think that facts about evidence didn’t just subvene facts about rationality; the former provided full and perhaps reductive explanations for the latter.\nI hold out no such hope for reductive explanations. Indeed, I’m closer in spirit to the kind of view you might read into Timothy Williamson (2000). As noted above, Williamson holds that one’s evidence is all and only what one knows. This thesis has become known as E=K. The notation here is instructive. It is commonplace to introduce new terms by definition by putting the new term on the left-hand side of an equality sign. \\(A =_{df} B\\) means that \\(A\\) is defined to be identical to \\(B\\), not the other way around. The E=K thesis suggests a form of evidentialism where evidence is in fact explanatorily posterior to rationality. Something is part of one’s evidence in virtue of the fact that one knows it, and arguably one only knows what one rationally believes.\nI’ve been a bit coy in the previous paragraph about what I’m attributing to Williamson, and what I’m just saying can be read into him. That’s because the view Williamson defends is not that rationality has explanatory priority, but that knowledge does. As he says in the first line of his book, his view is “knowledge first”  (Williamson 2000 v). And it’s consistent with ‘knowledge first’ to say that the explanatory relationship between evidence and rationality is complicated and multi-directional. Although I don’t endorse the knowledge first program, I agree with that last conclusion. The explanatory relationship between evidence and rationality is complicated and multi-directional. Evidentialism should not be construed as denying this claim.\nThe other way in which my version of evidentialism is weaker than it may be is that it really is restricted to being a claim about rationality. It isn’t a claim about justification. For all I say here, maybe something other than evidence determines whether a doxastic attitude is justified. For example, it may be that only true beliefs are justified. I don’t think that’s true, but if it is it would be consistent with evidentialism as I’m construing it.\nMore importantly for what follows, evidentialism also isn’t a claim about wisdom. It is very important to keep evaluations of agents apart from evaluations of acts or states. It is attitudes or states that are in the first instance rational or irrational. We can talk about rational or irrational agents, but such notions are derivative. Rational agents are those generally disposed to have rational attitudes, or be in rational states. Wisdom, on the other hand, is in the first instance a property of agents. Again, we can generalise the term to attitudes or states. A wise decision, for instance, is one that a wise person would make. But the wisdom of agents is explanatorily and analytically prior to the wisdom of their acts, judgments, decisions and attitudes. (I think that everything I’ve said in this paragraph is true of ordinary English. But I’m not committed to that, and it doesn’t matter if I’m wrong. You can read this paragraph as stipulating that ‘rational’ is to be used as a term that in the first instance applies to states, and ‘wise’ is to be used as a term that in the first instance applies to agents, and little will be lost.)\nEvidentialism is not a claim about the nature of wise agents. Perhaps a wise agent is one who always has rational attitudes. If so, then evidentialism will have quite strong implications for what wise agents are like. But that connection between wisdom and rationality is far from an obvious conceptual truth. For all I’ve said, it may well be wise to have doxastic attitudes that do not track one’s evidence. That is consistent with evidentialism, provided we understand the relevant situations as being ones where it is unwise to have rational attitudes.\nThe most important recent work on the connection between rationality and wisdom is by Maria Lasonen-Aarnio (2010, 2014a). And I agree with almost everything she says about the connection. The biggest difference between us is terminological. She uses ‘reasonable’ and ‘reasonableness’ where I use ‘wise’ and ‘wisdom.’ In my idiolect, I find it too easy to confuse ‘rational’ and ‘reasonable.’ So I’m using a different term, and one that, to me at least, more strongly suggests a focus on agents, not states. But this is a small point, and everything I say about the distinction draws heavily on Lasonen-Aarnio’s work.\nFinally, I’m not taking evidentialism to be committed to any kind of uniqueness thesis. It may be that different agents with the same evidence can have different views about p, and both be rational. That’s fine, as long as any agent with just that evidence could have either view about p and be rational. The view is that there’s a function from evidence and attitude to rational evaluation, not that there’s a function from evidence to rational attitude.\nMemory and Testimony\nIt’s natural to think about theories of memory by analogy to theories of testimony. Indeed, we see this strategy used in otherwise very different work by Sarah Moss (2012) and David James Barnett (2015). Moss and Barnett have very different views on memory, and very different views on the relationship between memory and testimony, but they both find it worthwhile to situate views about memory in relation to views about testimony. And I will follow this lead.\nFor an evidentialist, there are three interesting classes of theories of testimony. These almost, but not quite, track onto familiar categories of theories in the literature on testimony. I’ll use slightly idiosyncratic names for them, just to indicate that the categories aren’t exactly the same. In all cases, speaker S says that p on the basis of evidence E, and hearer H hears (and understands) the speaker. (And I’ll assume S is a she, and H a he.) I’m going to start with the case where S knows that p, and H has no reason to doubt S’s testimony; we’ll look at the complications that ensue when those assumptions are dropped presently.\nThe classes I’m interested in are divided by their answers to two questions:\nIs the evidence that H gets, in the first instance, that p, or that S said that p?\nIf the evidence is only that S said that p, is the fact that S said that p a ‘self sufficient’ reason to believe that p, or does it need to be supplemented?\nThe term ‘self sufficient’ is borrowed from Anna-Sara Malmgren (2006), who uses it in describing work by Crispin Wright (2002, 2004), James Pryor (2004) and Roger White (2005). Wright, Pryor and White are primarily concerned with whether perceptual appearances are self sufficient reasons to believe their contents, or they need to be supplemented. That isn’t the focus here; like Malmgren I’m focussing on testimony and memory.\nHere are the three classes of views that you get from the natural answers to those questions.\nIndirect Theories of Testimony.\nThe evidence is that S said that p, and this is not a self-sufficient reason to believe that p. This class closely corresponds to the class of so-called reductionist theories of testimony. Jennifer Lackey (2008) provides an important recent indirect theory.\n\nDirect Theories of Testimony.\nThe evidence is that S said that p, and this is a self-sufficient (though defeasible) reason to believe that p. Many theorists who reject reductionism about testimony endorse what I’m calling a direct theory. C. A. J. Coady (1995) provides an important recent direct theory.\n\nTransmission Theories of Testimony.\nThe evidence is that p, so it doesn’t matter how we answer the second question. Frederick Schmitt (2006) provides an important recent transmission theory.\n\nTransmission theories need not deny that H also gets the evidence that S said that p. And they need not take a stand on how good that evidence is as evidence that p. And direct theories need not deny that H may have independent evidence that if S says that p, then p is true. But in the other direction, I’m taking it as characteristic of the theories that they deny the core claims of the ones that come after them. So indirect theories deny that H immediately gets evidence that p, or that S says that p is a self sufficient reason to believe p. And direct theories deny that H immediately gets evidence that p.\nThe direct and transmission theories just say that a certain thing is possible. I haven’t said yet what they have to say about when it possible. To make matters a little less abstract, I’ll focus for now on theories that abide by the following constraints.\nS saying that p is only reason to believe that p in the absence of evidence against p, and in the absence of evidence against S’s reliability.\nH only gets to add p to their stock of evidence if it was in S’s stock of evidence to start with; testimony doesn’t generate evidence, except for evidence about what is said.\nA direct theory that didn’t comply with the first constraint really would be a charter for gullibility. Even with this constraint, direct theories possibly are too gullible, as Elizabeth Fricker (1994) has argued, but without this constraint they certainly are. And a transmission theory that didn’t comply with the second constraint would not deserve the name transmission; it would be a generative theory.\nWe can use these categories to draw three similar categories of memory. Here the case is that M forms a belief that p at \\(t_1\\), and has an apparent memory of p at \\(t_2\\). As we might put it, her memory reports that p at this time. As above, start with the simple case where M knows p at \\(t_1\\), and there is no counterevidence, or reason to doubt her own reliability, at later times. We’ll come back, in great detail, to cases where those assumptions are relaxed. What evidence does M get, in these simple cases, when her memory reports that p, and how good is this evidence?\nIndirect Theories of Memory\nThe evidence is that M’s memory reports that p, and this is not a self sufficient reason to believe that p.\n\nDirect Theories of Memory\nThe evidence is that M’s memory reports that p, and this is a self sufficient reason to believe that p.\n\nTransmission Theories of Memory\nThe evidence is that p.\n\nThe first two theories are temporally local, in the sense I started with, and the last is temporally extended. Again, we’ll put some restrictions in place.\nMemory’s reporting that p is only a self sufficient reason to believe that p in the absence of either evidence against p, or evidence that memory is unreliable.\nMemory only transmits evidence that p if p was genuinely among M’s pieces of evidence at an earlier time. And that requires, I’m assuming, that M knew that p at the earlier time.1\nSince I want to defend a temporally extended theory, that means I’m defending the transmission theory. And like Barnett, I do so while rejecting the corresponding theory of testimony.2 But once we set things out this way, we see that there are two distinct temporally local theories, and they fail for slightly different reasons. Before we get to why they fail, we’ll look at a reason for thinking one or other of them must work.\nShangri La\nThe Shangri La case introduced by Frank Arntzenius (2003) can be used to generate an argument that evidentialists are committed to the temporally local approach to evidence. This isn’t exactly how Arntzenius introduced it; he introduced it as a puzzle for conditionalisation. But the argument I’m interested in is related to the puzzle Arntzenius introduced. Here is how Michael Titelbaum describes the example.\n\nYou have reached a fork in the road to Shangri La. The guardians of the tower will flip a fair coin to determine your path. If it comes up heads, you will travel the Path by the Mountains; if it comes up tails, you will travel the Path by the Sea. Once you reach Shangri La, if you have traveled the Path by the Sea the guardians will alter your memory so you remember having traveled the Path by the Mountains. If you travel the Path by the Mountains they will leave your memory intact. Either way, once in Shangri La you will remember having traveled the Path by the Mountains. The guardians explain this entire arrangement to you, you believe their words with certainty, they flip the coin, and you follow your path. What does ideal rationality require of your degree of belief in heads once you reach Shangri La.  (Titelbaum 2014, 120)\n\nThe name of the person Titelbaum’s narrator is addressing isn’t given, so we’ll call him Hugh. And we’ll focus on the case where Hugh actually travels by the Mountains.\nOriginal Shangri La game; Hugh takes the right-hand pathThere is something very puzzling about Hugh’s case. On the one hand many philosophers (including Arntzenius and Titelbaum) report a strong intuition that once in Shangri La, Hugh should have equal confidence that he came by the mountains as that he came by the sea. On the other hand, it’s hard to tell a dynamic story that makes sense of that. When he is on the Path by the Mountans, Hugh clearly knows that he is on that path. It isn’t part of the story that the paths are so confusingly marked that it is hard to tell which one one is on. Then Hugh gets to Shangri La and, well, nothing happens. The most straightforward dynamic story about Hugh’s credences would suggest that, unless something happens, he should simply retain his certainty that he was on the Path by the Mountains.\nAnd you might think evidentialism is committed to the same thing as that dynamic story. To see why, imagine that Hugh is being terrifically sneaky, and wearing a small camera in his glasses. The camera is tracking what he sees, and Laurie is watching it on a distant TV monitor. The guardians can’t do anything to Laurie’s memory, so they don’t, just like they don’t do anything to Hugh. That night, it might seem Hugh and Laurie have the same evidence. Yet, according to some intuitions, it is rational for Laurie to believe that Hugh took the Path by the Mountains, and not rational for Hugh to believe this.\nHere’s a natural way out of that bind. Say that the evidence Hugh and Laurie have does not consist of what they saw as Hugh was ascending, but their current mnemonic seemings. Now their evidence is different. Hugh has the evidence that it seems to Hugh that Hugh ascended via the mountains, and Laurie has the evidence that it seems to Laurie that Hugh ascended via the mountains. And it is common knowledge that in either this world or a nearby one, Hugh’s mnemonic seemings are unreliable, while Laurie’s are reliable in all nearby worlds. So the temporally local theories can handle the problem, while one might think temporally extended theories cannot.\nThe most straightforward way to explain the common intuition about Shangri La is via the indirect theory of memory. On that theory, Hugh won’t know that he came to Shangri La via the mountains. That’s because the report of his memory, “We got here via the mountains, Hugh!” would be the same however he came up, and Hugh knows it. There is no basic entitlement, on this theory, to move from My memory says that p to p, and since Hugh does not even believe that a correlation obtains in practice between what he believes about his method of ascent and how he actually ascended, there is no earned entitlement.\nIt is a little tempting to read some of the published arguments that Hugh can’t know he came via the mountains as reasoning in just this way. Here is Arntzenius’s central argument. (Assume Arntzenius is talking to Hugh here, so ‘you’ picks out Hugh.)\n\nFor you will know that he would have had the memories that you have either way, and hence you know that the only relevant information that you have is that the coin was fair.  (Arntzenius 2003, 356)\n\nSarah Moss (2012) makes a similar claim about the case. (Again, her narration is addressed to Hugh.)\n\nIntuitively, even if you travel on the mountain path, you should have .5 credence when you gets to Shangri La that the coin landed heads. This is a case of abnormal updating: once you arrive in Shangri La, you can no longer be sure that you traveled on the mountain path, because you can no longer trust your apparent memory.  (Moss 2012, 241–42)\n\nNow it isn’t immediately clear why the fact that Hugh would have the same apparent memories in the two cases should matter. As far as I can see, the only way it could matter is if the following two things were true. First, we are using a temporally local theory, so the evidence is what Hugh’s memory reports when he is in Shangri La, not the evidence he acquired on the trip up the mountain. And second, what those appearances support is solely a function of things internal to the agent, and not, say, their connection to the truth. As an evidentialist, I’m committed to a version of that second assumption - at least, I’m committed to saying that things that over-ride evidence must themselves be evidence.\nLet’s focus for now on the assumption of temporal localism behind the arguments here. I’m going to offer a series of arguments against it, starting with a variant on the Shangri La case.\nIterated Shangri La\nHere’s a slightly more complicated variant of the Shangri La example.\n\nSati walks up to the base of the paths to Shangri La. “Have some toast and yeast extract,” says one of the attendants, somewhat stiltedly.\n“Yeast extract?” says Sati.\n“Yes, yeast extract. Vegemite or Marmite, your choice.”\n“Must I?” says Sati.\n“You must.”\n“Well, Vegemite then,” says Sati, recalling fond memories of having Vegemite in Australia, and dire memories of that trip to the English countryside.\n“Good choice,” says the attendant. Sati has her Vegemite on toast, and heads up the mountain path to Shangri La, as directed. On the way, she notices a worried looking person standing in front of a priest about to flip a coin. When she gets to Shangri La, she asks the attendant about that.\n“Oh,” says the attendant, “he chose Marmite.” Sati looks confused as to why this is relevant, so the attendant continues. “The priests don’t like people who choose Marmite, but they still must let them through. So they flip a coin to decide whether they will go by the sea or the mountains. Then, if they went by the sea, they will wipe the memory of that trip, and replace it with a memory of going through the mountains.”\n“I’m glad that didn’t happen to me. Lucky I chose Vegemite.”\n“Recently,” continued the attendant, “the priests decided to make things more complicated. They decided they would also wipe the memory of having eaten the Marmite, and hence facing the coin flip. Instead they would implant a false memory of having chosen Vegemite, indeed false memories of having preferred Vegemite to Marmite in the past, plus a false memory of seeing some other poor sap facing the coin flip. They really really don’t like Marmite eaters.”\n“So all the Marmite eaters get memories wiped?” asked Sati.\n“No, only if the coin lands the wrong way. So some people get to the top thinking they liked Marmite. But we only tell that memories of going by the sea will be wiped. In fact, knowing they chose Marmite is evidence they went by the Mountains, but they don’t know that.”\n“It all sounds horrible,” says Sati. “I’m so glad I remembered I liked Vegemite more than Marmite.”\n“Have a good day!” said the attendant, grinning.\n\nI think that Sati’s last statement is correct; she does remember that she likes Vegemite more than Marmite. Indeed, she knows this in virtue of her memory. But it’s not clear how a temporally local theory, either direct or indirect, can get that answer.\nImagine someone, call him Joe, who starts off in the same situation as Sati at the base of the mountain. Sadly, due to an unfortunate unbringing, he prefers Marmite to Vegemite, so he takes that. And then the coin lands the wrong way, and he is sent by the sea. Then his memories are wiped and replaced with fake memories when he gets to Shangri La.\nRevised Shangri La game; Sati takes the right-hand path, Joe the left- hand pathIf a temporally local theory is correct, then presumably Sati and Joe have the same evidence. And that means if evidentialism is true, then it is rational for them to believe the same things. Yet that is implausible; Joe should not be very confident that he had the Vegemite, came by the mountains, and so on.\nOn the other hand, it is overdetermined that Sati can know she came by the Mountains. The crucial difference between Sati and Hugh comes from the defeasibility conditions on the transmission theory. Past memories that p transform into current evidence that p unless they are forgotten, or the agent gets some good reason to suspect that her memory is unreliable. Hugh has such a reason; he is a coin flip away from having faulty memories. Sati does not have such a reason. She knows that had she had a very different kind of upbringing, and had she been on the bad end of a coin flip, she would have had faulty memories. But a reason to think that had things been different she would have reason to distrust her memories is not, itself, a reason to distrust her memories.\nSati’s case is not meant to be a close call. There are lots of relevant ways in which her case is different to cases in which the defeasibility clause is triggered. The fact that two different kinds of things need to have gone wrong here is relevant. And the fact that the first requires things going wrong for a long long time into the past is relevant. And the fact that the first is only a problem in very different possible worlds to actuality is relevant. In short, any plausible kind of defeasibility condition whatsoever on the transmission theory will mean that Joe’s memories of going by the sea are not transmitted, but only an implausibly strong defeasibility condition will prevent Sati’s memories from being transmitted.\nNote that I have not said that Sati can trust her memories because the probability of them being unreliable is so low. That is not the way to formulate defeasibility conditions. The sense of probability that is relevant here is evidential probability. And evidential probability is, as the name suggests, explanatorily posterior to evidence possession. We should not use evidential probability in our theory of what evidence the agent has. Sati knows she grew up liking Vegemite, despite the Shangri La shenanigans. But that’s not because it is so improbable that she had her memories wiped. Rather, it is improbable she had her memories wiped because she knows she does not meet the conditions under which memories are wiped.\nSo temporally extended theories can distinguish Sati’s case from Joe’s, as intuition requires that they be distinguished. But temporally local theories seem to have a problem here. Perhaps the problem here is not with the theory of mnemonic evidence that the the temporally local theories hold, but with evidentialism. Perhaps, that is, this is a case where we should say that Sati and Joe have the same evidence, but that this evidence supports different beliefs given the different reliability of their memories.\nBut there is little to be said to motivate such a theory. If we aren’t going to be evidentialists, it isn’t clear what the relevance of a theory of evidence is. And if we are going to say that historical events, like the fact that Joe’s memories were wiped and Sati’s weren’t, are relevant to contemporary rationality, it isn’t clear what we gain by having a temporally local theory of evidence. Either way, we have said that the existence of past events is relevant to the rationality of current beliefs. At this point we aren’t engaged in much more than a terminological dispute with the temporally extended theories.\nAgainst Indirect Theory\nAs Matthias Steup (2013) argues, the indirect theory of memory is implausible. It says that when one remembers that, say, the Chicago White Sox won the 2005 World Series, there are two things that are needed in order to ground the rational belief. The first is the apparent memory, and the second is some kind of reason to think that the memories are reliable. But the only reasons we could have for believing the second comes from what we have learned about the track record of memory, or perhaps of the role of memory in human functioning. And we couldn’t be rational in believing those things unless we could rationally rely on memory in forming beliefs. So we can never rationally form any belief on the basis of memory unless we antecedently have reason to trust memory. And that, plus the indirect theory of memory, leads to a vicious regress, and hence to an implausible scepticism.\nThe argument here is similar in form to an argument that has often been levelled against the indirect theory of testimony. This argument traces back at least to Coady (1995). The argument is that children can rely on testimony to get knowledge, and hence rational belief, but they don’t have the information or the cognitive capacity to rationally judge who is and isn’t reliable. So it can’t be, contra the indirect theorist, that such judgments of reliability are required in order to get rational belief and knowledge from testimony.\nOne problem with such an argument in the case of testimony is that it has relied, historically, on a very impoverished view of the cognitive capacities of young children. It is true that the capacity shown for explicit reasoning by children is often very weak. But they have rather amazing capacities for implicit reasoning, and there isn’t any reason to think they could not judge and track reliability of informants.3\nThe issue here is not capacity, it is information. No matter how much capacity you have, you can’t make rational judgments about the reliability of memory without information about memory. And you can’t have that information without being able to use memory. That’s the key problem.\nWe can use this idea to strengthen the arguments in the previous section about Sati. If the indirect theory of memory is wrong, we have to be a bit careful about why Hugh can’t know he came by the mountain path. It can’t just be that he lacks a reason to think his memories are reliable. Rather, it must be that what he was told at the bottom of the mountain is a reason to think his memories are not reliable. It must be the presence of reasons to doubt memory, not the absence of reasons to trust, that is doing the work.\nAnd, as noted, this is a big difference between Hugh’s case and Sati’s. Sati does not have any positive reason to doubt her memory. She is several steps removed from the situation where her memories would be in doubt. It’s true that her mnemonic beliefs are insensitive to the truth in a certain way. Arguably, the nearest world in which she came to Shangri La by the sea is one where she still believes she came by the mountains. But any kind of defeasible, direct theory of memory will allow for some rational but insensitive belief.\nAssume that our theory says that S can rationally use her memory to believe that p unless defeaters D are triggered. And S uses her memory to (accurately) remember \\(\\neg D\\). That is, she remembers that she is not in a situation where those defeaters are triggered. Presumably if D were true, her memory would be unreliable; that’s what makes D a defeater. So there isn’t any reason to think that this mnemonic belief in \\(\\neg D\\) is sensitive; she may well still have had it were D true. But the direct theory implies this doesn’t matter, and the direct theory is the only theory on the table given that the indirect theory leads to implausible scepticism.\nCould it be that Sati should not trust her memory because she is, and she knows she is, in a class of people whose memories are unreliable? Well, the mere fact that she is in such a class is not interesting. She knows, after all she is a member of the class consisting of her and all people with unreliable memories, and the memories of that class are as a group unreliable. But that’s not a reason to distrust her memory. Or, at least, it can’t be on pain of scepticism. What must matter is that she is in such a class, and it is epistemologically significant. But the significant class around here seems to be the class of people whose memories have been erased, or who have reason to suspect their memories have been erased. And that doesn’t include Sati. She knows she likes Vegemite, and has for a long time, and she knows that only Marmite-likers in Shangri La had their memories erased.\nHere’s what is true of Sati. She is, right now, phenomenally indistinguishable from a possible person whose memories are unreliable. But why should that matter? We all know brain in vat cases are possible, and each of us is phenomenally indistinguishable from such an unreliable ‘person.’ But that isn’t on its own grounds for doubt about memory. All that she learns from the attendant is that another kind of brain in vat case is possible. But she knew they were possible all along. The case isn’t actual and, unless we come up with a trigger for the defeater in the theory of memory, she has no reason to think it is actual.\nArgument from A Priority\nThere is another argument against the temporally local theories, and against both the direct and indirect theories, that we can derive from the work of Tyler Burge (1993, 1997). (I should note that there is considerable dispute about how best to interpret Burge. I’m not claiming that what follows is the best interpretation, or the only interpretation, just that it is an interesting argument inspired by, and quite arguably contained in, his work.)\nTamati is doing a proof. At one stage in the proof he appeals to Fermat’s Little Theorem, which says for any natural number \\(n\\), and any prime \\(p\\), \\(n^p \\equiv n \\text{ (mod } p)\\). Using this theorem, Tamati completes his proof, and derives a nice result \\(M\\). Intuitively, Tamati has not just come to know \\(M\\), but he has come to know \\(M\\) a priori.\nBut assume, now, that either kind of temporally local theory is true. At one stage of the proof, Tamati had to, at least implicitly, reason as follows. It seems to me that I remember that \\(n^p \\equiv n \\text{ (mod } p)\\), so (perhaps with an extra premise), \\(n^p \\equiv n \\text{ (mod } p)\\). And that can’t be a priori reasoning, since the premise about how things seem to Tamati is contingent and a posteriori. If the indirect theory of memory is right, the extra premise needed about the reliability of Tamati’s memory will also be contingent and a posteriori.\nIt would be a very strange and revisionary theory of the a priori to say that any proof is not a priori if it relies on remembered theorems without, perhaps, memory of the proof of that theorem. The proof of Fermat’s Little Theorem isn’t difficult, but it does go through several steps. It is hard to keep the whole proof in mind at once. Even proving it, that is, requires a little memory. On the temporally local theory, it isn’t clear that it could ever be a priori knowable for any normal person. And any theorem that required using it would similarly be a posteriori.\nPerhaps it could be said that Tamati’s reasoning is a priori because it doesn’t rely on sense perception, only on perception of how things seem to Tamati. But some such perception of how things seem yields a posteriori knowledge. If Tamati has a headache, and notices this at the same time he remembers Fermat’s Little Theorem, he gets a posteriori knowledge of the contingent truth that he has a headache, and a priori knowledge of the necessary truth of the theorem.\nIn short, a transmissive theory of memory is required to get the result that Tamati gets a priori knowledge of the mathematical theorem. As Burge argues, a transmissive theory of testimony gets the exciting result that when Tamati goes on to tell his friend about \\(M\\), the friend gets a priori knowledge of \\(M\\) as well. If one thinks it is intuitive that the friend’s knowledge is a priori, that’s a good reason to favour a transmissive view of testimony. But that the friend’s knowledge is a priori is not as intuitively obvious as that Tamati’s knowledge is a priori, so it isn’t obvious we must treat memory and testimony the same way here.\nI’ll end this section with a note about the dialectic. What would the argument of the paper lose if the arguments of this section didn’t work? This is an important question because of arguments, such as those by Daniele Sgaravatti (2012Ch. 3), that the a priori/a posteriori distinction can’t do the epistemological work that it is traditionally taken to do. The answer is that we’d lose one of the best arguments against the direct version of the temporally local theory, while the argument against the indirect version would not be significantly affected.\nAssume for now that one is happy with Tamati’s knowledge, and indeed all non-trivial mathematical knowledge, being a posteriori in this way, because it relies on mnemonic knowledge about one’s earlier self. There is still the question of how one gets from this knowledge about one’s earlier self to knowledge of mathematics. On this indirect theory, this goes via reasoning about the reliability of one’s earlier self. But that reasoning will have to use some non-trivial mathematics, and we’ll be back in the kind of circle we warned about in the previous section. On the direct theory, this won’t be a problem, since there isn’t any challenge in getting from I have an apparent memory that p to p. That inference is perfectly sound, as long as one lacks reasons to distrust it. It is still, I think, puzzling that we have to analyse mathematicians as reasoning this way, and generating a posteriori knowledge. But they key dialectical point is that sense of puzzlement is only relevant to thinking about the direct version of the temporally local theory; the indirect version is beset by a host of further and more serious problems.\nArgument from Laundering\nThe arguments involving Sati and Tamati were designed to show that not all rational mnemonic belief relies on inference from the existence of a current mnemonic seeming. But neither argument suggested that there was anything wrong with such inferences. It is fully compatible with what I said about both Sati and Tamati that they could also try to infer from how things seem to them to facts about how they got to Shangri La, or about modular arithmetic.\nBarnett’s argument for a temporally extended view takes the opposite tack. He thinks there is something problematic about these inferences, or at least a special class of them. And because of this, he infers that the inference from present seeming can’t be explanatorily important. And that gets him to a version of a temporally extended theory.\nSo what’s the problem? Here’s the schematic case that he focusses on.\n\nTwo Beliefs\nOn Monday you came to believe that \\(p\\) for good reasons that justified your belief, and on Tuesday you came to believe that \\(q\\) for bad reasons that failed to justify it (where \\(p\\) and \\(q\\) are independent). It is now Wednesday, and you have forgotten nothing, reconsidered nothing, and learned no new relevant evidence. You recall each conclusion without occurrently recalling your original reasons for those conclusions.  (Barnett 2015, 15)\n\n\nAgain, it’s a bit of an annoyance to use ‘you,’ especially since you, dear reader, would not do anything so foolhardy as come to believe \\(q\\). So let’s assume Barnett’s narration is directed at Kim. And the question is, is Kim’s belief that \\(q\\), on Wednesday, rational? Assume, to make the case most interesting, that this mistaken inference to \\(q\\) is completely out of character. Kim is, and knows he is, a very reliable processor of information, who rarely makes this kind of mistake.\nThe worry is that any temporally local theory will say that Kim’s belief on Wednesday is rational. After all, Kim has an apparent belief that \\(q\\), and not only lacks evidence of his unreliability, but knows he is reliable. Great! But, intuitively, his belief doesn’t go from being irrational to being rational just by the passage of time. It can’t get its irrationality laundered out in this way.\nBut it isn’t clear how big a problem this really should be. Note that Kim is supposed to have forgotten nothing. So the evidence on which \\(q\\) was based is still there. Now allow the temporally local theory a principle that they should want on independent grounds. That principle is that evidence screens judgment; the evidential force of the fact that an agent made a judgment is completely screened, for that agent, by the evidence the judgment was based on.4 I just stated that principle sychronically, so it doesn’t immediately have implications for Kim’s case. But it is plausible to say that as long as the judgment remains, its evidential force is screened off by the evidence it was based on.\nNow whether one has a direct or indirect theory, Kim is not obviously compelled to hold on to her belief that \\(q\\). And whether or not one believes the screening principle, the fact that Kim has forgotten nothing means that there is no symmetry between the cases of \\(p\\) and \\(q\\). The relevant evidence is different in the two cases. The only theorist who has a challenge here is one who thinks that only occurrent states are evidence, and that is a particularly implausible addition to the indirect theory.\nBarnett’s case is different in a couple of respects than an example Gilbert Harman (1986Ch. 4) uses to draw rather different conclusions. Working through the differences between them allows us to see something interesting about rational dilemmas, even if it isn’t immediately relevant to the debates about memory.\nIn Harman’s example, Karen first draws a conclusion \\(q\\). This is actually rational for her to draw given her evidence, but her evidence was extremely misleading. She then forgets why she came to believe \\(q\\), and gets new evidence that would show her the old evidence was misleading. But since she doesn’t remember why she believed \\(q\\), she doesn’t know that this new evidence affects her grounds for belief in \\(q\\), and retains the belief.\nHarman says that this is rational. Karen isn’t required to keep track of her evidence for each thing she believes. That seems right. It is hardly a rational failing of mine to not remember precisely why I think that the White Sox won the 2005 World Series; I don’t need to keep that level of detail in mind. And if Karen does not do that, she can’t be expected to adjust her beliefs when the evidence that, unbeknownst to her, they are based on is undermined.\nHarman thinks that our original intuition about Karen’s case is that her belief in \\(q\\) is irrational once it has been undermined. But he also thinks reflection on real life cases like Karen’s shows this intuition to be mistaken. The lesson he draws from this is that something like the direct theory is right; Karen can trust her memories unless she has a special reason to doubt them, even if in fact she couldn’t put together a positive argument for their reliability.\nBarnett’s case of Kim is different than Harman’s case of Karen in two respects. Kim retains his evidence; Karen loses hers. And Kim makes an irrational mistake; Karen is rationally misled by misleading evidence. Are those differences enough to think we should treat the cases differently? Or should we be worried that Karen’s case, like perhaps Kim’s, is one where intuition is not a reliable guide?\nI don’t actually have a firm view on this. The differences are significant. Harman himself thinks that the intuition in Karen’s case is driven by the mistaken assumption that Karen will track and retain her evidence. That’s not true in normal cases like Karen’s. But it is true, by stipulation, in Kim’s. So that is one big reason for treating the cases differently. Still, I do worry a little that we’re drifting into areas where intuition is unreliable.\nTo make that worry a little more concrete, consider this argument for the conclusion that Kim’s belief in \\(q\\) is actually rational.\nIt would be irrational for Kim to re-open inquiry into whether \\(q\\), given that it was settled, and no new evidence has come in.\nIt would be irrational or impossible for Kim to intentionally forget \\(q\\).\nKim cannot change his attitude to \\(q\\) without either re-opening inquiry into whether \\(q\\), or by forgetting \\(q\\).\nThere is some rational attitude towards \\(q\\) that Kim can take.\nSo, Kim is rational to retain belief in \\(q\\), since any other possible path would involve irrationality of some kind.\nPremises 2 and 3 aren’t, I think, particularly controversial, especially if ‘inquiry’ is read so broadly that any re-evaluation of \\(q\\) counts as re-opening inquiry. The issues are premises 1 and 4. Premise 4 is a no dilemmas principle. We’ll return to it later, though in this context it is notable that Barnett himself endorses it, as do many other epistemologists.  (Barnett 2015, 10)\nThe big issue is premise 1. I think it is true. It is a mistake to go around constantly reconsidering things that one has settled. Once a decision has been reached, it should be held, unless a reason comes along to reconsider it. That reason may be evidence that the decision was faulty, or reason to think the decision was badly made. But the mere passage of time is not a reason to reconsider, and nor is the fact that if inquiry were (properly) conducted, it would yield a different conclusion.\nThe picture I’m putting forward here owes a lot to Richard Holton (1999, 2009, 2014), as well as to a related idea due to Crispin Wright (2004). Holton argues that if one has an intention, rationality requires one to maintain that unless a good reason comes along to reconsider it. The fact that one would not form the intention again were one to reconsider it is not, he thinks, itself a good reason. Strikingly, he says that even in Kavka’s toxin puzzle  (Kavka 1983), the agent who intends to drink the toxin should not reconsider, because they have no reason to do so.  (Holton 2009, 162–65). And he suggests that we should think of belief along similar lines  (Holton 2014). To believe something is to commit to its truth, and we need a positive reason to give up our commitments. Wright argues that the sceptic tries to lure us into opening inquiries we can tell will not be completed. We should resist the lure. We have no reason to open the broad ranging inquiry into our own competencies that the sceptic wants us to hold, and good reason to avoid it.5\nWe can perhaps motivate the application of these ideas to cases like Kim’s by thinking of a similar case involving action.\n\nNed has been thinking about buying a new bed. He is deciding between a wood bed and a metal bed. And he just decided to get the wood bed. This is a bad mistake. He will like the metal bed much better, and this is in fact clear from the evidence available to Ned. But he’s made up his mind. The wood bed store is five miles east, the metal bed store is five miles west. And there’s Ned in his car, driving eastward. What does rationality require of Ned now?\n\nI think Ned’s in a rational dilemma. It is irrational to drive to the wood bed store and buy a wood bed. He won’t like it, and it is predictable that he won’t. What a mistake. But it is irrational to reopen inquiry. He’s made up his mind, and now he should focus on the road. He hasn’t received any new evidence about the qualities of the bed, or any reason to think he mis-evaluated the old evidence. And we can’t go around second guessing our past decisions all the time. That includes those of us (presumably all of us) who make mistakes. It is irrational to be fickle.\nSo what can Ned rationally do? The arguments of the previous paragraph suggest he’s in a rational dilemma. If you want to act rationally, you shouldn’t start where Ned is. If he keeps driving to the wood bed store, he’ll irrationally buy a sub-optimal bed. If he thinks again about the issue, he’ll be irrationally fickle. Rationality requires something that is practically impossible; changing his mind about what to buy without re-opening the issue of what to buy.\nThis is a dilemma for Ned, but it is one he could have avoided. He could have not made the mistaken decision in the first place. It may or may not be unfair if rationality makes incompatible demands on an agent without any chance to avoid them. But it isn’t unfair to think that agents who make mistakes at \\(t_1\\) are, in virtue of those mistakes, left without any good options at \\(t_2\\). Mistakes have consequences.\nIs this inconsistent with evidentialism? I said above that evidentialism says the rational status of a belief supervenes on the evidence that the agent has. Yet now I’m saying Ned is irrational to change his mind. But if he had, with the very same evidence, believed that he should get the metal bed, that would have been rational. This looks like a counterexample to evidentialism.\nHere’s why it isn’t a counterexample. What is irrational for Ned is re-opening inquiry into what bed to get. It is the activity of engaging in further consideration of the question that it is irrational. Moreover, this is irrational because the evidence that is available to make this decision does not support it. Should Ned irrationally engage in this activity, there is a uniquely rational way to finish it, which is to change his mind. But that’s not the same as saying that he should, rationally, change his mind.\nI am making a big assumption here, but one I think is true. Careful consideration involves thinking through a large amount of evidence. Decisions to engage in careful consideration must be made on the basis of flimsier amounts of evidence. After all, to bring all the evidence one has to bear on a question is to engage in careful consideration. So a decision to engage in such consideration must not use all that evidence. So evidentialism must say that the rationality of a belief, credence, decision etc must depend on, i.e., supervene on, the evidence available to the agent when they make that decision. And when deciding whether to carefully consider or reflect on the evidence, very little is available.\nThat’s why I think Ned isn’t a counterexample to evidentialism. And nor is Kim a counterexample to evidentialism, even if there is no way she can rationally lose her belief in q. For Kim too faces a dilemma, of just the same kind. So the argument I gave for the rationality of Kim’s continuing to believe q goes wrong at step 4. Kim has gotten herself into a mess, and there are no rational ways out.\nBut note that last conclusion cuts across the theories of memory we’ve considered here. The temporally extended theory has, as its distinctive claim, that some agents have a kind of evidence that the temporally local theory says no agent has. But Kim is not one of those agents. The evidence in question is evidence one gets when one acquires a piece of knowledge, and keeps that token mental state across time. And the relevant fact about Kim is that he has a belief in \\(q\\) that is well and truly not a piece of knowledge. So it doesn’t look like the case should tell the local and extended theories apart.\nWhat it does so is show that there is a new kind of argument for the possibility of rational dilemmas. People make mistakes. When they do, there might be no good way to undo the effect of the mistake. And then they’re in a dilemma. We don’t avoid that conclusion by giving people who don’t make mistakes more evidence.\nConclusion and Future Research\nI’ve argued for a transmissive, temporally extended, view of memory and the evidence it provides. When I remember that the White Sox won in 2005, it is the fact that they won which is my evidence, not my apparent memory. The role of memory is to preserve this fact in evidence, not to give me new evidence for it. Saying this invites any number of questions. I’ll end with a list of several ones that I find fascinating, but which I’m a long way from having answers to, divided up loosely into questions about metaphysics, and questions about epistemology.\nPreservation\nWhat is it for memory to preserve a belief? Sven Bernecker (2008) has written on this at length, and the issue turns out to be much more complicated than we might first suspect. I’m particularly unsure about cases like this one,\n\nAnkati.\nI haven’t had to change planes at O’Hare for over five years. That makes me happy.\n\nBojan.\nAre you sure? What about the trip to Vancouver? Or the one to Hong Kong?\n\nAnkati.\nVancouver was a direct flight. And I went to Hong Kong via New York. But, oh, you’re right, I came back via O’Hare. Sad face.\n\n\nAt the end Ankati remembers that she flew home from Hong Kong via O’Hare. Is this a belief that was stored in memory ever since it happened? If so, we have to say that Ankati had inconsistent beliefs at the start of the conversation. If not, then I think it is hard to say that the relevant evidence for the belief that saddens her is the fact that she transferred at O’Hare. In such a case, it seems to me that the temporally local theory is more plausible than in more usual cases.\nInitial Evidence and Over-Riding\nThe following two questions are related:\nWhat past states can constitute present evidence?\nWhat present states can over-ride, or defeat, past evidence?\nMy instinct is to defend an extremely restricted answer to this pair of questions. In particular, \\(S\\)’s attitude towards \\(p\\) at \\(t_1\\) can only be evidence for her at \\(t_2\\) if the following conditions obtain.\n\\(S\\) knows that \\(p\\) at \\(t_1\\).\n\\(S\\) does not receive a significant amount of evidence against \\(p\\) between \\(t_1\\) and \\(t_2\\).\n\\(S\\) does not receive (undefeated) reasons to distrust her ability to preserve information between \\(t_1\\) and \\(t_2\\).\nBut every one of these points is problematic.\nThe first point might imply some counterintuitive things about people who trust misleading evidence, such as ‘Karen’ in the earlier cited example by Gilbert Harman (1986Ch. 4). Let’s focus on an even simpler case than Harman’s. Unlike me, my doppleganger Nairb believes that the Astros beat the White Sox in the 2005 World Series. That’s because his web browser had been hacked on that crucial October morning, and it reported the wrong results. He hasn’t seen any relevant evidence since. He has forgotten why he thinks the Astros won in 2005, but has held on to the belief. Is this belief rational, and what’s his evidence for it?\nThe evidence can’t be that the Astros won in 2005; they didn’t. And it can’t be that his computer reported that; he’s forgotten that fact. Let’s say that it is his apparent memory that they won, which seems to be the only remaining option. That would mean that the rationality of his forming the belief in the first place is independent of whether his current belief that the Astros won is rational. That’s better than the alternative options, but it isn’t particularly happy either.\nThe second point leads us into the version of the dogmatism puzzle that Maria Lasonen-Aarnio (2014b) has developed. Assume that significant evidence can factor into insignificant parts. Pazu knows that \\(p\\), then gets three pieces of evidence \\(e_1, e_2\\) and \\(e_3\\) that tell against \\(p\\). The conjunction is significant evidence, the individual parts are not. But the parts come in sequentially. When \\(e_1\\) comes in, Pazu still knows \\(p\\); after all, it is insignificant evidence. So Pazu can conclude, i.e., know, that it is misleading evidence. And, intuitively, we can ignore evidence we know to be misleading. So he ignores \\(e_1\\). And for similar reasons he ignores \\(e_2\\). Then \\(e_3\\) comes in. Should he still ignore it? Presumably; it is on its own insignificant, and the only other evidence was known to be misleading, and so ignored. But it is odd that Pazu can hold onto his knowledge in \\(p\\) in the face of these three pieces of evidence, while he would have lost knowledge had they come in at once.\nFinally, we need to explain why evidence of unreliability of mnemonic processes can block mnemonic knowledge. If memory was a source of evidence, rather than a preserver of evidence, that would be an easy problem. In general, a source does not provide evidence to an agent if the agent has reason to believe that it is unreliable. The problem is how to motivate an extension of that principle to memory, which is in general not a source of evidence, but a preserver of it.\nWe could simply insist that the Shangri La case shows that the preservative role of memory can be defeated given sufficient grounds to doubt its accuracy. I think that’s right, we can insist that. But there is a puzzle still about why this should be so. And that puzzle remains work for another day, as do the other puzzles in this section.\nExternalism\nFinally, there are some tantalising possibilities for new angles into familiar epistemological debates between internalists and externalists. It is hardly news that this is possible; Goldman’s ‘problem of forgotten evidence’ is a familiar challenge to (certain) internalists  (Goldman 1999). But there might be other ways to make memory relevant to familiar debates.\nIf the temporally extended theory is true, then what is rational depends on something that is, well, extended. And if what is rational depends on something that is extended in time, we might think it is less surprising that is also depends on something that is extended in space. And that suggests the way to a kind of externalism.\nWe can do a bit better than that hand-waving metaphor though. There are versions of the New Evil Demon problem for transmissivism. If transmissivism is true anyway, that means those problems have solutions. Then we just have to find what those solutions are, and see if they generalise to solutions to the spatial version of the New Evil Demon problem. And if they do, we might have new ways to defend externalist theories of rationality, or at least new motivations for familiar ways to defend those theories.\n\n\nArntzenius, Frank. 2003. “Some Problems for Conditionalization and Reflection.” Journal of Philosophy 100 (7): 356–70. https://doi.org/10.5840/jphil2003100729.\n\n\nBarnett, David James. 2015. “Is Memory Merely Testimony from One’s Former Self?” Philosophical Review 124 (3): 353–92. https://doi.org/10.1215/00318108-2895337.\n\n\nBernecker, Sven. 2008. The Metaphysics of Memory. New York: Springer.\n\n\nBurge, Tyler. 1993. “Content Preservation.” Philosophical Review 102 (4): 457–88. https://doi.org/10.2307/2185680.\n\n\n———. 1997. “Interlocution, Perception, and Memory.” Philosophical Studies 86 (1): 21–47. https://doi.org/10.1023/A:1004261628340.\n\n\nCoady, C. A. J. 1995. Testimony: A Philosophical Study. Oxford: Clarendon Press.\n\n\nDokic, Jérôme. 2001. “Is Memory Purely Preservative?” In Time and Memory. Issues in Philosophy and Psychology, edited by Christoph Hoerl and Teresa McCormack, 213–32. Oxford: Oxford University Press.\n\n\nDummett, Michael. 1994. “Testimony and Memory.” In Knowing from Words, edited by A. Chakrabarti and B. K. Matilal, 1–23. Dordrecht: Kluwer.\n\n\nFricker, Elizabeth. 1994. “Against Gullibility.” In Knowing from Words, edited by A. Chakrabarti and B. K. Matilal, 125–61. Dordrecht: Kluwer.\n\n\nGoldman, Alvin. 1999. “Internalism Exposed.” Journal of Philosophy 96 (6): 271–93. https://doi.org/10.2307/2564679.\n\n\nGopnik, Alison, David M. Sobel, Laura E. Schulz, and Clark Glymour. 2001. “Causal Learning Mechanisms in Very Young Children: Two-, Three-, and Four-Year-Olds Infer Causal Relations from Patterns of Variation and Covariation.” Developmental Psychology 37 (5): 620–29. https://doi.org/10.1037//0012-1649.37.5.620.\n\n\nHamilton, Sue. 2001. Indian Philosophy: A Very Short Introduction. Oxford: Oxford University Press.\n\n\nHarman, Gilbert. 1986. Change in View. Cambridge, MA: Bradford.\n\n\nHarris, Paul L., and Kathleen H. Corriveau. 2011. “Young Children’s Selective Trust in Informants.” Philosophical Transactions of the Royal Society B 366: 1179–87. https://doi.org/10.1098/rstb.2010.0321.\n\n\nHolton, Richard. 1999. “Intention and Weakness of Will.” The Journal of Philosophy 96 (5): 241–62. https://doi.org/10.2307/2564667.\n\n\n———. 2009. Willing, Wanting, Waiting. Oxford: Oxford University Press.\n\n\n———. 2014. “Intention as a Model for Belief.” In Rational and Social Agency: Essays on the Philosophy of Michael Bratman, edited by Manuel Vargas and Gideon Yaffe, 12–37. Oxford: Oxford University Press.\n\n\nHorowitz, Sophie. 2014. “Epistemic Akrasia.” Noûs 48 (4): 718–44. https://doi.org/10.1111/nous.12026.\n\n\nJaswal, Vikram K., David A. McKercher, and Mieke VanderBorght. 2008. “Limitations on Reliability: Regularity Rules in the English Plural and Past Tense.” Child Development 79 (3): 750–60. https://doi.org/10.1111/j.1467-8624.2008.01155.x.\n\n\nKavka, Gregory S. 1983. “The Toxin Puzzle.” Analysis 43 (1): 33–36. https://doi.org/10.1093/analys/43.1.33.\n\n\nKoenig, Mellisa A., Fabrice Clément, and Paul L. Harris. 2004. “Trust in Testimony: Children’s Use of True and False Statements.” Psychological Science 15 (10): 694–98. https://doi.org/10.1111/j.0956-7976.2004.00742.x.\n\n\nLackey, Jennifer. 2005. “Memory as a Generative Epistemic Source.” Philosophy and Phenomenological Research 70 (3): 636–58. https://doi.org/10.1111/j.1933-1592.2005.tb00418.x.\n\n\n———. 2008. Learning from Words: Testimony as a Source of Knowledge. Oxford: Oxford University Press.\n\n\nLasonen-Aarnio, Maria. 2010. “Is There a Viable Account of Well-Founded Belief.” Erkenntnis 72 (2): 205–31. https://doi.org/10.1007/s10670-009-9200-z.\n\n\n———. 2014a. “Higher-Order Evidence and the Limits of Defeat.” Philosophy and Phenomenological Research 88 (2): 314–45. https://doi.org/10.1111/phpr.12090.\n\n\n———. 2014b. “The Dogmatism Puzzle.” Australasian Journal of Philosophy 92 (3): 417–32. https://doi.org/10.1080/00048402.2013.834949.\n\n\nLewis, David. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nMalmgren, Anna-Sara. 2006. “Is There a Priori Knowledge by Testimony?” Philosophical Review 115 (2): 199–241. https://doi.org/10.1215/00318108-115-2-199.\n\n\nMoss, Sarah. 2012. “Updating as Communication.” Philosophy and Phenomenological Research 85 (2): 225–48. https://doi.org/10.1111/j.1933-1592.2011.00572.x.\n\n\nPryor, James. 2004. “What’s Wrong with Moore’s Argument?” Philosophical Issues 14 (1): 349–78. https://doi.org/10.1111/j.1533-6077.2004.00034.x.\n\n\nSaffran, Jenny R., Richard N. Aslin, and Elissa L. Newport. 1996. “Statistical Learning by 8-Month-Old Infants.” Science 274 (5294): 1926–28. https://doi.org/10.1126/science.274.5294.1926.\n\n\nSaffran, Jenny R., Elissa L. Newport, and Richard N. Aslin. 1996. “Word Segmentation: The Role of Distributional Cues.” Journal of Memory and Language 35 (4): 606–21. https://doi.org/10.1006/jmla.1996.0032.\n\n\nSchmitt, Frederick F. 2006. “Testimonial Justification and Transindividual Reasons.” In The Epistemology of Testimony, edited by Jennifer Lackey and Ernest Sosa, 193–224. Oxford University Press.\n\n\nSgaravatti, Daniele. 2012. “Down to Earth Philosophy: An Anti-Exceptionalist Essay on Thought Experiments and Philosophical Methodology.” PhD thesis, University of St Andrews.\n\n\nSteup, Matthias. 2013. “Is Epistemic Circularity Bad?” Res Philosophica 90 (2): 215–35. https://doi.org/http:// dx.doi.org/ 10.11612/ resphil.2013.90.2.8.\n\n\nTetlock, Philip E., and Dan Gardner. 2015. Superforecasting: The Art and Science of Prediction. New York: Crown.\n\n\nTitelbaum, Michael. 2014. Quitting Certainties: A Bayesian Framework for Modeling Degrees of Belief. Oxford: Oxford.\n\n\nWeatherson, Brian. 2012. “Knowledge, Bets and Interests.” In Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.\n\n\nWhite, Roger. 2005. “Explanation as a Guide to Induction.” Philosophers’ Imprint 5 (2): 1–29. http://hdl.handle.net/2027/spo.3521354.0005.002.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nWright, Crispin. 2002. “(Anti-)Sceptics Simple and Subtle: G.e. Moore and John McDowell.” Philosophy and Phenomenological Research 65 (2): 330–48. https://doi.org/10.1111/j.1933-1592.2002.tb00205.x.\n\n\n———. 2004. “Warrant for Nothing (and Foundations for Free)?” Proceedings of the Aristotelian Society, Supplementary Volume 78 (1): 167–212. https://doi.org/10.1111/j.0309-7013.2004.00121.x.\n\n\nAs with the transmissive view on testimony, I don’t take it to be essential to the transmissive view that all mnemonic knowledge is transmitted. Perhaps, as Lackey (2005) argues, memory can sometimes generate new knowledge. Even so, as long as it sometimes plays a purely preservative role, the transmissive theory is true.↩︎\nMichael Dummett (1994) also defends a transmissive account of memory, though the analogy between testimony and memory is important in his argument. Jérôme Dokic (2001) endorses Dummett’s position on memory.↩︎\nOn children’s capacities to learn, see Saffran, Aslin, and Newport (1996; Saffran, Newport, and Aslin 1996) and Gopnik et al. (2001). For applications of this directly to the judgments of credibility, see among many others, Koenig, Clément, and Harris (2004) and Harris and Corriveau (2011). Jaswal, McKercher, and VanderBorght (2008) show that children don’t just track credibility of informants, they trade off credibility of informant against credibility of what is currently being said. In general, the lesson from the last 10 to 20 years of research is that children have more than enough capacity to perform the cognitive tasks that indirect theorists require of them.↩︎\nI haven’t actually defended this in print yet, but it is correctly attributed to me by Sophie Horowitz (2014, 25).↩︎\nSue Hamilton (2001, 78) says that this idea, that it is wrong to open an inquiry you know you can’t complete, plays a central role in the epistemology of the important Nyāya philosopher Gotama. The discussion of forecasting in Tetlock and Gardner (2015) might cast doubt on whether the kind of conservatism I’m endorsing here is empirically sound. There is a suggestion there that people who tinker with their credal states more frequently end up with more accurate credences. This is a topic that deserves revisiting as more data comes in.\n\n↩︎\n",
    "preview": "posts/2021-02-03-memory-belief-and-time/shangrila.jpg",
    "last_modified": "2021-02-05T20:45:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-humean-supervenience/",
    "title": "Humean Supervenience",
    "description": "Humean supervenience is the conjunction of three theses: Truth supervenes on being, Anti‐haecceitism, and Spatiotemporalism. The first clause is a core part of Lewis's metaphysics. The second clause is related to Lewis's counterpart theory. The third clause says there are no fundamental relations beyond the spatiotemporal, or fundamental properties of extended objects. This paper sets out why Humean Supervenience was so central to Lewis's metaphysics, and why we should care about it even if there are empirical argumens against Spatiotemporalism. The project of defending Humean Supervenience was part of a larger project of philosophical compatibilism, of showing how the folk picture of the world and the scientific picture could be made to cohere with relatively little damage to the former and none to the latter. And Lewis's contributions to that project are independent of whether the scientific picture of the world ultimately includes Spatiotemporalism.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2015-03-06",
    "categories": [
      "David Lewis",
      "metaphysics",
      "Humeanism"
    ],
    "contents": "\n\nContents\nWhat is Humean Supervenience?\nSupervenience\nWhat is Perfect Naturalness?\nHumean Supervenience and other Humean Theses\nWhy Care about Humean Supervenience\nPoints, Vectors and Lewis\n\nWhat is Humean Supervenience?\nAs with many aspects of David Lewis’s work, it is hard to provide a better summary of his views than he provided himself. So the following introduction to what the Humean Supervenience view is will follow the opening pages of Lewis (1994a) extremely closely. But for those readers who haven’t read that paper, here’s the nickel version.\n\nPublished in A Companion to David Lewis, edited by Barry Loewer and Jonathan Schaffer, Blackwell, 2015, 99-115.\nHumean Supervenience is the conjunction of three theses.\nTruth supervenes on being (Bigelow 1988). That is, all the facts about a world supervene on facts about which individuals instantiate which fundamental properties and relations.\nAnti-haeccaetism. All the facts about a world supervene on the distribution of qualitative properties and relations; rearranging which properties hang on which ‘hooks’ doesn’t change any facts.\nSpatio-temporalism. The only fundamental relations that are actually instantiated are spatio-temporal, and all fundamental properties are properties of points or point-sized occupants of points.\nThe first clause is a core part of Lewis’s metaphysics. It is part of what it is for some properties and relations to be fundamental that they characterize the world. Indeed, Lewis thinks something stronger, namely that the fundamental properties and relations characterize the world without redundancy (Lewis 1986a, 60). This probably isn’t true, for a reason noted in Sider (1993). Consider the relations earlier than and later than. If these are both fundamental, then there is some redundancy in the characterisation of the world in terms of fundamental properties and relations. But there is no reason to believe that one is fundamental and the other isn’t. And it is hard to see how we could give a complete characterisation of the world without either of these relations. So we’ll drop the claim that the fundamental properties relations characterise the world without redundancy, and stick to the weaker claim, namely that the fundamental properties and relations characterize the world completely.\nThe second clause is related to Lewis’s counterpart theory. Consider what it would be like for anti-haeccaetism to fail. There would have to be two worlds, with the same distribution of qualitative properties, but with different facts obtaining in each. These facts would have to be non-qualitative facts, presumably facts about which individual plays which role. So perhaps, to use a well-known example, there could be a world in which everything is qualitatively as it is in this world, but in which Barack Obama plays the Julius Caeser role, and vice versa. So Obama conquers Gaul and crosses the Rubicon, Caeser is born in Hawai’i and becomes President of the United States. But what could make it the case that the Gaul-conqueror in that world is really Obama’s counterpart, and not Caeser’s? Nothing qualitative, and nothing else it seems is available. So this pseudo-possibility is not really a possibility. And so on for all other counterexamples to anti-haeccaetism.\nThe third clause is the most striking. It says there are no fundamental relations beyond the spatio-temporal, or fundamental properties of extended objects. If we assume that ‘properties’ of objects with parts are really relations between the parts, and anything extended has proper parts, then the second clause reduces to the first. I think it isn’t unfair to read Lewis as holding both those theses.\nSince for Lewis the fundamental qualities are all intrinsic, the upshot is that the world is characterized by a spatio-temporal distribution of intrinsic qualities. As Lewis acknowledged, this was considerably more plausible given older views about the nature of physics than it is now. We’ll return to this point at great length below. But for now the key point to see the kind of picture Humean Supervenience offers. The world is like a giant video monitor. The facts about a monitor’s appearance supervene, plausibly, on intrinsic qualities of the pixels, plus facts about the spatial arrangement of the pixels. The world is 4-dimensional, not 2-dimensional like the monitor, but the underlying picture is the same.\nSupervenience\nGiven the name Humean Supervenience  you might expect it to be possible to state Humean Supervenience as a supervenience thesis. But this turns out to be hard to do. Here is one attempt at stating Humean Supervenience as a supervenience thesis that is happily clear, and unhappily false.\nStrong Modal Humean Supervenience\nFor any two worlds where the spatio-temporal distribution of fundamental qualities is the same, the contingent facts are the same.\n\nBut Humean Supervenience does not make a claim this strong. It is consistent with Humean Supervenience that there could be fundamental non-spatio-temporal relations. The only thing Humean Supervenience claims is that no such relations are instantiated. In a pair of possible worlds where there are such relations, and the relations vary but the arrangement of qualities is the same, Strong Modal Humean Supervenience will fail. In the Introduction to Lewis (1986b), he suggested the following weaker version.\nLocal Modal Humean Supervenience\nFor any two worlds at which no alien properties or relations are instantiated, if the spatio-temporal distribution of fundamental qualities is the same at each world, the contingent facts are also the same.\n\nAn alien property(/relation) is a fundamental property(/relation) that is not actually instantiated. So this version of Humean Supervenience says that to get a difference between two worlds, you have to either have a change in the spatio-temporal arrangement of qualities, or the instantiation of actually uninstantiated fundamental properties or relations.\nBut Lewis eventually decided that wouldn’t do either. In response to Haslanger (1994), he conceded that enduring objects would generate counterexamples to Local Modal Humean Supervenience even if there were no alien properties or relations. So he fell back to the following, somewhat vaguely stated, thesis. (See Lewis (1994a) for the concession, and Hall (2010) for an argument that he should not have conceded this to Haslanger.)\nFamiliar Modal Humean Supervenience\nIn any two “worlds like ours,” if the spatio-temporal distribution of fundamental qualities is the same at each world, the contingent facts are also the same (Lewis 1994a, 475).\n\nWhat’s a \"world like ours’? It isn’t, I fear, entirely clear. But this doesn’t matter for the precise statement of Humean Supervenience. The three theses in section 1 are clear enough, and state what Humean Supervenience is. The only difficulty is in stating it as a supervenience thesis.\nWhat is Perfect Naturalness?\nThat definintion does, however, require that we understand what it is for some properties and relations to be fundamental, or, as Lewis put it following his discussion in Lewis (1983), perfectly natural. The perfectly natural properties and relations play a number of interconnected roles in Lewis’s metaphysics and his broader philosophy.\nMost generally, they characterise the difference between real change and ‘Cambridge change,’ and the related difference between real similarity, and mere sharing of grue-like attributes. This somewhat loose idea is turned, in Plurality, into a definition of duplication.\n\n…two things are duplicates iff (1) they have exactly the same perfectly natural properties, and (2) their parts can be put into correspondence in such a way that corresponding parts have exactly the same perfectly natural properties, and stand in the same perfectly natural relations. (Lewis 1986a, 61)\n\nThe intrinsic properties are then defined as those that are shared between any two (possible) duplicates. So, as noted above, Humean Supervenience says that the spatio-temporal distribution of intrinsic features of points characterises worlds like ours.\nI’ve gone back and forth between describing these properties as fundamental and describing them as perfectly natural. And that’s because for Lewis, the perfectly natural properties are in a key sense fundamental. For reasons to do with the nature of vectorial properties, I think this is probably wrong (Weatherson 2006). That is, we need to hold that some derivative properties are perfectly natural in order to get the definition of intrinsicness terms of perfect naturalness to work. But for Lewis, the perfectly natural properties and relations are all fundamental.\nPart of what Lewis means by saying that some properties are fundamental is that all the facts about the world supervene on the distribution. (This is Bigelow’s thesis that truth supervenes on being.) But I think he also means something stronger. The non-fundamental facts don’t merely supervene on the fundamental facts; those non-fundamental facts are true because the fundamental facts are true, and in virtue of the truth of the fundamental facts.\nThe perfectly natural properties play many other roles in Lewis’s philosophy besides these two. They play a key role in the theory of laws, for instance. They are a key part of Lewis’s solution to the New Riddle of Induction (Goodman 1955). And they play an important role in Lewis’s theory of content, though just exactly what that role is is a matter of some dispute. (See Sider (2001) and Weatherson (2003) for one interpretation, and Schwarz (2009) for a conflicting interpretation.)\nNow it is a pretty open question whether any one division of properties can do all these roles. One way to solve the New Riddle (arguably Lewis’s way, though this is a delicate question of interpretation) is to be a dogmatist (in the sense of Pryor (2000)) about inductive projections involving a privileged class of properties. Lewis’s discussion of the New Riddle at the end of Lewis (1983) sounds like he endorses this view, with the privileged class being the very same class as fundamentally determines the structure of the world, and makes for objective similarity and difference. But why should these classes be the same? It might make more sense to, for instance, endorse dogmatism about inductive projections of observational properties, rather than about microphysical properties.\nLewis doesn’t attempt to give a theoretically neutral definition of the perfectly natural properties. Rather, the notion of a perfectly natural property is introduced by the theoretical role it serves. But that theoretical role is very ambitious, covering many areas in metaphysics, epistemology and the theory of content. We might wonder whether claims like Humean Supervenience have any content if it turns out nothing quite plays that theoretical role. I think there is still a clear thesis we can extract, relying on the connection between intrinsicness and naturalness. It consists of the following claims:\nThere is a small class of properties and relations such that the contingent facts at any world supervene on the distribution of these properties and relations.\nEach of these properties is an intrinsic property.\nAt the actual world, the only relations among these which are instantiated are spatio-temporal, and all the contingent facts supervene not merely on the distribution of fundamental qualities and relations, but also on the distribution of fundamental qualities and relations over points and point-sized occupants of points.\nThose theses are distinctively Lewisian, they are clearly entailed by Humean Supervenience as Lewis’s conceives of them, they are opposed in one way or another by those who take themselves to reject Humean Supervenience, but they are free of any commitment to there being a single class of properties and relations that plays all the roles Lewis wants the perfectly natural properties and relations to play. So from now on, when I discuss the viability of Humean Supervenience, I’ll be discussing the viability of this package of views.\nHumean Supervenience and other Humean Theses\nLewis endorsed many views that we might broadly describe as ‘Humean.’ Of particular interest here are the following three.\nHumean Supervenience.\nNomological Reductionism. Nomological properties and relations (including lawhood, chance and causation) are not among the fundamental properties and relations.\nModal Combinatorialism. Roughly, anything can co-exist with anything else.\nWe’ve stated Modal Combinatorialism extremely roughly, and will persist with using a fairly informal version of it throughout. For an excellent study of more careful versions of it, see Nolan (1996). But those details aren’t as important to this debate. What is important for now is that all three of these theses are associated with what are known as Humean approaches to metaphysics in the contemporary literature. But how closely connected are they to each other, or for that matter to Hume.\nOne question about Humean Supervenience is just how it connects to the work of the historical Hume. This would be a little easier to answer if there was a broad scholarly consensus that Hume actually believed the kind of simple regularity thesis of causation that Lewis attributes to him at the start of Lewis (1973). But it isn’t clear that this is Hume’s view (Strawson 2000). What is true is that Hume was sceptical that we could know more about causation than that it was manifested in certain distinctive kinds of correlations. But it is a further step to say that Hume inferred that causation just consists of these distinctive kinds of correlations.\nA second question is how Humean Supervenience, which perhaps should be referred to as so-called “Humean Supervenience,” or perhaps even better as “Lewisian Supervenience,” relates to the kind of regularity theory that Lewis attributes to Hume, or to the prohibition on necessary connections between distinct existences that underlies Modal Combinatorialism. Lewis seemed to see the three theses as related. Here he is explaining how he chose to name Humean Supervenience (and recall that this isn’t backed up by any detailed exegesis of Hume).\n\nHumean Supervenience is named in honour of the great denier of necessary connections. It is the doctrine that all there is to the world is a vast mosaic of local matters of particular fact just one little thing and then another. (Lewis 1986b ix)\n\nThis is a slightly confusing passage, since it isn’t clear why a violation of Humean Supervenience would constitute a necessary connection of any kind. We will return to this point below. But it does seem to make clear that Lewis thought that Humean Supervenience and Modal Combinatorialism were connected, since Modal Combinatorialism is much more closely connected to the denial that they can be necessary connections between distinct existences.\nCompare how Lewis introduces Humean Supervenience when discussing the role of possible worlds in formulating trans-world supervenience theses in Plurality.\n\nAre the laws, chances, and causal relationships nothing but patterns which supervene on this point-by-point distribution of properties? Could two worlds differ in their wars without differing, somehow, somewhere, in local qualitative character? (I discuss this question of ‘Humean Supervenience,’ inconclusively, in the Introduction to my Philosophical Papers, volume II.) (Lewis 1986a, 14)\n\nThis seems to connect Humean Supervenience closely to Nomological Reductionism, since it makes the reducibility of the nomological properties and relations central to the question of whether Humean Supervenience is true. We can also, I think, see Lewis connecting Modal Combinatorialism and Nomological Reductionism in a later passage in Plurality where he discusses why he doesn’t believe that laws are necessary truths.\n\nAnother use of Modal Combinatorialism is to settle – or as opponents might say, to beg – the question whether the laws of nature are strictly necessary. They are not … Episodes of bread-eating are possible because actual; as are episodes of starvation. Juxtaposed duplicates of the two, on the grounds that anything can follow anything; here is a possible world to violate the law bread nourishes. … It is no surprise that Modal Combinatorialism prohibited strictly necessary connections between distinct existences. What I have done is to take a Humean view about laws and causation, and use it instead as a thesis about possibility. Same thesis, different emphasis. (Lewis 1986a, 91)\n\nSo for Lewis, these three theses are meant to be closely connected. And it is true that in the contemporary literature all three of them are frequently described as ‘Humean’ theses. (Or at least they are so described in metaphysics and philosophy of science; again, we’re bracketing questions of historical interpretation here.) But on second glance, it isn’t as clear what the connection between the three theses could amount to. One immediate puzzle is that Humean Supervenience is for Lewis a contingent thesis, while the other two theses are necessary truths. The accounts of causation, lawhood and chance that he gives in defending Nomological Reductionism are clearly meant to hold in all kinds of worlds, not just worlds like ours. (Consider the amount of effort that is spent in Lewis (2004a) at defending the theory of causation from examples involving wizards, action at a distance and so on.) And the formulation of Modal Combinatorialism in Plurality leaves little doubt that it is meant to be necessarily true.\nThis difference in modal status means that the theses can’t be in any way equivalent. But you might think that they are in some way reinforcing. Even that isn’t so clear. Consider the most dedicated kind of denier of Modal Combinatorialism, namely the fatalist who thinks that every truth is a necessary truth. She will endorse Humean Supervenience. After all, she thinks that all the truths about the world supervene on any category of truths whatsoever, so they’ll supervene on intrinsic properties of point-sized objects.\nIn the other direction, failures of Humean Supervenience don’t motivate compromising Modal Combinatorialism. Imagine a world where occasionally there are pairs of people who can know what each other is thinking, even though there is no independent informational chain between the two of them. It is just that a telepathic connection exists. Moreover, there is no rhyme or reason to when a pair of people will be telepathic; it is simply the case that some pairs of people are. In such a world, it is plausible that being a telepathic pair will be a fundamental relation. That’s not a problem for Humean Supervenience, since there aren’t any such pairs in this world. But it does mean Humean Supervenience is false in that world.\nAssume that Daniels and O’Leary are a telepathic pair. Any duplication of the pair of them will also be telepathic, since by Lewis’s preferred definition of duplication, duplication preserves all fundamental properties and relations. Does that mean there’s a necessary connection between Daniels and O’Leary? Not really. The spirit of Modal Combinatorialism is that you can duplicate any parts of any worlds, and combine them. One part of our world is Daniels. A duplicate of him need not include any telepathic connection to O’Leary; indeed, he has duplicates in worlds in which O’Leary is absent. Another part of the world is O’Leary; duplicates of him need not include a connection to Daniels. Putting the two together, there is a world where there are duplicates of Daniels and O’Leary, but no telepathic connection between the two. So Modal Combinatorialism suggests that even when Humean Supervenience fails, there won’t be a necessary connection between distinct objects. So Humean Supervenience really isn’t that important to the idea that there are no necessary connection between distinct existences.\nWhat’s closer to the truth, I think, is that Humean Supervenience is interesting because of Modal Combinatorialism. If Modal Combinatorialism fails, then Humean Supervenience doesn’t capture anything important. In particular, it doesn’t capture the idea that the nomic is somehow less fundamental than (some features of) the non-nomic. It is only given Modal Combinatorialism that we can make these kinds of priority claims in modal terms. Think about the philosopher who denies Modal Combinatorialism on the grounds that laws of nature are necessarily true. That philosopher will say that the laws supervene on the distribution of intrinsic properties of points, because the laws supervene on any set of facts that you like. But they will deny that this makes the distribution of intrinsic properties of points more fundamental than the laws. It is only given Modal Combinatorialism that we can claim that supervenience theses are any guide whatsoever to fundamentality.\nWhat about the connection between Nomological Reductionism and Humean Supervenience? It can’t be equivalence, since Lewis agrees that Humean Supervenience fails in worlds in which Nomological Reductionism is true. For the same reason, it can’t be that failures of Humean Supervenience entail failures of Nomological Reductionism. What about the other direction? Could we imagine Nomological Reductionism failing while Humean Supervenience holds? I think this is a coherent possibility, but not at all an attractive one. (Compare, in this respect, the discussion of theories that “qualify technically as Humean” at (Lewis 1994a, 485).) It requires that some of the irreducible, nomological properties be intrinsic properties of point-sized objects. Well, we could imagine two worlds where \\(F\\) and \\(G\\) are co-extensive, intrinsic properties of points, and in one of them it is a law that all \\(F\\)s are \\(G\\)s, and in the other it is a law that all \\(G\\)s are \\(F\\)s, and there are further intrinsic properties of all the points which are \\(F\\) and \\(G\\) which underlie these laws without making a difference to any of the other facts. So we imagine that the property being F in virtue of being G is held by all these things in one world but not in the other, and this is a fundamental perfectly natural property. I don’t think any of this is literally inconsistent, and I think filling out the details could give us a way for Nomological Reductionism to fail while the letter of Humean Supervenience holds. But it would clearly violate the spirit of Humean Supervenience  and it isn’t clear why we should believe in such ‘possibilities’ anyway.\nSo in practice, I think that any philosopher who rejects Nomological Reductionism is probably going to want to reject Humean Supervenience. And I think that Lewis saw some of the deepest challenges to Humean Supervenience as coming from threats to Nomological Reductionism. In particular, Lewis thought that the biggest challenges to Humean Supervenience came from the difficulties in providing a reductive account of chance, and the appeal of non-reductive series of causation.\nThe difficulties in providing a reductive account of chance are discussed at length in the introduction to Lewis (1986b), and in the only paper that has ‘Humean Supervenience’ in its title, i.e., Lewis (1994a). Here is a quick version of the problem. Chances are not fundamental, so they must supervene on the distribution of qualities. At least in the very early stages of the universe, there aren’t enough facts about the distribution of qualities in the past and present to form a suitable subvenient base for the chances. So whether the chance of \\(p\\) is \\(x\\) or \\(y\\) will, at least some of the time, depend on how the future of the world turns out. Now let \\(p\\) the proposition that tells the full story about the future of the world. And assume that \\(p\\) is a proposition such that what its chance is depends on how that future goes. If it goes the way \\(p\\) says it will go, the chance of \\(p\\) is \\(x\\); if it goes some other way, the chance of \\(p\\) is \\(y\\). Given a Humean theory of chance, Lewis says that this is going to be possible.\nBut now there’s a problem. What Lewis calls the Principal Principle says that if we know the chance of \\(p\\) is \\(y\\), and have no further information, then our credence in \\(p\\) should be \\(y\\). But in this case, if we knew the chance of \\(p\\) was \\(y\\), we could be sure that \\(p\\) would not obtain. So our credence in \\(p\\) should be 0. Here we seem to have reached a contradiction, and it is a contradiction to Lewis for a long time feared undermined the prospect of giving a reductive account of chance. The solution he eventually settled on in Lewis (1994a) was to slightly modify the Principal Principle, with the modification being designed to make very little difference in regular cases, but avoid this contradiction.\nLewis discusses the appeal of non-reductive theories of causation in several places, most notably for our purposes Lewis (2004a) and Lewis (2004c). Much of his attention is focused on the theory developed by Peter Menzies (1996). Menzies suggests that causation is the intrinsic relation that does the best job of satisfying folk platitudes about causation. A consequence of Menzies’s view is that there is something that makes a difference to the intrinsic properties of pairs of causes and effects which doesn’t supervene on either the intrinsic properties of the two ends of the causal chain, or on the spatio-temporal relations that hold between them. This something will either be causation or will be something on which causation depends. Either way there is a problem for Humean Supervenience, since there will have to be a perfectly natural relation that is not spatio-temporal.\nLewis’s response is to raise problems for the idea that causation could be an intrinsic relation. One class of worries concerns the very idea that causation could be a relation. Lewis says that absences can be causes and effects, but absences can’t stand in any relations, so causation must not be a relation. Another class of worries concerns the idea that causation could be intrinsic. Causation by double prevention, says Lewis, doesn’t look like it could be intrinsic. But intuitively there could be causation by double prevention. Yet another class of worries concerns the idea that causation could be a natural relation, or that there could be any one thing that satisfies all the platitudes about causation. The vast array of different ways in which causes can bring about their effects in the actual world, he says, undermines this possibility.\nNote that in both cases Lewis defends Humean Supervenience simply by defending Nomological Reductionism. So I think it is fair to say that there’s a close connection between the two in Lewis’s overall theory.\nWhy Care about Humean Supervenience\nAs is well-known, some surprising results in quantum mechanics suggest that entanglement relations are somehow fundamental (Maudlin 1994). This suggests that Humean Supervenience is actually false. If that’s right, why should we care about philosophical arguments for Humean Supervenience? Lewis’s response to this challenge is somewhat disconcerting.\n\nReally, what I uphold is not so much the truth of Humean Supervenience as the tenability of it. If physics itself were to teach me that it is false, I wouldn’t grieve.\nThat might happen: maybe the lesson of Bell’s Theorem is exactly that … But I am not ready to take lessons in ontology from quantum physics as it now is. … If, after quantum theory has been cleaned up, it still teaches non-locality, I shall submit willingly to the best of authority.\nWhat I want to fight are philosophical arguments against Humean Supervenience. When philosophers claim that one or another commonplace feature of the world cannot supervene on the arrangement of qualities, I make it my business to resist. Being a commonsensical fellow (except where unactualised possible worlds are concerned) I will seldom deny that the features in question exist. I grant their existence, and do my best to show how they can, after all, supervene on the arrangement of qualities. (Lewis 1986b xi)\n\nWe can, I think, dismiss the point about quantum physics as it was in 1986. The theory has been cleaned up in just the way Lewis wanted, and the claims about non-locality remain. Indeed, by the end of his life Lewis was willing to take lessons in ontology from quantum physics. See, for example, Lewis (2004b). So what is at issue here is whether or not there are philosophical arguments against Humean Supervenience.\nBut at this point we might wonder why we should care. If a theory is false, what does it matter whether its falsehood is shown by philosophy or by physics? We might compare the dismissive attitude Lewis takes towards Plantinga’s attempts to show that reconstructions of the problem of evil as an argument do not rely solely on things provable in first-order logic (Lewis 1993).\nThe answer I offered in Weatherson (2009) was that the philosophical defence of Humean Supervenience was connected to the point of the last paragraph quoted above. Lewis wanted to save various features of our commonsensical picture of the world. And he wanted to do this without saying that philosophical reflection showed us that the picture of the world given to us by signs of somehow incomplete. He wanted to defend what I called ‘compatibilism,’ something that I contrasted with eliminativism and expansionism. The eliminativists want to say that science shows us that some commonsensical feature of reality doesn’t really exist. (See, for example, Churchland (1981) for eliminativism about folk psychological states.) The expansionists want to say that since science (or at least physics) doesn’t recognise certain features of reality, but they obviously exist, we need to posit that science (or at least physics) is incomplete. There are many stripes of philosophical expansionists, from theists to dualists to believers in agent causation.\nLewis wasn’t averse in principle to either eliminativism or expansionism. One could, depending on exactly how one interpreted folk theory and science, classify him as an eliminativist about gods, and an expansionist about unactualised possible worlds. But his first tendency was always to support compatibilism. Compatibilists face what Frank Jackson (1998) called the ‘location problem.’ They have to show where the commonsensical features are located in the scientific picture. That is, they have to show how to reduce (in at least some sense of ‘reduce’) or commonsensical concepts to scientific concepts. (Many compatibilists may bristle at the idea that they have to be reductionists; in recent decades the world has abounded with ‘non-reductive physicalists,’ who are precisely compatibilists in my sense, but who reject what they call ‘reductionism.’ But as Lewis (1994b) argued, these rejections often turn on reading too much into the notion of reduction. For that reason, Lewis would not have objected to being described as a reductionist about many everyday concepts.)\nOne way to perform such a reduction would be to wait until the best scientific theory is developed, and show where within it we find minds, meanings, morals and all the other exciting features of our ordinary worldview. But that could take a while, and philosophers could use something to do while waiting. In the meantime we could look for a recipe that should work no matter what physical theory the scientists settle on, or at least should work in a very wide range of cases. I think we can see Lewis’s defence of Humean Supervenience as providing such a recipe.\nIt is important to note here that Lewis’s defence of Humean Supervenience was largely constructive. He didn’t try to give a proof that there couldn’t be more to the world than the arrangement of local qualities. At least, he didn’t rest a huge amount of weight on such arguments. The arguments we will look at below for a functional construal of the nomological are, perhaps, hints at arguments of this type. But, in general, Lewis defended Humean Supervenience by explicitly showing where the ordinary concepts fitted in to a sparse physical picture of reality, under the assumption that physics tells us that the world consists of nothing but a spatio-temporal arrangement of intrinsic qualities.\nNow physics tells us no such thing. But it shouldn’t matter. If the recipe Lewis provides works in the case of the ‘Humean’ world, it should also work in the world physics tells us we actually live in. The reduction of laws to facts about the distribution of fundamental qualities, and the reduction of chances and counterfactual dependencies to facts about laws, and the reduction of causation to facts about chances and counterfactual dependencies, and the reduction of mind to facts about causation and the distribution of qualities, and the reduction of value to facts about minds, and so on are all independent of whether physics tells us that we have to recognise relationships like entanglement as fundamental. In other words, if we can solve the location problem for the Humean world, we can solve it for the actual world. And solving the location problem is crucial to defending compatibilism. And whether it is possible to defend compatibilism is a central concern of metaphysics.\nI quoted above a passage from 1986 in which Lewis links Humean Supervenience to compatibilism. It’s worth noting that he returns to the point in 1994.\n\nThe point of defending Humean Supervenience is not to support reactionary physics, but rather to resist philosophical arguments that there are more things in heaven and earth in physics has dreamt of. Therefore if I defend the philosophical tenability of Humean Supervenience, that defence can doubtless be adapted to whatever better supervenience thesis may emerge from better physics. (Lewis 1994a, 474)\n\nThat is, the defence of Humean Supervenience just is part of the argument against expansionism, and hence for compatibilism. That was the defence I offered in Weatherson (2009) for the interest of Lewis’s defence of Humean Supervenience, even if it were to turn out that Humean Supervenience was refuted by physics. I still think much of it is correct. In particular, I still think that Lewis wanted to defend compatibilism, and that the defence of Humean Supervenience is key to the defence of Humean Supervenience. Indeed, I think there is pretty strong textual evidence that it was a major part of Lewis’s motivation for defending Humean Supervenience. But this explanation of why the defence of Humean Supervenience is significant can’t explain why Lewis was so worried about the failures of Humean theories of chance. After all, if all we are trying to do is show that science and commonsense are compatible, we could just take chances to be one of the fundamental features of reality given to us by science. There isn’t any need, from the perspective of trying to reconcile science and common sense, to give a reductive account of chance. Yet Lewis clearly thought that giving a reductive account of chance was crucial to the defence of Humean Supervenience. As he said,\n\nThere is one big bad bug: chance. It is here, and here alone, that I fear defeat. But if I’m beaten here, then the entire campaign goes kaput. (Lewis 1986b xiv)\n\nI now think that attitude is very hard to explain if my earlier views about the significance of Humean Supervenience are entirely correct. The natural conclusion is that there is something more that the defence of Humean Supervenience is supposed to accomplish. One plausible interpretation is that what it is supposed to accomplish is a vindication of the idea that the key nomological concepts are, in a sense, descriptive. It’s easiest to say what this sense is by contrasting it with the kind of view that Lewis rejected.\nWe’re all familiar with the standard story about ‘water.’ Our ordinary usage of the term latches onto some stuff in the physical world. That stuff is H\\(_2\\)O. Some people think that’s because our ordinary usage determines a property which H\\(_2\\)O satisfies, others because we demonstratively pick out H\\(_2\\)O in ordinary demonstrations of what it is we’re talking about when we use the term ‘water.’ Either way, we get to be talking about H\\(_2\\)O when we use the word ‘water,’ even if we are so ignorant of chemistry that we can’t tell hydrogen and oxygen apart. Moreover, our term continues to pick out ‘water’ even in worlds that are completely free of hydrogen and oxygen, and even if such worlds have other stuff that plays a very similar functional role to the role water plays in the actual world.\nLewis was somewhat sceptical of this standard story about ‘water’ (Lewis 2002). He thought that the ordinary term was ambiguous between our usage on which it picked out H\\(_2\\)O, and usage on which it picked out a role, a role that happens to be played by H\\(_2\\)O in the actual world but which could be played by other substances in other worlds. But if he thought the standard story about ‘water’ was at best, part right, he thought applying a similar story to ‘law,’ ‘cause’ and ‘chance’ was wildly implausible.\nIf such a story were right, then we would expect to find worlds where there was some relation other than causation which played the causal role. Since the actual world is physical, any world in which nonphysical things stand in the kind of relations that causes and effects typically stand in should do. So, for instance, if we have a world where the castings of spells are frequently followed by transformations from human to toad form, we should have a world where spells don’t cause such transformations but rather the spellcasting and the transformation stand in a kind of fool’s cause relationship. But we see no such thing. In such magical worlds, spells cause transformations.\nSo whatever causation is, it doesn’t look to be the kind of thing whose essence can be discovered by physics. Physics couldn’t tell us anything about the essence of the relationship between the spell and the transformation into a toad. But, we think, physics can tell us a lot about the fundamental properties and relations are instantiated in the actual world. So causation must not be one of them.\nLewis has a number of other arguments against anti-descriptivist views about individual nomological concepts. These arguments strike me as rather strong in the case of lawhood and causation, and less strong in the case of chance.\nIf being F and being F in virtue of a law are both fundamental properties, then a plausible principle of modal recombination would suggest they could come apart. But they cannot; or at least they cannot in one direction. We want being F in virtue of a law to entail being F. That’s easy if lawhood is defined in terms of fundamental properties of things; but it’s hard to see how it could be if lawhood itself is fundamental (Lewis 1986b xii).\nA similar argument goes for causation. Assume that causation is a fundamental intrinsic relation that holds between things at different times. Consider, for instance, the causal relationship which holds between a throw of a rock (call it \\(t\\)) and the shattering of the window (call it \\(s\\)). As we noted above in the case of Daniels and O’Leary, several applications of Modal Combinatorialism suggest that there will be a world just like this one in which \\(t\\) is followed by \\(s\\), but in which \\(t\\) does not cause \\(s\\). But such a world seems to be impossible. As we also noted above, such a view runs into trouble with causation by double prevention, which does not look to be intrinsic.\nThe last two paragraphs have been extremely quick arguments, but in both cases it seems to me that they can be tightened up so as to provide good arguments for some kind of descriptivist stance towards laws and causation. Chance is another matter.\nThe first problem is that recombination arguments if anything point away from descriptivism about chance. Any such account will imply that chances can’t, in general, point too far away from frequencies. But recombination arguments suggest that chances and frequencies can come arbitrarily far apart. Consider some particular event type \\(e\\) that has a one-half chance of occurring in circumstances \\(c\\). Start with a world where \\(c\\) occurs frequently, and about half the time it is followed by \\(e\\). Now use recombination to generate a world where all the \\(c \\wedge \\neg e\\) events are deleted, so \\(c\\) is always followed by \\(e\\). Unless we add a lot of bells and whistles to our theory of chance, it will no longer be the case that the chance of \\(e\\) given \\(c\\) is one-half. That is odd; we can’t simply take the first circumstance where \\(c\\) occurred and at that moment there was a one-half chance of it being followed by \\(e\\), and patch it into an arbitrary world. Bigelow, Collins, and Pargetter (1993) turn this idea into a more careful argument against descriptivism about chance. They say that chances should satisfy the following principle. (In this principle, \\(Ch\\) is the chance function, and various subscripts relativise it to times and worlds.)\n\nSuppose \\(x > 0\\) and \\(Ch_{tw}(A) = x\\). Then \\(A\\) is true in at least one of those worlds \\(w^{\\prime}\\) that matches \\(w\\) up to time \\(t\\) and for which \\(Ch_t(A) = x\\). (Bigelow, Collins, and Pargetter 1993, 459)\n\nThat is, if the chance of \\(A\\) at \\(t\\) is \\(x\\), and \\(x > 0\\), then \\(A\\) could occur without changing the history prior to \\(t\\), and without changing the chance of \\(A\\) at \\(t\\). This seems like a plausible principle of chance, but it entails the not-so-Humean view that chances at \\(t\\) supervene on history to \\(t\\), not on the full state of the world.\nNow as it turns out Lewis doesn’t rest on recombination arguments against rival views of chance, and in my view he is wise to do so. Instead he rests on epistemological arguments. He takes the following two things to be data points.\nSomething like the Principal Principle is true. The original Principal Principle said that if you knew the chance of \\(p\\) at \\(t\\) was \\(x\\), and didn’t have any ‘inadmissible’ information (roughly, information about how the world developed after \\(t\\)), then your credence in \\(p\\) should be \\(x\\). Lewis tinkered with this slightly, as we noted above, but he took it to be a requirement on a theory of chance that the Principal Principle turn out at least roughly right.\nThe correct theory of chance will explain the Principal Principle.\nLewis frequently wielded this second requirement against rival theories of chance. Here’s one example.\n\nI can see, dimly, how it might be rational to conform my credences about outcomes to my credencs about history, symmetries and frequencies. I haven’t the faintest notion how it might be rational to conform my credences about outcomes to my credences about some mysterious unHumean magnitude. Don’t try to take away the mystery my saying that this unHumean magnitude is none other than chance! (Lewis 1986b xv)\n\nBut this also seems like a weak argument. For one thing, chances are actually correlated very well with frequencies, and this correlation does not look at all accidental. It seems very plausible to me that we should line up our credences with things that are actually correlated well with frequencies. But, you might protest, shouldn’t we have an explanation of why the Principal Principle is an a priori principle of rationality? I think that before we ask for such an explanation, we should check how confident we are that the Principal Principle, or anything else, is part of an a priori theory of rationality. I’m not so confident that we’ll be able to do this (Weatherson 2005, 2007).\nThere are other replies too that we might make. It seems plausible that we should minimise the expected inaccuracy of our credences (Joyce 1998). This is true when we consider not just the subjective expected inaccuracy of our credences, but the objective expected inaccuracy of our credences. That is, when we calculate the expected inaccuracy of someone’s credences, using chances as the probabilities for generating the expectations, it is good if this expected inaccuracy is as low as possible. But, assuming that we are using a proper scoring rule for measuring the accuracy of credences, this means that we must have credences match chances.\nMore generally, I’m very sceptical of theories that insist our metaphysics be designed to have complicated epistemological theses fall out as immediate consequences. Rationality requires that we be inductivists. Why is that? Here’s a bad way to go about answering it: find a theory of persistence that makes induction obviously rational, and then require our metaphysics to conform to that theory. I don’t think you’ll get a very good theory of persistence that way, and, relatedly, you won’t get a very Lewisian theory of persistence that way. The demand that the theory of chance play a central role in an explanation of the Principal Principle strikes me as equally mistaken.\nIf what I’ve been saying so far is correct, then chance interacts with the motivation for Humean Supervenience in very different ways to how laws and causation interact. Neither of the two kinds of motivations for defending Humean Supervenience against philosophical attacks provides us with good reason to leave chances out of the subvenient base on which we say all contingent facts supervene. This is not to yet offer anything like a positive argument for chances to be part of the fundamental furniture of reality. Rather, what I’ve argued here is that a metaphysics that takes chances as primitives would not be as far removed from a recognisably Lewisian metaphysics as a metaphysics that takes laws or causes as primitive, let alone one that takes mind, meanings or morals as primitive.\nPoints, Vectors and Lewis\nThe other main point from the discussion of the previous section is that the fact that quantum mechanics raises problems for Humean Supervenience does not undercut the philosophical significance of Lewis’s defence of Humean Supervenience. But is Humean Supervenience even compatible with classical physics? Perhaps not.\n\nEven classical electromagnetism raises a question for Humean Supervenience as I stated it. Denis Robinson (1989) has asked: is a vector field an arrangement of local qualities? I said qualities were intrinsic; that means they can never differ between duplicates; and I would have said offhand that two things can be duplicates even if they point in different directions. May be this last opinion should be reconsidered, so that vector-valued magnitudes may count as intrinsic properties. What else could they be? Any attempt to reconstruct with them as relational properties seems seriously artificial. (Lewis 1994a, 474)\n\nThe opinion that the Lewis proposes to discard here seems more than an offhand judgement. It seems to follow from the very way that we introduce the notion of duplication. Here is Lewis’s own attempt to introduce the notion.\n\nWe are familiar with cases of approximate duplication, e.g., when we use copying machines. And we understand that if these machines were more perfect than they are, the copies they made would be perfect duplicates of the original. Copy and original would be alike in size and shape and chemical composition of the ink marks and the paper, alike in temperature and magnetic alignment and electrostatic charge, alike even in the exact arrangement of their electrons and quarks. Such duplicates would be exactly alike we say. They would match perfectly, they would be qualitatively identical, they would be indiscernible. (Lewis 1983, 355)\n\nIf Lewis is right that vector-valued magnitudes may count as intrinsic properties, then there is yet another condition that the perfect copying machine must satisfy. The original and the duplicate must be parallel. This isn’t the case in most actual copying machines. Usually, the original is laid flat, while the duplicate is a small angle to make it easier to collect. This is a feature, not a bug. It is not a way in which the machine falls short of perfect copying. But if vector-valued magnitudes are intrinsic qualities, and duplicates share their intrinsic qualities, it would be. So Lewis is wrong to think that these vector-valued magnitudes may be intrinsic.\nMoreover, the little argument that Lewis gives seems to rest on a category mistake. What matters here is the division of properties into intrinsic and extrinsic. But the properties on the kind of things that can be relational or non-relational. As Humberstone (1996) shows, concepts and not properties of the things that can be relational and non-relational. For instance the concept being the same shape as David Lewis actually was at noon on January 1, 1970, is a relational concept that presumably picks out an intrinsic property, namely a shape property. Whether they are valued magnitudes are intrinsic or extrinsic properties, is somewhat orthogonal question of whether it is best to pick them out by means of relational or non-relational concepts.\nThere is a further issue about the compatibility of Humean Supervenience with classical physics. This is a point that has been made well by Jeremy Butterfield (2006), and we can see the problem by looking at the different ways in which Lewis introduces Humean Supervenience.\n\nHumean Supervenience says that in a world like ours, the fundamental properties are local qualities: perfectly natural intrinsic properties of points, or of point-sized occupants of points. (Lewis 1994a, 474)\n\nLewis goes back and forth between local properties and intrinsic properties of points here. These aren’t the same thing. As Butterfield notes, ‘local’ is used in a few different ways throughout physics. One simple usage identifies local properties of a point with properties that supervene on intrinsic features of arbitrarily small regions around the point. To take an important example, the slope of a curve at a point may be a local property of the curve at that point without being intrinsic property of the point.\nThis raises a question: can we do classical physics with only intrinsic properties of points, and not even these further local properties? Butterfield argues, persuasively, that the answer is no. He notes, however, that there are some very mild weakenings of Humean Supervenience that avoid this difficulty. Here is a very simple one.\nCall Local Supervenience the following thesis. For any length \\(\\varepsilon\\) greater than 0, there is a length \\(d\\) less than \\(\\varepsilon\\) with the following feature. All the facts about the world supervene on intrinsic features of objects and regions with diameter at most \\(d\\), plus facts about the spatio-temporal arrangement of these objects and regions. This will mean that we can include all local qualities in the subvenient base, without assuming that these are intrinsic qualities of points. If the theory of intrinsicness in Weatherson (2006) is correct, we’ll also be able to include vector-valued magnitudes in the subvenient base without assuming that these are intrinsic properties of points. (On my view, they will end up being intrinsic properties of asymmetrically shaped regions.) We still won’t be able to accommodate entanglement relationships, but we will be able to capture classical physics. And, for the reasons discussed in the previous section, it would still be worthwhile to ask whether there are philosophical objections to Local Supervenience. A negative answer would greatly assist the arguments for compatibilism, and for nomological descriptivism.\nButterfield offers from theses like Local Supervenience to Lewis as friendly suggestions. But he thinks Lewis’s focus on points and their properties would have led him to reject it. I don’t want to get into the business of making counterfactual speculation about what Lewis would or would not have accepted. But I think he should have been happy to weaken Humean Supervenience to something like Local Supervenience. If the point of defending Humean Supervenience is not to defend its truth, but rather to assist in larger arguments for compatibilism, and for nomological descriptivism, then the big question to ask is whether a defence of Local Supervenience (against distinctively philosophical objections) would have served those causes just as well. And I think it’s pretty clear that it would have. Showing that we have no philosophical reason to posit fundamental non-local features of reality would be enough to let us “resist philosophical arguments that there are more things in heaven and earth in physics has dreamt of” (Lewis 1994a, 474). Lewis’s work in defending Humean Supervenience has been invaluable to those of us who want to join this resistance. It wouldn’t have been undermined if he’d allowed some local properties into the mix.\n\n\n\nBigelow, John. 1988. The Reality of Numbers: A Physicalist’s Philosophy of Mathematics. Oxford: Oxford.\n\n\nBigelow, John, John Collins, and Robert Pargetter. 1993. “The Big Bad Bug: What Are the Humean’s Chances?” The British Journal for the Philosophy of Science 44 (3): 443–62. https://doi.org/10.1093/bjps/44.3.443.\n\n\nButterfield, Jeremy. 2006. “Against Pointillisme about Mechanics.” British Journal for the Philosophy of Science 57 (4): 709–53. https://doi.org/10.1093/bjps/axl026.\n\n\nChurchland, Paul. 1981. “Eliminative Materialism and the Propositional Attitudes.” Journal of Philosophy 78 (2): 67–90. https://doi.org/10.2307/2025900.\n\n\nGoodman, Nelson. 1955. Fact, Fiction and Forecast. Cambridge: Harvard University Press.\n\n\nHall, Ned. 2010. “David Lewisś Metaphysics.” In The Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta, Fall 2010. http://plato.stanford.edu/archives/fall2010/entries/lewis-metaphysics/; Metaphysics Research Lab, Stanford University.\n\n\nHaslanger, Sally. 1994. “Humean Supervenience and Enduring Things.” Australasian Journal of Philosophy 72 (3): 339–59. https://doi.org/10.1080/00048409412346141.\n\n\nHumberstone, I. L. 1996. “Intrinsic/Extrinsic.” Synthese 108 (2): 205–67. https://doi.org/10.1007/bf00413498.\n\n\nJackson, Frank. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nJoyce, James M. 1998. “A Non-Pragmatic Vindication of Probabilism.” Philosophy of Science 65 (4): 575–603. https://doi.org/10.1086/392661.\n\n\nLewis, David. 1973. “Causation.” Journal of Philosophy 70 (17): 556–67. https://doi.org/10.2307/2025310.\n\n\n———. 1983. “New Work for a Theory of Universals.” Australasian Journal of Philosophy 61 (4): 343–77. https://doi.org/10.1080/00048408312341131.\n\n\n———. 1986a. On the Plurality of Worlds. Oxford: Blackwell Publishers.\n\n\n———. 1986b. Philosophical Papers. Vol. II. Oxford: Oxford University Press.\n\n\n———. 1993. “Evil for Freedom’s Sake?” Philosophical Papers 22 (3): 149–72. https://doi.org/10.1080/05568649309506401.\n\n\n———. 1994a. “Humean Supervenience Debugged.” Mind 103 (412): 473–90. https://doi.org/10.1093/mind/103.412.473.\n\n\n———. 1994b. “Reduction of Mind.” In A Companion to the Philosophy of Mind, edited by Samuel Guttenplan, 412–31. Oxford: Blackwell. https://doi.org/10.1017/CBO9780511625343.019.\n\n\n———. 2002. “Tharp’s Third Theorem.” Analysis 62 (2): 95–97. https://doi.org/10.1093/analys/62.2.95.\n\n\n———. 2004a. “Causation as Influence.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 75–106. Cambridge: MIT Press.\n\n\n———. 2004b. “How Many Lives Has Schrödinger’s Cat?” Australasian Journal of Philosophy 82 (1): 3–22. https://doi.org/10.1080/713659799.\n\n\n———. 2004c. “Void and Object.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 277–90. Cambridge: MIT Press.\n\n\nMaudlin, Tim. 1994. Quantum Non-Locality and Relativity: Metaphysical Intimations of Modern Physics. Oxford: Blackwell.\n\n\nMenzies, Peter. 1996. “Probabilistic Causation and the Pre-Emption Problem.” Mind 105 (417): 85–117. https://doi.org/10.1093/mind/105.417.85.\n\n\nNolan, Daniel. 1996. “Recombination Unbound.” Philosophical Studies 84 (2-3): 239–62. https://doi.org/10.1007/BF00354489.\n\n\nPryor, James. 2000. “The Sceptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\nRobinson, Denis. 1989. “Matter, Motion and Humean Supervenience.” Australasian Journal of Philosophy 67 (4): 394–409. https://doi.org/10.1080/00048408912343921.\n\n\nSchwarz, Wolfgang. 2009. David Lewis: Metaphysik Und Analyse. Paderborn: Mentis-Verlag.\n\n\nSider, Theodore. 1993. “Naturalness, Intrinsicality and Duplication.” PhD thesis, University of Massachusetts - Amherst.\n\n\n———. 2001. “Criteria of Personal Identity and the Limits of Conceptual Analysis.” Philosophical Perspectives 15: 189–209. https://doi.org/10.1111/0029-4624.35.s15.10.\n\n\nStrawson, Galen. 2000. “David Hume: Objects and Power.” In The New Hume Debate, edited by Rupert Read and Kenneth A. Richman, 31–51. London: Routledge.\n\n\nWeatherson, Brian. 2003. “What Good Are Counterexamples?” Philosophical Studies 115 (1): 1–31. https://doi.org/10.1023/A:1024961917413.\n\n\n———. 2005. “Scepticism, Rationalism and Externalism.” Oxford Studies in Epistemology 1: 311–31.\n\n\n———. 2006. “The Asymmetric Magnets Problem.” Philosophical Perspectives 20: 479–92. https://doi.org/10.1111/j.1520-8583.2006.00116.x.\n\n\n———. 2007. “The Bayesian and the Dogmatist.” Proceedings of the Aristotelian Society 107: 169–85. https://doi.org/10.1111/j.1467-9264.2007.00217.x.\n\n\n———. 2009. “David Lewis.” In Stanford Encyclopedia of Philosophy, edited by Edward N. Zalta. Metaphysics Research Lab, Stanford University.\n\n\n\n\n",
    "preview": "posts/2021-01-03-humean-supervenience/companion.jpg",
    "last_modified": "2021-02-04T15:23:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-probability-and-scepticism/",
    "title": "Probability and Scepticism",
    "description": "One way to motivate scepticism is by looking at the ways we might possibly know we aren’t brains in vats. Could we know we aren’t brains in vats a priori? Many will say no, since it is possible to be a brain in a vat. Could we know it on the basis of evidence? The chapter argues that given some commonly held assumptions, the answer is no. In particular, there is a kind of sceptical hypothesis whose probability is decreased by conditionalising on the evidence we have. Using this fact, I argue that if we want to say our knowledge that we aren’t brains in vats is a posteriori, we have to give up the view that all updating on evidence is by conditionalisation.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2014-06-10",
    "categories": [
      "epistemology",
      "scepticism"
    ],
    "contents": "\n\nContents\nThe Humean Sceptical Argument\nA Probabilistic Argument for the a posteriori Premise\nClassical and Non-Classical Credences\nThe Updating Theorem\nUpdating and Conditionalising\nLearning and Credence\nLearning and Knowing\n\nLearning and Defeaters\nLearning, Probability and Interests\nLotteries and Learning\nInterest-Relativity, Knowledge and Justification\n\nConclusions\n\nThe Humean Sceptical Argument\nThe following, broadly Humean, sceptical argument is fascinating for many reasons.1 In the argument \\(E\\) is the agent’s evidence, \\(H\\) is some hypothesis derived by ampliative reasoning from her evidence, and \\(\\supset\\) is the (classical) material conditional, i.e., \\(\\neg E \\vee H\\).2\nIt is not possible for the agent to know \\(E \\supset H\\) a priori.\nIt is not possible for the agent to know \\(E \\supset H\\) a posteriori.\nSo, it is not possible for the agent to know \\(E \\supset H\\).\nIf we add as an extra premise that if the agent does know \\(H\\), then it is possible for her to know \\(E \\supset H\\) by using \\(\\vee\\)-introduction, we get the conclusion that the agent does not really know \\(H\\). But even without that closure premise, or something like it, the conclusion seems quite dramatic.\nPublished in Scepticism and Perceptual Justification, edited by Dylan Dodd and Elia Zardini, OUP 71-86.\nPicture by aa.thompson via Creative Commons.\nOne possible response to the argument, floated by both Descartes and Hume, is to accept the conclusion and embrace scepticism. We cannot know anything that goes beyond our evidence, so we do not know very much at all. This is a remarkably sceptical conclusion, so we should resist it if at all possible.\nA more modern response, associated perhaps most strongly with Timothy Williamson’s view that our evidence just is as our knowledge, is to accept the conclusion but deny it is as sceptical as it first appears (Williamson 1998, 2000). The Humean argument, even if it works, only shows that our evidence and our knowledge are more closely linked than we might have thought. Perhaps that’s true because we have a lot of evidence, not because we have very little knowledge.\nThere’s something right about this response I think. We have more evidence than Descartes or even Hume thought we had. But I think we still need the idea of ampliative knowledge. It stretches the concept of evidence to breaking point to suggest that all of our knowledge, including knowledge about the future, is part of our evidence. So the conclusion really is unacceptable. Or, at least, I think we should try to see what an epistemology that rejects the conclusion looks like.\nI’m going to argue here that such an epistemology has to deviate in one way or another from orthodox views. In particular, I’ll argue that it has to accept deeply contingent a priori knowledge, or reject the idea that probabilistic updating should always go by conditionalisation.\nA Probabilistic Argument for the a posteriori Premise\nRejecting the conclusion would be easy if it was easy to reject the premises. But in fact there are quite strong defences of each of the premises. Let’s look at some of them.\nThe simplest argument in favour of premise 1 uses a little bit of empiricism. It could turn out to be true that \\(E \\supset H\\). What could turn out to be false can only be known a posteriori.3 So we can’t know a priori that \\(E \\supset H\\). The crucial premise there, about the limits of the a priori, is the distinctively empiricist assumption, but it is shared by a lot of contemporary philosophers.4\nThe simplest argument in favour of premise 2 uses a little bit of rationalism, though I think it takes a little more to see that it is a rationalist assumption. Here’s the argument in premise-conclusion form; we’ll go through each of the premises at some length below. So as to avoid confusion with the Humean argument, I’ve named the premises rather than numbered them.\nCredences are Classical Probabilities (CCP)\nCr is a classical probability function.\n\nUpdating Theorem (UT)\nLet \\(E = E_1 \\wedge \\dots \\wedge E_n\\), \\(\\Pr(E) > 0, \\Pr(H) < 1, \\Pr(E \\supset H) < 1\\) and for each \\(i\\), \\(\\Pr(E_i) < 1\\). And assume \\(\\Pr\\) is a classical probability function. Then \\(\\Pr(E \\supset H| E_i) < \\Pr(E \\supset H)\\).\n\nUpdating is Conditionalisation (UIC)\nIf we use Cr to measure our rational agent’s credences, and Cr\\(_Y\\) to be her credences after updating with evidence \\(Y\\), then Cr\\(_Y(X) = \\textit{Cr}(X | Y)\\) for all \\(X, Y\\).\n\nLearning Doesn’t Lower Credence (LDLC)\nIt is impossible for a rational agent to learn \\(X\\) on the basis of evidence \\(Y\\) if Cr\\(_Y(X) < \\textit{Cr}(X)\\).\n\nKnowledge Requires Learning (KRL)\nIf the agent knows \\(E \\supset H\\) a posteriori, i.e., on the basis of her empirical evidence, then there is some part of her evidence \\(E_i\\) on the basis of which she learned \\(E \\supset H\\), and before she learned it, her credence in \\(E_i\\) is less than 1.5\n\nHumean Conclusion (HC)\nSo, it is impossible to know \\(E \\supset H\\) a posteriori.6\n\nNow if someone wants to reject the Humean argument at premise 2, they better reject one of these five principles. But the principles are each reasonably strong.\nClassical and Non-Classical Credences\nThere is a huge literature on whether credence functions should be probability functions. For a good recent overview, see Hájek (2008). Most of that literature has assumed that the underlying logic we use in reasoning under uncertainty should be classical. But this assumption can be questioned too, as I did in Weatherson (2003). It turns out this matters for the argument here. Without some extensive use of classical assumptions, it doesn’t always hold that \\(\\Pr(E \\supset H| E) < \\Pr(E \\supset H)\\). (For more on this, see Jehle and Weatherson (2012).) In principle, that’s one possible way out of the argument. But I imagine it will be too costly a way out for most philosophers.\nThe Updating Theorem\nThis is a theorem, so it is harder to reject! It’s not a new theorem by any stretch; in fact it is a fairly simple result. Here’s a proof of it. The proof uses the following very familiar result from the classical probability calculus.\n\\[\\Pr(X) = \\Pr(X | Y)\\Pr(Y) + \\Pr(X | \\neg Y)\\Pr(\\neg Y)\\]\nWe’ll substitute \\(E \\supset H\\) for \\(X\\) and \\(E_i\\) for \\(Y\\), to get.\n\\[\\Pr(E \\supset H) = \\Pr(E \\supset H| E_i)\\Pr(E_i) + \\Pr(E \\supset H| \\neg E_i)\\Pr(\\neg E_i)\\]\nSince \\(\\neg E_i\\) entails \\(E \\supset H\\) it follows that \\(\\Pr(E \\supset H| \\neg E_i) = 1\\). And since \\(\\Pr(E \\supset H| E_i) \\leq 1\\), it follows that \\(\\Pr(E \\supset H| E_i) \\leq \\Pr(E \\supset H| \\neg E_i)\\). So substituting \\(\\Pr(E \\supset H| E_i)\\) for \\(\\Pr(E \\supset H| \\neg E_i)\\) on the right-hand side of that equation, and noting that we can’t make the right-hand side larger by that substitution, we get,\n\\[\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)\\Pr(E_i) + \\Pr(E \\supset H| E_i)\\Pr(\\neg E_i)\\]\nwith equality only if \\(\\Pr(E \\supset H| E_i) = \\Pr(E \\supset H| \\neg E_i) = 1\\) or \\(\\Pr(\\neg E_i) = 0\\). But we assumed that \\(\\Pr(E_i) < 1\\), so \\(\\Pr(\\neg E_i) \\neq 0\\). We’ll come back to the argument that \\(\\Pr(E \\supset H| E_i) < 1\\). Note for now that we can rewrite that inequality by factoring out \\(\\Pr(E \\supset H| E_i)\\), to get\n\\[\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)(\\Pr(E_i) + \\Pr(\\neg E_i))\\]\nBut \\(\\Pr(E_i) + \\Pr(\\neg E_i) = 1\\), is a trivial theorem of the classical probability calculus, so this just reduces to:\n\\[\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)\\]\nSince \\(\\Pr(E \\supset H) \\geq \\Pr(E \\supset H| E_i)\\), and we assumed \\(\\Pr(E \\supset H) < 1\\), it follows that \\(\\Pr(E \\supset H| E_i) < 1\\). But that means neither condition for the inequality introduced above to not be strict is satisfied. So in fact we can conclude:\n\\[\\Pr(E \\supset H) > \\Pr(E \\supset H| E_i)\\]\nas required.\nUpdating and Conditionalising\nIn Weatherson (2007), I argue that philosophers who are sympathetic to empiricism (broadly construed) should reject (UIC). That’s because (UIC) embodies a very implausible picture of the relationship between evidence and hypotheses. We can see this more clearly if we think about the non-probabilistic case first. Consider the following hypothesis.\nAfter learning \\(E\\), an agent should believe \\(H\\) iff they believed \\(E \\supset H\\) before learning \\(E\\).\nThis picture suggests that all a rational agent has to do is line up all their thoughts at the beginning of time, or I guess of inquiry, and then go around collecting evidence and applying modus ponens. Indeed, it says there is nothing else that would be rational to do. This strikes me as implausible in the extreme. There are many more rules we can use to get from evidence to conclusion than modus ponens applied to pre-known conditionals. Sometimes, it is only by getting some evidence that we are in a position to see what that evidence supports.7\nNow the rule that we should always update by conditionalisation is like the rule that we should always update by modus ponens in the way just suggested. Instead of saying that learning \\(E\\) doesn’t change which conditionals with antecedent \\(E\\) we can know to be true, it says that learning \\(E\\) doesn’t change the conditional probability of anything given \\(E\\). And it seems equally implausible for just the same reason.\nSo I don’t think (UIC) is right, and I suspect at the end of the day rejecting it is the best way to avoid the Humean sceptical argument. But I do think that there are many people who are not so sceptical (as a casual perusal of the literature on conditionalisation will show). And there may be several others who are implicitly committed to (UIC), whether or not they explicitly acknowledge that fact. So I think it is interesting to see how (UIC) can promote a certain kind of scepticism.8\nLearning and Credence\nWe will look at a lot of cases that seem to raise problems for (LDLC) below. But first I just wanted to note that the restriction to rational agents avoids one quick problem for the principle. An irrational agent might simply ignore very good evidence for \\(p\\), and then come to believe \\(p\\) on the basis of evidence that undermines that initial evidence for \\(p\\), but provides an independent (but weaker) basis for believing \\(p\\). She really could learn \\(p\\) on the second basis, even though its probability was lowered.\nThe restriction to rational agents is intended to rule out such a case. We assume that the agent has antecedently taken correct account of the available evidence. If that isn’t the case, then something which lowers the probability of \\(p\\) can ground knowledge that \\(p\\), perhaps because it reinforces evidence that \\(S\\) had, but was not properly using. What’s interesting is whether we can have violations of (LDLC) without irrationality.\nLearning and Knowing\nYou might think that the last premise, (KRL), would be the easiest one to defend. Arguably something even stronger is an analytic truth, namely that \\(S\\) learns \\(p\\) at \\(t\\) iff \\(S\\) knows \\(p\\) at \\(t\\) but not before \\(p\\). Indeed, I used to think this. But it isn’t actually true. What is plausibly true, as we’ll see by some reflections on learning, is that knowing requires either innate knowledge or learning. But the relationship between the learning and the knowing may be very complicated indeed. Let’s turn to that relationship now.\nLearning and Defeaters\nIn an earlier version of this paper, I worked with a much simpler premise, namely that coming to know required probability non-decrease. But that isn’t right.9 The problem is that the view in question doesn’t account for defeaters.\nHere’s a schematic version of the kind of case that causes problems. Assume \\(S\\) has a justified true belief that \\(p\\). Assume also that there is some defeater \\(D\\) that blocks \\(S\\)’s belief from being knowledge. Now imagine an event happens that (a) slightly lowers the evidential probability of \\(p\\) for \\(S\\), and (b) defeats the defeater \\(D\\). Then after the event, it may be that \\(S\\) knows that \\(p\\), although she does so in part in virtue of an event that lowered the probability of \\(p\\).\nThe schematic version of this argument is much more plausible than any particular case, since defeaters are often very hard to get clear judgments about. But here are three cases that may illustrate the kind of thing I have in mind.\n\nDead Dictator\nCarol is trapped in Gilbert Harman’s dead dictator story (Harman 1973, 75). At \\(t_1\\) she reads the one newspaper that correctly (and sensitively) reported that the dictator has died. She hasn’t seen the copious other reports that the dictator is alive, but the existence of those reports defeats her putative knowledge that the dictator is alive. At \\(t_2\\), all the other news sources change their tune, and acknowledge the dictator has died. Carol doesn’t see any of those newspapers; she’s too busy playing Farmville. But Carol’s memory very slowly degrades over time (as most memories do), so at \\(t_2\\) her evidence that the dictator died is slightly weaker than at \\(t_1\\). Still, over the time between \\(t_1\\) and \\(t_2\\) while she played Farmville, she came to know the dictator had died, even while the (evidential) probability of that decreased.\n\n\nFake Barns\nBob starts our story in Fake Barn Country (Goldman 1976). At \\(t_1\\), he starts looking straight at a genuine barn on a distant hill, and forms the belief that there is a barn on that hill. Since he’s in fake barn country, he doesn’t know there is a barn on the hill. At \\(t_2\\), while Bob is still looking at the one genuine barn, all the fake barns are instantly destroyed by a visiting spaceship, from a race which doesn’t put up with nonsense like fake barns. The mist from the vaporised barns slightly clouds Bob’s vision, so he doesn’t have quite as clear a view of the barn on the hill. But he still has an excellent view, so after the barns are destroyed, Bob’s belief that there is a barn on that hill is knowledge. So at \\(t_2\\) he comes to know, for the first time, that there is a barn on that hill. But the vaporisation of the fake barns, which is what lets him come to know that there is a barn on that hill, doesn’t raise the (evidential) probability that there is a barn there.10 Indeed, by making Bob’s vision a little cloudier, it lowers that probability.\n\n\nGettier Cases\nTed starts our story believing (truly, at least in the world of the story) that Bertrand Russell was the last analytic philosopher to win the Nobel Prize in literature. The next day, the 2011 Nobel Prize in literature is announced. At \\(t_1\\), a trustworthy and very reliable friend of Ted’s tells him that Fred has won the Nobel Prize in literature. Ted believes this, and since Fred is an analytic philosopher, Ted reasonably infers that, as of 2011 at least, Bertrand Russell was not the last analytic philosopher to win the Nobel Prize in literature. This conclusion is true, but not because Fred won. In fact, Ed, who is also an analytic philosopher, won the 2011 Nobel Prize in literature. At \\(t_2\\), Ted is told by a friend who is just slightly less reliable than the first friend that it is Ed, not Fred, who won the prize.11 Since Ted knows that Ed is also an analytic philosopher, this doesn’t change his belief that Bertrand Russell was not the last analytic philosopher to win the Nobel Prize in literature. But it does change that belief from a mere justified true belief into knowledge.\nAt \\(t_1\\), Ted didn’t know that Bertrand Russell was not the last analytic philosopher to win the Nobel Prize in literature, since his true belief was based on a falsehood.12 At \\(t_2\\), he did know this, on the basis of the second friend’s testimony. But since the second friend was less reliable, and since the second piece of testimony raised doubts about the first in ways that render each of them suspect, the probability of Ted’s conclusion was lower at \\(t_2\\) than \\(t_1\\). So the second piece of testimony both lowered the probability of Ted’s conclusion, and turned it into knowledge.\n\nIn every one of those cases, something happens that ever so slightly lowers the probability of \\(p\\), and also defeats a defeater of the agent’s knowledge that \\(p\\). So the agent gets knowledge that \\(p\\) in virtue of an event that lowers the probability of \\(p\\).\nBut there is, in general, something odd about events that bring about a conclusion by double prevention. There’s a big difference between being responsible for a pot of soup in virtue of preparing and cooking it, and being responsible for it in virtue of removing the banana peel that the chef would have slipped on when bringing the pot to the table. The same goes for knowledge; things that remove defeaters of knowledge are importantly different in kind to the underlying bases for knowledge.\nThe difference in question is one that we mark in language. We say that the chef cooked, or prepared, the soup. We don’t say that the banana peel remover did either of those things, although she may have caused the soup to be ready to eat. In the three cases described above, I think it’s odd to say that the agent learns that \\(p\\) in virtue of the defeater being defeated.13\nCarol can’t learn that the dictator has died while she is busy playing Farmville, and not being in any contact (of the right kind) with the outside world. So the passage of time from \\(t_1\\) to \\(t_2\\) doesn’t cause her to learn the dictator has died. If she ever learned this, she learned it at \\(t_1\\). And surely she did learn it. It wasn’t innate knowledge, and it wasn’t knowledge that was somehow implanted in her, in the way characters in the movie The Matrix can have knowledge implanted directly into their brain.14 So she learned the dictator died, and the only learning she did took place at \\(t_1\\), so she learned that the dictator died at \\(t_1\\).\nI think the same thing is true in the other cases. Bob learns that there is a barn on that hill at \\(t_1\\), but doesn’t know this until \\(t_2\\). And Ted learns that Russell is not the last analytic philosopher to win at \\(t_1\\), but doesn’t know this until \\(t_2\\). So actually cases where defeaters are defeated by probability lowerers are not counterexamples to (LDLC).\nOfficially, that completes my defence of (LDLC) from this kind of objection. But I know that not everyone agrees with my judgments about these three cases, especially the last. So I wanted to say a bit about why the overall argument is not overly affected even if I’m wrong about (LDLC).\nNote that in all three of the cases, there are two distinctive things that happen at \\(t_1\\). The agent gets a lot of evidence in favour of \\(p\\). And the agent gets some kind of defeater that prevents beliefs based on that evidence turning into knowledge. Now let’s say that the probabilistic argument that \\(E \\supset H\\) can’t be known a posteriori fails because of an analogy with these cases. That is, let’s suppose that \\(E \\supset H\\) can be known a posteriori even though all the empirical evidence lowers its probability, and the explanation for how this is possible is by analogy with cases like Dead Dictator. Then we should be able to find analogies for these two properties: something sometime raises the probability of \\(p\\), and there is a defeater that prevents \\(p\\) being known despite having a high probability.\nThe first putative point of analogy obviously fails. After all, \\(E \\supset H\\) was designed so that the agent never gets evidence that raises its probability. So we should already be suspicious of such an analogy going through. But the second putative point of analogy is actually pretty interesting. Could there be a defeater that prevents someone knowing a priori that \\(E \\supset H\\) even though the a priori probability of \\(E \\supset H\\) is very high?15\nI don’t have a conclusive argument that there is no such defeater, but it’s worth noting that most of the usual suspects don’t seem to work.\nSensitivity\nIt’s true that the a priori belief that \\(E \\supset H\\) is insensitive. That is, even if it were false, it would still be held. But the a posteriori belief that \\(E \\supset H\\) is also insensitive. So if insensitivity is a barrier to knowledge, this is a quick argument for the conclusion of the Humean sceptical argument, not a way to block a premise in an argument for premise 2.16\n\nSafety\nThe belief that \\(E \\supset H\\) is true seems to be safe. After all, any world in which it is false must be rather distant. If not, then we don’t know very much about the external world, which means we have a direct argument for the conclusion of the Humean sceptical conclusion, not a way to block a premise in an argument for premise 2.\n\nReliability\nThere are a few reliable ways in which \\(E \\supset H\\) could be believed. One is the rule, in any circumstance, believe \\(E \\supset H\\). More practically, the rule that says whenever \\(X\\) is good evidence for \\(Y\\), good enough to ground knowledge that \\(Y\\), and one doesn’t have any evidence for \\(X \\wedge \\neg Y\\), then believe \\(X \\supset Y\\) seems fairly reliable too. So there isn’t an obvious reliability argument that \\(E \\supset H\\) is not knowable a priori.\n\nFalse Belief\nIt’s possible to infer \\(E \\supset H\\) a priori from a false premise. But it isn’t necessary. The inference from the premise that \\(E\\) is good evidence for \\(H\\) to the conclusion \\(E \\supset H\\) seems reasonable, and based on true (indeed knowable) premises.\n\nIn short, the following position looks untenable to me: It’s possible to have a priori a justified true belief in \\(E \\supset H\\), but defeaters always conspire to ensure that this cannot rise to the level of knowledge. There just aren’t the defeaters around to ensure this works.\nA corollary to this is that it is impossible to learn \\(E \\supset H\\) on the basis of a probability lowerer that simultaneously defeats an a priori defeater to \\(E \\supset H\\). There just aren’t enough defeaters around for that strategy to work.\nLearning, Probability and Interests\nA slightly different kind of objection to (LDLC) comes from considerations about lottery cases. My reply, in short, is going to be that standard treatments of lottery cases are not very promising, that we should adopt a kind of interest-relative approach to lottery cases instead, and when we do that the problem goes away. But first I’ll set out the problem.17\nLotteries and Learning\nThe case we will focus on concerns testimony from a source not certain to be reliable or knowledgeable, and we need a way to model that. I’ll assume that if \\(Ra\\) is the proposition that \\(R\\) is a knowledgeable testifier, and \\(Sap\\) the proposition that \\(a\\) said that \\(p\\), then our agent’s credences satisfy the following constraints for any testifier \\(a\\).\nCr\\((p | Ra \\wedge Sap) = 1\\)\nCr\\((p | \\neg Ra \\wedge Sap) = \\textit{Cr}(p | \\neg Sap)\\)\nThat is, testimony from a knowledgeable source is maximally valuable testimony, while testimony from other sources has no evidential value. The second assumption is a little extreme18, but more moderate models will also generate the kind of example we’re interested in here.19\nThe case concerns a lottery that is based around a series of coin flips. Each lottery ticket consists of a 20-character string of H’s and T’s. A fair coin is flipped 20 times in a row. The agent wins iff the sequence of H’s and T’s on one’s ticket matches the sequence of Heads and Tails that come up as the coin is flipped. The rational agent has one ticket in this lottery, so their initial credence that they will lose the lottery is \\(1 - 2^{-20}\\). Let \\(X\\) be the proposition that they will lose the lottery.\nThe agent will get some testimony from two sources, first \\(b\\), then \\(c\\). The agent’s prior credence in \\(Rb\\) is 1. That is, she is certain that what \\(b\\) says is true. And her credence in \\(Rc\\) is 0.99, which is reasonably high. (But we’ll come back to the question of just how high it is by everyday standards.) Still, she does allow there is a non-zero probability that \\(c\\)’s vision was inaccurate, or that their memory was inaccurate, or that they are being deliberately misleading, or that any one of the myriad ways in which individual testifiers fail to be accurate infected \\(c\\)’s testimony. The agent then gets the following two pieces of evidence.\nThe agent is told by \\(b\\) that the first 19 characters on their ticket match the first 19 flips of the coin.\nThe agent is told by \\(c\\) that the last character on their ticket does not match the last flip of the coin.\nIn both cases we’ll assume that the testifiers know the truth of their assertions, though we won’t make any assumptions yet about whether the agent shares in this knowledge. After she gets the first piece of evidence, her credence in \\(X\\) drops to 0.5. After she gets the second piece of evidence, her credence in \\(X\\) rises back up to 0.995. That’s high, but notably it is less than her prior credence in \\(X\\).\nStill, we might think that the agent is now in a position to know \\(X\\), and she wasn’t before getting this evidence. She has learned that her ticket lost from a knowledgeable source. (Strictly, she has learned something that entailed this, but this doesn’t affect the overall argument.) To be sure, she has some minor reservations about the reliability of this source, but those reservations are no greater than most of us have about the testimony we get from friends and acquaintances everyday. And we typically take that testimony to produce knowledge. So it looks like, if \\(Y\\) is the combination of these two pieces of testimony, then \\(Y\\) lowers her credence in \\(X\\), as we’ll put it, it makes \\(X\\) less credible, but it also grounds knowledge of \\(X\\). That’s a counterexample to (LDLC), or so it looks.\nSomeone might object here that for many everyday pieces of knowledge, the prior credibility of our testifier is greater than 0.99. That doesn’t mean the testifier is right 99% of the time, just that on this occasion the credibility of their knowledgeability is greater than 0.99. I’m sympathetic to this line of criticism—I think we often overestimate the likelihood of error in everyday settings. But I don’t think it matters much here. For one thing, we often learn things by testimony when our credence in the reliability of the testifier is much lower than 0.99. For another, we could make the prior credence in \\(c\\)’s knowledgeability as high as \\(1 - 2^{-19}\\) without affecting the argument. (And by increasing the number of coin flips, we can make the credence even higher; arbitrarily close to 1 if need be.) And that’s a very high degree of credibility indeed. It seems to me that \\(c\\) is a lot like an ordinary testifier, and rejecting \\(c\\)’s testimony as a grounds for knowledge puts one at grave risk of embracing an overarching scepticism about testimonial knowledge. That is a sufficient reason to stay away from this kind of objection.\nThe first thing to note about this example is that what we have here is a case where there is no single piece of evidence that both lowers the credibility of \\(X\\) and grounds knowledge of \\(X\\). True, if we take \\(Y\\) to be the combination of the two pieces of evidence the agent gets, then \\(Y\\) both lowers the credibility of \\(X\\) and grounds knowledge of \\(X\\). But that’s because \\(Y\\) has two parts, and one part lowers the credibility of \\(X\\) while not grounding knowledge of it, and the other raises the credibility of \\(X\\) and grounds knowledge of it. If we restrict our attention to single pieces of evidence, says the objector, then (LDLC) is clearly true, and is untouched by this objection.\nIt isn’t at all clear that anything similar is happening in the case of \\(E\\) grounding knowledge of \\(E \\supset H\\). After all, the point of the theorem we earlier proved was that every single part of \\(E\\) lowers the probability of \\(E \\supset H\\). Now I don’t want to rest too much on a theory of how evidence divides into parts, and maybe there won’t be any way to make sense of the notion of parts of evidence in a way that is needed for the point I’m making here to work. If we are to have a theory of parts of evidence, I like a causal theory of evidence that naturally lends itself to individuating parts as being evidence that arrives via different causal chains. But I don’t think we know nearly enough about the ontology of evidence to make this kind of response compelling.\nSo if we are to defend (LDLC), and hence defend the Humean argument from attack at this point, we need to say what goes wrong with the example. I will offer a somewhat disjunctive response, with both disjuncts turning on the interest-relative account of justified belief that I defend in Weatherson (2005a) and Weatherson (2011). I’ll argue on the one hand that philosophers have been too quick to accept that we do not know we’ll lose lotteries. As David Lewis (1996) pointed out, in many contexts it seems perfectly reasonable to say that people do have such knowledge. I’ll argue that it often sounds right to say that because it’s often true. On the other hand, I’ll argue that in those settings where we do not know that the ticket will lose, \\(c\\)’s testimony does not help us gain knowledge.\nInterest-Relativity, Knowledge and Justification\nIn Weatherson (2005a) I defended an interest-relative theory of belief. This implied an interest-relative theory of justified belief, even though the theory of justification was not, fundamentally, interest-relative. Rather, that theory held that what it was to justifiably believe that \\(p\\) was to have a high enough credence to believe \\(p\\), and for that credence to be justified. What is ‘high enough?’ That, I claimed, was interest-relative. The agent’s credence in \\(p\\) is high enough for her to believe \\(p\\) if her attitudes conditional on \\(p\\) matched her unconditional attitudes on every issue that was relevant to her. In particular, I said that for her to believe \\(p\\), then for any \\(A\\) and \\(B\\) where the choice between doing \\(A\\) and \\(B\\) is a live question (in a sense I describe in much more detail in the earlier paper), and \\(U\\) is her utility function, then \\([U(A) > U(B)] \\leftrightarrow [U(A | p) > U(B | p)]\\).\nIn that paper I also noted that sometimes the theoretical interests of the agent could be relevant to what she knows, but I don’t think I went far enough down that road. Here’s what I should have said.20 The idea behind my theory was that if you believe \\(p\\), taking \\(p\\) as given in any inquiry doesn’t change the results of that inquiry. If you believe \\(p\\), you’ve already factored it in. Now one of the things that we can inquire into is the evidential probability of certain propositions. If we already believe \\(p\\), the results of those inquiries shouldn’t change when we conditionalise on \\(p\\). In particular, we should have the following two constraints on belief that \\(p\\).\nIf whether \\(q\\) is more probable than \\(x\\) is a live question, then Cr\\((q) > x \\leftrightarrow \\textit{Cr}(q | p) > x\\).\nIf the comparative probability of \\(r\\) and \\(s\\) is a live question, then Cr\\((r) > \\textit{Cr}(s) \\leftrightarrow \\textit{Cr}(r | p) > \\textit{Cr}(s | p)\\).\nThe restriction to live questions here is important. If our credence in \\(p\\) is less than 1, even marginally less than 1, then there will be some inquiries whose results are altered by conditionalising on \\(p\\). For instance, the question of whether \\(p\\)’s probability is or isn’t exactly 1 will be affected by whether we conditionalise on \\(p\\). But that doesn’t mean that belief requires probability 1. It means that not all inquiries are relevant to all agents, and in particular, the question of whether \\(p\\)’s credence is exactly 1 isn’t always relevant.\nBut consider one special case. Assume the agent is interested in exactly what the probability of \\(p\\) is. That is, for all \\(x\\), the question of whether \\(\\Pr(p) > x\\) is live for her. And assume that she judges that probability, on her evidence, to be less than 1. Assume also that she’s rational enough to know that \\(\\Pr(p | p) = 1\\). Then she can’t believe that \\(p\\), because there will be some \\(x\\) such that \\(\\Pr(p) < x\\), but \\(\\Pr(p | p) > x\\), and whether \\(\\Pr(p) > x\\) is live.\nI think that’s a quite nice result. When we’re trying to say what the relation is between credence and outright belief, it is tempting for many reasons to say that belief requires credence 1. One reason for that is that if we know the objective chance of \\(p\\), and it’s less than 1, it can feel very odd to say, without qualification, that we believe that \\(p\\). It’s much better to say that we believe \\(p\\) is probable. But it’s very implausible to say that in general belief requires credence 1, because that would mean we believe very little.\nThe interest-relative view makes sense of this conundrum. On the one hand, belief does not in general require credence 1. On the other hand, when the agent is themselves focussed on the probability of \\(p\\), they must judge that probability to be 1 to outright believe that \\(p\\). I think that’s a nice way to steer between the conflicting intuitions here.\nGiven all this, it’s probably easy to imagine what I’ll say about the challenge to (LDLC). The idea behind the challenge was two-fold. First, purely probabilistic evidence is not enough for knowledge. Second, other sources of evidence, such as testimony, can be the basis for knowledge even if we would, if pressed, say that they do not provide more support than purely probabilistic evidence. I’m going to accept the second claim (with some qualifications) but reject the first.\nI think there are circumstances where we can, with Lewis, say the following.\n\nPity poor Bill! He squanders all his spare cash on the pokies, the races, and the lottery. He will be a wage slave all his days. We know he will never be rich. (Lewis 1996: 443)\n\nHow, you might ask, can we know Bill will never be rich? The answer is that we know the odds are massively against him winning the lottery. That justifies a very high credence in his losing. For anything we care about, the odds are close enough to 1 that the difference doesn’t matter. So our high credence is belief, and since it is justified, true, and undefeated, it is knowledge.21\nBut wait, you say, isn’t there some chance of Bill winning the lottery, and hence being rich? Why yes, there is. And doesn’t that mean that we don’t know he’ll never be rich? Indeed it does. And doesn’t that mean the previous paragraph is all mistaken? No, it doesn’t. It means that asking all these questions changes the subject. In particular, it raises the question of whether the chance of Bill winning is equal to zero or greater than zero to salience. And once that question is salient, our degree of belief that Bill will lose is not close enough to 1 that the difference doesn’t matter. The difference matters a lot, to the question you just raised. So I insist that given what I cared about a paragraph ago, I was speaking truly.22\nThis explains why we think we can’t get knowledge on probabilistic grounds. Here’s what we can’t do. We can’t simultaneously try to figure out what the probability of \\(p\\) is, conclude it is less than 1, and believe \\(p\\). But that’s simply because once the question of \\(p\\)’s probability is live, we lose the belief that \\(p\\). We can, I think, investigate whether the probability of \\(p\\) is, say, over 0.9, conclude that it is, and conclude on that basis that \\(p\\). As long as there are no further questions whose answer turns on whether \\(p\\)’s probability is 1 or a little less, that could be enough for knowledge.\nThe converse is true about testimony. It’s true that we can gain knowledge from testimony. And it’s true that, if pressed, we may admit that that testimony is less than perfectly reliable. But what I deny we can do is admit the unreliability, work on figuring out just how unreliable it is, and hold onto the knowledge gained from testimony. But it’s fairly intuitive that this would be impossible. Simultaneously thinking that my only reason for believing \\(p\\) is that \\(S\\) told me that \\(p\\), and holding that \\(S\\) is somewhat unreliable, and may have been mistaken on this occasion, but nevertheless simply believing \\(p\\), is an unstable state.\nThe difference between probabilistic grounds for belief, as when we believe we’ll lose the lottery, and testimonial grounds then is not that one of them requires higher standards. It is rather that when we use explicitly probabilistic grounds, we tend to make probabilistic questions salient, and hence live.23 And the salience of those questions destroys belief, and hence destroys knowledge. If we make the same questions salient in both the probabilistic and testimonial cases, we get the same criteria for knowledge. Hence the kind of case we’ve been considering is not a threat to (LDLC). Indeed, it is hard to see what could be a threat to (LDLC), without changing the salience of probabilistic questions. So I think (LDLC) survives, and anyone who wants to resist the Humean conclusion will have to look elsewhere to find the weak link in the argument.\nHere’s a crude summary of these reflections. If questions about the precise probability of \\(H\\) or \\(E \\supset H\\) are salient, then \\(E \\supset H\\) can’t be known before or after learning \\(E\\). If they aren’t, \\(E \\supset H\\) can be known both a priori and a posteriori. The only way we get that \\(E \\supset H\\) is only knowable after learning \\(E\\) is if we equivocate between the two positions on what is salient.\nConclusions\nSo I think (LDLC) is invulnerable to these kinds of objections. Since it is intuitively a very plausible principle, and these attempts to counterexample it have failed, I think we should adopt as a working hypothesis that it is true. That means, I think, that we really have two options for responding to the Humean argument.\nAccept that \\(E \\supset H\\) is a priori knowable.\nReject (UIC), and say some updating is not by conditionalisation.\nI don’t think either of these are bad options. You can read Weatherson (2005b) as an attempt to defend the first, and Weatherson (2007) as an attempt to defend the second. But I do think these options aren’t available to everyone.\nIf \\(E \\supset H\\) is a priori knowable, then any kind of ‘modal’ account of the a priori has to fail. That is, we can’t understand a priority as any kind of metaphysical necessity, since \\(E \\wedge \\neg H\\) is clearly possible.24 It’s just that we have defeasible, fallible a priori knowledge that it isn’t true. And I noted above that (UIC) will follow from some other independently attractive views about what we can known a priori about epistemology, and when it is that conditionalising seems wrong. Many years ago, I held both (UIC) and that deeply contingent truths like \\(E \\supset H\\) could not be known a priori. I now think that’s an unstable combination of views; it leaves you without resources to turn back the Humean argument.\n\n\nArntzenius, Frank. 2003. “Some Problems for Conditionalization and Reflection.” Journal of Philosophy 100 (7): 356–70. https://doi.org/10.5840/jphil2003100729.\n\n\nBonJour, Laurence. 1997. In Defense of Pure Reason. Cambridge: Cambridge University Press.\n\n\nFantl, Jeremy, and Matthew McGrath. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nGettier, Edmund L. 1963. “Is Justified True Belief Knowledge?” Analysis 23 (6): 121–23. https://doi.org/10.2307/3326922.\n\n\nGoldman, Alvin I. 1976. “Discrimination and Perceptual Knowledge.” The Journal of Philosophy 73 (20): 771–91. https://doi.org/10.2307/2025679.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\nHájek, Alan. 2008. “Arguments for-or Against-Probabilism?” British Journal for the Philosophy of Science 59 (4): 793–819. https://doi.org/10.1093/bjps/axn045.\n\n\nJehle, David, and Brian Weatherson. 2012. “Dogmatism, Probability and Logical Uncertainty.” In New Waves in Philosophical Logic, edited by Greg Restall and Gillian Russell, 95–111. Bassingstoke: Palgrave Macmillan.\n\n\nLewis, David. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nNelkin, Dana. 2000. “The Lottery Paradox, Knowledge, and Rationality.” Philosophical Review 109 (3): 373–409. https://doi.org/10.2307/2693695.\n\n\nOkasha, Samir. 2001. “What Did Hume Really Show about Induction?” The Philosophical Quarterly 51 (204): 307–27. https://doi.org/10.1111/1467-9213.00231.\n\n\n———. 2005. “Does Hume’s Argument Against Induction Rest on a Quantifier-Shift Fallacy?” Proceedings of the Aristotelian Society 105 (1): 237–55. https://doi.org/10.1111/j.0066-7373.2004.00113.x.\n\n\nSmith, Martin. 2010. “What Else Justification Could Be.” Noûs 44 (1): 10–31. https://doi.org/10.1111/j.1468-0068.2009.00729.x.\n\n\nVogel, Jonathan. 1987. “Tracking, Closure and Inductive Knowledge.” In The Possibility of Knowledge: Nozick and His Critics, edited by Stephen Luper-Foy, 197–215. Totowa, NJ: Rowman & Littlefield.\n\n\n———. 1990. “Cartesian Skepticism and Inference to the Best Explanatio.” Journal of Philosophy 87 (11): 658–66. https://doi.org/10.5840/jphil1990871123.\n\n\nWarfield, Ted A. 2005. “Knowledge from Falsehood.” Philosophical Perspectives 19: 405–16. https://doi.org/10.1111/j.1520-8583.2005.00067.x.\n\n\nWeatherson, Brian. 2003. “From Classical to Intuitionistic Probability.” Notre Dame Journal of Formal Logic 44 (2): 111–23. https://doi.org/10.1305/ndjfl/1082637807.\n\n\n———. 2005a. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\n———. 2005b. “Scepticism, Rationalism and Externalism.” Oxford Studies in Epistemology 1: 311–31.\n\n\n———. 2006. “Questioning Contextualism.” In Epistemology Futures, edited by Stephen Cade Hetherington, 133–47. Oxford: Oxford University Press.\n\n\n———. 2007. “The Bayesian and the Dogmatist.” Proceedings of the Aristotelian Society 107: 169–85. https://doi.org/10.1111/j.1467-9264.2007.00217.x.\n\n\n———. 2011. “Knowledge, Bets and Interests.” In Forthcoming Volume on Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.\n\n\nWilliamson, Timothy. 1998. “Conditionalizing on Knowledge.” British Journal for the Philosophy of Science 49 (1): 89–121. https://doi.org/10.1093/bjps/49.1.89.\n\n\n———. 2000. “Scepticism and Evidence.” Philosophy and Phenomenological Research 60 (3): 613–28. https://doi.org/10.2307/2653819.\n\n\nYablo, Stephen. 2002. “Coulda, Woulda, Shoulda.” In Conceivability and Possibility, edited by Tamar Szabó Gendler and John Hawthorne, 441–92. Oxford: Oxford University Press.\n\n\nZardini, Elia. 2012. “Confirming the Less Likely, Discovering the Unknown Dogmatisms: Surd and Doubly Surd, Natural, Flat and Doubly Flat.” In Scepticism and Perceptual Justification, edited by Dylan Dodd and Elia Zardini, 33–70. Oxford: Oxford University Press. https://doi.org/10.1093/acprof:oso/9780199658343.001.0001.\n\n\nOn how closely this argument resembles Hume’s argument for inductive scepticism, see Okasha (2001, 2005). I’ve previously discussed the argument in Weatherson (2005b) and Weatherson (2007).↩︎\nI’m going to assume throughout that we aren’t dealing with the special case where the prior credence of \\(E\\) is 0, or of \\(H\\) is 1. That will do some work in section 2.↩︎\nI’m using ‘could turn out to be false’ in the sense described in Yablo (2002).↩︎\nWhen I say it is an ‘empiricist’ assumption, I mean that the two ways of rejecting it correspond to two things classical empiricists rejected. One is that we can reason our way, perhaps abductively, to substantive knowledge about the external world, a la Vogel (1990) or BonJour (1997) . The other is that we have substantial innate knowledge about the external world, and this is not justified by empirical evidence, but perhaps by its reliability. It’s interesting that these two forms of rejection are associated with very different views in contemporary philosophy, but they seem both anti-empiricist to me.↩︎\nWhen I say ‘part’ here, I just mean that \\(E_i\\) is one of the conjuncts of \\(E\\). This may require relabelling, if for instance the basis for \\(E \\supset H\\) consists of many conjuncts of \\(E\\) under some representation; just collect all those into a single conjunct.↩︎\nProof: Assume for reductio that we can know \\(E \\supset H\\) a posteriori. So by (KRL) there is some \\(E_i\\) that is the basis of this knowledge, such that before she learned it, her credence in it was less than 1. When she does learn \\(E_i\\), she conditionalises her credences, as required by (UIC). So updating (i.e., conditionalising) on \\(E_i\\) raised its credence to 1, but by (LDLC) did not lower the agent’s credence in \\(E \\supset H\\). As we noted in footnote 2, we’re assuming Cr\\((E) > 0\\), and by (CCP) we’re assuming (Cr) is a classical probability function, so Cr\\((E_i) > 0\\). We also assumed Cr\\((H) < 1\\). So the conditions for applying (UIT) are all satisfied, and hence her credence in \\(E \\supset H\\) goes down when she updates in \\(E_i\\). That contradicts our earlier conclusion that it does not go down, completing the reductio.↩︎\nIn (Weatherson 2007) I argue for this by considering agents with radically different kinds of evidence to ours, and noting how much we could know about what kinds of conclusions their evidence supports, and what they could know about what kinds of conclusions our evidence supports.↩︎\nOf course, I’m hardly the only person to promote doubts about (UIC). See Arntzenius (2003) for some very different kind of criticisms.↩︎\nThe essential reason it isn’t right was pointed out by Martin Smith in comments on this paper at the 2009 Arché scepticism conference. This section is basically a response to the good objections he raised to the earlier version of the paper.↩︎\nIt does raise the probability that a randomly selected barn-like structure in Bob’s vicinity is a barn, but that’s not the evidential probability for Bob of there being a barn in that hill.↩︎\nPresumably for (Gettier 1963).↩︎\nI’m not presupposing here that we can never get knowledge from false beliefs, just that the falsity of Ted’s initial belief explains why his subsequent belief is not knowledge. For more on this point, see Warfield (2005).↩︎\nA quick sample of informants suggests that this is much less odd in the Gettier case than in the other two cases. We’ll come back to this point below.↩︎\nIt’s a delicate question whether this kind of procedure is properly called learning. I’m inclined to say that it is, but I suspect a lot of people aren’t, so didn’t want to presuppose my own idiosyncratic usage here. Thanks here to Jonathan Livengood and Daniele Sgaravatti.↩︎\nWhy are we interested in whether we can prevent a priori knowledge of \\(E \\supset H\\)? Because we’re interested in ways in which \\(E \\supset H\\) can be known a posteriori, and by definition that means that it isn’t known a priori. The idea I’m floating here, which I don’t think will work, is that the first knowledge of \\(E \\supset H\\) is after the agent gets some evidence, and because she gets that evidence, although \\(E \\supset H\\) has maximal probability a priori, i.e., before she gets any evidence.↩︎\nVogel (1987) makes a similar point that sensitivity and induction don’t mix.↩︎\nI’m grateful to David Chalmers, Crispin Wright and Elia Zardini for pressing me on the need to address these cases. The cases are discussed in more detail in Smith (2010) and Zardini (2012).↩︎\nI’m interpreting \\(R\\) in such a way that \\(Ra\\) entails what \\(a\\) says is true, so \\(Ra \\wedge Sap\\) entails \\(p\\), so the first assumption is natural. Making the second assumption more realistic would just increase the complexity of the model without revealing anything insightful. Since this model is meant to raise problems for my view, I think it is fine to use an extreme case, and not complain about its extremity.↩︎\nI think this kind of model is more realistic than a model that is based around Jeffrey-conditionalising, where we have to specify in advance what the posterior probability of some salient proposition is. That’s not required here; the posterior probability of \\(p\\) is an output of the prior probabilities of \\(p\\) and \\(Ra\\), not an input to a Jeffrey-conditionalising formula.↩︎\nI go into much more detail on this in Weatherson (2011).↩︎\nSo I’m disagreeing with those such as Nelkin (2000) who think high probability can’t suffice for knowledge. But I think the comments below help explain away the motivations for such views.↩︎\nTwo technical points about how what I said relates to the broader debates about interest-relativity.\nI think that what’s going on in cases like these involves the interest-relativity of belief, not in the first-instance the interest-relativity of knowledge. Does that mean that if an agent held on to their beliefs across changes of interest, then their knowledge would not be affected by changes of interest? No; because the only way to hold on to beliefs when interests change may involve raising one’s credence so high that it would be irrational, and when credences are irrational the resulting beliefs are irrational, and irrational beliefs can’t constitute knowledge.\nMy positive view is a form a interest-relative invariantism; that is, I don’t think contextualism is true about ‘knows.’ But I haven’t relied on that here, just on the interest-relativity. If one wanted to hold a form of interest-relative contextualism, a la Fantl and McGrath (2009), this explanation would still go through. There are puzzles that might push one towards interest-relative contextualism, but I think there are larger puzzles that should push one back towards invariantism (Weatherson 2006).↩︎\nSalient to the person doing the reasoning that is. As an invariantist, I think that matters. But a contextualist who thought what’s relevant to subjects is thereby relevant could say the same thing.↩︎\nI mean both that it’s true in some possible worlds, and in some worlds considered as actual, so a ‘two-dimensional’ equation of a priority with a kind of metaphysical possible is ruled out.\n\n↩︎\n",
    "preview": "posts/2021-03-04-probability-and-scepticism/demon.jpg",
    "last_modified": "2021-03-05T14:15:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-06-centrality-and-marginalisation/",
    "title": "Centrality and Marginalisation",
    "description": "A commentary on Herman Cappelen's \"Philosophy without Intuitions\".",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2014-02-22",
    "categories": [
      "methodology",
      "book symposium",
      "on books",
      "history of analytic"
    ],
    "contents": "\n\nContents\nWelcome to the History of Late Analytic Philosophy\nCentrality and Its Discontents\nIntuitions in Detective Work\nPhilosophy: A Negative Characterisation\nLetting Go\nStrength and Fragility\nSome Lewisian Case Studies\nSummary\n\nWelcome to the History of Late Analytic Philosophy\nIt’s a good time to be doing history of late analytic philosophy. There is flurry of new and exciting work on how philosophy got from the death pangs of positivism and ordinary language philosophy to where it is today. Some may see this as a much needed gap in the literature. Indeed, there are a couple of reasons for scepticism about there being such a field as history of late analytic philosophy, both of which are plausible but wrong.\nPhilosophical Studies 171: 517-533. Thanks to Herman Cappelen and Ishani Maitra for many discussions about the material in this paper.\nOne reason is that it is too recent. But it can’t be too recent for general historical study; there are courses in history departments on September 11, so it’s not like looking at philosophy from thirty to forty years ago is rushing in where historians fear to tread. And indeed, if logical positivism could be treated historically in the 1960s, and ordinary language philosophy could be treated historically at the turn of the century, it seems a reasonable time to look back at the important works of the 1970s that established the contemporary era in philosophy.\nAnother reason is that we all know it so well. We are still so engaged with the key works by Kripke, Lewis, Burge, Perry, Thomson and so on that we don’t need to also look at them the way we look at Descartes, Locke and Hume. But this, it turns out, is not true. Books by Daniel Nolan (2005) and Wolfgang Schwarz (2009) changed the way that some philosophers, even those who knew the Lewisian corpus fairly well, changed the way they read Lewis. There has also been a minor flurry of work on how important the Gödel/Schmidt case is to the argument of Naming and Necessity (Devitt 2011; Ichikawa, Maitra, and Weatherson 2012; Machery et al. 2012).\nBut that’s nothing compared to the bombshell that is Philosophy Without Intuitions. (Cappelen 2012; all page citations, unless otherwise noted, to this book.) Herman Cappelen shows, extremely convincingly to my eyes at least, that intuitions play a much smaller role in late analytic philosophy than many philosophers thought. Indeed, there is a lot of textual evidence both for the claim that intuitions don’t do much philosophical work, and for the claim that many people have said that they do. The first of these claims is all to the good, says Cappelen, since there isn’t a particularly good epistemological defence of the use of intuitions.\nThe evidence for Cappelen’s claims comes in two parts. The first part, which I won’t discuss much here, is an extended argument that words like ‘intuitively,’ or ‘counterintuitive,’ as they appear in philosophical discourse, don’t in general function to pick out, or even draw attention to, any distinctive kind of mental state we could call an ‘intuition.’ The second part argues that when we look at the actual introduction of thought experiments into late analytic philosophy, we don’t see the appeal to intuitions that many philosophers seem to think go along with thought experiments. Rather, we see a whole host of interesting philosophical moves. Sometimes a thought experiment functions to highlight an explanandum. Sometimes it gives us a prima facie plausible thesis that we then argue for (or against) at great length. Sometimes it just raises a puzzle.\nOne upshot of this historical work, one that Cappelen I think does a good job highlighting, is that contemporary philosophy is much more interesting than its practitioners sometimes take it to be. Philosophy is a way of investigating hard questions about the world, often at great expense in terms of human capital, but with thankfully little in the way of other expenses. It isn’t a matter of tidying up conceptual space. Thinking of philosophy this way should, I think, help us see why so many different kinds of projects are philosophically important.\nCentrality and Its Discontents\nThe big goal of Cappelen’s book is to refute the view, which he dubs Centrality, that intuitions (of a certain kind) are central to analytic philosophy, and in particular that they are a primary source of evidence for analytic philosophers. The intuitions that he has in mind have these three characteristics. (The quotes are from pages 112-3, where these features are articulated.)\nF1: Phenomenology\n“An Intuitive Judgment has a distinctive phenomenology” .\n\nF2: Rock\n“An intuitive judgment has a special epistemic status …Intuitive judgments justify, but they need no justification.”\n\nF3: Conceptual\nA judgment is an intuition “only if it is justified solely by the subjects’ conceptual competence.”\n\nThere’s some more detail on F2, but we’ll get to that in section 6. And there’s a fourth characteristic of intuitions that I want to add.\nF4: Speed\nIntuitions are rapid reactions..1\n\nI’m going to spend much of this paper defending a view that intuitions characterised by F2 and F4 do play a role, though perhaps not a central role, in philosophy. But I do think that intuitions characterised by F1 and F3 are just not important to philosophy. Indeed, I think it’s a very important fact that they are not that important.\nThe claim that intuitions have a distinctive phenomenology is mostly harmless but, it seems to me, false. I certainly don’t find anything in common when I introspect my judgments that, say, no set is a member of itself, or that losing a limb would seriously reduce my happiness, or that the only language I think in is English. It will fall out of the view I’m defending that the best intuitions have no phenomenology, but I don’t think that’s a particularly important fact about them.\nBut the claim that intuitions derive solely from conceptual competencies, plus the claim that these are the central source of evidence in philosophy, is both wrong and dangerous. If that conjunction were true, we’d expect most philosophical conclusions to be conceptual truths (whatever those are). I’m not going to take a stand on whether there are conceptual truths, but I think it is pretty obvious that conceptual truths won’t help much resolve the following debates. (Compare the list E1-E6 on pages 200-201, which I’m basically just extending.)\nDo bans on pornography involve trading off speech rights versus welfare considerations, or do they just involve evaluating the free speech interests of different groups?\nIs it permissible to eat whales?\nUnder what circumstances is it permissible to end a terminally ill patient’s life, or to withhold life-saving treatment?\nIs all context dependency in language traceable to the presence of bindable variables?\nDoes belief have a phenomenology?\nWhich animals (and which non-animals) have beliefs?\nIf philosophy uses largely conceptual evidence, these aren’t philosophical questions. More generally, if Centrality (in Cappelen’s sense) is true of philosophy, then feminist philosophy, legal philosophy, political philosophy, bioethics, philosophy of language and (most of) philosophy of mind are not part of philosophy. (This list is far from exhaustive; making philosophy Centrality-friendly would involve writing out huge swathes of the discipline.)\nModus tollens obviously beckons. But as Cappelen notes (213), one occasional reaction to this is to identify certain parts of philosophy as the ‘Core’ of the discipline, and say Centrality is true of those. If Centrality is true of the core of philosophy, then feminist philosophy etc., are not part of the core of the field. Maybe now some people would be disposed to use modus ponens not modus tollens.\nThat would be a large mistake. It would have shocked Plato, and Locke, and Hume, and practically every other major figure in the history of philosophy to learn that political philosophy wasn’t central to the field. I do think (contra some of what Cappelen says) that some philosophy involves a priori and conceptual investigation. Indeed, I even do some of it. But it’s not true that when I’m doing that I’m doing work that’s deeper, or more philosophical, or more central to philosophy than the work that, for example, Rae Langton or Susan Moller Okin or Tamar Szabó Gendler or Sarah-Jane Leslie do.\nThis reason alone suffices for me to hope that Cappelen’s book has a very wide readership. Centrality isn’t true, but it is I think widely believed to true of at least some parts of the field. (Cappelen quotes many people endorsing this view.) I suspect that on the basis of this mistake, the parts of philosophy about which Centrality is not obviously false (especially metaphysics and epistemology) have been seen as more central to the discipline than they really ought to be. That’s not a bad state of affairs for metaphysicians and epistemologists, but it’s not good for philosophy, and I hope that Cappelen’s book helps put a stop to it.\nIntuitions in Detective Work\nDespite my very broad sympathy with Cappelen’s project, I do think there’s a role for intuitions of some kind in philosophy. Just what this kind is, and what this role is, will take some spelling out to avoid Cappelen’s arguments. So that’s what I’ll do for the next few pages.\nThe intuitions I have in mind are characterised by F2 and F4; they are default justified, and they are fast. Here’s how I think these kinds of intuitions could matter philosophically.\nWhen humans are growing up, they develop a lot of cognitive skills. Some of these skills are grounded in specific bits of propositional knowledge. We learn to count in part by learning that 2 comes after 1, and 3 comes after 2, and so on. But not all of them are. We learn how to tell causation from correlation, at least in simple cases, by developing various heuristics, none of which come close to a full theory of causation. Indeed, none of these heuristics would even be true, if stated as universal generalisations. But this ability to pick out which of the many predecessors of an event is its cause is one we develop very early (Gopnik 2009, 33–44), and it is vital to navigating the world.\nI think we develop a lot of skills like that; skills which either go beyond our propositional knowledge, or at the very least are hard to articulate in terms of propositions. That we have these kinds of skills should hardly be news to philosophers; under the label ‘heuristics’ they have become quite familiar thanks to the work of, among others, Daniel Kahneman. They occasionally get a bad press, because one central way in which psychologists detect them is by seeing where they lead to errors that careful thought would correct. (For instance, our heuristics sometimes say that a conjunction is more probable than one of the conjuncts, and careful thinking would correct this.) But this should not blind us to the fact that these incredibly fast heuristics are often very reliable; reliable enough to be an independent check on our theorising.\nThe use of the term ‘intuition’ to pick out these heuristics isn’t particularly idiosyncratic; Kahneman (2011) himself moves back and forth freely between the two terms. He approvingly cites Herbert Simon’s remark that “intuition is nothing more and nothing less than recognition,” which I think is basically right. We intuit that \\(a\\) is \\(F\\) by recognising that it has the tell-tale signs of \\(F\\)hood. Of course we’re a million miles from conceptual or a priori reasoning here; as I said, I agree entirely with Cappelen that F3 is not a feature of any philosophically significant source of evidence. Here are a couple of cases, one real life and one fictional, that draw out far removed intuitive thinking can be from a priori or conceptual thinking. The first is from Kahneman’s description of a case reported by Gary Klein (1999); the second is from (Norwegian) crime novelist Jo Nesbø (2009). First Kahneman,\n\nA team of firefighters entered a house in which the kitchen was on fire. Soon after they started hosing down the kitchen, the commander heard himself shout “Let’s get out of here!” without realizing why. The floor collapsed almost immediately after the firefighters escaped. Only after the fact did the commander realize that the fire had been unusually quiet and that his ears had been unusually hot ... He had no idea what was wrong, but he knew something was wrong. (Kahneman 2011, 11)\n\nNow Nesbø. In the story, Harry is the hero, Harry Hole, and Beate is a talented forensic detective.\n\n‘Forget what you have or haven’t got,’ Harry said. ‘What was your first impression? Don’t think, speak.’\nBeate smiled. She knew Harry now. First, intuition, then the facts. Because intuition provides facts too; it’s all the information the crime scene gives you, but which the brain cannot articulate straight off. (Nesbø 2009, 126)\n\nThere’s at least a family resemblence between Harry Hole’s instruction here and Lewis’s instruction to his readers at the start of “Elusive Knowledge” (Lewis 1996).\n\nIf you are a contented fallibilist, I implore you to be honest, be naive, hear it afresh. ‘He knows, yet he has not eliminated all possibilities of error.’ Even if you’ve numbed your ears, doesn’t this overt, explicit fallibilism still sound wrong? (Lewis 1996, 550)\n\nReviewers of Nesbø’s books often describe his hero as ‘intuitive.’ That’s a little misleading; Harry Hole thinks intuition has a key role to play in detective work, but the adjective suggests that he relies heavily on his own intuition. That’s not right; he’s just as often badgering his colleagues to give him their impressions of a crime scene, or an interview subject. In these scenes he reminds me of no one so much as a colleague constantly wanting to know what one thinks about some thought experiment or variation on a familiar case. (These are often the best kind of colleague - full of inspiring ideas!)\nSo I think a lot of philosophical progress is made by drawing on, and drawing out, these skills. But isn’t this just to say something uncontroversial and uninteresting, namely that philosophy relies on implicit knowledge? As Cappelen puts it,\n\nIt is not controversial that conversations have propositions in the common ground. Nor is it controversial that all arguments start with premises that are not argued for. (155)\n\nWell, there’s something a bit interesting here, namely that the ‘common ground’ and the ‘not argued for’ premises have much greater overlap in philosophy than in other fields. A book starting with observations about the Galápagos Islands starts with premises that are not argued for, but are asserted on the basis of observations. These premises surely weren’t in the common ground before the ‘conversation’ starts. I’ll say more about this in the next section.\nBecause first I want to fuss a little about just what ‘common ground’ is. We’ll start with an observation Cappelen makes about the Ginet/Goldman case of Henry and the fake barns (Goldman 1976). Many philosophers take it to be an interesting fact that in one scenario, Henry knows there’s a barn, while in another he does not. Cappelen says that these facts are “presented as being pre-theoretically in the common ground” (172). That seems false at first blush. Before reading Goldman’s paper, it’s not clear philosophers are in a position to form singular thoughts about Henry. That’s an uncharitable reading though. A more plausible claim is to say that we are pre-theoretically disposed to accept some long sentence that roughly says that an agent in such-and-such scenario knows there is a barn, while an agent in a slightly different scenario does not.\nWe might gloss that last claim as saying that we implicitly knew something about these scenarios. I’m not sure that’s right though. We do surely have lots of implicit knowledge. I know, and so do you, that the Sydney Opera House is south of the Royal Albert Hall, even if you’d never articulated that thought to yourself or another. But do our dispositions to respond to quite finely drawn, and often reasonably long, vignettes count as implicit beliefs, or should they count as things we were in a position to know, but only learned once a philosopher had done the work of drawing the vignette? I can see merit in both positions, and don’t see firm grounds for preferring one.\nLet’s introduce some terminology to avoid taking a stance on this question. Say that a subject has Socratic knowledge that \\(p\\) when the following two conditions are met:\nOnce the agent is asked to consider \\(p\\) in the right way, they will come to know \\(p\\).\nThe evidential basis for this knowledge that \\(p\\) is not the asking itself.\nThe first clause says that anyone who reacts to a Gettier case with “Oh, of course that’s justified true belief without knowledge” has Socratic knowledge that such a case is a counterexample to the JTB theory of knowledge. And they have this Socratic knowledge before the case is even raised. The second clause says that if the person reacts instead with “Oh, some philosophers use thought experiments that don’t make sense unless you know which cars come from which countries,” that won’t count as Socratic knowledge. They would be expressing some knowledge, to be sure, but the telling of the example would play an evidential role.\nIf you are very liberal about which dispositions count as implicit beliefs, and implicit knowledge, then Socratic knowledge will just be a special kind of implicit knowledge. But if you think considering examples can lead to learning new facts, not just drawing out dispositions, then you will think ‘Socratic’ is like ‘alleged,’ a non-factive modifier. As I’ve defined it, once you hear Gettier cases once, that they are counterexamples to the JTB theory ceases to be Socratic knowledge, and becomes regular knowledge. Note also that we can make sense of some implicit states being more or less Socratic than others; some dispositions to assent require very careful work to trigger.\nWhy is the class of propositions that we Socratically know so rich and fertile? It’s because of the central role of heuristics in our cognitive lives. Our interactions with the world don’t just furnish us with a set of truths about the world. They also furnish us with skills that we can apply to generate more truths. I suspect that something like this observation is at the heart of the endorsement of F3, that intuitions reveal conceptual truths. When we intuit that \\(p\\), we don’t always merely recall a prior belief that \\(p\\), or infer \\(p\\) from what we antecedently explicitly knew. But nor do we observe that \\(p\\). So what is it? It must be something internal, but not memory or inference. Conceptual competence isn’t a bad first guess, but Cappelen shows that isn’t the right answer. I think the right answer has to do with cognitive skills, i.e., heuristics.\nPhilosophy: A Negative Characterisation\nSo intuitions matter because they reveal Socratic knowledge, and Socratic knowledge, when made explicit, is a very good guide to the world. That implies that intuitions should not be confined to philosophy. And, indeed, they are not. If an economic theorist claimed the standard of living among English men was higher in 1915 than in 1935, it would be perfectly reasonable to reply that intuitively that cannot be right, because in 1915 a rather large number of English men were living on the Western Front in catastrophically poor conditions. What is distinctive of philosophy then?\nWe need to clarify this question before we can answer it. Philosophy is both a discipline with a history over many millennia, and an organisational unit inside modern universities. These two things overlap well, but not perfectly. Once we note that they are distinct, we can separate out the following three questions.\nWhat questions are philosophical questions?\nWhat questions are, within the academy, primarily addressed by researchers in philosophy departments?\nWhat questions should be, at least within the academy, primarily addressed by researchers in philosophy departments?\nThe three questions don’t overlap. When Milton Friedman (1953) writes about economic methodology, I think he’s addressing a philosophical question, but work like this is, and probably should be, carried out in economics departments. Questions about professional ethics are philosophical questions that I think should be researched in philosophy departments, but in the United States at least typically receive more attention in professional schools. Let’s focus on the third question; what should a philosophy department do?\nMy colleagues at Michigan and St Andrews work on an incredibly wide range of questions, from the interpretation of quantum physics through history of logic through moral psychology and so on. And I think philosophy departments should have this range of interests. But what do all these questions have in common?\nIt’s not anything to do with necessity or a priority. Those categories seriously cross cut philosophy, as Cappelen points out. Historical investigations into disputes about the parentage of various might-have-been-royals, or mathematical investigation into the nature of the primes are not philosophical, but have to do with necessity and a priority. Whether there’s a language of thought is contingent, a posteriori, and almost paradigmatically philosophical.\nIt’s not really anything to do with depth, at least on a natural understanding of that. Why pandas have thumbs, and humans have appendices, turn out to be reasonably deep questions, but they are for biologists, not philosophers. Under what circumstances is democracy compatible with a strong executive is, at least to me, an incredibly deep and important question, but it’s a question to be answered, primarily, in history and political science departments.2 On the other hand, whether we can tell a plausible supervaluationist story about belief reports is not particularly deep, but a perfectly good subject for a philosophical inquiry as in Weatherson (2003a).\nBetter, I think, is to say that philosophical questions are those where implicit or Socratic knowledge, including crucially intuitions, can plausibly play a large role in getting to an answer. Philosophy is a little recursive, so it includes investigations into its own investigations, including historical work and metaphilosophical work. (Two fields which, prior to Cappelen’s book, had surprisingly little interaction.) That’s not to say we’re always right that Socratic knowledge can answer the questions philosophy sets. Maybe some questions in mind and language are best answered with the aid of neurological or phonological work that requires powerful measuring devices. But the questions are ones where starting with the knowledge and skills we already have seems like a plausible starting point, or at least not entirely crazy. This makes philosophy distinct from, say, history. We use intuitions in history too, especially intuitions about what explains what. But we need more; intuition won’t help if you want to know how many troops Henry had at Agincourt.\nThis hypothesis explains, I think, one of the historically important facts about philosophy. Philosophy gives birth to disciplines. Physics, economics, psychology and cognitive science were all, at one time, part of philosophy. In some cases, the split was very recent. The economics tripos at Cambridge only split from philosophy in 1903 (Tribe 2002). The Australasian Journal of Philosophy was the Australasian Journal of Psychology and Philosophy until 1946. Why does philosophy give rise to disciplines like these?\nI think having a negative characterisation of philosophy helps explain it. Philosophy has a lot in common, methodologically, with physics, economics, psychology and so on. All those fields use intuitions and other forms of Socratic knowledge. But the other fields use other things too, especially observation. It’s when it becomes clear that armchair methods play too small a role in the research that the field leaves philosophy.\nOf course, philosophers care more about their questions than their methods, so when the need for non-armchair methods becomes pressing, some of the individual philosophers will go along, picking up more and more observational knowledge and experimental skills. Note how much more empirical research informs the recent work by (for example) Gilbert Harman, Kim Sterelny and Peter Carruthers, compared to their earlier work (Harman 1973; Kilkarni and Harman 2011; Devitt and Sterelny 1987; Sterelny 2012; Carruthers 1990, 2011). From the other direction, our armchairs come with more knowledge now than they used to, which is partially why engaging with Laura Ruetsche’s work in philosophy of science requires more empirical knowledge engaging with William Whewell’s (Ruetsche 2011; Whewell 1840). But still I think the general picture holds; a question is fit for philosophy iff it is plausible that the intuitive, armchair methods which are part of every academic’s toolkit can, on their own, generate serious progress on the question.\nLetting Go\nI’ve said that Lewis’s instruction at the start of “Elusive Knowledge” is to look to intuitions, not to theoretical beliefs. But that might involve reading more into Lewis than is really there. What he literally asks the reader is to not appeal to their preferred theory of knowledge. Is that the same as an appeal to intuitions?\nIt need not always be. Sometimes, asking people to let go of their prior theory involves asking them to engage in a complex cognitive task. In Meditation One, Descartes has us go through quite a lot of thoughts before we can be pre-theoretical in the way he wants us to be.\nBut I don’t think that’s what’s going on with Lewis. For one thing, he doesn’t guide us back to a pre-theoretic naı̈veté the way Descartes does. But more generally, I think getting snap judgments is a way of letting go of some prior theories.\nThe picture I have here, and it is nothing more than a picture, is that intuitions are judgments delivered by heuristics, heuristics are deployed by Fodorian modules, and Fodorian modules are informationally encapsulated (J. A. Fodor 1983; J. Fodor 2000). That is, when we rely on a heuristic, we don’t use all of the information at our disposal. The classic example of this is eyesight; we may know that there are no elephants on Market Street in St Andrews, but given the right visual stimuli, our eyes will still insist that there is an elephant right there. The background theory about the spatial distribution of elephants isn’t encoded into the visual module. More generally, to rely on a heuristic just is to make a judgment using a part of our mind that doesn’t believe some of the things that we do. And that’s good, because it is a kind of independent check on the beliefs we have.3\nBut isn’t the idea that snap judgments are essential to philosophy inconsistent with the fact that we work very hard on getting our examples just right, and (as Cappelen shows), argue at great length over what to say about various examples? I think it isn’t, because there are two respects in which our practice reveals a sensitivity to snap judgments, and a respect for their use as a check on theorising.\nLet me tell you a small secret. I haven’t heard anything that even sounds like a counterexample to the broadly Stalnakerian theory of indicative conditionals that I like for about a decade. That’s not because there aren’t any intuitive counterexamples. It’s just because my intuitions have been trained to accord with this kind of theory.4 So what do I do? Do I give up on the use of intuitions as a test of theory? No, I ask colleagues for their intuitions. Sometimes I ask them a lot of different questions, and sometimes I work rather hard on refining the question, or (when they sadly disagree with my theory) finding ways to undermine their intuitions. Given the number of similar questions I get from other colleagues, I don’t think my methodology here is distinctive. In short, we can work very hard before and after getting the snap judgments, while giving those judgments a role.\nThis might be more idiosyncratic, but I also do a bunch of things in papers to draw out snap judgments. The main idea is to distract the reader from the fact that they are about to be prompted for an intuition, one that may not accord with their preferred theory. So I’ll use deliberately absurd props (like Vinny the talking vulture), or start an example without flagging that it is an example. My favourite move along these lines is to set up an example in such a way that the example doesn’t make sense unless some theoretical claim I want to argue for is true. Then, after much discussion of the correct verdict on the case, I can announce that the very sensibility of the prior discussion is proof that, at least intuitively, the theory I’m pushing must be true.\nWe’re going to come back to this theme a bit later, because I think it’s rather important. The cases you can remember from papers are probably not the ones where intuition mattered. The big role for intuition in philosophy (and in many other disciplines) is in checking the small steps along the way. That’s why I join Cappelen in opposing the methodological rationalists; I don’t think intuitions are distinctive to philosophy, and these small steps don’t have much of a phenomenology. But that doesn’t mean they are unimportant.\nStrength and Fragility\nOne of the big trends in late 20th Century epistemology has been the separation of two senses of strength of evidence. This might mean\nHow strong a doxastic state is supported by the evidence.\nHow resilient the force of the evidence is in the face of counterevidence.\nOne thing that conservative epistemologies (e.g., Harman (1986)) and dogmatic epistemologies (e.g., Pryor (2000)) have in common is that sources which might be very strong in the first sense might be very weak in the second sense. In particular, there can be sources of evidence that ground knowledge, and hence be rather strong in tthe first sense, but easily overturned by conflicting evidence. I prefer to reserve the terms ‘strong’ and ‘weak’ for the first sense, and use the terms ‘resilient’ or ‘fragile’ for the presence or absence of the second property. In that language, the important insight of the conservatives and dogmatists is that evidence can be strong but fragile.\nThat’s roughly how I think of intuitions – they are strong but rather fragile. So they can be unjustified justifiers, which is how I read Cappelen’s feature F2 (i.e., Rock).5\nCappelen notes it is hard to tell whether something is being used as a starting point, or an unjustified justifier, so he gives three diagnostics for this. I mostly agree with one, and disagree with the other two. I agree that intuitions are non-inferential, and they aren’t based on any particular experience, which is his criteria F2.1. (Though they usually are based on experiences taken collectively.) But I would alter the following suggestion, which he gives as a second diagnostic.\n\nF2.2 Evidence Recalcitrance: Intuitions are evidence recalcitrant; i.e., holders of them are not disposed to give them up even when their best arguments for those intuitions are shown to fail. (Compare pg 112)\n\nI would rather offer something normative here. What’s true of intuitions is that they might provide a stronger ground for belief than the best evidence we can offer for them. Compare the case of Gettier. As Cappelen carefully notes (194n3), Gettier doesn’t appeal to a raw intuition. He gives an argument that his subjects don’t know. Unfortunately, it isn’t a compelling argument, since it takes as a premise that we can’t get knowledge from a false belief, and that isn’t quite right (Warfield 2005). But Gettier was, to some extent, justified in believing these subjects didn’t know to a greater degree than he was justified in believing this argument was sound. And that, I think, is not uncommon.\nThis is why I don’t think Cappelen’s ‘Rough Guide to Rock Detection’ (121), the third of the diagnostics, is perfectly reliable. He says that if evidence is given for \\(p\\) in a context, that’s evidence that \\(p\\) isn’t an unjustified justifier in that context. But sometimes we give arguments for judgments that we think could rest without them. Compare this little dialogue.\n\nA: Is ‘John happiness’ a well-formed sentence?\nB: No; it doesn’t have a verb.\n\nHere B gives a judgment, then offers a little argument for it. The argument has a strong premise, namely that all sentences have verbs. That’s debatable; ‘Lo, gavagai!’ may be a counterexample. But B’s judgment isn’t undermined by examples that undermine her argument. As in the Gettier case, we may give an argument that doesn’t capture the full normative force of the judgment.\nTo say that intuitions are unjustified justifiers is not to say they are particularly special. If some conservative or dogmatic epistemology is true, there will be other unjustified justifiers. And if not, then this story about intuitions will be pretty implausible.\nThis picture of intuitions as strong but fragile meshes well, I think, with the picture from section 5. There I said the important intuitions are the ones you barely notice or remember. That’s because the intuitions are fragile; if you remembered them enough to argue about them (or experimentally test them), the fragility conditions had probably been triggered, and the intuition probably wasn’t doing much argumentative work.6\nBut why not think that intuitions are so fragile that they have no use in any philosophical debate? This question deserves more space than I can give it, but here are three sketches of answers.\nIntuitions might be valuable checks on theory, and might be resilient enough to perform a valuable checking role.\nJust like heuristics have characteristic errors, it might be that careful reasoning has characteristic errors, and there are cases where our first impressions are more reliable. See Gladwell (2005) for a summary of some relevant evidence.\nSomewhat surprisingly, there may be cases when it is best to trust the less reliable source. The case for this is a bit detailed, and not original to me, so I’ll just include a brief footnote for those interested.7\nSome Lewisian Case Studies\nI’ve described one kind of mental state that deserves the name ‘intuition,’ and which could play a role in philosophical activity. But, as Cappelen presses, we have to work to convert that ‘could’ to a ‘does.’ Do we really rely in intuitive, or heuristic-driven, judgments about cases in analytic philosophy?\nAs Cappelen shows, the answer is “A lot less than you may have guessed.” We argue a lot more than we intuit, especially about the famous cases.8 The bit of analytic philosophy I’m most familiar with is David Lewis’s corpus, and since that doesn’t play much of a role in Cappelen’s story, I’ll illustrate his point with some examples from it.\nGoing from memory, I would have guessed the clearest example of a case refuting a theory was the use of finkish dispositions to refute the conditional analysis of dispositions. But go to the opening pages of “Finkish Dispositions” (Lewis 1997a), and you find not an intuition about a case, but an argument that finks are possible. And even though that argument is followed up with more cases, Lewis rather explicitly argues for his conclusions about each one. See, for example, the glass loving sorcerer on page 147. Lewis doesn’t avert to an intuition that the loved glass is fragile, rather he “wield[s] an assumption that dispositions are an intrinsic matter.” (Lewis 1997a, 147)\nThe discussion of causation turns out to be a little more fertile. From (the longer version of) “Causation as Influence,” I count the following appeals to intuitions about cases.\nThe chancy bomb example which shows simple probabilistic analyses of indeterministic causation won’t work (Lewis 2004a, 79).\nThe Merlin and Morgana example which shows that trumping is possible, and matters for what is the cause (Lewis 2004a, 81).\nThe variant on Billy and Suzy that raises problems for quasi-dependence (Lewis 2004a, 83).\nThe crazed President example which shows that causation by double prevention is possible, and that causation is not an intrinsic relation (Lewis 2004a, 84).\nThe Frankfurt example which shows we can have causation without dependence (Lewis 2004a, 95).\nThere’s a strong sense, I think, in which none of the judgments in these cases are argued for. Indeed, they arise as problems for theories that are otherwise doing rather well. If there was an argument around, it would be for the negation of the intuited judgment. So I think there’s a role for intuition here.\nBut we should not imagine that this is normal for philosophers, or even for Lewis. Cases, it is true, play a large role in Lewis’s writing. But they are very rarely simple refutations of existing theories. We could perhaps distinguish four roles that cases play, or perhaps four types of philosophical cases.\nRefutation of theories, as in these causation cases.\nIllustrations that help explain what’s going on in an argument, as in the examples from “Finkish Dispositions.” For a more extensive version of this, see Lewis’s version of Puzzling Pierre (Lewis 1981).\nTools for showing that we must distinguish various concepts, such as the discussion of Ned Kelly’s proof that there’s no honest cop (Lewis 1988).\nSimplified versions of the real world, on which we can test various explanatory hypotheses, such as the footy and rugby people in “Naming the Colors” (Lewis 1997b).\nAnd that list is probably incomplete. The last is fairly fascinating as a case study actually.9 Some of you may have had the following experience when programming, or indeed doing anything that looks like working with code (such as writing in LaTeX). A bug arises. It helps to find a minimal example in which the bug arises, i.e., a smallest program that produces the same bug. This helps you spot what’s going on, and if you still need help, it helps your interlocutors focus on the central problem. It’s important that you haven’t changed the problem; the example must be of the same kind as what you started with. But the example could be much simpler than the case you’re most interested in. Some philosophy examples are, I suspect, like that. Their value lies in revealing that some striking feature of reality would persist even if the world were simpler. So, probably, the explanation of the feature lies in some respect the real messy world shares with the simple example world. (Compare Cappelen’s discussion of Perry’s messy shopper in section 8.1.)\nIt is perhaps no coincidence that the easiest place to find examples of type 1 in Lewis’s work is in the papers on causation. Lewis thinks there is no such thing as causation (Lewis 2004a, 2004b). Whatever our theory of ‘causes’ should be, it shouldn’t match that verb with a binary property. Rather, the aim of philosophical work on causation is to give a reductive analysis of causal thought and talk. In such a project, judgments about how we use ‘causes’ are more likely to be central.\nIt’s also not coincidental that when an example is central to a paper, such as the ‘dishonest’ cop and Puzzled Pierre, they really don’t look like type 1. That’s one big and important lesson from Cappelen’s work. Philosophers do use examples to refute theories, but they are rarely the big famous examples. If an example is central to a philosophy paper, it typically plays one of the other three roles.\nSummary\nLet’s take stock. I’ve argued for the following theses:\nSocratic knowledge is important to philosophy.\nThe distinctive feature of philosophy is that it addresses questions that can, at least prima facie, be productively worked on while relying primarily on Socratic knowledge.\nIntuitions are manifestations of cognitive skills, and much Socratic knowledge is constituted by the possession of such cognitive skills.\nLike other forms of Socratic knowledge, intuitions are mostly a posteriori, and have roles outside philosophy as well as inside it.\nIntuitions are default justified; that is, they can be unjustified justifiers.\nThis default is very weak; intuitions can easily be overridden by other considerations.\nRelatedly, it is rare for any one intuition to be central to a philosophical work; philosophical intuitions mostly concern the little cases we see along the way to larger projects.\nI also hinted at, without developing, an argument for\nThe right intuition can stop even a plausible theory dead in its tracks; and we have (thanks to Ben Levinstein) a mathematical model for why this can be so even if intuitions are much less reliable than theories.\nI opened with a discussion of why it matters to philosophy’s self-conception that point 4 is correct. Since Cappelen also endorses 4, I probably don’t need to say more about that here. But I think there is more to say about 7.\nThe first thing I want to note is that 7 is of course consistent with Cappelen’s textual research on important work in late analytic philosophy. In just about any thought experiment that you can remember, the intuitions about it don’t carry much philosophical weight in the work in which it is introduced. The intuitions that matter are the little ones, the ones that go by so quickly that no one questions them and are largely forgotten by all but the cognoscenti in that field. Even these intuitions aren’t that common. There are less of them in Lewis than I would have guessed.\nStill, I disagree with Cappelen that philosophy is without these intuitions. And so I disagree that there’s no role for double checking, experimentally if need be, whether these intuitions are really intuitive. If a well run survey showed that most people disagree with Lewis’s judgment about, say, the chancy bombs example, I’d reconsider my views about probabilistic causation. But I’d be really surprised to see this.\nThe second thing to note is that while 7 is true, it’s not the case that intuitions about one case are never central to a philosophical project. There is one big counterexample: the Gettier literature. Like Cappelen (194n3), I think this literature is incredibly unrepresentative of philosophy. And I think that’s in part because it was methodologically flawed. I tried to make this point in an earlier paper (Weatherson 2003b), but I didn’t get it quite right. (What I should have said was more like what Elijah Chudnoff (2011) does say.) When we saw the Gettier example, this should have been an invitation to try and find out what feature of knowledge was driving the fact that the belief in the main examples didn’t amount to knowledge. Gettier suggested it was inference from a false premise, but that doesn’t quite work (Warfield 2005). You might think it is insensitivity, but that doesn’t quite work. At this point there should have been one of two paths taken - attempts to find some other explanation of the data, or a reconsideration of whether our initial judgment about the case was wrong. That’s what the picture of philosophy sketched here would have predicted, and (this is the point I was trying but failing to make in the earlier paper) that’s what reflection on our successes in other areas of philosophy would have recommended. But the first kind of project ended up intertwined with attempts to analyse knowledge, and stalled for decades. And the second project wasn’t seriously undertaken, with some honorable exceptions such as Sartwell (1992) and Hetherington (2001). Now eventually this didn’t matter, because we discovered that safety based explanations of the Gettier case would work, even if there is no safety based analysis of knowledge, and even if there is some work to be done in getting the safety condition just right (Williamson 1994, 2000; Sainsbury 1995; Lewis 1996; Weatherson 2004). So if we strengthened 7 into a universal claim it would be false – thirty years of epistemological struggle attest to this. But it was really when epistemology fell into line with practice in other fields of philosophy that it made progress on the Gettier case.\n\n\nCarruthers, Peter. 1990. The Metaphysics of the Tractatus. Cambridge: Cambridge University Press.\n\n\n———. 2011. The Opacity of Mind: An Integrative Theory of Self-Knowledge. Oxford: Oxford University Press.\n\n\nChudnoff, Elijah. 2011. “What Should a Theory of Knowledge Do?” Dialectica 65 (4): 561–79. https://doi.org/10.1111/j.1746-8361.2011.01285.x.\n\n\nDevitt, Michael. 2011. “Experimental Semantics.” Philosophy and Phenomenological Research 82 (2): 418–35. https://doi.org/ppr201182222.\n\n\nDevitt, Michael, and Kim Sterelny. 1987. Language and Reality: An Introduction to the Philosophy of Language. Cambridge, MA.: MIT Press.\n\n\nFodor, Jerry. 2000. The Mind Doesn’t Work That Way. Cambridge, MA: MIT Press.\n\n\nFodor, Jerry A. 1983. The Modularity of Mind. Cambridge, MA: MIT Press.\n\n\nFriedman, Milton. 1953. “The Methodology of Positive Economics.” In Essays in Positive Economics, 3–43. Chicago: University of Chicago Press.\n\n\nGladwell, Malcolm. 2005. Blink: The Power of Thinking Without Thinking. New York: Little, Brown.\n\n\nGoldman, Alvin I. 1976. “Discrimination and Perceptual Knowledge.” The Journal of Philosophy 73 (20): 771–91. https://doi.org/10.2307/2025679.\n\n\nGopnik, Alison. 2009. The Philosophical Baby: What Children’s Minds Tell Us about Truth, Love, and the Meaning of Life. New York: Farrar, Straus; Giroux.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\n———. 1986. Change in View. Cambridge, MA: Bradford.\n\n\nHetherington, Stephen. 2001. Good Knowledge, Bad Knowledge: On Two Dogmas of Epistemology. Oxford: Oxford University Press.\n\n\nIchikawa, Jonathan, Ishani Maitra, and Brian Weatherson. 2012. “In Defence of a Kripkean Dogma.” Philosophy and Phenomenological Research 85 (1): 56–68. https://doi.org/10.1111/j.1933-1592.2010.00478.x.\n\n\nKahneman, Daniel. 2011. Thinking Fast and Slow. New York: Farrar, Straus; Giroux.\n\n\nKilkarni, Sanjeev, and Gilbert Harman. 2011. An Elementary Introduction to Statistical Learning Theory. Hoboken, NJ: Wiley.\n\n\nKlein, Gary A. 1999. Sources of Power. Cambridge, MA.: MIT Press.\n\n\nLewis, David. 1981. “What Puzzling Pierre Does Not Believe.” Australasian Journal of Philosophy 59 (3): 283–89. https://doi.org/10.1080/00048408112340241.\n\n\n———. 1988. “The Trap’s Dilemma.” Australasian Journal of Philosophy 66 (2): 220–23. https://doi.org/10.1080/00048408812343301.\n\n\n———. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\n———. 1997a. “Finkish Dispositions.” The Philosophical Quarterly 47 (187): 143–58. https://doi.org/10.1111/1467-9213.00052.\n\n\n———. 1997b. “Naming the Colours.” Australasian Journal of Philosophy 75 (3): 325–42. https://doi.org/10.1080/00048409712347931.\n\n\n———. 2004a. “Causation as Influence.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 75–106. Cambridge: MIT Press.\n\n\n———. 2004b. “Void and Object.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 277–90. Cambridge: MIT Press.\n\n\nMachery, Eduoard, Ron Mallon, Shaun Nichols, and Stephen Stich. 2012. “If Folk Intuitions Vary, Then What?” Philosophy and Phenomenological Research 86 (3): 618–35. https://doi.org/10.1111/j.1933-1592.2011.00555.x.\n\n\nNagel, Jennifer. 2007. “Epistemic Intuitions.” Philosophy Compass 2 (6): 792–819. https://doi.org/10.1111/j.1747-9991.2007.00104.x.\n\n\n———. 2013. “Defending the Evidential Value of Epistemic Intuitions: A Reply to Stich.” Philosophy and Phenomenological Research 86 (1): 179–99. https://doi.org/10.1111/phpr.12008.\n\n\nNesbø, Jo. 2009. The Redeemer. London: Vintage Books.\n\n\nNolan, Daniel. 2005. David Lewis. Chesham: Acumen Publishing.\n\n\nPryor, James. 2000. “The Sceptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\nRuetsche, Laura. 2011. Interpreting Quantum Theories. Oxford: Oxford University Press.\n\n\nSainsbury, Mark. 1995. “Vagueness, Ignorance and Margin for Error.” British Journal for the Philosophy of Science 46: 589–601. https://doi.org/10.1093/bjps/46.4.589.\n\n\nSartwell, Crispin. 1992. “Why Knowledge Is Merely True Belief.” Journal of Philosophy 89 (4): 167–80. https://doi.org/10.2307/2026639.\n\n\nSchwarz, Wolfgang. 2009. David Lewis: Metaphysik Und Analyse. Paderborn: Mentis-Verlag.\n\n\nSterelny, Kim. 2012. The Evolved Apprentice: How Evolution Made Humans Unique. Cambridge, MA.: Bradford.\n\n\nSugden, Robert. 2000. “Credible Worlds: The Status of Theoretical Models in Economics.” Journal of Economic Methodology 7 (1): 1–31. https://doi.org/10.1080/135017800362220.\n\n\n———. 2009. “Credible Worlds, Capacities and Mechanisms.” Erkenntnis 70 (1): 3–27. https://doi.org/10.1007/s10670-008-9134-x.\n\n\nTribe, Kevin. 2002. “The Cambridge Economics Tripos 1903–55 and the Training of Economists.” The Manchester School 68 (2): 222–48. https://doi.org/10.1111/1467-9957.00191.\n\n\nWarfield, Ted A. 2005. “Knowledge from Falsehood.” Philosophical Perspectives 19: 405–16. https://doi.org/10.1111/j.1520-8583.2005.00067.x.\n\n\nWeatherson, Brian. 2003a. “Many Many Problems.” The Philosophical Quarterly 53 (213): 481–501. https://doi.org/10.1111/1467-9213.00327.\n\n\n———. 2003b. “What Good Are Counterexamples?” Philosophical Studies 115 (1): 1–31. https://doi.org/10.1023/A:1024961917413.\n\n\n———. 2004. “Luminous Margins.” Australasian Journal of Philosophy 82 (3): 373–83. https://doi.org/10.1080/713659874.\n\n\nWhewell, William. 1840. The Philosophy of the Inductive Sciences, Founded Upon Their History. London: John W. Parker.\n\n\nWilliamson, Timothy. 1994. Vagueness. Routledge.\n\n\n———. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nMy own views about the importance of this, as well as much else in this paper, owe a lot to Jennifer Nagel (2007, 2013).↩︎\nThis is not to say that political philosophers couldn’t help with this question. There are lots of questions that should have as their research centre some other department, but to which philosophers can usefully help. Indeed, the examples from economic methodology and evolutionary explanation I just mentioned are two more such questions.↩︎\nPhilosophers sometimes understate the importance of independent checks. We can know a scale is working, but if we want to check its reliability we don’t use it, we use something else. I suspect that a certain amount of theory-independence is part of the explanation of the value of intuitions.↩︎\nRelatedly, I haven’t seen Liverpool get awarded an undeserved free kick for about that long.↩︎\nThere’s an ambiguity in Cappelen’s text that I’m not sure I’m interpreting the right way. Let’s that someone intuits that in a particular case, \\(c\\) doesn’t cause \\(e\\). Call the content of that intuition, i.e., what is intuited, \\(p_d\\). And call the proposition that the person has this intuition, i.e., the event of the intuiting, \\(p_g\\). Plausibly both \\(p_d\\) and \\(p_g\\) could be evidence in the right cases, though most of the time the salient evidence will be \\(p_d\\). I think \\(p_d\\) can be an unjustified justifier in the sense that other beliefs, e.g., that a particular theory of causation is false, can be justified on the basis of \\(p_d\\), but no other beliefs the agent has justify \\(p_d\\). But you might want a stronger sense of ‘unjustified,’ where it means not just not justified by anything else, but not justified at all. I think in these kinds of cases, \\(p_d\\) is justified, just not justified by anything else. And the justification is, as I’ll get to below, strong but fragile. If when Cappelen says that intuitions, according to Centrality, are unjustified justifiers he means that the belief that \\(p_d\\) is unjustified, then I’m not defending Centrality. I just mean that the agent need not have any other mental states which justify the belief \\(p_d\\), or indeed any access to anything that justifies \\(p_d\\). But for all that it might be that the belief that \\(p_d\\) is justified, and the grounds for the justification include what the agent learned about causation as a child, plus perhaps her competence in distinguishing causes from non-causes.↩︎\nI’m simplifying a little here. My preferred position is that intuiteds provide strong but fragile evidence, while intuitings provide weak but resilient evidence. The reason this is relevant is related to footnote 7.↩︎\nAt one point in Ben Levinstein’s doctoral dissertation, he considers whether there’s a general rule for deciding which of two conflicting sources we can trust. There turns out to be very little in general one can say. In particular, trust the more reliable source turns out not in general to be good advice. If sources have characteristic errors, it might be that given what the two sources have said, it is better on this occasion to trust the less reliable source, because the verdicts the sources deliver provide evidence that we are seeing one of the characteristic errors of the more reliable source. It takes more space than I have here to fill in the details of this argument, and most of the details I’d include would be Levinstein’s not mine. But here’s the big conclusion. Assume that intuitions are often wrong, but rarely dramatically wrong. The reason for that is that heuristics are bad at getting things exactly right, and good at getting in the ballpark. And that careful reasoning is often right, but sometimes dramatically wrong. This is trickier to motivate, but I think true. Then when intuition dramatically diverges from theory, and we don’t have independent reason to think that intuition is mistaken about the kind of case that’s in question, we should trust the intuition more than the theory.↩︎\nThere is interesting work to be done on the relative role of intuitions and arguments about principles, but I’m going to leave that for another day, and focus here on cases. The principles/cases distinction can be a bit slippery, but paradigm cases are easy to identify, and we’ll be working with fairly paradigmatic cases here.↩︎\nSee Sugden (2000, 2009) for much more on this use of thought experiments.\n\n↩︎\n",
    "preview": "posts/2021-01-06-centrality-and-marginalisation/cappelen.jpg",
    "last_modified": "2021-03-12T15:12:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-06-running-risks-morally/",
    "title": "Running Risks Morally",
    "description": "I defend normative externalism from the objection that it cannot account for the wrongfulness of moral recklessness. The defence is fairly simple—there is no wrong of moral recklessness. There is an intuitive argument by analogy that there should be a wrong of moral recklessness, and the bulk of the paper consists of a response to this analogy. A central part of my response is that if people were motivated to avoid moral recklessness, they would have to have an unpleasant sort of motivation, what Michael Smith calls “moral fetishism”.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2014-01-01",
    "categories": [
      "ethics",
      "games and decisions",
      "moral uncertainty"
    ],
    "contents": "\n\nContents\nMoral Uncertainty\nPrinciples\nWelfare and Rationality\nDuelling Analogies\nAn Alternative Analogy\nObjections and Replies\n\nThis paper is part of a project defending normative externalism. This is the view that the most important norms concerning the guidance and evaluation of action and belief are external to the agent being guided or evaluated. The agent simply may not know what the salient norms are, and indeed may have seriously false beliefs about them. The agent may not have any evidence that makes it reasonable to have true beliefs about what the salient norms are, and indeed may have misleading evidence about them. But this does not matter. What one should do, or should believe, in a particular situation is independent of what one thinks one should do or believe, and (in some key respects) of what one’s evidence suggests one should do or believe.\nPublished in Philosophical Studies 167: 141-163.\nPhoto by Michel Osmont via Creative Commons.\nThere are three important classes of argument relevant to the debate between normative externalists, in the sense of the first paragraph, and normative internalists. One class concerns intuitions about cases. For instance, we might try to defend normative externalism by arguing that according to the internalist, but not the externalist, there is something bad about Huckleberry Finn’s actions in helping Jim escape. Nomy Arpaly (2002) uses this example as part of an argument for a sophisticated form of externalism. Another class concerns views about the nature of norms. Internalists think that externalists have missed the need for a class of subjective norms, that are sensitive to agents’ views about the good. Externalists think that the norms internalists put forward are incoherent, or do not meet the internalists’s needs. I’ll gesture at these arguments below, but they are made in much more detail in recent work by Elizabeth Harman (2015) responding to internalist proposals.\n\nI’ve discussed this paper with just about everyone I know. Thanks to Elizabeth Anderson, Rachael Briggs, Lara Buchak, Sarah Buss, Justin D’Arms, Tom Dougherty, Dmitri Gallow, Alex Guerrero, Elizabeth Harman, Scott Hershovitz, Ishani Maitra, Julia Markovits, Jill North, Timothy Schroeder, Andrew Sepielli, Ted Sider, Rohan Sud, Sigrún Svavarsdóttir and Julie Tannenbaum for suggestions that particularly improved the paper.\nBut there’s a third class of argument where the internalist may seem to have an edge. Internalists can argue that there is a wrong of moral recklessness, and externalists cannot explain what is wrong about moral recklessness. My response will be fairly blunt; I do not think moral recklessness is wrong. But I’ll start by trying to state the case for the wrongness of moral recklessness as strongly as I can, including clarifying just what moral recklessness is, before moving onto a response on behalf of the externalist.\n\nThis paper was presented to the EDGe group at the University of Michigan and the philosophy department at Ohio State University, and I got valuable feedback at both of those presentations.\nMoral Uncertainty\nSome of our moral opinions are pretty firmly held. Slavery really is wrong; rescuing drowning children is good; and so on. But others might be more uncertain. To use an example I’ll return to a lot, even a lot of carnivores worry that it isn’t obvious that killing animals to eat their flesh is morally permissible.\n\nThe paper also served as my inaugural lecture as the Marshall M. Weinberg Professor at the University of Michigan. Marshall has been a wonderful supporter of the University of Michigan for many years, and especially of its philosophy department, and this was a tremendous honour.\nWe might wonder whether this uncertainty should have practical consequences. Uncertainty in general does have practical, and even moral, consequences. If you’re pretty sure the bridge is safe, but not completely certain, you don’t cross the bridge. If you’re only sorta kinda confident that an action won’t kill any innocent bystanders, and there is no compelling reason to do the action, it would be horribly immoral of you to do it.\nAnd the paper was presented at the 2013 Bellingham Summer Philosophy Conference. This is close to the Platonic Ideal of a philosophy conference. I’m incredibly grateful to Ned Markosian, and to all of the people who work with him to make this conference happen every year. And I’m very happy to have been able to present this paper at the 2013 conference.\nSo to Marshall and to Ned, thanks.\nThere are (at least) two ways to be uncertain about the morally significant consequences of your action. You might know the moral significance of everyone who might be harmed by your action, but not know how many of them will be harmed, or how seriously they will be harmed. Someone who habitually runs red lights is in this position. They know there’s an elevated risk that they’ll kill another human this way, and they know the human they would kill is morally valuable. Alternatively, you might know who or what is affected by your action, but not be sure of their moral status. The hesitant carnivore is like this. They know that steak dinners require killing cows, but they aren’t sure how morally significant the cows are.\nPerhaps that’s a distinction without a difference though. In both cases, the action results in a higher probability of something morally significant being killed. And, one might think, that’s enough to give the actor reason to pause before acting, and enough to give us reason to condemn the action.\nAs may be clear from the introduction, that’s not how I think of the cases. I think the distinction I just flagged is very important both practically and morally. Being uncertain about the physical consequences of your actions should matter both to what you do, and how you are assessed. The red light runner is immoral, even if she never actually harms anyone, because she endangers morally significant humans. But the meat eater cannot be condemned on the same grounds. If she is wrong that meat eating is morally acceptable, that would be one thing. But a mere probability that meat eating is immoral should not change one’s actions, or one’s evaluations of meat eaters.\nNow I won’t pretend this is a particularly intuitive view. In fact, quick reflection on a few cases may make it seem that it is extremely unintuitive. Let’s look at three such cases.\n\nCake\nCarla is baking a cake for a fundraiser. She wants to put some sweetening syrup into the cake to improve its taste. She reaches for an unmarked bottle, which she is pretty sure contains the sweetener she wants. But then she remembers that last week she had some arsenic in a similar bottle. She is pretty sure she threw the arsenic out, but not exactly certain. As a matter of fact, the syrup in the bottle is sweetener, not arsenic, but Carla isn’t certain of this. What should she do?\n\n\nDinner\nMartha is deciding whether to have steak or tofu for dinner. She prefers steak, but knows there are ethical questions around meat-eating. She has studied the relevant biological and philosophical literature, and concluded that it is not wrong to eat steak. But she is not completely certain of this; as with any other philosophical conculsion, she has doubts. As a matter of fact, Martha is right in the sense that a fully informed person in her position would know that meat-eating was permissible, but Martha can’t be certain of this. What should she do?\n\n\nAbortion\nAgnes is twelve weeks pregnant, and wants to have an abortion. She has studied the relevant medical and philosophical literature, and is pretty sure that foetuses at this stage of development are not so morally significant as to make abortion wrong. But she is not completely certain of this; as with any other philosophical conclusion, she has doubts. As a matter of fact, Agnes is right in the sense that a fully informed person in her position would know that abortion was permissible, but Martha can’t be certain of this. What should she do?\n\nThe setup of the last two cases is a bit cumbersome in one key respect; I had to refer to what a fully informed person in Martha or Agnes’s position would know. I did this so as to not beg any questions against the internalist. I would rather say simply that Martha and Agnes were simply right in their beliefs. But I’m not sure how to make sense of this from an internalist perspective. If what’s right to do is a function of your moral evidence and beliefs, perhaps there is a sense in which meat-eating or abortion is objectively permissible, but Martha and Agnes can’t truly believe it is permissible, since it isn’t permissible in their subjective state, and that’s the really important kind of permissibility. So the retreat to talking about what a fully informed person would know is my attempt to find an objective point at which the internalist and externalist can agree. It doesn’t signal that I think there’s anything special about fully informed agents; I’m just trying to avoid being question-begging here.\nYou might also think that one or other of these cases is very far removed from reality. Perhaps what counts as meat or a foetus would have to be very different for these cases to be possible, perhaps so different that they wouldn’t deserve the label ‘meat’ or ‘foetus.’ I don’t think this should worry us. I don’t particularly care if the cases are metaphysically possible or not. There’s a world, epistemically if not metaphysically possible, where the medical and biological facts are as they are and meat-eating and abortion are permissible, and that’s the world I mean these examples to be set in. By allowing that my thought experiments may well be set in metaphysically impossible worlds, I am going against some recent views on thought experiments as put forward by, e.g, Timothy Williamson (2007) and Anna-Sara Malmgren (2011), but it would take us too far afield to defend this bit of apostasy. Instead, I’ll just use the cases as they are.\nFinally, note that I’ve set up the cases where the protagonists are almost, but not entirely, sure of something that is in fact true. And I’m going to argue in the moral case that they should act as if they are right. That’s not because I think that a view one is almost sure of should be acted on; one should act on the moral truths, and Agnes and Martha are close to certain of the actual truth. The reason for picking these cases is that they make the issue of recklessness most salient. If any of the three women do anything wrong (and I think Carla does) it is only because they are reckless.\nThat said, there is something interestingly in common to the three cases. In each case, the agent has a choice that is, if taken freely, clearly morally acceptable. Carla can leave out the syrup, Agnes can continue the pregnancy, and Martha can order the tofu. At least, that’s true on the most natural ways to fill out the details of the case.1 So assume that Carla, Martha and Agnes are correctly completely certain that they have a morally safe option. Also assume, if it isn’t clear already, that their only motivation for taking the safe option is to hedge against a possibility that they think is rather unlikely. Hedges can be valuable, so the fact that this is their only motivation is not a reason to not take the safe option.\nIn contemporary debates, it’s not often you see pro-vegetarianism and anti-abortion arguments run side by side. Especially in America, these debates have been caught up in culture war politics, and on the whole vegetarians are on one side of this debate, and anti-abortion activists on the other side. But the debates do have some things in common, and it is their commonality that will interest us primarily here. In particular, we’ll be looking at the idea that one should be vegetarian, and refrain from having abortions, on the grounds that these are the good safe options to take. (This connection between the debates is not a novel observation. D. Moller (2011, 426) notes it, and makes some pointed observations about how it affects the philosophical landscape.)\nI’m going to argue that the idea that all three women should ‘play it safe’ is entirely the wrong lesson to take from the cases. I think the cases are in important respects disanalogous. It is seriously morally wrong for Carla to include the syrup in the cake, but it is not wrong in the same way for Martha to eat the steak, or for Agnes to have the abortion. A little more precisely, I’m going to be arguing that there is no good way to fill in the missing premise of this argument.\n\nThe ‘Might’ Argument\n\nIn the circumstances that Agnes/Martha are in, having an abortion /eating a steak might be morally wrong.\nIn the circumstances that Agnes/Martha are in, continuing the pregnancy /eating vegetables is definitely morally permissible.\nMissing Premise\nSo, Agnes should not have the abortion, and Martha should not eat the steak.\nWhen I argue that the ‘Might’ Argument cannot be filled in, I’m arguing against philosophers who, like Pascal, think they can convince us to act as if they are right as soon as we agree there is a non-zero chance that they are right. I’m as a rule deeply sceptical of any such move, whether it be in ethics, theology, or anywhere else.\nBut note like someone responding to Pascal’s Wager, I’m focussing on a relatively narrow target here. Rejecting Pascal’s Wager does not mean rejecting theism; it means rejecting Pascal’s argument for being a theist. Similarly, rejecting the ‘Might’ Argument does not mean rejecting all ethical arguments against meat-eating or abortion. It just means rejecting this one.\nI’m also not arguing about public policy here. The ‘Might’ Argument can be generalised to any case where there is an epistmic asymmetry. The agent faces a choice where one option is morally risky, and the other is not. Public policy debates are rarely, if ever, like that. A legislator who bans meat-eating or abortion takes a serious moral risk. They interfere seriously with the liberties of the people of their state, and perhaps do so for insufficient reason. (This point is well made by Moller (2011, 442).) So there isn’t a ‘play it safe’ reason to support anti-meat or anti-abortion legislation, even if I’m wrong and there is such a reason to think that individuals should not eat meat or have abortions.\nThere are two ways to try to fill out the ‘Might’ Argument. We could try to offer a particular principle that implies the conclusion given the rest of the premises. Or we could try to stress the analogy between the three cases that I started with. I’m going to have a brief discussion of the first option, and then spend most of my time on the analogy. As we’ll see, there are many possible principles that we could try to use here, but hopefully what I say about a some very simple principles, plus what I say about the analogy, will make it clear how I want to respond to most of them.\nPrinciples\nOne way to fill in the Missing Premise is to have a general principle that links probabilities about morality with action. The simplest such principle that would do the trick is this.\n\nProbWrong\nIf an agent has a choice between two options, and one might be wrong, while the other is definitely permissible, then it is wrong to choose the first option.\n\nI think ProbWrong does a reasonable job of capturing the intuition that Agnes and Martha would be running an impermissible risk in having an abortion or eating meat. But ProbWrong has clearly implausible consequences. Imagine that an agent has the following mental states:\nShe is sure that ProbWrong is true.\nShe is almost, but not completely, sure that eating meat is permissible for her now.\nShe is sure that eating vegetables is permissible for her now.\nShe is sure that she has states 1–3.\nA little reflection shows that this is an incoherent set of states. Given ProbWrong, it is simply wrong for someone with states 2 and 3 to eat meat. And the agent knows that she has states 2 and 3. So she can deduce from her other commitments and mental states that eating meat is, right now, wrong. So she shouldn’t be almost sure that eating meat is permissible; she should be sure that it is wrong.\nThis argument generalises. If 1, 3 and 4 are true of any agent, the only ways to maintain coherence are to be completely certain that meat eating is permissible, or completely certain that it is impermissible. But that is, I think, absurd; these are hard questions, and it is perfectly reasonable to be uncertain about them. At least, there is nothing incoherent about being uncertain about them. But ProbWrong implies that this kind of uncertainty is incoherent, at least for believers in the truth of ProbWrong itself. Indeed, it implies that in any asymmetric moral risk case, an agent who knows the truth of ProbWrong and is aware of her own mental states cannot have any attitude between certainty that both options are permissible, and certainty that the risky action is not, for her, permissible. That is, I think, completely absurd.\nNow most philosophers who advocate some principle or other as the Missing Premise don’t quite advocate ProbWrong. We can position some of the rival views by abstracting away from ProbWrong as follows.\n\nGeneral Principle\nIf an agent has a choice between two options, and one might be X, while the other is definitely not X, then it is Y to choose the first option.\n\nWe get ProbWrong by substituting ‘wrong’ for both X and Y. But we saw a decisive objection to that view. And we get a version of that objection for any substitution where X and Y are the same. So a natural move is to use different substitutions. If you replace X with ‘wrong’ and Y with ‘irrational,’ you get something like a principle defended by Ted Lockhart (2000).\n\nWhat Might be Wrong Is Irrational\nIf an agent has a choice between two options, and one might be wrong, while the other is definitely not wrong, then it is irrational to choose the first option.\n\nNow at this stage we could look at whether this principle is plausible, and if not whether alternative principles offered by Alex Guerrero (2007), Andrew Sepielli (2009) or others are any better. You can probably guess how this would go. We’d spend some time on counterexamples to the principle. And we’d spend some time on whether the conclusion we get in this particular case is really plausible. (Is it true that Martha is not in any way immoral, but is irrational in virtue of moral risk? That doesn’t sound at all like the right conclusion.)\nBut I’m not going to go down that path. Shamelessly stealing an analogy from Jerry Fodor (2000), I’m not going to get into a game of Whack-a-Mole, where I try to reject a principle that could fill in for the Missing Premise, and if I succeed, another one pops up. I’m not playing that game because you never actually win Whack-a-Mole; by going through possible principles one at a time it isn’t clear how I could ever show that no principle could do the job.\nWhat I need to show is that we shouldn’t look for a principle to fill in as Missing Premise. One reason we shouldn’t is that the intuitions behind principles like Lockhart’s is really an intuition in favour of ProbWrong, and as such should be suspect. But a better reason is that the analogy between Carla’s case and Agnes/Martha’s cases that motivated the thought that there should be some principle here is mistaken. Once we see how weak that analogy is, I think we’ll lose motivation for trying to fix ProbWrong.\nWelfare and Rationality\nSo my primary opponent the rest of the way is someone who wants to defend the ‘Might’ Argument by pressing the analogy between Carla’s case and the two more morally loaded cases.2 My reply will be that there are better analogies than this which point in the opposite direction. In particular, I’m going to draw an analogy between Agnes and Martha’s cases with some tricky cases concerning prudential reasoning. To set up the case, I’ll start with an assumption that guides the discussion.\nThe assumption is that deliberately undermining your own welfare, for no gain of any kind to anyone, is irrational. Indeed, it may be the paradigmatic form of irrationality. This is, I think, a widely if not universally held view. There is a radically Humean view that says that welfrae just consists of preference satisfaction, and rationality is just a matter of means-end reasoning. If that’s right then this assumption is not only right, it states the only kind of irrationality there is. But you don’t have to be that radical a Humean, or really any kind of Humean at all, to think the assumption is true.\nThe assumption doesn’t just mean that doing things that you know will undermine your welfare for no associated gain is irrational. It means that taking serious risks with your welfare for no compensating gain is irrational. Here is a clear example of that.\n\nEating Cake\nRicky is baking a cake for himself. He wants to put some sweetening syrup into the cake to improve its taste. He reaches for an unmarked bottle, which he is pretty sure contains the sweetener he wants. But then he remembers that last week he had some arsenic in a similar bottle. He is pretty sure that he threw the arsenic out, but not exactly certain. As a matter of fact, the bottle does contain sweetener, not arsenic, but Ricky isn’t completely sure of this. What should he do?\n\nI hope it is plausible enough that it would be irrational for Ricky to put the syrup in the cake. The risk he is running to his own welfare – he literally will due if he’s wrong about what’s in the bottle – isn’t worth the gain in taste, given his level of confidence.\nWith that said, consider two more examples, Bob and Bruce. Bob has thought a bit about philosophical views on welfare. In particular, he has spent a lot of time arguing with a colleague who has the G. E. Moore-inspired view that all that matters to welfare is the appreciation of beauty, and personal love.3 Bob is pretty sure this isn’t right, but he isn’t certain, since he has a lot of respect for both his colleague and for Moore.\nBob also doesn’t care much for visual arts. He thought that art is something he should learn something about, both because of the value other people get from art, and because of what you can learn about the human condition from it. And while he’s grateful for what he learned while trying to inculcate an appreciation of art, and he has become a much more reliable judge of what’s beautiful and what isn’t, the art itself just leaves him cold. I suspect most of us are like Bob about some fields of art; there are genres that we feel have at best a kind of sterile beauty. That’s how Bob feels about most visual art. This is perhaps unfortunate; we should feel sorry for Bob that he doesn’t get as much pleasure from great art as we do. But it doesn’t make Bob irrational, just unlucky.\nFinally, we will suppose, Bob is right to reject his colleague’s Moorean view on welfare. Appreciation of art isn’t a constituent of welfare. In the example we’ll suppose welfare is a matter of health, happiness and friendship. So a fairly restricted version of an objective list theory of welfare is correct in Bob’s world. And for people who like art, appreciating art can produce a lot of goods. Some of these are direct - art can make you happy. And some are indirect - art can teach you things and that learning can contribute to your welfare down the line. But if the art doesn’t make you happy, as it doesn’t make Bob happy, and one has learned all one can from a genre, as has Bob, there is no welfare gain from going to see art. It doesn’t in itself make you better off, as Bob’s Moorean colleague thinks.\nNow Bob has to decide whether to spend some time at an art gallery on his way home. He knows the art there will be beautiful, and he knows it will leave him cold. There isn’t any cost to going, but there isn’t anything else he’ll gain by going either. Still, Bob decides it isn’t worth the trouble, and stays out. He doesn’t have anything else to do, so he simply takes a slightly more direct walk home, which (as he knows) makes at best a trifling gain to his welfare.\nI think Bob is perfectly rational to do this. He doesn’t stand to gain anything at all from going to the gallery. In fact, it would be a little perverse, in a sense we’ll return to, if he did go.\nBruce is also almost, but not completely certain, that health, happiness and friendship are the sole constituents of welfare.4 But he worries that this is undervaluing art. He isn’t so worried by the Moorean considerations of Bob’s colleagues. But he fears there is something to the Millian distinction between higher and lower pleasures, and thinks that perhaps higher pleasures contribute more to welfare than lower pleasures. Now most of Bruce’s credence goes to alternative views. He is mostly confident that people think higher pleasures are more valuable than lower pleasures because they are confusing causation and constitution. It’s true that experienceing higher pleasures will, typically, be part of experiences with more downstream benefits than experiences of lower pleasures. But that’s the only difference between the two that’s prudentially relevant. (Bruce also suspects the Millian view goes along with a pernicious conservatism that values the pop culture of the past over the pop culture of the present solely because it is past. But that’s not central to his theory of welfare.) And like Bob, we’ll assume Bruce is right about the theory of welfare in the world of the example.\nNow Bruce can also go to the art gallery. And, unlike Bob, he will like doing so. But going to it will mean he has to miss a night playing video games that he often goes to. Bruce knows he will enjoy the video games more. And since playing video games with friends helps strengthen friendships, there may be a further reason to skip the gallery and play games. Like Bob, Bruce knows that there can be very good consequences of seeing great art. But also like Bob, Bruce knows that none of that relevant here. Given Bruce’s background knowledge, he will have fun at the exhibition, but won’t learn anything significant.\nStill, Bruce worries that he should take a slightly smaller amount of higher pleasure rather than a slightly larger amount of lower pleasure. And he’s worried about this even though he doesn’t give a lot of credence to the whole theory of higher and lower pleasures. But he doesn’t go to the gallery. He simply decides to act on the basis of his preferred theory of welfare, and since that welfare is correct, he maximises his welfare by doing this.\nNow I think both Bob and Bruce are rational in what they do. But there is an argument that they are not. I’ll focus on Bob, but the points here generalise.\nGoing to the gallery might increase his welfare substantially, since it will lead to more appreciation of beauty, and appreciation of beauty might be a key constituent of welfare.\nNot going to the gallery definitely won’t increase his welfare by more than a trivial amount.\nIt is irrational to do something that might seriously undermine your own welfare for no compensating gain.\nSo it is irrational for Bob to skip the gallery.\nI think that argument is wrong. Bob’s case is rather unlike Ricky’s. There is a sense in which Bob might be undermining his own welfare in skipping the gallery. But it is not the relevant sense. We can distinguish the two senses making the scope of various operators explicit. The first of these claims is plausibly true; the second is false.\nBob’s welfare is such that it is irrational for him to do something that might undermine it for no compensating gain.\nIt is irrational for Bob to do something that might undermine his welfare, whatever that turns out to be, for no compensating gain.\nIf welfare turns out to be health, happiness and learning, then the first claim says that it is irrational to risk undermining your health, happiness and learning for no compensating gain. And that is, I think, right. But the second claim says that for any thing, if that thing might be welfare, and an action might undermine it, it is irrational to perform the action without a compensating gain. That’s a much stronger, and a much less plausible, claim.\nImportantly, Bob’s ‘Might’ Argument doesn’t go through with the first claim. Given that appreciation of beauty is not directly a component of welfare, and that the various channels through which appreciating beauty might lead to an increase in welfare are blocked for Bob, there is no chance that going to the gallery will increase his actual welfare. Going to the gallery will increase something, namely his appreciation of beauty, that is for all Bob knows part of welfare. But that’s not the same thing, and it isn’t relevant to rationality.\nOne caveat to all this. On some theories of welfare, it will not be obvious that even the first claim is right. Consider a view (standard among economists) that welfare is preference satisfaction. Now you might think that even the first claim is ambiguous, between a claim that one’s preferences are such that it is irrational to undermine them (plausibly true), and a claim that it is irrational to undermine one’s preference satisfaction. The latter claim is not true. If someone offers me a pill that will make me have preferences for things that are sure to come out true (I want the USA to be more populous than Monaco; etc.), it is rational to refuse it. And that’s true even though taking the pill will ensure that I do well by preference satisfaction. The point is that taking the pill does not, as things stand, satisfy my preferences. If I prefer X to Y, I should aim to bring about X. But I shouldn’t aim to bring about a state of having satisfied preferences; that could lead to rather perverse behaviour, like taking this pill.\nDuelling Analogies\nHere’s how I see the five cases we’ve discussed so far fitting together.\n\nFactual Uncertainty\nNormative Uncertainty\nPrudential\nRicky\nBob\nRisk\n\nBruce\n\n\n\nMoral\nCarla\nAgnes\nRisk\n\nMartha\nOn the left-hand column, we have agents who are uncertain about a simple factual question; is this syrup sweetener or arsenic? On the right-hand column, we have agents who are uncertain about a question about the nature of value; does the decision I’m facing right now have serious evaluative consequences?\nIt’s even easier to see what is separating the rows. Ricky, Bob and Bruce face questions that, in the first instance, just concern their own welfare. Carla, Agnes and Martha face questions that concern the morality of their actions. I don’t mean to say that there’s a hard line between these two. Perhaps being moral is an important part of the good life. And perhaps one has a moral duty to live well. I’m a little doubtful on both scores actually. But even if the questions bleed into each other in one or other way, we can separate questions that are in the first instance about the agent’s own welfare from questions that bear directly on the morality of the agent. (Recognising, as always, that there will be borderline cases.) And that’s how we’ve split the rows.\nOne way to motivate the ‘Might’ Argument is to stress the analogy between Carla and Agnes/Martha. After all, both of them risk killing someone (or something) statused if they act in a certain way. But once we look at the table more broadly, it is easy to see why we should resist the analogy between Carla and Agnes/Martha. The analogy between Bob/Bruce and Agnes/Martha is much stronger. We can see that by thinking about their motivations.\nWhy would Bruce go to the gallery? Not for pleasure; he’ll get more pleasure out of playing video games with his friends. Not for the educational value; he won’t learn more by looking at these kind of paintings again. His only reason for going is that he thinks it might increase his welfare. That is, he can only be motivated to go if he is motivated to care about welfare as such, and not about the things that make up welfare. There is something perverse about this motivation. It is healthy and natural to want the things that make up a good life. It is less healthy, and less natural, to directly desire a good life whatever that may be.\nNow think about Martha. Why should she turn down the steak? Not because she values the interests of the cow over her dining. She does not. And not because she should have that value. By hypothesis, she need not do so. (Remember we’re only interested in replying to people who argue from The ‘Might’ Argument to vegetarianism; if you think there’s a direct argument that Martha should value the cow so highly that she doesn’t eat meat, that’s a different debate.) Rather, she has to care about morality as such. And that seems wrong.\nThe argument I’m making here owes a lot to a similar argument offered for a somewhat different conclusion by Michael Smith (1994). He compared the person who desires to do what is actually right, as he put it, desires the right de re, with the person who desires to do what is right whatever that turns out to be, as he put it, desires the right de dicto.\n\nGood people care non-derivatively about honesty, the weal and woe of their children and friends, the well-being of their fellows, people getting what they deserve, justice, equality, and the like, not just one thing: doing what they believe to be right, where this is read de dicto and not de re. Indeed, commonsense tells us that being so motivated is a fetish or moral vice, not the one and only moral virtue.  (Smith 1994, 75)\n\nI think that’s all true. A good person will dive into a river to rescue a drowning child. (Assuming that is that it is safe enough to do so; it’s wrong to create more rescue work for onlookers.) And she won’t do so because it’s the right thing to do. She’ll do it because there’s a child who needs to be rescued, and that child is valuable.\nThe analogy with the welfare case strengthens this conclusion. The rational person values their health, happiness and friendships (and whatever goes into the actual list of things that constitute welfare.). They don’t simply value their welfare, and desire to increase it. That’s why it would be perverse for Bruce to go to the gallery. He would only go if he had a strange motivation. And it is why it would be perverse for Martha to turn down the steak. To do so she would have to care about morality, whatever it is, not about the list of things that Smith rightly says a good person will care about.\nAn Alternative Analogy\nMoller offers the following analogy to back up something like the ‘Might’ Argument.5\n\nSuppose Frank is the dean of a large medical school. Because his work often involves ethical complications touching on issues like medical experimentation and intellectual property, Frank has an ethical advisory committee consisting of 10 members that helps him make difficult decisions. One day Frank must decide whether to pursue important research for the company in one of two ways: plan A and plan B would both accomplish the necessary research, and seem to differ only to the trivial extent that plan A would involve slightly less paperwork for Frank. But then Frank consults the ethics committee, which tells him that although everyone on the committee is absolutely convinced that plan B is morally permissible, a significant minority - four of the members - feel that plan A is a moral catastrophe. So the majority of the committee thinks that the evidence favors believing that both plans are permissible, but a significant minority is confident that one of the plans would be a moral abomination, and there are practically no costs attached to avoiding that possibility. Let’s assume that Frank himself cannot investigate the moral issues involved - doing so would involve neglecting his other responsibilities. Let’s also assume that Frank generally trusts the members of the committee and has no special reason to disregard certain members’ opinions. Suppose that Frank decides to go ahead with plan A, which creates slightly less paperwork for him, even though, as he acknowledges, there seems to be a pretty significant chance that enacting that plan will result in doing something very deeply wrong and he has a virtually cost-free alternative. (436)\n\nThe intuitions are supposed to be that this is a very bad thing for Frank to do, and that this illustrates that there’s something very wrong with ignoring moral risk. But once we fill in the details of the case, it is clear that this can’t be the right diagnosis.\nThe first thing to note is that there is something special about decision making as the head of an organization. Frank doesn’t just have a duty to do what he thinks is best. He has a duty to reflect his school’s policies and viewpoints. A dean is not a dictator, not even an enlightened, benevolent one. Not considering an advisory committee’s report is bad practice qua dean of the medical school, whether or not Frank’s own decisions should be guided by moral risk.\nWe aren’t told whether A or B are moral catastrophes. If B is a moral catastrophe, and A isn’t, there’s something good about what Frank does. Of course, he does it for the wrong reasons, and that might undercut our admiration of him. But it does seem relevant to our assessment to know whether A or B are actually permissible.\nAssuming that B is actually permissible, the most natural reading of the case is that Frank shouldn’t do A. Or, at least, that he shouldn’t do A for this reason. But that doesn’t mean he should be sensitive to moral risk. Unless the four members who think that A is a moral catastrophe are crazy, there must be some non-moral facts that make A morally risky. If Frank doesn’t know what those facts are, then he isn’t just making a decision under moral risk, he’s making a decision involving physical risk. And that’s clearly a bad thing to do.\nIf Frank does know why the committee members think that the plan is a moral catastrophe, his action is worse. Authorising a particular kind of medical experimentation, when you know what effects it will have on people, and where intelligent people think this is morally impermissible, on the basis of convenience seems to show a striking lack of character and judgment. Even if Frank doesn’t have the time to work through all the ins and outs of the case, it doesn’t follow that it is permissible to make decisions based on convenience, rather than based on some (probably incomplete) assessment of the costs and benefits of the program.\nBut having said all that, there’s one variant of this case, perhaps somewhat implausible, where it doesn’t seem that Frank should listen to the committee at all. Assume that both Frank and the committee have a fairly thick understanding of what’s involved in doing A and B. They know which actions maximise expected utility, they know that which acts are consistent with the categorical imperative, they know which people affected by the acts would be entitled to complain about our performance, or non-performance, of each act, they know which acts are such that everyone could rationally will it to be true that everyone believes those acts to be morally permitted, and so on. What they disagree about is what rightness and wrongness consist in. What’s common knowledge between Frank, the majority and the minority is that both A and B pass all these tests, with one exception: A is not consistent with the categorical imperative. And the minority members of the committee are committed Kantians, who think that they have a response to the best recent anti-Kantian arguments.\nIt seems to me, intuitively, that this shouldn’t matter one whit. I think the extreme view I’m defending in this paper is not, in general, intuitive. But it is worth noting how counterintuitive the opposing view is in this extreme case. A moral agent simply won’t care what the latest journal articles have been saying about the relative importance of Kant’s formulation of the categorical imperative versus either contemporary variants or approaches from very different traditions. It’s possible (though personally I doubt it), that learning of an action that it violates the categorical imperative would be relevant to one’s motivations. It’s not possible that learning that some people you admire think the categorical imperative is central to morality could change one’s motivation to perform, or not perform, actions one knew all along violated the categorical imperative. At least that’s not possible without falling into the bad kind of moral fetishism that Smith rightly decries.\nSo here’s my general response to analogies of this kind, one that shouldn’t be surprising given the previous sections. Assuming the minority committee members are rational, either they know some facts about the impacts of A and B that Frank is unaware of, or they hold some philosophical theory that Frank doesn’t. If it’s the former, Frank should take their concerns into account; but that’s not because he should be sensitive to moral risk, it’s because he should be sensitive to non-moral risk. If it’s the latter, Frank shouldn’t take their concerns into account; that would be moral fetishism.\nObjections and Replies\nI’ve discussed this paper with many people, and they almost all have objections. I’m going to respond to some of the most pressing, and end with three objections that I don’t have a particularly satisfying response to. The most important objection, from my perspective, is the second; it’s what most closely links the discussion of this paper to the broader issues about normative externalism that I find most fascinating.\nObjection: All you’ve shown so far is that moral recklessness isn’t objectively wrong. But that’s trivial. There’s a sense in which ordinary recklessness isn’t objectively wrong either. What matters is that both are subjectively wrong, where this tracks what the agent believes.\nReply: Distinguish between two things: doing things that produce bad outcomes, and doing the wrong thing. Unless you are sure that actualist consequentialism is a conceptual truth, this is a conceptually coherent distinction. Among actions that produce bad outcomes, there are easily detectable distinctions we draw that seem to track whether the actions are wrong.\nIn the paper so far I’ve usually been focussed on people who are almost certain of the truth. But let’s change tack for a minute and look at people who have catastrophically false beliefs. In particular, consider Hannah and Hannibal. (I’m taking the Hannibal example from work by Elizabeth Harman (2011), who uses it for a related purpose.)\nHannah takes her spouse out for what is meant to be a pleasant anniversary dinner. It’s a nice restaurant, and there’s no reason to think anything will go wrong. But the restaurant gets bad supplies that day, and Hannah’s spouse gets very sick as a consequence of going there.\nHannibal is a 1950s father with sexist attitudes that were sadly typical. He has a son and a daughter, and makes sure to put together a good college savings fund for his son, but does not do the same for his daughter. Indeed, if he had tried to do the same for his daughter, he would not have been able to support his son as well as he actually did. As a consequence, his daughter cannot afford to go to college.\nHannah was mistaken about a matter of fact; whether the food at the restaurant was safe. Hannibal was mistaken about a moral matter; whether one should treat one’s sons and daughters equally. Now consider what happens when both see the error of their ways. Hannah should feel bad for her spouse, but there is no need for any kind of self-reproach. It’s hard to imagine she would feel ashamed for what she did. And there’s no obligation for her to feel guilty, though it’s easier to imagine she would feel some guilt. Hannibal, on the other hand, should feel both ashamed and guilty. And I think it’s natural that a father who realised too late that he had been guilty of this kind of sexism would in fact feel the shame and guilt he should feel. The fact that his earlier sexist attitudes were widely shared, and firmly and sincerely held, simply seems irrelevant here.\nThe simplest explanation of this emotional difference is that what Hannibal does is, in an important sense, wrong, and what Hannah does is not wrong. But the wrongness at issue is missing from the objective/subjective distinction the objector here makes. Both Hannah and Hannibal do things that make things objectively worse. Both Hannah and Hannibal do things that are good given their beliefs at the time they act. Yet there is a distinction between them. It’s this distinction that the normative externalist wants to stress. There’s a normative status that is not wholly objective, insofar as it doesn’t reproach Hannah, but not wholly subjective, insofar as it does reproach Hannibal.\nObjection: But still, we need a standard that can guide the agent, that an agent can live by. Do the right thing, whatever it turns out to be, is not such a standard. And what motivates internalism is the thought that this kind of agent-centred norm is most important.\nReply: If this is the motivation for internalism, it is vulnerable to a nasty regress. The problem is that internalists disagree amongst themselves, and there is no internalist-friendly way to resolve the disagreement.6 (Much of what I say here draws on arguments that Elizabeth Harman (2015) makes about the nature of internalist norms.)\nThe examples that illustrate this point are a little convoluted, so I’ll just state one example schematically to make the point. And I’ll put numerical values on options because it is hard to state the internalist views without doing this.\nAn agent faces a choice between four options: A, B, C and D. Option A is the right option, both in the sense that the externalist will praise people who take it and criticise others, and in the sense that a fully informed intrnalist would do A. But our agent is, sadly, not fully informed. She thinks A is a completely horrible thing do to. Her credences are split over three moral theories, X, Y and Z, with credence 0.5 in X, 0.1 in Y, and 0.4 in Z. The moral values of each action according to each moral theory are given by this table. (Higher values are better; non-negative values are for actions that are permissible according to the theory.)\n@RCCC@ &X&Y&Z\nB&0&0&–20\nC&0&–30&–10\nD&–1&–5&0\nSo the probability, according to the agent, that each action is permissible is 0.6 for B, 0.5 for C and 0.4 for D. The expected moral value of each action is –8 for B, –7 for C, and –2 for D.\nOur agent at this stage is a bit confused. And reading some philosophy doesn’t help. She reads Ted Lockhart (2000) saying that what she should do is the thing that is most probably permissible. And she reads Andrew Sepielli (2009) saying that what she should do is the thing that maximises expected moral value. But these pieces of advice pull in opposite directions. She could try and come up with a theory of how to resolve the tension, but that is just as hard as resolving the dispute between Lockhart and Sepielli in the first place. She eventually settles on the rule Don’t do what any plausible meta-theory says is the worst thing to do. Since Lockhart says D is the worst thing to do (having the lowest probability of permissibility), and Sepielli says that B is the worst thing to do (having the lowest expected moral value), she does C.\nHere’s the lesson of this little parable. There is a worry that externalism is not sufficiently action guiding, and can’t be a norm that agents can live by. But any philosophical theory whatsoever is going to have to say something about how to judge agents who ascribe some credence to a rival theory. That’s true whether the theory is the first-order theory that Jeremy Bentham offers, or the second-order theory that Andrew Sepielli offers. Once you’re in the business of theorising at all, you’re going to impose an external standard on an agent, one that an agent may, in good faith and something like good conscience, sincerely reject. The externalist says that it’s better to have that standard be one concerned with what is genuinely valuable in the world, rather than a technical standard about resolving moral uncertainty. But every theorist has to be a little bit externalist; the objector who searches for a thoroughly subjective standard is going to end up like Ponce de Leon.\nObjection: You’ve focussed on the case where Martha is almost sure that meat-eating is permissible. What do we say about the person who is almost sure that meat-eating is impermissible, eats meat anyway, and gets lucky, because they are in a world where it is permissible? The normative externalist says that they are beyond reproach, but something seems wrong here.\nReply: The externalist is only committed to the view that the most important evaluative concepts are independent of the agent’s beliefs. There is something rather simple to say about this person; they are a hypocrite.\nObjection: Wait a minute! We wanted something reproachful to say about this person. But all you’ve said is that they are a hypocrite, by which you presumably mean they don’t act in accord with their beliefs about what’s valuable. And Huckleberry Finn is a hypocrite in that sense, but also beyond reproach.\nReply: Good point, but I think we can still say something. Huckleberry Finn acts against what he believes to be most valuable in order to preserve a great good: Jim’s freedom. Our imagined meat-eater acts against what he believes to be most valuable in order to get a tastier lunch. Someone who will do what they believe to be wrong in order to produce a gain which is both trivial, and entirely accrues to them, reveals a bad character. The gain that Huckleberry Finn’s actions produce, note, are neither trivial nor selfish, and that’s why his actions do not indicate a character defect. But giving up on morality for a trivial, selfish gain is a sign that things will go very badly wrong, very soon.7\nObjection: How can you even acknowledge such a thing as hypocrisy? Isn’t the positing of such a norm vulnerable to the same regress arguments as you’ve run against the internalist?\nReply: No, because we can be an externalist about what is and is not hypocritical. We can, at least in theory, imagine these two cases. The first case is a person whose beliefs, credences and values indicate that the best thing to do is B, but who thinks the best thing to do given those beliefs, credences and values is C. They do C. They are hypocritical, although they (falsely) do not believe they are. The second case is a person who is exactly like this, except they do B. They are not acting hypocritically. Or, at least, they are not a first-order hypocrite. Perhaps we can recognise a distinct state of second-order hypocrisy, and say that they fall under it. And you can imagine even higher-orders. The externalist can say all of these exist. They aren’t the worst offences ever, but it is coherent to posit all of them.\nObjection: Once you recognise hypocrisy, there is a way to reinstate the ‘Might’ Argument. Martha and Agnes are hypocrites. They shouldn’t be hypocrites. So they shouldn’t eat meat, or have an abortion.\nReply: I simply deny that they are hypocrites. Compare these three statuses.\nDoing that which you disvalue.\nDoing that which you believe to be less valuable.\nDoing that which you have some credence is less valuable.\nThe first is clearly hypocrisy, and the second seems similar. But there’s no reason to say the third is hypocritical. The following example, closely modelled on one offered by Lara Buchak (2014) makes this point.\nAnnie values her close relationship with her brother Jack. One day, she receives some evidence that marginally raises her credence that Jack did something horrible. She is pretty sure Jack is innocent, but her credence in his guilt does rise a notch. Still, Annie values her relationship with Jack just as much as she did before. If Jack did the horrible thing, she would not value the relationship. But getting some (almost surely misleading) evidence that Jack did something horrible does not change her values at all.\nThe lesson here is that credences about what is valuable can quite coherently float free from valuings. There is a tricky question about what happens to beliefs about what is valuable in these cases. Buchak thinks they should go with valuings, and this is a problem for theories that reduce credence to belief. I don’t agree with this extension of her argument, but I certainly agree that small changes in credence about what is valuable need not, and often should not, change what one values.\nObjection: The externalist can’t explain why moral ignorance exculpates.\nReply: The short reply is that, following for example Elizabeth Harman (2011), I don’t think moral ignorance does exculpate. But the longer reply is that the internalist can’t explain why moral ignorance is at best an excuse, not a defence, and why it only works in special circumstances.\nWe already saw one distinctive aspect of moral ignorance above, in the Hannibal example. Hannibal should feel ashamed, and guilty, about what he did. That’s because even if he had an excuse, he did the wrong thing. And this doesn’t just mean he made the world worse. This notion of wrongness is an externalist one, even if we allow an internalist friendly excuse for the wrong action.\nBut when we turn to classic defenders of the idea that moral ignorance can be exculpatory, such as Susan Wolf (1980) and Cheshire Calhoun (1989), we see that it is meant to be an excuse with a very limited scope. And whether the circumstances are such as to furnish this excuse will not always be clear to the wrong-doer. (Indeed, it might be that they are not, and could not, be clear.) So even if moral ignorance was exculpatory, this wouldn’t be much help to the internalist. Since on everyone’s view some moral ignorance is blameworthy, and the factors that may make moral ignorance an excuse are external to the agent, only the externalist can offer a plausible theory on which moral ignorance is exculpatory.\nObjection: Even if it is fetishistic to be motivated by the good as such, this doesn’t extend to thick moral properties. Indeed, the quote from Smith you use explicitly contrasts the thinnest of moral properties with ever so slightly thicker ones. So your objections to arguments from moral uncertainty don’t extend to arguments from what we might call virtue uncertainty.\nReply: I agree with this. Here are some things that seem like be non-fetishistic motivations to avoid doing action A.\nIt would be cowardly to do A.\nDoing A would be free-riding.\nI would not appreciate if others did A-like actions that could disadvantage me.\nThe objector draws attention to the distinction between thick and thin moral properties, and I think that’s the right way to highlight what’s at issue here. But note how thin these are getting. I’m conceding that the fact that something violates the Golden Rule could be a motivation, as could the fact that it violates the categorical imperative.8 What I deny is that the wrongness of the action could be an extra motivation over and above these. This was the point of the discussion of Moller’s executive in the previous section.\nFor each of these motivations, there are cases where the risk of violating the relevant standard can be motivating. So one might not do something because there is a risk that it would be cowardly, or free-riding, or violate the Golden Rule or categorical imperative. I don’t mean to object to any argument along these lines.\nObjection: Now you’ve conceded that a version of the ‘Might’ Argument can work. After all, there are vices that might be manifest by eating meat or having an abortion.\nReply: True, but the fact that some action might manifest a vice can hardly be a decisive consideration against doing it. If the vice in question is relatively small, or the chance of manifesting it is relatively small, it is easy to see how this kind of consideration could be overridden.\nFor instance, imagine an argument for vegetarianism as follows. Eating meat you haven’t killed yourself might be cowardly. It certainly isn’t obvious that letting someone else do the dirty work isn’t a manifestation of cowardice. So that’s a reason to not eat meat. I can grant it is a reason while thinking that (a) this kind of cowardice isn’t a particularly heinous vice, and (b) it isn’t that likely that meat eating is really cowardly in this way, so the reason is a relatively weak one, that can easily be overridden.\nBut the concession I want to make is that there could be an argument along these lines that works. In earlier presentations of this paper, I’d tried to extend my argument to respond to the arguments Alex Guerrero (2007) makes for vegetarianism. But I’m no longer sure that was a good idea. But I think Guerrero’s arguments can be understood in such a way that they rely only on the idea that we shouldn’t risk instantiating certain particular vices. And I don’t have a systematic objection to every argument of this form. After all, I do think we have a reason to avoid running a risk of being free-riders, or cowards, even if the action under consideration would not be cowardly, or an act of free-riding.\nObjection: Even without getting into debates about moral uncertainty, there are other uncertainty arguments against meat eating or abortion. There is some probability that cows or foetuses have souls, and it is a very serious harm to kill something that has a soul.\nReply: Nothing I say here helps respond to this argument. If one thinks that what’s wrong with killing is that it kills a soul, thinks that there’s a non-trivial chance that cows or foetuses have souls, and eats meat or has an abortion anyway, then one really is being immoral. Whether this should be called recklessness is tricky, since one could understand ‘recklessness’ as being concerned only with risks that are in a certain sense objective. But it certainly seems that such a person would be morally on a par with the people I’ve said are immoral in virtue of the risks they pose to others. It’s an empirical question, and one I don’t have any good evidence about, whether arguments from uncertainty about abortion and meat eating primarily concern uncertainty about facts, as this objection suggests, uncertainty about virtues (broadly construed) as the previous objection suggests, or uncertainty about right and wrong.\nObjection: It may be wrong to be only concerned with right and wrong, but it isn’t wrong to have this be one of your considerations.\nReply: I don’t think you get the ‘Might’ Argument to work unless concern with right and wrong, whatever they turn out to be, are the only considerations. Assume that they are only one consideration among many. Then even if they point in one direction, they may be overridden by the other considerations. And if the ‘Might’ Argument doesn’t work, then normative internalism, in its strongest forms, is false. So I really only need to appeal to the plausible view that right and wrong as such shouldn’t be our only motivations to get the conclusions I want.\nBut actually I think the stronger, prima facie implausible, view is true: rightness and wrongness as such shouldn’t even be part of our motivation. My reasons for thinking this are related to my responses to the next three objections. Unfortunately, these are the least developed, and least satisfying, of the responses I’ll offer. But I’ll conclude with them to leave you with a sense of where I think the debate is at, and what I think future research could assist with.\nObjection: Here’s one occasion where we do seem motivated by the good as such, or by welfare as such – when we’re doing moral or prudential reflection. Sometimes we stop and think, What would be the best thing to do in a certain kind of case? In philosophy departments, people might do that solely because they’re interested in the answer. But most people will think that these projects have some practical consequences. And the strong form of Smith’s fetishism objection that you’re relying on can’t explain why this is a good practice.\nReply: I agree this is a good practice. But I think it is consistent with what I’ve said so far. Start with an observation also by Michael Smith, that moral inquiry has “a certain characteristic coherentist form”  (Smith 1994, 40–41). I think (not originally) that this is because we’re not trying to figure out something about this magical thing, the good, but rather because we’re trying to systematise and where necessary reconcile our values. When we’re doing moral philosophy, we’re often doing work that more at the systematising end, trying to figure out whether seemingly disparate values have a common core. When we’re trying to figure out what is right in the context of deciding what to do, we’re often trying to reconcile, where possible, conflicting values. But as long as we accept that there are genuinely plural values, both in moral and prudential reasoning, we shouldn’t think that a desire to determine what is right is driven by a motivation to do the right thing, or to live a good life, as such.\nObjection: Sometimes people act from moral conscience. At least by their own account, they do something that involves no small amount of personal sacrifice because it is the right thing to do. And, at least some of the time, these people are highly praiseworthy. The strong version of the fetishism objection you’re using can’t account for this.\nObjection: So I have to bite some bullets here. I have to offer a slightly unnatural reformulation of these cases. In particular, in cases where someone acts from conscience, I have to say that there is something they value greatly, and they are acting on that value. What the value is will depend on the case. It might be welfare, or freedom, or keeping promises, or justice. It might even, and this is the version of the case that’s trickiest for me, be a value they can’t clearly articulate. A person can know something is the right thing to do and not be in any position to say why it is the right thing to do. And they may do it, even at great sacrifice. I think I’m required to say here that their motivation is the feature of the act that makes it right, not the rightness of the act. That’s not optimal, especially since it isn’t how the agent themself would describe the motivation. But I don’t think we should assume that agents have perfect access to their own motivations.\nI take myself to be here largely in agreement with a line suggested by Sigrún Svavarsdóttir (1999) when she says, in defence of an externalist theory of moral motivation.\n\nThe externalist account I propose does not ascribe to the good person a particular concern with doing the right thing. Rather it ascribes to him a more general concern with doing what is morally valuable or required, when that might include what is just, fair, honest, etc.  (Svavarsdóttir 1999, 197–98)\n\nThere are two points here that are particularly relevant to the current project. The good person has a plurality of motivations, not just one. And the fetishism argument really has a very narrow application: it really only works against theories which say goodness is a matter of having the thinnest of possible moral motivations. It’s odd to be solely concerned with doing the right thing as such. (It’s even odd, I say, to have this as one of your concerns, though that’s not central to my argument.) It’s not odd to have fairness as one of one’s concerns, even an important one. Svavarsdóttir suggests that once the range of the fetishism argument is restricted in this way, it can’t do the work that Smith needs it to do in his attack on motivational externalism. I don’t need to take a stand on this, since I’m not taking sides in the debate between motivational externalists and internalists. All I need is that Smith’s objection to fetishism can work, as long as it is suitably restricted.\nObjection: Is there any coherent meta-ethical view that can licence all the moves you’ve made? On the one hand, normative claims must be distinctive enough that uncertainty about them has a very different effect on deliberation and motivation than everyday factual claims. On the other hand, your externalism is the view that the moral facts matter more than anyone’s (reasonable) beliefs about the moral facts. The first consideration suggests a strong kind of moral anti-realism, where moral claims are different in kind to factual claims. But the second suggests a strong kind of moral realism, where there are these wonderful moral facts around to do the work that reasonable moral beliefs cannot do. Is this even consistent? And if it is, is there a meta-ethical view we should want to hold consistent with all of it?\nReply: The inconsistency charge isn’t, I think, too hard to meet. As long as the ‘facts’ that I talk about when I say the moral facts matter are construed in an extremely deflationary way, then I’m not being inconsistent. Any kind of sophisticated expressivist or quasi-realist view that allows you to talk about moral facts, while perhaps not meaning quite the same thing by ‘fact’ as a realist does, will be consistent with everything I’ve said.\nThe second challenge is harder, and I don’t know that I have a good response. I would like to make the theory I’ve presented here consistent with a fairly thoroughgoing moral realism, and I’m not sure that’s possible. (I’d like to do that simply because I don’t want the fate of the theory tied up with contentious issues in meta-ethics.) I think the way to make the view consistent with this kind of realism is to defend the view that neither the metaphysical status of a truth (as necessary or contingent, analytic or synthetic, and so on) has very little to do with its appropriate role in deliberation or evaluation. But defending that, and showing how it suffices to make moral cognitivism consistent with the view I’m describing, is more than I know how to do now.\n\n\nArpaly, Nomy. 2002. “Moral Worth.” Journal of Philosophy 99 (5): 223–45. https://doi.org/10.2307/3655647.\n\n\n———. 2003. Unprincipled Virtue. Oxford: Oxford University Press.\n\n\nArpaly, Nomy, and Timothy Schroeder. 1999. “Praise, Blame and the Whole Self.” Philosophical Studies 93 (2): 161–88. https://doi.org/10.1023/A:1004222928272.\n\n\n———. 2014. In Praise of Desire. Oxford: Oxford University Press.\n\n\nBuchak, Lara. 2014. “Belief, Credence and Norms.” Philosophical Studies 169 (2): 285–311. https://doi.org/10.1007/s11098-013-0182-y.\n\n\nCalhoun, Cheshire. 1989. “Responsibility and Reproach.” Ethics 99 (2): 389–406. https://doi.org/10.1086/293071.\n\n\nFinnis, John. 2011. Natural Law and Natural Rights. Second. Oxford: Oxford University Press.\n\n\nFodor, Jerry. 2000. “It’s All in the Mind: Noam Chomsky and the Arguments for Internalism.” Times Literary Supplement 23 June: 3–4.\n\n\nGuerrero, Alexander. 2007. “Don’t Know, Don’t Kill: Moral Ignorance, Culpability and Caution.” Philosophical Studies 136 (1): 59–97. https://doi.org/10.1007/s11098-007-9143-7.\n\n\nHarman, Elizabeth. 2011. “Does Moral Ignorance Exculpate?” Ratio 24 (4): 443–68. https://doi.org/10.1111/j.1467-9329.2011.00511.x.\n\n\n———. 2015. “The Irrelevance of Moral Uncertainty.” Oxford Studies in Metaethics 10: 53–79. https://doi.org/10.1093/acprof:oso/9780198738695.003.0003.\n\n\nLockhart, Ted. 2000. Moral Uncertainty and Its Consequences. Oxford University Press.\n\n\nMalmgren, Anna-Sara. 2011. “Rationalism and the Content of Intuitive Judgements.” Mind 120 (478): 263–327. https://doi.org/10.1093/mind/fzr039.\n\n\nMoller, D. 2011. “Abortion and Moral Risk.” Philosophy 86 (3): 425–43. https://doi.org/10.1017/S0031819111000222.\n\n\nSepielli, Andrew. 2009. “What to Do When You Don’t Know What to Do.” Oxford Studies in Metaethics 4: 5–28.\n\n\nSmith, Michael. 1994. The Moral Problem. Oxford: Blackwell.\n\n\nSvavarsdóttir, Sigrún. 1999. “Moral Cognition and Motivation.” Philosophical Review 108 (2): 161–219. https://doi.org/10.2307/2998300.\n\n\nWeatherson, Brian. 2013. “Disagreements, Philosophical and Otherwise.” In The Epistemology of Disagreement: New Essays, edited by David Christensen and Jennifer Lackey, 54–73. Oxford: Oxford University Press.\n\n\nWilliamson, Timothy. 2007. The Philosophy of Philosophy. Blackwell.\n\n\nWolf, Susan. 1980. “Asymmetrical Freedom.” Journal of Philosophy 77 (3): 151–66. https://doi.org/10.2307/2025667.\n\n\nHere is one argument against the claims of the last two sentences. Assume that, as is realistic, Agnes wants an abortion because her life will be worse in significant ways if she becomes a parent (again) in the near future. And assume that Agnes has a moral duty to herself; making her own life worse in significant ways for no sufficient reason is immoral. Then it could be immoral for her to continue the pregnancy. I don’t find this reason particularly compelling; it seems to me odd to say that people who make heroic sacrifices are immoral in virtue of paying insufficient regard to their own welfare. But the issues here are difficult, and I certainly don’t have a strong argument that we should give no credence to the view that there are substantial duties to self that make misguided sacrifices on behalf of others immoral. Still, I’m going to set this whole line of reasoning aside for most of the paper, while just noting that this could be a way even for an internalist to reject the practical arguments I’ll discuss below. I’m grateful to conversations with Elizabeth Anderson here (but not only here!).↩︎\nD. Moller (2011) offers an interesting different analogy to motivate something like the ‘Might’ Argument. I think that analogy is a little messier than the one I’m focussing on, and I’ll discuss it separately below.↩︎\nIt would be a bit of a stretch to say this is Moore’s own view, but you can see how a philosopher might get from Moore to here. Appreciation of beauty is one of the constituents of welfare in the objective list theory of welfare put forward by John Finnis (2011, 87–88).↩︎\nThanks to Julia Markovits for suggesting the central idea behind the Bruce example, and to Jill North for some comments that showed the need for it.↩︎\nThough note that Moller’s own position is more moderate than what the ‘Might’ Argument suggests; he thinks moral risk should play a role in reasoning, but not necessarily so strong a role as to make the ‘Might’ Argument go through. I’m advocating what he calls the “extreme view, we never need to take moral risk into account; it is always permissible to take moral risks.” (435).}↩︎\nIn Weatherson (2013) I make a similar objection to normative internalism in epistemology. It’s this point of connection that’s made me focus on normative internalism and externalism, not moral internalism and externalism. The issues in ethics and in epistemology are very closely connected here.↩︎\nThe Huckleberry Finn case has been discussed extensively by Nomy Arpaly and Timothy Schroeder  (Arpaly 2002, 2003; Arpaly and Schroeder 1999, 2014), and I’m relying heavily on their analysis of the case in what I say here and elsewhere about Huckleberry Finn. More generally, the picture I’m assuming of moral motivation owes a lot to those works.↩︎\nTo be clear, I’m conceding that these motivations are consistent with the argument of the paper. My own view is that while realising that something violates the Golden Rule could be a motivation, as is evident from how we teach morality to children, realising that it violates the categorical imperative should not be motivating. But the argument of the paper doesn’t turn on my quirky views here. What matters is that we distinguish wrongness itself from properties like harming another person, not what other properties we group in with wrongness.\n\n↩︎\n",
    "preview": "posts/2021-01-06-running-risks-morally/bullfight.jpg",
    "last_modified": "2021-03-12T15:14:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-06-in-defense-of-the-acas-medicaid-expansion/",
    "title": "In Defense of the ACA's Medicaid Expansion",
    "description": "The only part of the Patient Protection and Affordable Care Act (hereafter, 'the ACA') struck down was a provision expanding Medicaid. We will argue that this was a mistake; the provision should not have been struck down. We'll do this by identifying a test that C.J. Roberts used to justify his view that this provision was unconstitutional. We'll defend that test against some objections raised by J. Ginsburg. We'll then go on to argue that, properly applied, that test establishes the constitutionality of the Medicaid provision.",
    "author": [
      {
        "name": "Ishani Maitra",
        "url": "https://lsa.umich.edu/philosophy/people/faculty/imaitra.html"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2013-07-01",
    "categories": [
      "political philosophy"
    ],
    "contents": "\n\nContents\nIntroduction\nSpending Power and Coercion\nThe Roberts Rule\nOntological Questions\nCoercion in Continuing Relationships\nRelation Between Programs\nReasons for Bundling\nConclusion\n\nIntroduction\nThe only part of the Patient Protection and Affordable Care Act (hereafter, ‘the ACA’) struck down in National Federation of Independent Business (NFIB) et al. v. Sebelius, Secretary of Health and Human Services, et al. was a provision expanding Medicaid.1 We will argue that this was a mistake; the provision should not have been struck down. We’ll do this by identifying a test that C.J. Roberts used to justify his view that this provision was unconstitutional. We’ll defend that test against some objections raised by J. Ginsburg. We’ll then go on to argue that, properly applied, that test establishes the constitutionality of the Medicaid provision.\nPublished in Public Affairs Quarterly 27: 267-288.\nPhoto by LaDawna Howard via Creative Commons.\nTo say just what the provision in question is, it will help to have before us the distinctive structure of Medicaid. Each state runs its own Medicaid program, with substantial financial support from the federal government. There are several conditions that a state Medicaid program must satisfy in order to qualify for this federal support, and which all fifty states do currently satisfy. In particular, there have always been minimum coverage requirements.2 Before the ACA, these minimum coverage requirements were a bit of a hodgepodge. As J. Ginsburg notes,\n\nTo receive federal Medicaid funds, States must provide health benefits to specified categories of needy persons, including pregnant women, children, parents, and adults with disabilities. Guaranteed eligibility varies by category: for some it is tied to the federal poverty level (incomes up to 100% or 133%); for others it depends on criteria such as eligibility for designated state or federal assistance programs.3\n\nThe ACA introduced a broad new category of Medicaid eligibility. It said that the states must extend Medicaid eligibility to pretty much anyone whose income is below 133% of the federal poverty level.4 In addition, it provided quite generous federal support for these newly eligible claimants: over 90% of the costs of covering these individuals would be reimbursed to the states by the federal government.5 (For other categories of claimants, the reimbursement rate is considerably lower.6) But importantly to NFIB v. Sebelius, the ACA made provision of Medicaid services to these newly eligible individuals a condition of continuing federal support. That is, if a state did not expand its Medicaid program to accommodate these newly eligible individuals, the ACA gave the Secretary of Health and Human Services the authority to withhold all of the Medicaid funds the state would otherwise be entitled to.7\nIt is this last provision that the Supreme Court found to be unconstitutional in the current case. By a margin of 7–2, the Court ruled that it was unconstitutional to make federal support for continuation of the old Medicaid program conditional on states’ participation in this expansion. We’re going to argue that that was a mistake. We’ll mostly focus on the opinion written by C.J. Roberts (and joined, in this respect, by J. Kagan and J. Breyer), and the dissent written by J. Ginsburg (and joined by J. Sotomayor).\nOur discussion in this paper takes place in a relatively constrained ideological space. Our aim is to argue against one part of the Court’s decision in this case, but to keep things manageable, we’ll do this while ignoring some very interesting philosophical and policy questions in the vicinity. Thus, for instance, we won’t concern ourselves with arguments against the constitutionality of the ACA’s Medicaid provision that, if successful, would also judge the old Medicaid program to be unconstitutional. And we’ll assume that there are constitutional constraints on what the federal government can do via its spending power, that go beyond what is explicitly stated in the U.S. Constitution. While we think these issues are very much worth pursuing, we’ll leave them aside as much as possible in this paper.\nIn what follows, it will be helpful to have some terminology for the various Medicaid requirements. We will use the following:\n‘Old Medicaid’ refers to the set of requirements and funding levels that existed prior to the passage of the ACA.\n‘Expanded Medicaid’ refers to the requirement that those not covered by Old Medicaid, but whose earnings fall below 133% of the federal poverty line, now be covered, plus the set of requirements and funding levels for these newly eligible individuals stipulated by the ACA.\n‘New Medicaid’ refers to the conjunction of Old Medicaid and Expanded Medicaid, i.e., Medicaid as it was envisioned to work after the passage of the ACA.\n‘The ACA’s Offer’ refers to the ACA’s offering the states the option of participating in (all of) New Medicaid, or not participating at all, but not the option of participating in just Old Medicaid.\nOne question that became surprisingly central to the ruling in NFIB v. Sebelius was the ontological relationship between Old Medicaid and New Medicaid. As we’ll see, J. Ginsburg held that these were the same program; the addition of Expanded Medicaid was just one of the many modifications that have been made to Old Medicaid over the years, without destroying its identity.8 C.J. Roberts disagreed, arguing that the Medicaid expansion provision “accomplishes a shift in kind, not merely degree.”9\nWe’re not going to take a strong stand on this ontological question. That’s partly because we think that questions like this are the wrong kinds of questions to be asking here. We don’t really have views on the criteria of identity through time for federal-state cooperative programs. And we’re not convinced that these questions have determinate answers. But even if they did, we still wouldn’t think that these answers were relevant to the constitutionality of proposed changes/supplements to those programs. Rather, on our view, what matters is the functional relationship between the old and new programs.\nWe’ll have more on this presently. But first, we need to look at why the Court thought the relevant sections of the ACA should be struck down.\nSpending Power and Coercion\nThe Spending Clause of the U.S. Constitution gives Congress the power “to pay the Debts and provide for the … general Welfare of the United States.”10 The justices in NFIB v. Sebelius agreed that the Spending Clause gives Congress a “broad authority” to interpret what that “general Welfare” consists in, and to apportion funds accordingly.11 This includes the power to offer the states funds as an inducement to take certain actions - such as establishing and operating certain programs - that accord with Congress’ understanding of the “general Welfare.” Old Medicaid is just such a federal-state cooperative program, established by Spending Clause legislation.\nWhat the Spending Clause does not allow is for the federal government to require the states to implement a particular federal-state cooperative program, or to accept the associated funding package. That includes either directly ordering the states to do these things, or ‘indirectly’ coercing the states into doing them. As we’ll see in the next section, one of the main issues in NFIB v. Sebelius is whether the ACA’s Offer constitutes an attempt to unconstitutionally coerce the states into realizing a federal spending objective.12\nThe justices in NFIB v. Sebelius emphasized that Spending Clause legislation has the nature of a contract.13 The federal government offers the states funds conditional on their satisfying certain conditions. That is, it offers them money in exchange for doing something. That looks like a contract. And, arguably, some contracts are coercive.\nWhen we talk about contracts being coercive, we don’t mean that there is coercion involved in getting one of the parties to accept the offer. Rather, we mean that there is something about the offer itself that makes it coercive. Call coercion of the first kind ‘coercion alongside the contract.’ In simple cases, where the parties have roughly equal power, there can be coercion of this sort - i.e., coercion alongside the contract - but it’s hard to see how the offer itself can be coercive. And in such cases, as long as there is no coercion alongside the contract, the party to whom the offer is made can simply refuse it. By contrast, when the parties are unequal in power, the mere making of the offer can sometimes amount to an abuse of the extra power. We’ll illustrate this point with some examples in section 5. For now we want to note two further points about the notion of coercion at issue here.\nFirst, the question of whether a particular piece of Spending Clause legislation is unconstitutionally coercive should be distinguished from the question of whether that legislation runs contrary to the U.S. system of federalism. If the federal government offered each state a million dollars to introduce a filibuster rule into their state Senate procedures (perhaps because the U.S. Senate was embarrassed to be so idiosyncratic), that would arguably be unconstitutional. But that’s not because the offer would be unconstitutionally coercive. Rather, it would be because the Spending Clause isn’t an open invitation to let the federal government interfere with every power a state has, including powers over their own legislative procedures. To put it another way, the Spending Clause doesn’t allow the federal government to do an end run around the system of federalism.\nSecond, even if an offer is judged to be unconstitutionally coercive, it doesn’t follow that it will be voided. In NFIB v. Sebelius, the Supreme Court did not strike down the federal government’s attempt to expand Medicaid. Rather, it amended the terms of the offer so they were no longer unconstitutionally coercive. Rather than states having a choice between New Medicaid and nothing, the Court held that they would have a choice between Expanded Medicaid, Old Medicaid, and nothing. This isn’t the usual remedy when a contract is found to be coercive; usually, it’s simply voided.14\nThe Roberts Rule\nAs we discussed in the previous section, the Spending Clause permits the federal government to offer the states financial inducement to implement particular programs, as long as the offer itself isn’t coercive. Typically, this just means that the states must have the option not to participate. As C.J. Roberts put the point,\n\nIn the typical case we look to the States to defend their prerogatives by adopting “the simple expedient of not yielding” to federal blandishments when they do not want to embrace the federal policies as their own. ... The States are separate and independent sovereigns. Sometimes they have to act like it.15\n\nBut sometimes, merely having the option not to participate is not enough. That’s the case if, for example, there’s something about the nature of the federal government’s offer that means that the states don’t have a genuine choice about participating. According to C.J. Roberts, there were three aspects of the ACA’s Offer that, together, made it the case that the states didn’t have a genuine choice about accepting. So, the offer was unconstitutionally coercive.\nFirst, the ACA’s Offer conditioned the granting of funds for an already existing program (Old Medicaid) on states’ participation in another program (Expanded Medicaid). Call this phenomenon - i.e., conditioning the funding for an existing program on states’ participation in another program - ‘bundling.’ Bundling is importantly different from conditioning the funding for a program on states’ willingness to operate that very program in some particular way. The latter, according to C.J. Roberts, is just the federal government providing for the “general Welfare” as permitted by the Spending Clause.16\nBut second, even bundling may be permitted when it serves some legitimate purpose. The bundling involved in the ACA’s Offer, however, “serves no purpose other than to force unwilling States to sign up for the dramatic expansion in health care coverage effected by the Act.”17 To put the point another way, the only purpose of the bundling proposed by the ACA was to force the states to participate in Expanded Medicaid.18\nFinally, the states’ financial stake in participating (or not) in the bundled programs was enormous. Federal support for Old Medicaid makes up more than 10% of the typical state’s budget.19 Threatening the states with losses of that magnitude was, according to C.J. Roberts, a kind of “economic dragooning.”20 When the financial stake at issue is so large, the states have no genuine choice about participating.\nWhy think that these features of the ACA’s Offer are sufficient to constitute it as unconstitutionally coercive? C.J. Roberts doesn’t spell out his reasoning here, but perhaps the thought is this. Bundling of federal-state cooperative programs can serve various purposes. For example, bundling can encourage the states to participate in a pair of programs, where the existence of each program helps the other one operate more efficiently. Or else, bundling can encourage the states to implement a new program that helps the existing program achieve its aims better. When bundling serves these (and other) legitimate public policy purposes, it’s plausible that the federal government is attempting to provide for the general Welfare in accordance with the Spending Clause.\nIf, on the other hand, the only purpose served by the bundling is to get the states to participate in one of the bundled programs, and further, the financial inducement offered is so large that it effectively serves as a “gun to the head” of the states, then that just amounts to the federal government requiring the states to participate in a particular program.21 And as we observed in the previous section, that’s not permitted under the Spending Clause. (We’ll have much more to say about this sort of reasoning in later sections of this paper.)\nIn sum, then, C.J. Roberts argued that the ACA’s Offer was unconstitutionally coercive because it satisfied the following three conditions:\nThe offer bundles two independent programs (Old Medicaid and Expanded Medicaid);\nThat bundling serves no other purpose than to force the states to participate in one of the bundled programs (Expanded Medicaid);\nFailure to participate in the bundled programs exposes the states to enormous financial loss, and so, constitutes a kind of “economic dragooning.”\nAs we understand the Roberts Rule, conditions (1)-(3) are not only jointly sufficient for unconstitutional coerciveness, but each of them is individually necessary as well. To see why, we can consider the conditions in turn.\nWithout (1), the states could potentially opt out of any modification to an existing federal-state cooperative program. Imagine that the federal government established new standards for effectiveness of cancer treatments, and decided that Medicaid would henceforth only fund programs that complied with those new standards. The new parts of this Medicaid package are bundled together with the old parts to force the states to comply, and failure to participate in the bundled programs exposes the states to enormous loss. But this isn’t unconstitutionally coercive; the imagined policy could just be a good instance of quality control on government spending.22\nWithout (2), federal government actions that by hypothesis have a legitimate purpose could be ruled out. That would be a bad result. Here’s one sort of case that illustrates this point. Imagine that the federal government in its wisdom subsidizes widget production through conditional grants to the states. The government then discovers that widget production has serious environmental consequences. So it decides to fund cleanup operations, again through conditional grants to the states. It seems reasonable to bundle these two grants together, since the federal government has a legitimate interest in not subsidizing a certain industry without also subsidizing the elimination of its external costs. But that could be true even if widget production and the cleanup operations look like independent programs, and each is a signifiant component of state budgets.\nFinally, (3) is needed to ensure that South Dakota v. Dole is not overturned.23 That case concerned the introduction of a new condition on federal highway funding, namely, that the states enforce a drinking age of 21. States that failed to comply with this condition stood to lose up to 5% of the highway funds they would otherwise be entitled to. Several states, including South Dakota, argued that this was unconstitutionally coercive. And they had a point; setting the drinking age and repairing highways look like quite independent programs. Further, it was clear that the point of the bundling was simply to get the states to comply with the federal government objective of raising drinking ages to 21. But C.J. Roberts held that this didn’t matter, because the threatened financial loss for noncompliance with the federal objective was small enough so as not to amount to unconstitutional coercion.24\nOur reading of C.J. Roberts’ argument - and in particular, of his test for unconstitutional coerciveness - is quite similar to J. Ginsburg’s.25 But there’s one point of difference. While J. Ginsburg includes conditions very much like our (1)-(3) in her reading of the test, she adds a further condition as well.\nThe expansion (Expanded Medicaid) was unforeseeable by the states when they signed onto the already existing program (Old Medicaid).26\nWe agree with J. Ginsburg that C.J. Roberts commits himself to the truth of (4).27 But we don’t think that (4) is relevant to the issue of whether the ACA’s Offer is unconstitutionally coercive (and it’s not clear to us that C.J. Roberts does either).28\nAs we see it, (4) speaks to a different question, namely, whether Medicaid program as envisioned by the ACA (New Medicaid) is the same program as the already existing one (Old Medicaid). And as we’ll argue next, that ontological question is irrelevant to whether the ACA’s Offer is unconstitutionally coercive.\nOntological Questions\nRecall that the Roberts Rule says that an offer by the federal government to help establish a federal-state cooperative program is unconstitutionally coercive if the following conditions are satisfied:\nThe offer bundles two independent programs;\nThat bundling serves no other purpose than to force the states to participate in one of the bundled programs;\nFailure to participate in the bundled programs exposes the states to enormous financial loss, and so, constitutes a kind of “economic dragooning.”\nCondition (1) requires that that the programs be independent, not that their conjunction (in this case, New Medicaid) be distinct from the previously existing program (Old Medicaid). Thus, the Roberts Rule, as we understand it, places no emphasis on whether the federal government’s offer creates a new program, or merely modifies an already existing program. We’ll have a lot more to say about independence later (in section 6), but it should be clear that whether two programs are independent, and whether conjoining them creates a new program, are different questions. Independence has to do with how the programs function, what they do, not with their ontological status, what they are. So, even if New Medicaid turned out to be the same program as Old Medicaid, the two major parts of New Medicaid - Old Medicaid and Expanded Medicaid - could be quite independent of each other.\nAt first reading, it seems that both C.J. Roberts and J. Ginsburg take the ontological question (about whether New Medicaid is the same program as Old Medicaid) seriously. For instance, C.J. Roberts notes, in what seems like a positive way, the states’ claim that “the expansion is in reality a new program and that Congress is forcing them to accept it by threatening the funds for the existing Medicaid program.”29 And he says that Ginsburg’s reply, which assumes New Medicaid and Old Medicaid are the same program, “begs the question” against the states.30 But we think that while J. Ginsburg does take a stance on the ontological question, C.J. Roberts in the end does not. And on this point, we side with the latter, at least to the extent that we think that the ontological question is irrelevant to the issue of unconstitutional coerciveness.\nTo see why, consider a rival test to the Roberts Rule which says that what matters for unconstitutional coerciveness is that the programs in question (say, A and B) be numerically distinct. Would we get a rule that is better than the Roberts Rule? Actually, that breaks down into two questions. First, would the revised rule make for better law? And second, would the revised rule be a better interpretation of C.J. Roberts? We answer both questions negatively, with the first negative answer being some part of our reason for the second negative answer.\nQuestions about individuation criteria, and criteria of identity over time, for governmental programs are rather hard. We might think we could make progress by looking at the area where philosophers have made the most thorough investigation of identity criteria - namely, personal identity - and carrying the lessons from there over to debates about identity of governmental programs. But this would be useless twice over. For one thing, the debates about personal identity are so far from being settled that we have little to go on. For another, different views are going to be plausible in the two cases. A thoroughgoing conventionalism about personal identity is a rather unpopular view (though it is ably defended by Caroline West).31 But conventionalism about identity criteria for conventionally established programs, like Medicaid, seems much more plausible.\nSo in general we start knowing very little about identity criteria for governmental programs. But what we do know should give us pause before putting identity criteria into a substantial legal rule. It will often be indeterminate whether A and B are the same program, or different ones. (Governmental programs provide as clear an example of indeterminate identity as anything in Terrence Parsons’s study of indeterminate identity.)32 If we made the identity, or otherwise, of A and B relevant to whether a particular law was unconstitutional, we would risk concluding that it is indeterminate whether that law is unconstitutional. That doesn’t feel like an acceptable outcome.\nIn the next section, we’ll consider a thought experiment suggested by J. Ginsburg about what would have happened if Congress had repealed Old Medicaid and then enacted New Medicaid as a replacement.33 We’ll argue that the thought experiment isn’t particularly revealing, because it differs from what actually happened in a striking way. But perhaps we should question this assumption. Why should we say that the ACA merely enacted Expanded Medicaid, rather than saying that it actually repealed Old Medicaid, and enacted New Medicaid in its place? Indeed, if such a reading would make the ACA constitutional (as J. Ginsburg suggests), wasn’t the Court obliged to read the Act that way?34\nThe relevant reason is presumably that neither the ACA, nor the debate around it, reads like it was repealing and replacing Old Medicaid. That is, the rhetoric around the ACA reads like it was an expansion of Old Medicaid, not a replacement of it. And that’s enough to make it be the case that the ACA merely enacted an expansion of Old Medicaid, not a replacement of it. This talk about how the rhetoric matters to the ontological description of what the ACA did is part of what we meant above by saying that a conventionalist theory of identity for governmental programs is plausible. We all treated the ACA as expanding, not replacing, Old Medicaid, and hence it really was an expansion, not a replacement, of Old Medicaid. That’s so even though a functionally equivalent Act could have replaced Old Medicaid.\nBut while this kind of rhetoric can matter for ontological questions, it can hardly matter for the constitutional legitimacy of the ACA. C.J. Roberts held that one central part of the ACA, namely, the individual mandate, was a valid exercise of the taxing power, even though it was never marketed as such during Congressional debates.35 And that seems right to us. What matters for constitutionality is whether Congress has a power, not how they talk about their powers.36 But to make the ontological relationships between Old Medicaid, Expanded Medicaid, and New Medicaid relevant to the constitutionality of the latter would be to grant this talk, these “recitals of the powers,” too much significance. It’s hard to see that that is right, or that C.J. Roberts, in the very opinion where he upheld the individual mandate as an exercise of the taxing power, would do that.\nSo we conclude that the right way to read C.J. Roberts’ test is in terms of the relationship between what A and B do, not what A and B are. That’s how we take his comments that what matters is that the Medicaid provision in the ACA brought about a “shift in kind, not merely degree,” which would “transform it [i.e., Medicaid] so dramatically.”37 Looking at what Congress is trying to do is a much better guide to the constitutionality of its actions than looking at its talk.\nHaving said all that, we suspect that if one did take ontological distinctness to be important in testing for constitutionality, then the Medicaid provision of the ACA would be constitutional. It’s actually rather tricky to motivate a position on the ontology of Medicaid that makes that provision problematic. To see this, note that there are three ontological options here. (At least, there are three determinate options; there are also views on which the truth is indeterminate between these.)\nFirst, it might be that New Medicaid and Old Medicaid are the same program, and adding Expanded Medicaid is just a familiar way for Medicaid to grow. That’s roughly J. Ginsburg’s position.38 Second, it might be that New Medicaid and Old Medicaid are distinct programs, with New Medicaid having two components - Old Medicaid and Expanded Medicaid - that run in parallel alongside each other. On this option, New Medicaid and Old Medicaid are no more one program than Social Security plus the Defense Department are one program. This feels like the option most in tune with the Court’s ruling. Third, it might be that Old Medicaid was repealed by the ACA, and New Medicaid is a new program put in its place. We’ve discussed this possibility a couple of times already, and will have more to say about it in the next section.\nNow to get an ontological view that supports C.J. Roberts’s ruling (on the assumption that ontology is constitutionally relevant), we’d have to say that New Medicaid is sufficiently different from Old Medicaid that the first option is ruled out.39 But we couldn’t say that they are so different that the third option becomes the only plausible one.40 A middle ground has to be found, and that ground doesn’t look stable to us. Note in particular that Expanded Medicaid is a program whose eligibility is defined largely in terms of who is not eligible for Old Medicaid. It isn’t that Expanded Medicaid is for everyone earning less than 133% of the poverty line. Rather, it’s for everyone earning less than 133% of the poverty line, who wasn’t already covered by Old Medicaid. That’s a little odd. It’s especially odd because many Old Medicaid supported state programs were already more generous than the federal minimums, and so already covered many of the people who fall under Expanded Medicaid.41\nNow ultimately none of these ontological speculations matter. But perhaps the underlying considerations do matter a little. The oddness of thinking of New Medicaid as a distinct program from Old Medicaid will be reflected in the oddness of operating it as one. And as we’ll consider in sections 6–7, that latter oddness is relevant to the test C.J. Roberts set out.\nBefore turning to applications of the Roberts Rule, however, we’ll consider an important objection to the Rule itself. We’ll argue that the objection is unsuccessful, but that it highlights some important aspects of the Rule, including the vitality of its third condition.\nCoercion in Continuing Relationships\nIn her dissent, J. Ginsburg offered the following thought experiment.\n\nConsider also that Congress could have repealed Medicaid ... Thereafter, Congress could have enacted Medicaid II, a new program combining the pre-2010 coverage with the expanded coverage required by the ACA. By what right does a court stop Congress from building up without first tearing down?42\n\nWe take the point of this thought experiment to be something like this. There would have been nothing unconstitutionally coercive about Congress repealing (Old) Medicaid, and enacting Medicaid II. But the end product of that repeal-and-replace effort (Medicaid II) would have been functionally equivalent to New Medicaid, though arrived at in a different manner. So, if enacting Medicaid II by repealing-and-replacing would have been constitutional, enacting New Medicaid by just expanding Old Medicaid (and so, altering an already existing agreement) must be constitutional as well. Therefore, any test that holds the ACA’s Offer to be unconstitutionally coercive - such as the Roberts Rule - should be rejected.\nWe think this is a bad way to assess whether a proposed use of Congress’ spending power is unconstitutionally coercive. To see why, it will help to consider two examples of agreements. The first involves contracts, as C.J. Roberts suggests we should understand agreements between the federal government and the states. The second is a more informal agreement. What the two agreements have in common is that they’re both continuing agreements. As we’ll argue below, that’s significant for assessing their coerciveness.\n\nThe Monopsonist\nSupplier makes widgets, and their largest customer by far is MegaCorp. For many years, MegaCorp has had an annual order for one million widgets. The price that MegaCorp pays has basically tracked inflation since the deal was first established, and they now pay $10 per widget. This is a decent deal for Supplier, since it costs them $8 to make each widget. Although the supply contracts are explicitly only for a year at a time, it is generally understood that the contracts will be renewed, and the norm in the widget industry is that these contracts are renewed.\nOne year, MegaCorp says it is only interested in continuing the deal if Supplier also sells it a million gimcracks for $10 per gimcrack. This isn’t a great deal for Supplier, since it costs $11 to make each gimcrack. But Supplier will likely go out of business without the deal to sell widgets to MegaCorp.\n\nWe think MegaCorp’s proposal is coercive. Supplier has no real choice but to take on an extra supply contract that does not even cover their costs. And we think that the offer is coercive even though the combined offer MegaCorp makes, namely, $20 million for a million widgets and a million gimcracks, is not a bad deal for Supplier. Indeed, Supplier stands to profit on the combined deal. Nevertheless, the newly added part of the deal is basically a gift to MegaCorp, and MegaCorp is using their monopsony power to extract that gift. That makes the deal coercive.\nThe inequality in power between MegaCorp and Supplier matters here. Had Supplier been flooded with potential buyers for their widgets, MegaCorp could still make an offer of “Widgets and gimcracks, or nothing,” but they wouldn’t be in a position to make that offer credible. That’s because it isn’t credible, in the envisaged circumstances, that if Supplier had countered with an offer of “Widgets at the old price, and nothing more,” MegaCorp would have stuck to their guns and refused the mutually beneficial deal.43 Under those circumstances, the offer might not have been coercive. But those are not the circumstances in our example.\nMany of the same points apply to our second example of a continuing agreement.\n\nThe Conditional Philanthropist\nChild has recently graduated college, and has her first job. As with many first jobs, the pay is not fantastic. But it’s enough to afford a (barely) tolerable apartment in a safe enough neighborhood. Her Parent offers to pay the difference in rent that would allow her to live in a nicer apartment in a safer neighborhood, and Child takes up this offer. We assume that Parent is not under any obligation to do this; Child’s living situation without parental support is sub-optimal, but acceptable. It’s just a nice gift from Parent.\nSome years later, after Child has established roots in the neighborhood that she can live in thanks to Parent’s gift, Parent informs her that he won’t keep providing financial support unless she agrees to assist with one of Parent’s political causes. As it happens, it is a cause that Child does not agree with.\n\nWe think this offer is also coercive. It would have been acceptable for Parent to simply never provide support for Child’s rental expenses. It would also have been acceptable for Parent to make clear from the start that the offer of financial support was conditional on reciprocal political support. Making an offer like that would be distasteful, and frankly strikes us as an appalling way to relate to one’s own child. But if Child could have an acceptable standard of living without this extra money, it isn’t coercive to offer her a little more money in exchange for political support. Once the arrangement has commenced, though, and Child has structured her life around it, threatening to take it away unless Child supports a political cause does seem coercive.\nAnd this is why we think J. Ginsburg’s thought experiment fails. It’s true that it would be constitutional for Congress to repeal Old Medicaid. It’s also true that had Old Medicaid never existed, and Congress had enacted New Medicaid all at once in the ACA, there would be no constitutional question here. J. Ginsburg suggests that this is enough to show that the expansion is constitutional.\nBut the facts in J. Ginsburg’s thought experiment aren’t the facts at hand. The existing federal support for Old Medicaid creates an important kind of relationship between the states and the federal government. The states have structured a significant part of their operations around the (quite reasonable) assumption that this relationship would persist. Requiring the states to do something new in exchange for the preservation of that relationship is potentially coercive for just the same reasons that Parent and MegaCorp’s offers are coercive.\nPut another way, in the context of a continuing relationship, particularly one in which there is an imbalance in power, we have to look at how the relationship is changing, and not just at the end result, to see whether we have a case of coercion. Even if the federal government’s offer of New Medicaid, appearing as a deux ex machina, would have been constitutionally acceptable, it doesn’t follow that adding Expanded Medicaid to Old Medicaid (in the way the ACA does) is acceptable.\nOur argument in this section highlights the importance of condition (3) of the Roberts Rule. That condition focuses on the possibility of “enormous financial loss.” If Old Medicaid and Expanded Medicaid were both first enacted as part of the ACA, and they were bundled in the sense that a state was not free to participate in one but not the other, then this bundling would not expose the states to any losses. Rather, the bundling would just make it a little harder for the states to receive funds offered by the federal government. But the fact that Old Medicaid existed, and had been incorporated into the states’ financial planning, means that a threat to any state’s continued participation is a threat of loss to that state, as required under condition (3) of the Roberts Rule. And as we’ve been arguing, this is signifiant for assessing coerciveness.\nWhen we said that MegaCorp’s and Parent’s offers were coercive, we were not offering an opinion about whether there are, or should be, legal remedies available to Supplier or Child. It could well be argued that the potential costs of involving the courts in relationships like these outweigh the costs of allowing some coercive offers to be made.\nBut when we look at legislation that alters the relationship between the states and the federal government, it is more plausible that there is a role for the courts in preventing coercion. If one level of government is coercing others, that is not something that should be allowed to stand. In fact, allowing it to stand seems incompatible with the U.S. system of federalism.\nSo we reject this argument of J. Ginsburg’s. As we read her, this is the only objection she makes to the test C.J. Roberts proposes. As we’ll see in the next section, she makes several further points that can be used as objections to his application of the test. We will endorse some of those objections. But she doesn’t appear to offer other objections to the test itself. And we too will assume, from here on, that the Roberts Rule is a good test for striking down proposed uses of Congress’ spending power on grounds of unconstitutional coerciveness.\nThis is primarily a paper on constitutional questions, so we’ll keep this digression brief. But we do want to note that our disagreement with J. Ginsburg here has wider ramifications. It is common in several walks of life to have agreements between two parties that are year-to-year on paper, but are expected by both parties to continue somewhat indefinitely. Many employment arrangements are like that. And, although less common in the United States, arrangements to rent housing in many parts of the world are also like that. In those cases, when considering whether proposed changes to the relationship by the more powerful party (usually the employer or the landlord) are coercive, we think it’s important to look at the changes themselves, and not just to whether the new agreement would be acceptable taken on its own. The same kind of reasoning should apply to continuing agreements between the states and the federal government.\nRelation Between Programs\nWhile we agree that an expansion of Medicaid could be unconstitutionally coercive, we don’t think the expansion envisioned in the ACA actually is. Further, we think that the Roberts Rule, properly applied, gets this result. In this section, we’ll argue that Old Medicaid and Expanded Medicaid are sufficiently closely related that the first condition of the Roberts Rule is not satisfied. In the next section, we’ll argue that there is legitimate reason to bundle the two programs together, so the second condition is also not satisfied. Thus, even accepting the Roberts Rule as a good test for unconstitutional coerciveness, the ACA’s Offer turns out to not be unconstitutionally coercive.\nHere’s the Roberts Rule once more. It says that an offer by the federal government to help establish a federal-state cooperative program is unconstitutionally coercive if the following conditions are satisfied:\nThe offer bundles two independent programs;\nThat bundling serves no other purpose than to force the states to participate in one of the bundled programs;\nFailure to participate in the bundled programs exposes the states to enormous financial loss, and so, constitutes a kind of “economic dragooning.”\nA pair of federal-state cooperative programs may be independent in at least two different senses: first, if the programs have substantially different purposes; and second, (even) if they have the same (or closely related) purposes, but attempt to achieve those purposes in substantially different ways.\nWhen it comes to Old Medicaid and Expanded Medicaid, that the programs are not independent in either of the senses just outlined seems obvious on its face. After all, the two programs share an overall purpose; both have the aim of improving access to health services for the neediest Americans. In fact, as we’ve already mentioned (in section 4), eligibility criteria for one of the programs (Expanded Medicaid) is defined partly in terms of ineligibility for the other one. That suggests that the programs are designed to work together to achieve their overall purpose. Moreover, they try to achieve this purpose in the same way, via the same circuitous means; both feature the federal government encouraging the states to provide health care to their poorest residents by paying a (large) percentage of the costs, conditional on the states meeting certain conditions for minimum care. That looks like enough to make it the case that the programs are closely related, contra condition (1) of the Roberts Rule.\nObviously, C.J. Roberts disagreed with this assessment. We find in his opinion four considerations that might be used to argue that the programs are not suitably related.44 The first two purport to identify significant differences in purpose between Old Medicaid and Expanded Medicaid, while the third and fourth point to differences in how they’re intended to operate. None of these strikes us as persuasive. (Three of these considerations were also discussed by J. Ginsburg in her dissent, and as will be clear below, we largely sympathize with her responses.45)\nThe first consideration is that Old Medicaid covered discrete “categories of the needy: the disabled, the blind, the elderly, and needy families with dependent children.”46 But Expanded Medicaid had a somewhat blunter condition of eligibility, namely, that the claimants be earning less than 133% of the federal poverty level (and not already be covered by Old Medicaid). It’s hard to see how this amounts to a source of unrelatedness. It’s as if C.J. Roberts thought of Old Medicaid as not merely having a disjunctive essence, but as being essentially disjunctive. He seems to be suggesting that any program that didn’t have a long list of eligibility criteria could not be closely related to Old Medicaid. This strikes us as absurd, especially since the list of eligibility criteria for Old Medicaid were hardly arbitrary or ad hoc. It was meant to reflect, as C.J. Roberts recognized, ways of being especially needy. A simpler means of determining need would accomplish the same thing.\nThis brings us to the second consideration, that by covering up to 133% of the poverty level (rather than merely covering the discrete categories mentioned in the previous paragraph), Expanded Medicaid was no longer reserved for the neediest among us. But a family of four has to earn less than $31,322 to qualify under this condition.47 To exclude those earning so little from among the neediest seems bizarre.48\nOf course, it would be possible to keep expanding Medicaid, in something akin to the manner envisioned by the ACA, until it no longer covers just the neediest, but more closely resembles a program of universal health care. And it may even be indeterminate where the line between these lies. But C.J. Roberts offers no reason to think that Expanded Medicaid crosses this line. Moreover, in light of well-known criticisms of the federal poverty measure - criticisms which charge that the measure, developed in the 1960s, is now outdated and significantly undercounts poverty in the U.S. - there’s good reason to think that 133% of the federal poverty level is not an especially generous threshold.49\nThe third consideration is that the federal subsidies for Expanded Medicaid are more generous than those for Old Medicaid.50 But it’s hard to see how the generosity, or lack thereof, of the federal government speaks to the relatedness of the two programs. Would more stinginess on the part of the federal government have made the two programs more closely related?\nAnd the final consideration C.J. Roberts offers is that the conditions imposed on the states by Old Medicaid and Expanded Medicaid are different. But as J. Ginsburg notes, things aren’t quite so simple. While it’s true that the conditions imposed under Expanded Medicaid are different from those traditionally required under Old Medicaid, they aren’t any different from what has been required under Old Medicaid since 2006. So Expanded Medicaid isn’t different, in this sense, from Medicaid as it was at the time ACA was passed.51\nNone of this is meant to deny that there are differences between Old Medicaid and Expanded Medicaid. But by itself, that doesn’t tell us much about whether the ACA’s Offer was unconstitutionally coercive. Condition (1) of the Roberts Rule requires not merely that programs in question be different in some way(s), but that they be independent. We’ve been arguing that the differences cited by C.J. Roberts don’t speak to independence of the two programs in any relevant sense.\nReasons for Bundling\nIn this section, we describe a further way in which the ACA’s Offer fails to be unconstitutionally coercive under the Roberts Rule. We argue that the federal government has at least three legitimate reasons for bundling the programs together. Thus, it’s not true that the only reason for the bundling is to force the states to participate in Expanded Medicaid, contra condition (2) of the Roberts Rule.\nThe first (and more minor) reason concerns the complexity that will arise when future modifications are made to Medicaid. As we’ve already had reason to note, Medicaid requirements are not set in stone.52 They change, often substantially, from year to year. But the Court’s decision in NFIB v. Sebelius has just made it considerably more complex to make any such changes. As things stand, if Congress wants to enact changes to both Old Medicaid and Expanded Medicaid, it will have to apply the change to each program separately.\nOf course, this point is related to our discussion in the previous section; if the two programs were genuinely unrelated, we wouldn’t expect many changes that would apply to both programs. But given that the two programs work in substantially similar ways, at least some such changes - maybe even many - will be forthcoming.\nThe second reason concerns potential savings for the states. Though both C.J. Roberts53 and the authors of the Joint Dissent54 emphasized the extra financial burden Expanded Medicaid would impose on the states, there’s ample research to suggest that that burden is in fact quite minor.55 For example, the Center on Budget and Policy Priorities estimated that the expansion would cost the states just 2.8% more than they would otherwise spend on Medicaid between 2014 and 2022.56 Even more strikingly, that figure overstates the increase in state spending once we add in the savings to the states from no longer having to provide uncompensated care to those currently uninsured (or underinsured). The Urban Institute estimated that the states would end up saving money - anywhere from $92 to $129 billion between 2014 and 2019 - by taking up the ACA’s Offer.57 Those are savings that can be used by the states to bolster their other health care programs, including Old Medicaid. So, implementing Expanded Medicaid might in fact put the states in a position to run Old Medicaid better.\nThe third reason has to do with improving health outcomes generally. There’s research suggesting that high rates of uninsurance in a community adversely affect health outcomes for everyone there, including those with insurance.58 The difficulty and expense involved in treating the uninsured when they are finally driven to seek health care tends to reduce the quality of care that might otherwise be available for the insured. So, by reducing the ranks of the uninsured, Expanded Medicaid can help states achieve better health outcomes via their other health care programs, including Old Medicaid.\nIn assessing the costs and benefits of programs like Medicaid, it’s important to remember the ways in which basic health care is unlike many other goods. In particular, even if a state chooses not to participate in something like Expanded Medicaid, it still must share the financial burden of providing health care to the population that would have been covered by such a program. States still have to provide emergency rooms, and in practice, are not compensated for a significant portion of the care these provide to the uninsured. If some of the people who move onto Expanded Medicaid were previously uninsured, a state could see an overall reduction in its health care spending. Some of these savings would come from moving state costs onto the federal component of Medicaid. But some would come from the fact that patients would be moving from a form of health care that’s very expensive to provide, i.e., emergency room care, to the more efficient forms that are mostly available to the insured.59 These savings could strengthen Old Medicaid, and the health system as a whole. So expanding Medicaid could make Old Medicaid more financially self-sufficient.\nAgain, the fact that the programs are not unrelated is relevant. It’s because these are both health care programs - and moreover, health care programs for those who might otherwise be uninsured - that it’s plausible to think that savings incurred in one program strengthen the other program, rather than just strengthen the federal government’s balance sheet.\nSo the expansion of Medicaid fails two of Roberts’ three criteria for being unconstitutionally coercive. That’s why we can agree with his test, but disagree with the conclusion that the expansion was unconstitutionally coercive.\nConclusion\nWe’ve argued in this paper for several conclusions:\nThe ontological relationship between Old Medicaid and New Medicaid is irrelevant to the constitutionality of the ACA.\nThe facts that Old Medicaid was well-established prior to the passage of the ACA, and that the states relied on its continuation, are relevant to the constitutionality of the ACA, and, in fact, open up the possibility that the ACA’s Offer is unconstitutionally coercive.\nBut that Offer is not, after all, unconstitutionally coercive, for at least two reasons: first, because it bundles together two programs (Old Medicaid and Expanded Medicaid) that are closely related, and second, because there are legitimate reasons for that bundling.\nOur discussion has left open several interesting questions. That’s partly for space reasons, and partly because we’re not sure what the right answers are. We’ll end our discussion by listing three of those questions.\nFirst, are offers by the federal government to help establish federal-state cooperative programs unconstitutionally coercive only when the states are exposed to significant losses? We are inclined to think that this is not the case, and that this poses a problem for C.J. Roberts’ attempt to distinguish the current case from South Dakota v. Dole via condition (3) of the Roberts Rule.\nSecond, is the Roberts Rule a good test for unconstitutional coerciveness of offers by the federal government in other federal jurisdictions? For example, does it throw light on whether the Australian High Court ruled correctly in the Uniform Tax Cases?60\nThird, is the Roberts Rule a good test for coerciveness of proposed changes to continuing relationships in business, or in residential tenancy? If so, this case could have implications for areas far removed from constitutional law. Although the previous two sections have provided a number of reasons to doubt that the ACA’s Medicaid provision is unconstitutionally coercive, the issues raised here are relevant to the broader question of when the more powerful party in a continuing relationship can force changes to that relationship.61\n567 U.S. ___ (2012), available at http://www.supremecourt.gov/opinions/11pdf/11--393c3a2.pdf.↩︎\nJonathan Engel, Poor People’s Medicine: Medicaid and American Charity Care since 1965 (Durham, NC: Duke University Press, 2006), 48–51.↩︎\nNFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 38 (2012).↩︎\n42 U.S.C. 1396a(a)(10)(A)(i)(VIII).↩︎\n42 U.S.C. 1396d(y)(1). More specifically, the ACA required the federal government to bear 100% of the costs of covering these newly eligible claimants through 2016. The level of federal support was then allowed to gradually decline, but to no lower than 90% of these costs.↩︎\nIn fiscal year 2012, federal funds offset between 50 and approximately 74% of states’ costs of covering claimants eligible for Medicaid prior to the passage of the ACA. Kaiser Commission on Medicaid and the Uninsured, “An Overview of Changes in the Federal Medical Assistance Percentages (FMAPs) for Medicaid,” July 2011, available at http://www.kff.org/medicaid/upload/8210.pdf, 2.↩︎\n42 U.S.C. 1396c.↩︎\nNFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 41–44 (2012).↩︎\nId., Roberts, C.J., slip opin. at 53.↩︎\nU.S. Constitution, Article I, Section 8, Clause 1.↩︎\nNFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 50 (2012). See also Roberts, C.J., slip opin. at 45–46, and Joint Dissent, slip opin. at 29–32.↩︎\nIn our discussion, we’ll leave open the question of whether an offer by the federal government can be coercive without being unconstitutionally coercive.↩︎\nId., Roberts, C.J., slip opin. at 46, and Joint Dissent, slip opin. at 33.↩︎\nThis is actually a surprising fact about the Court’s decision. Some prominent court watchers seemed to assume, before the decision, that if the court found the expansion to be constitutionally problematic, it would simply be voided. See, for instance, the pre-decision discussions in Kaiser Family Foundation, “The Health Reform Law’s Medicaid Expansion: A Guide to the Supreme Court Arguments,” March 2012, available at http://www.kff.org/healthreform/upload/8288.pdf, and Lyle Denniston, “Argument Preview: Health Care, Part IV - The Medicaid Expansion,” 23 March 2012, available at http://www.scotusblog.com/2012/03/argument-preview-health-care-part-iv-the-medicaid-expansion/. Both of these discuss the possibility the expansion will be struck down, but neither seems to mention the possibility that the expansion will be made voluntary. We suspect they were assuming the court would either find the expansion non-coercive, and hence let it stand, or coercive, and hold that coercive offers are void.↩︎\nId., Roberts, C.J., slip opin. at 49, citation omitted.↩︎\nId. at 50.↩︎\nId.↩︎\nWe’ll talk throughout this paper about purposes served by bundling, rather than the purposes Congress had in mind by bundling. This is in keeping with both C.J. Roberts’ and J. Ginsburg’s discussions.↩︎\nIn fiscal year 2012, spending on Old Medicaid comprised nearly 24% of total spending by the states. The National Association of State Budget Officers, “State Expenditure Report: Examining Fiscal 2010–2012 State Spending,” available at http://www.nasbo.org/sites/default/files/State%20Expenditure%20Report_1.pdf, 44. Federal support offset at least 50% of that state spending, and for some states, quite a lot more. See note 6.↩︎\nNFIB v. Sebelius, 567 U.S. ___, Roberts, C.J., slip opin. at 52 (2012).↩︎\nId. at 51.↩︎\nNote that Old Medicaid has evolved in just this way, with modifications to the program being bundled together with unchanged parts. For example, the Balanced Budget Act of 1997 (P.L. 105–33) required the states to expand their coverage of home health visits required by Medicare beneficiaries. The federal government met all of the costs of this expansion, but did not make it voluntary. For more on this, see Melvina Ford, Richard Price, and Jennifer Neisner, “Medicaid: 105th Congress,” Congressional Research Service, February 4 1998, available at http://www.policyarchive.org/handle/10207/508, 12–13.↩︎\n483 U.S. 203 (1987).↩︎\nNFIB v. Sebelius, 567 U.S. ___, Roberts, C.J., slip opin. at 50–51 (2012). But in a footnote, C.J. Roberts seems to argue that any amount of threatened financial loss makes the federal government’s offer coercive. Id. at 52n2. Perhaps he intended to draw a distinction between an offer being coercive, and it being unconstitutionally coercive. But that distinction doesn’t figure anywhere else in his opinion. So, we’re left at a loss about how to square this footnote with the rest of C.J. Roberts’ argument.↩︎\nId., Ginsburg, J., slip opin. at 39.↩︎\nId.↩︎\nId., Roberts, C.J., slip opin. at 54. “A State could hardly anticipate that Congress’ reservation of the right to”alter\" or “amend” the Medicaid program included the power to transform it so dramatically.\"↩︎\nThe result of adding (4) to the Roberts Rule is implausible as a test for unconstitutional coerciveness. Imagine that the federal government conditions federal highway spending on states enforcing a drinking age of 21 (as in South Dakota v. Dole), but this time, threatens to withhold 100% of highway funds from any states that fail to comply. That offer seems unconstitutionally coercive. And this verdict isn’t changed if the federal government threatens to add such a condition to federal highway spending for many years before actually doing so. So the foreseeability (or lack thereof) of the condition doesn’t seem to affect whether the offer is unconstitutionally coercive.↩︎\nId. at 52.↩︎\nId.↩︎\nCaroline West, “Personal Identity: Practical or Metaphysical?” in Kim Atkins and Catriona Mackenzie (eds.), Practical Identity and Narrative Agency (New York: Routledge, 2008), 56–77.↩︎\nTerrence Parsons, Indeterminate Identity: Metaphysics and Semantics (Oxford: Oxford University Press, 2000).↩︎\nNFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 51 (2012).↩︎\nId.↩︎\nId., Roberts, C.J., slip opin. at 33–40.↩︎\n“[T]he constitutionality of an action taken by Congress does not depend on recitals of the powers which it undertakes to exercise.” Woods v. Cloyd W. Miller Co., 333 U.S. 138, 144 (1948), emphasis added. Quoted in NFIB v. Sebelius, 567 U.S. ___, Roberts, C.J., slip opin. at 39 (2012).↩︎\nId. at 53, 54.↩︎\nId., Ginsburg, J., slip opin. at 41–44.↩︎\nFor if ontology were constitutionally relevant, and New Medicaid and Old Medicaid were substantially similar, then it’s hard to see why the former would be unconstitutionally coercive but the latter not.↩︎\nAs J. Ginsburg suggests, there’s little doubt about the constitutionality of the third option. Id. at 51.↩︎\nKaiser Commission on Medicaid and the Uninsured, “Federal Core Requirements and State Options in Medicaid: Current Policies and Key Issues,” April 2011, available at http://www.kff.org/medicaid/upload/8174.pdf.↩︎\nNFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 51 (2012), citation omitted.↩︎\nWe are drawing here on the literature in game theory on credible threats. For more detailed discussion see, for example, Avinash Dixit and Susan Skeath, Games of Strategy, second edition (New York: Norton, 2004), Chapter 10.↩︎\nId., Roberts, C.J., slip opin. at 53–54.↩︎\nId., Ginsburg, J., slip opin. at 50–51. J. Ginsburg treats these considerations as comprising C.J. Roberts’ argument for the view that Old Medicaid and New Medicaid are distinct programs. We’ve explained (in section 4) why we don’t take C.J. Roberts to subscribe to this view.↩︎\nId., Roberts, C.J., slip opin. at 53.↩︎\nU.S. Department of Health & Human Services, “2013 Poverty Guidelines,” available at http://aspe.hhs.gov/poverty/13poverty.cfm.↩︎\nJ. Ginsburg makes a similar point. NFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 50 (2012).↩︎\nJared Bernstein, “More Poverty than Meets the Eye,” Economic Policy Institute, April 11 2007, available at http://www.epi.org/publication/webfeatures_snapshots_20070411/. Bernstein writes, “When it comes to poverty in America, almost every analyst agrees that the official measure is terribly out-of-date and no longer provides a valid indication of economic deprivation.” See also David M. Betson, Constance F. Citro, and Robert T. Michael, “Recent Developments for Poverty Measurement in U.S. Official Statistics,” Journal of Official Statistics, vol. 16, no. 2, 2000, 87–111.↩︎\nSee notes 5 and 6.↩︎\nNFIB v. Sebelius, 567 U.S. ___, Ginsburg, J., slip opin. at 50–51 (2012).↩︎\nSee note 22.↩︎\nId., Roberts, C.J., slip opin. at 52n12.↩︎\nId., Joint Dissent, slip opin. at 45–46.↩︎\nSee, for example, January Angeles, “How Health Reform’s Medicaid Expansion Will Impact State Budgets: Federal Government Will Pick Up Nearly All Costs, Even as Expansion Provides Coverage to Millions of Low-Income Uninsured Americans,” Center on Budget and Policy Priorities, July 25, 2012, available at http://www.cbpp.org/files/7--12--12health.pdf, and John Holahan and Irene Headen, “Medicaid Coverage and Spending in Health Reform: National and State-by-State Results for Adults at or Below 133% FPL,” Kaiser Commission on Medicaid and the Uninsured, May 2012, available at http://www.kff.org/healthreform/upload/medicaid-coverage-and-spending-in-health-reform-national-and-state-by-state-results-for-adults-at-or-below--133-fpl.pdf.↩︎\nAngeles, op. cit., 1.↩︎\nMatthew Buttguens, Stan Dorn, and Caitlyn Carroll, “Consider Savings as well as Costs: State Governments Would Spend at Least $90 Billion Less With the ACA than Without It from 2014 to 2019,” The Urban Institute, July 2011, available at http://www.urban.org/UploadedPDF/412361-consider-savings.pdf, 1. This projection includes savings from moving some adults currently covered by various state Medicaid programs onto federal subsidies via the health care exchanges to be established under the ACA.↩︎\nFor an useful overview of some research on this point, see the Institute of Medicine, America’s Uninsured Crisis: Consequences for Health and Health Care (Washington, D.C.: National Academies Press), especially Chapter 4.↩︎\nIn its first full year of implementing health reform substantially similar to the ACA, Massachusetts saw an astounding 38% drop in its spending on uncompensated care. Angeles, op. cit., 5.↩︎\nSouth Australia v Commonwealth 65 CLR 373 (1942) and Victoria v Commonwealth 99 CLR 575 (1957).↩︎\nThanks to the editors and referees of this journal for many helpful comments.\n\n↩︎\n",
    "preview": "posts/2021-01-06-in-defense-of-the-acas-medicaid-expansion/obamacare.jpg",
    "last_modified": "2021-02-05T15:26:06-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-disagreements-philosophical-and-otherwise/",
    "title": "Disagreements, Philosophical and Otherwise",
    "description": "The Equal Weight View of disagreement says that if an agent sees that an epistemic peer disagrees with her about p, the agent should change her credence in p to half way between her initial credence, and the peer's credence. But it is hard to believe the Equal Weight View for a surprising reason; not everyone believes it. And that means that if one did believe it, one would be required to lower one's belief in it in light of this peer disagreement. Brian Weatherson explores the options for how a proponent of the Equal Weight View might respond to this difficulty, and how this challenge fits into broader arguments against the Equal Weight View.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2013-06-10",
    "categories": [
      "epistemology",
      "disagreement"
    ],
    "contents": "\n\nContents\nPeers and EW\nCircumstances of Evaluation\nA Story about Disagreement\nSumming Up\n\nThis paper started life as a short note I wrote around New Year 2007 while in Minneapolis. It was originally intended as a blog post. That might explain, if not altogether excuse, the flippant tone in places. But it got a little long for a post, so I made it into the format of a paper and posted it to my website. The paper has received a lot of attention, so it seems like it will be helpful to see it in print. Since a number of people have responded to the argument as stated, I’ve decided to just reprint the article warts and all, and make a few comments at the end about how I see its argument in the context of the subsequent debate.\n\nPublished in The Epistemology of Disagreement: New Essays, edited by David Christensen and Jennifer Lackey, OUP 54-75\n\nDisagreeing about Disagreement (2007)\n\nI argue with my friends a lot. That is, I offer them reasons to believe all sorts of philosophical conclusions. Sadly, despite the quality of my arguments, and despite their apparent intelligence, they don’t always agree. They keep insisting on principles in the face of my wittier and wittier counterexamples, and they keep offering their own dull alleged counterexamples to my clever principles. What is a philosopher to do in these circumstances? (And I don’t mean get better friends.)\nOne popular answer these days is that I should, to some extent, defer to my friends. If I look at a batch of reasons and conclude \\(p\\), and my equally talented friend reaches an incompatible conclusion \\(q\\), I should revise my opinion so I’m now undecided between \\(p\\) and \\(q\\). I should, in the preferred lingo, assign equal weight to my view as to theirs. This is despite the fact that I’ve looked at their reasons for concluding \\(q\\) and found them wanting. If I hadn’t, I would have already concluded \\(q\\). The mere fact that a friend (from now on I’ll leave off the qualifier ‘equally talented and informed,’ since all my friends satisfy that) reaches a contrary opinion should be reason to move me. Such a position is defended by Richard Feldman (2005, 2006), David Christensen (2007) and Adam Elga (2007).\nThis equal weight view, hereafter EW, is itself a philosophical position. And while some of my friends believe it, some of my friends do not. (Nor, I should add for your benefit, do I.) This raises an odd little dilemma. If EW is correct, then the fact that my friends disagree about it means that I shouldn’t be particularly confident that it is true, since EW says that I shouldn’t be too confident about any position on which my friends disagree. But, as I’ll argue below, to consistently implement EW, I have to be maximally confident that it is true. So to accept EW, I have to inconsistently both be very confident that it is true and not very confident that it is true. This seems like a problem, and a reason to not accept EW. We can state this argument formally as follows, using the notion of a peer and an expert. Some people are peers if they are equally philosophically talented and informed as each other, and one is more expert than another if they are more informed and talented than the other.\nThere are peers who disagree about EW, and there is no one who is an expert relative to them who endorses EW.\nIf 1 is true, then according to EW, my credence in EW should be less than 1.\nIf my credence in EW is less than 1, then the advice that EW offers in a wide range of cases is incoherent.\nSo, the advice EW offers in a wide range of cases is incoherent.\nThe first three sections of this paper will be used to defend the first three premises. The final section will look at the philosophical consequences of the conclusion.\nPeers and EW\nThomas Kelly (2005) has argued against EW and in favour of the view that a peer with the irrational view should defer to a peer with the rational view. Elga helpfully dubs this the ‘right reasons’ view. Ralph Wedgwood (2007 Ch. 11) has argued against EW and in favour of the view that one should have a modest ‘egocentric bias,’ i.e. a bias towards one’s own beliefs. On the other hand, as mentioned above, Elga, Christensen and Feldman endorse versions of EW. So it certainly looks like there are very talented and informed philosophers on either side of this debate.\nNow I suppose that if we were taking EW completely seriously, we would at this stage of the investigation look very closely at whether these five really are epistemic peers. We could pull out their grad school transcripts, look at the citation rates for their papers, get reference letters from expert colleagues, maybe bring one or two of them in for job-style interviews, and so on. But this all seems somewhat inappropriate for a scholarly journal. Not to mention a little tactless.1 So I’ll just stipulate that they seem to be peers in the sense relevant for EW, and address one worry a reader may have about my argument.\nAn objector might say, “Sure it seems antecedently that Kelly and Wedgwood are the peers of the folks who endorse EW. But take a look at the arguments for EW that have been offered. They look like good arguments, don’t they? Doesn’t the fact that Kelly and Wedgwood don’t accept these arguments mean that, however talented they might be in general, they obviously have a blind spot when it comes to the epistemology of disagreement? If so, we shouldn’t treat them as experts on this question.” There is something right about this. People can be experts in one area, or even many areas, while their opinions are systematically wrong in another. But the objector’s line is unavailable to defenders of EW.\nIndeed, these defenders have been quick to distance themselves from the objector. Here, for instance, is Elga’s formulation of the EW view, a formulation we’ll return to below.\n\nYour probability in a given disputed claim should equal your prior conditional probability in that claim. Prior to what? Prior to your thinking through the claim, and finding out what your advisor thinks of it. Conditional on what? On whatever you have learned about the circumstances of how you and your advisor have evaluated the claim. (Elga 2007, 490)\n\nThe fact that Kelly and Wedgwood come to different conclusions can’t be enough reason to declare that they are not peers. As Elga stresses, what matters is the prior judgment of their acuity. And Elga is right to stress this. If we declared anyone who doesn’t accept reasoning that we find compelling not a peer, then the EW view will be trivial. After all, the EW view only gets its force from cases as described in the introduction, where our friends reject reasoning we accept, and accept reasons we reject. If that makes them not a peer, the EW view never applies. So we can’t argue that anyone who rejects EW is thereby less of an expert in the relevant sense than someone who accepts it, merely in virtue of their rejection of EW. So it seems we should accept premise 1.\nCircumstances of Evaluation\nElga worries about the following kind of case. Let \\(p\\) be that the sum of a certain series of numbers, all of them integers, is 50. Let \\(q\\) be that the sum of those numbers is \\(400e\\). My friend and I both add the numbers, and I conclude \\(p\\) while he concludes \\(q\\). It seems that there is no reason to defer to my friend. I know, after all, that he has made some kind of mistake. The response, say defenders of EW, is that deference is context-sensitive. If I know, for example, that my friend is drunk, then I shouldn’t defer to him. More generally, as Elga puts it, how much I should defer should depend on what I know about the circumstances.\nNow this is relevant because one of the relevant circumstances might be that my friend has come to a view that I regard as insane. That’s what happens in the case of the sums. Since my prior probability that my friend is right given that he has an insane seeming view is very low, my posterior probability that my friend is right should also, according to Elga, be low. Could we say that, although antecedently we regard Wedgwood and Kelly as peers of those they disagree with, that the circumstance of their disagreement is such that we should disregard their views?\nIt is hard to see how this would be defensible. It is true that a proponent of EW will regard Kelly and Wedgwood as wrong. But we can’t say that we should disregard the views of all those we regard as mistaken. That leads to trivialising EW, for reasons given above. The claim has to be that their views are so outrageous, that we wouldn’t defer to anyone with views that outrageous. And this seems highly implausible. But that’s the only reason that premise 2 could be false. So we should accept premise 2.\nA Story about Disagreement\nThe tricky part of the argument is proving premise 3. To do this, I’ll use a story involving four friends, Apollo, Telemachus, Adam and Tom. The day before our story takes place, Adam has convinced Apollo that he should believe EW, and organise his life around it. Now Apollo and Telemachus are on their way to Fenway Park to watch the Red Sox play the Indians. There have been rumours flying around all day about whether the Red Sox injured star player, David Ortiz, will be healthy enough to play. Apollo and Telemachus have heard all the competing reports, and are comparing their credences that Ortiz will play. (Call the proposition that he will play \\(p\\).) Apollo’s credence in \\(p\\) is 0.7, and Telemachus’s is 0.3. In fact, 0.7 is the rational credence in p given their shared evidence, and Apollo truly believes that it is.2 And, as it turns out, the Red Sox have decided but not announced that Ortiz will play, so \\(p\\) is true.\nDespite these facts, Apollo lowers his credence in \\(p\\). In accord with his newfound belief in EW, he changes his credence in \\(p\\) to 0.5. Apollo is sure, after all, that when it comes to baseball Telemachus is an epistemic peer. At this point Tom arrives, and with a slight disregard for the important baseball game at hand, starts trying to convince them of the right reasons view on disagreement. Apollo is not convinced, but Telemachus thinks it sounds right. As he puts it, the view merely says that the rational person believes what the rational person believes. And who could disagree with that?\nApollo is not convinced, and starts telling them the virtues of EW. But a little way in, Tom cuts him off with a question. “How probable,” he asks Apollo, “does something have to be before you’ll assert it?”\nApollo says that it has to be fairly probable, though just what the threshold is depends on just what issues are at stake. But he agrees that it has to be fairly high, well above 0.5 at least.\n“Well,” says Tom, “in that case you shouldn’t be defending EW in public. Because you think that Telemachus and I are the epistemic peers of you and Adam. And we think EW is false. So even by EW’s own lights, the probability you assign to EW should be 0.5. And that’s not a high enough probability to assert it.” Tom’s speech requires that Apollo regard he and Telemachus as Apollo’s epistemic peers with regard to this question. By premises 1 and 2, Apollo should do this, and we’ll assume that he does.\nSo Apollo agrees with all this, and agrees that he shouldn’t assert EW any more. But he still plans to use it, i.e. to have a credence in \\(p\\) of 0.5 rather than 0.7. But now Telemachus and Tom press on him the following analogy.\nImagine that there were two competing experts, each of whom gave differing views about the probability of \\(q\\). One of the experts, call her Emma, said that the probability of \\(q\\), given the evidence, is 0.5. The other expert, call her Rae, said that the probability of \\(q\\), given the evidence, is 0.7. Assuming that Apollo has the same evidence as the experts, but he regards the experts as experts at evaluating evidence, what should his credence in \\(q\\) be? It seems plausible that it should be a weighted average of what Emma says and what Rae says. In particular, it should be 0.5 only if Apollo is maximally confident that Emma is the expert to trust, and not at all confident that Rae is the expert to trust.\nThe situation is parallel to the one Apollo actually faces. EW says that his credence in \\(p\\) should be 0.5. The right reason view says that his credence in \\(p\\) should be 0.7. Apollo is aware of both of these facts. So his credence in \\(p\\) should be 0.5 iff he is certain that EW is the theory to trust, just as his credence in \\(q\\) should be 0.5 iff he is certain that Emma is the expert to trust. Indeed, a credence of 0.5 in \\(p\\) is incoherent unless Apollo is certain EW is the theory to trust. But Apollo is not at all certain of this. His credence in EW, as is required by EW itself, is 0.5. So as long as Apollo keeps his credence in p at 0.5, he is being incoherent. But EW says to keep his credence in p at 0.5. So EW advises him to be incoherent. That is, EW offers incoherent advice. We can state this more carefully in an argument.\nEW says that Apollo’s credence in p should be 0.5.\nIf 5, then EW offers incoherent advice unless it also says that Apollo’s credence in EW should be 1.\nEW says that Apollo’s credence in EW should be 0.5.\nSo, EW offers incoherent advice.\nSince Apollo’s case is easily generalisable, we can infer that in a large number of cases, EW offers advice that is incoherent. Line 7 in this argument is hard to assail given premises 1 and 2 of the master argument. But I can imagine objections to each of the other lines.\nObjection: Line 6 is false. Apollo can coherently have one credence in p while being unsure about whether it is the rational credence to have. In particular, he can coherently have his credence in p be 0.5, while he is unsure whether his credence in p should be 0.5 or 0.7. In general there is no requirement for agents who are not omniscient to have their credences match their judgments of what their credences should be.\nReplies: I have two replies to this, the first dialectical and the second substantive.\nThe dialectical reply is that if the objector’s position on coherence is accepted, then a lot of the motivation for EW fades away. A core idea behind EW is that Apollo was unsure before the conversation started whether he or Telemachus would have the most rational reaction to the evidence, and hearing what each of them say does not provide him with more evidence. (See the ‘bootstrapping’ argument in Elga (2007) for a more formal statement of this idea.) So Apollo should have equal credence in the rationality of his judgment and of Telemachus’s judgment.\nBut if the objector is correct, Apollo can do that without changing his view on EW one bit. He can, indeed should, have his credence in \\(p\\) be 0.7, while being uncertain whether his credence in p should be 0.7 (as he thinks) or 0.3 (as Telemachus thinks). Without some principle connecting what Apollo should think about what he should think to what Apollo should think, it is hard to see why this is not the uniquely rational reaction to Apollo’s circumstances. In other words, if this is an objection to my argument against EW, it is just as good an objection to a core argument for EW.\nThe substantive argument is that the objector’s position requires violating some very weak principles concerning rationality and higher-order beliefs. The objector is right that, for instance, in order to justifiably believe that \\(p\\) (to degree \\(d\\)), one need not know, or even believe, that one is justified in believing \\(p\\) (to that degree). If nothing else, the anti-luminosity arguments in Williamson (2000) show that to be the case. But there are weaker principles that are more plausible, and which the objector’s position has us violate. In particular, there is the view that we can’t both be justified in believing that \\(p\\) (to degree \\(d\\)), while we know we are not justified in believing that we are justified in believing \\(p\\) (to that degree). In symbols, if we let \\(Jp\\) mean that the agent is justified in believing \\(p\\), and box and diamond to be epistemic modals, we have the principle MJ (for Might be Justified).\nMJ\n\\(Jp \\rightarrow \\diamond JJp\\)\n\nThis seems like a much more plausible principle, since if we know we aren’t justified in believing we’re justified in believing \\(p\\), it seems like we should at least suspend judgment in \\(p\\). That is, we shouldn’t believe \\(p\\). That is, we aren’t justified in believing \\(p\\). But the objector’s position violates principle MJ, or at least a probabilistic version of it, as we’ll now show.\nWe aim to prove that the objector is committed to Apollo being justified in believing \\(p\\) to degree 0.5, while he knows he is not justified in believing he is justified in believing \\(p\\) to degree 0.5. The first part is trivial, it’s just a restatement of the objector’s view, so it is the second part that we must be concerned with.\nNow, either EW is true, or it isn’t true. If it is true, then Apollo is not justified in having a greater credence in it than 0.5. But his only justification for believing p to degree 0.5 is EW. He’s only justified in believing he’s justified in believing \\(p\\) if he can justify his use of EW in it. But you can’t justify a premise in which your rational credence is 0.5. So Apollo isn’t justified in believing he is justified in believing \\(p\\). If EW isn’t true, then Apollo isn’t even justified in believing \\(p\\) to degree 0.5. And he knows this, since he knows EW is his only justification for lowering his credence in \\(p\\) that far. So he certainly isn’t justified in believing he is justified in believing \\(p\\) to degree 0.5 Moreover, every premise in this argument has been a premise that Apollo knows to obtain, and he is capable of following all the reasoning. So he knows that he isn’t justified in believing he is justified in believing \\(p\\) to degree 0.5, as required.\nThe two replies I’ve offered to the objector complement one another. If someone accepts MJ, then they’ll regard the objector’s position as incoherent, since we’ve just shown that MJ is inconsistent with that position. If, on the other hand, someone rejects MJ and everything like it, then they have little reason to accept EW in the first place. They should just accept that Apollo’s credence in p should be, as per hypothesis the evidence suggests, 0.7. The fact that an epistemic peer disagrees, in the face of the same evidence, might give Apollo reason to doubt that this is in fact that uniquely rational response to the evidence. But, unless we accept a principle like MJ, that’s consistent with Apollo retaining the rational response to the evidence, namely a credence of 0.7 in p. So it is hard to see how someone could accept the objector’s argument, while also being motivated to accept EW. In any case, I think MJ is plausible enough on its own to undermine the objector’s position.3\nObjection: Line 5 is false. Once we’ve seen that the credence of EW is 0.5, then Apollo’s credence in first-order claims such as p should, as the analogy with q suggests, be a weighted average of what EW says it should be, and what the right reason view says it should be. So, even by EW’s own lights, Apollo’s credence in p should be 0.6.\nReplies: Again I have a dialectical reply, and a substantive reply.\nThe dialectical reply is that once we make this move, we really have very little motivation to accept EW. There is, I’ll grant, some intuitive plausibility to the view that when faced with a disagreeing peer, we should think the right response is half way between our competing views. But there is no intuitive plausibility whatsoever to the view that in such a situation, we should naturally move to a position three-quarters of the way between the two competing views, as this objector suggests. Much of the argument for EW, especially in Christensen, turns on intuitions about cases, and the objector would have us give all of that up. Without those intuitions, however, EW falls in a heap.\nThe substantive reply is that the idea behind the objection can’t be coherently sustained. The idea is that we should first apply EW to philosophical questions to work out the probability of different theories of disagreement, and then apply those probabilities to first-order disagreements. The hope is that in doing so we’ll reach a stable point at which EW can be coherently applied. But there is no such stable point. Consider the following series of questions.\nQ1\nIs EW true?\n\nTwo participants say yes, two say no. We have a dispute, leading to our next question.\nQ2\nWhat is the right reaction to the disagreement over Q1?\n\nEW answers this by saying our credence in EW should be 0.5. But that’s not what the right reason proponents say. They don’t believe EW, so they have no reason to move their credence in EW away from 0. So we have another dispute, and we can ask\nQ3\nWhat is the right reaction to the disagreement over Q2?\n\nEW presumably says that we should again split the difference. Our credence in EW might now be 0.25, half-way between the 0.5 it was after considering Q2, and what the right reasons folks say. But, again, those who don’t buy EW will disagree, and won’t be moved to adjust their credence in EW. So again there’s a dispute, and again we can ask\nQ4\nWhat is the right reaction to the disagreement over Q3?\n\nThis could go on for a while. The only ‘stable point’ in the sequence is when we assign a credence of 0 to EW. That’s to say, the only way to coherently defend the idea behind the objection is to assign credence 0 to EW. But that’s to give up on EW. As with the previous objection, we can’t hold on to EW and object to the argument.\nSumming Up\nThe story I’ve told here is a little idealised, but otherwise common enough. We often have disagreements both about first-order questions, and about how to resolve this disagreement. In these cases, there is no coherent way to assign equal weight to all prima facie rational views both about the first order question and the second order, epistemological, question. The only way to coherently apply EW to all first order questions is to put our foot down, and say that despite the apparent intelligence of our philosophical interlocutors, we’re not letting them dim our credence in EW. But if we are prepared to put our foot down here, why not about some first-order question or other? It certainly isn’t because we have more reason to believe an epistemological theory like EW than we have to believe first order theories about which there is substantive disagreement. So perhaps we should hold on to those theories, and let go of EW.\n\nAfterthoughts (2010)\n\nI now think that the kind of argument I presented in the 2007 paper is not really an argument against EW as such, but an argument against one possible motivation for EW. I also think that alternate motivations for EW are no good, so I still think it is an important argument. But I think it’s role in the dialectic is a little more complicated than I appreciated back then.\nMuch of my thinking about disagreement problems revolves around the following table. The idea behind the table, and much of the related argument, is due to Thomas Kelly (2010). In the table, \\(S\\) and \\(T\\) antecedently had good reasons to take themselves to be epistemic peers, and they know that their judgments about \\(p\\) are both based on \\(E\\). In fact, \\(E\\) is excellent evidence for \\(p\\), but only \\(S\\) judges that \\(p\\); \\(T\\) judges that \\(\\neg p\\). Now let’s look at what seems to be the available evidence for and against \\(p\\).\n\nEvidence for \\(p\\)\nEvidence against \\(p\\)\n\\(S\\)’s judgment that \\(p\\)\n\\(T\\)’s judgment that \\(\\neg p\\)\n\\(E\\)\n\n\nNow that doesn’t look to me like a table where the evidence is equally balanced for and against \\(p\\). Even granting that the judgments are evidence over and above \\(E\\), and granting that how much weight we should give to judgments should track our ex ante judgments of their reliability rather than our ex post judgments of their reliability, both of which strike me as false but necessary premises for EW, it still looks like there is more evidence for \\(p\\) than against \\(p\\).4 There is strictly more evidence for \\(p\\) than against it, since \\(E\\) exists. If we want to conclude that \\(S\\) should regard \\(p\\) and \\(\\neg p\\) as equally well supported for someone in her circumstance, we have to show that the table is somehow wrong. I know of three possible moves the EW defender could make here.\nDavid Christensen (2011), as I read him, says that the table is wrong because when we are representing the evidence \\(S\\) has, we should not include her own judgment. There’s something plausible to this. Pretend for a second that \\(T\\) doesn’t exist, so it’s clearly rational for \\(S\\) to judge that \\(p\\). It would still be wrong of \\(S\\) to say, “Since \\(E\\) is true, \\(p\\). And I judged that \\(p\\), so that’s another reason to believe that \\(p\\), because I’m smart.” By hypothesis, \\(S\\) is smart, and that smart people judge things is reason to believe those things are true. But this doesn’t work when the judgment is one’s own. This is something that needs explaining in a full theory of the epistemic significance of judgment, but let’s just take it as a given for now.5 Now the table, or at least the table as is relevant to \\(S\\), looks as follows.\n\nEvidence for \\(p\\)\nEvidence against \\(p\\)\n\\(E\\)\n\\(T\\)’s judgment that \\(\\neg p\\)\n\nBut I don’t think this does enough to support EW, or really anything like it. First, it won’t be true in general that the two sides of this table balance. In many cases, \\(E\\) is strong evidence for \\(p\\), and \\(T\\)’s judgment won’t be particularly strong evidence against \\(p\\). In fact, I’d say the kind of case where \\(E\\) is much better evidence for \\(p\\) than \\(T\\)’s judgment is against \\(p\\) is the statistically normal kind. Or, at least, it is the normal kind of case modulo the assumption that \\(S\\) and \\(T\\) have the same evidence. In cases where that isn’t true, learning that \\(T\\) thinks \\(\\neg p\\) is good evidence that \\(T\\) has evidence against \\(p\\) that you don’t have, and you should adjust accordingly. But by hypothesis, \\(S\\) knows that isn’t the case here. So I don’t see why this should push us even close to taking \\(p\\) and \\(\\neg p\\) to be equally well supported.\nThe other difficulty for defending EW by this approach is that it seems to undermine the original motivations for the view. As Christensen notes, the table above is specifically for \\(S\\). Here’s what the table looks like for \\(T\\).\n\nEvidence for \\(p\\)\nEvidence against \\(p\\)\n\\(S\\)’s judgment that \\(p\\)\n\n\\(E\\)\n\n\nIt’s no contest! So \\(T\\) should firmly believe \\(p\\). But that isn’t the intuition anyone gets, as far as I can tell, in any of the cases motivating EW. And the big motivation for EW comes from intuitions about cases. Once we acknowledge that these intuitions are unreliable, as we’d have to do if we were defending EW this way, we seem to lack any reason to accept EW.\nThe second approach to blocking the table is to say that \\(T\\)’s judgment is an undercutting defeater for the support \\(E\\) provides for \\(p\\). This looks superficially promising. Having a smart person say that your evidence supports something other than you thought it did seems like it could be an undercutting defeater, since it is a reason to think the evidence supports something else, and hence doesn’t support what you thought it does. And, of course, if \\(E\\) is undercut, then the table just has one line on it, and the two sides look equal.\nBut it doesn’t seem like it can work in general, for a reason that Kelly (2010) makes clear. We haven’t said what \\(E\\) is so far. Let’s start with a case where \\(E\\) consists of the judgments of a million other very smart people that \\(p\\). Then no one, not even the EW theorist, will think that \\(T\\)’s judgment undercuts the support \\(E\\) provides to \\(p\\). Indeed, even if \\(E\\) just consists of one other person’s judgment, it won’t be undercut by \\(T\\)’s judgment. The natural thought for an EW-friendly person to have in that case is that since there are two people who think \\(p\\), and one who thinks \\(\\neg p\\), then \\(S\\)’s credence in \\(p\\) should be \\(\\frac{2}{3}\\). But that’s impossible if \\(E\\), i.e., the third person’s judgment, is undercut by \\(T\\)’s judgment. It’s true that \\(T\\)’s judgment will partially rebut the judgments that \\(S\\), and the third party, make. It will move the probability of \\(p\\), at least according to EW, from 1 to \\(\\frac{2}{3}\\). But that evidence won’t be in any way undercut.\nAnd as Kelly points out, evidence is pretty fungible. Whatever support \\(p\\) gets from other people’s judgments, it could get very similar support from something other than a judgment. We get roughly the same evidence for \\(p\\) by learning that a smart person predicts \\(p\\) as learning that a successful computer model predicts \\(p\\). So the following argument looks sound to me.\nWhen \\(E\\) consists of other people’s judgments, the support it provides to \\(p\\) is not undercut by \\(T\\)’s judgment.\nIf the evidence provided by other people’s judgments is not undercut by \\(T\\)’s judgment, then some non-judgmental evidence is not undercut by \\(T\\)’s judgment.\nSo, not all non-judgmental evidence is not undercut by \\(T\\)’s judgment.\nSo it isn’t true in general that the table is wrong because \\(E\\) has been defeated by an undercutting defeater.\nThere’s another problem with the defeat model in cases where the initial judgments are not full beliefs. Change the case so \\(E\\) provides basically no support to either \\(p\\) or \\(\\neg p\\). In fact, \\(E\\) is just irrelevant to \\(p\\), and the agent’s have nothing to base either a firm or a probabilistic judgment about \\(p\\) on. For this reason, \\(S\\) declines to form a judgment, but \\(T\\) forms a firm judgment that \\(p\\). Moreover, although both \\(S\\) and \\(T\\) are peers, that’s because they are both equally poor at making judgments about cases like \\(p\\). Here’s the table then:\n\nEvidence for \\(p\\)\nEvidence against \\(p\\)\n\\(T\\)’s judgment that \\(p\\)\n\n\nSince \\(E\\) is irrelevant, it doesn’t appear, either before or after we think about defeaters. And since \\(T\\) is not very competent, that’s not great evidence for \\(p\\). But EW says that \\(S\\) should ‘split the difference’ between her initial agnositicism, and \\(T\\)’s firm belief in \\(p\\). I don’t see how that could be justified by \\(S\\)’s evidence.\nSo that move doesn’t work either, and we’re left with the third option for upsetting the table. This move is, I think, the most promising of the lot. It is to say that \\(S\\)’s own judgment screens off the evidence that \\(E\\) provides. So the table is misleading, because it ‘double counts’ evidence.\nThe idea of screening I’m using here, at least on behalf of EW, comes from Reichenbach’s The Direction of Time, and in particular from his work on deriving a principle that lets us infer events have a common cause. The notion was originally introduced in probabilistic terms. We say that \\(C\\) screens off the positive correlation between \\(B\\) and \\(A\\) if the following two conditions are met.\n\\(A\\) and \\(B\\) are positively correlated probabilistically, i.e. \\(Pr(A | B) > Pr(A)\\).\nGiven \\(C\\), \\(A\\) and \\(B\\) are probabilistically independent,\ni.e. \\(Pr(A | B \\wedge C) = Pr(A | C)\\).\nI’m interested in an evidential version of screening. If we have a probabilistic analysis of evidential support, the version of screening I’m going to offer here is identical to the Reichenbachian version just provided. But I want to stay neutral on whether we should think of evidence probabilistically.6 When I say that \\(C\\) screens off the evidential support that \\(B\\) provides to \\(A\\), I mean the following. (Both these clauses, as well as the statement that \\(C\\) screens off \\(B\\) from \\(A\\), are made relative to an evidential background. I’ll leave that as tacit in what follows.)\n\\(B\\) is evidence that \\(A\\).\n\\(B \\wedge C\\) is no better evidence that \\(A\\) than \\(C\\) is.7\nHere is one stylised example of where screening helps conceptualise things. Detective Det is trying to figure out whether suspect Sus committed a certain crime. Let \\(A\\) be that Sus is guilty, \\(B\\) be that Sus’s was seen near the crime scene near the time the crime was committed, and \\(C\\) be that Sus was at the crime scene when the crime was committed. Then both clauses are satisfied. \\(B\\) is evidence for \\(A\\); that’s why we look for witnesses who place the suspect near the crime scene. But given the further evidence \\(C\\), then \\(B\\) is neither here nor there with respect to \\(A\\). We’re only interested in finding if Sus was near the crime scene because we want to know whether he was at the crime scene. If we know that he was there, then learning he was seen near there doesn’t move the investigation along. So both clauses of the definition of screening are satisfied.\nWhen there is screened evidence, there is the potential for double counting. It would be wrong to say that if we know \\(B \\wedge C\\) we have two pieces of evidence against Sus. Similarly, if a judgment screens off the evidence it is based on, then the table ‘double counts’ the evidence for \\(p\\). Removing the double counting, by removing \\(E\\), makes the table symmetrical. And that’s just what EW needs.\nSo the hypothesis that judgments screen the evidence they are based on, or JSE for short, can help EW respond to the argument from this table. But JSE is vulnerable to regress arguments. I now think that the argument in ‘Disagreeing about Disagreement’ is a version of the regress argument against JSE. So really it’s an argument against the most promising response to a particularly threatening argument against EW.\nUnfortunately for EW, those regress arguments are actually quite good. To see ths, let’s say an agent makes a judgment on the basis of \\(E\\), and let \\(J\\) be the proposition that that judgment was made. JSE says that \\(E\\) is now screened off, and the agent’s evidence is just \\(J\\). But with that evidence, the agent presumably makes a new judgment. Let \\(J^\\prime\\) be the proposition that that judgment was made. We might ask now, does \\(J^\\prime\\) sit alongside \\(J\\) as extra evidence, is it screened off by \\(J\\), or does it screen off \\(J\\)? The picture behind JSE, the picture that says that judgments on the basis of some evidence screen that evidence, suggest that \\(J^\\prime\\) should in turn screen \\(J\\). But now it seems we have a regress on our hands. By the same token, \\(J^{\\prime \\prime}\\), the proposition concerning the new judgment made on the basis of \\(J^\\prime\\), should screen off \\(J^\\prime\\), and the proposition \\(J^{\\prime \\prime \\prime}\\) about the fourth judgment made, should screen off \\(J^{\\prime \\prime}\\), and so on. The poor agent has no unscreened evidence left! Something has gone horribly wrong.\nI think this regress is ultimately fatal for JSE. But to see this, we need to work through the possible responses that a defender of JSE could make. There are really just two moves that seem viable. One is to say that the regress does not get going, because \\(J\\) is better evidence than \\(J^\\prime\\), and perhaps screens it. The other is to say that the regress is not vicious, because all these judgments should agree in their content. I’ll end the paper by addressing these two responses.\nThe first way to avoid the regress is to say that there is something special about the first level. So although \\(J\\) screens \\(E\\), it isn’t the case that \\(J^\\prime\\) screens \\(J\\). That way, the regress doesn’t start. This kind of move is structurally like the move Adam Elga (2010) has recently suggested. He argues that we should adjust our views about first-order matters in (partial) deference to our peers, but we shouldn’t adjust our views about the right response to disagreement in this way.\nIt’s hard to see what could motivate such a position, either about disagreement or about screening. It’s true that we need some kind of stopping point to avoid these regresses. But the most natural stopping point is the very first level. Consider a toy example. It’s common knowledge that there are two apples and two oranges in the basket, and no other fruit. (And that no apple is an orange.) Two people disagree about how many pieces of fruit there are in the basket. \\(A\\) thinks there are four, \\(B\\) thinks there are five, and both of them are equally confident. Two other people, \\(C\\) and \\(D\\), disagree about what \\(A\\) and \\(B\\) should do in the face of this disagreement. All four people regard each other as peers. Let’s say \\(C\\)’s position is the correct one (whatever that is) and \\(D\\)’s position is incorrect. Elga’s position is that \\(A\\) should partially defer to \\(B\\), but \\(C\\) should not defer to \\(D\\). This is, intuitively, just back to front. \\(A\\) has evidence that immediately and obviously entails the correctness of her position. \\(C\\) is making a complicated judgment about a philosophical question where there are plausible and intricate arguments on each side. The position \\(C\\) is in is much more like the kind of case where experience suggests a measure of modesty and deference can lead us away from foolish errors. If anyone should be sticking to their guns here, it is \\(A\\), not \\(C\\).\nThe same thing happens when it comes to screening. Let’s say that \\(A\\) has some evidence that (a) she has made some mistakes on simple sums in the past, but (b) tends to massively over-estimate the likelihood that she’s made a mistake on any given puzzle. What should she do? One option, in my view the correct one, is that she should believe that there are four pieces of fruit in the basket, because that’s what the evidence obviously entails. Another option is that she should be not very confident there are four pieces of fruit in the basket, because she makes mistakes on these kinds of sums. Yet another option is that she should be pretty confident (if not completely certain) that there are four pieces of fruit in the basket, because if she were not very confident about this, this would just be a manifestation of her over-estimation of her tendency to err. The ‘solution’ to the regress we’re considering here says that the second of these three reactions is the uniquely rational reaction. The idea behind the solution is that we should respond to the evidence provided by first-order judgments, and correct that judgment for our known biases, but that we shouldn’t in turn correct for the flaws in our self-correcting routine. I don’t see what could motivate such a position. Either we just rationally respond to the evidence, and in this case just believe there are four pieces of fruit in the basket, or we keep correcting for errors we make in any judgment. It’s true that the latter plan leads either to regress or to the kind of ratificationism we’re about to critically examine. But that’s not because the disjunction is false, it’s because the first disjunct is true.\nA more promising way to avoid the regress is suggested by some other work of Elga’s, in this case a paper he co-wrote with Andy Egan (Egan and Elga 2005). Their idea, as I understand them, is that for any rational agent, any judgment they make must be such that when they add the fact that they made that judgment to their evidence (or, perhaps better given JSE, replace their evidence with the fact that they made that judgment), the rational judgment to make given the new evidence has the same content as the original judgment. So if you’re rational, and you come to believe that \\(p\\) is likely true, then the rational thing to believe given you’ve made that judgment is that \\(p\\) is likely true.\nNote that this isn’t as strong a requirement as it may first seem. The requirement is not that any time an agent makes a judgment, rationality requires that they say on reflection that it is the correct judgments. Rather, the requirement is that the only judgments rational agents make are those judgments that, on reflection, she would reflectively endorse. We can think of this as a kind of ratifiability constraint on judgment, like the ratifiability constraint on decision making that Richard Jeffrey uses to handle Newcomb cases Jeffrey (1983).\nTo be a little more precise, a judgment is ratifiable for agent \\(S\\) just in case the rational judgment for \\(S\\) to make conditional on her having made that judgment has the same content as the original judgment. The thought then is that we avoid the regress by saying rational agents always make ratifiable judgments. If the agent does do that, there isn’t much of a problem with the regress; once she gets to the first level, she has a stable view, even once she reflects on it.\nIt seems to me that this assumption, that only ratifiable judgments are rational, is what drives most of the arguments in Egan and Elga’s paper on self-confidence, so I don’t think this is a straw-man move. Indeed, as the comparison to Jeffrey suggests, it has some motivation behind it. Nevertheless it is false. I’ll first note one puzzling feature of the view, then one clearly false implication of the view.\nThe puzzling feature is that in some cases there may be nothing we can rationally do which is ratifiable. One way this can happen involves a slight modification of Egan and Elga’s example of the directionaly-challenged driver. Imagine that when I’m trying to decide whether \\(p\\), for any \\(p\\) in a certain field, I know (a) that whatever judgment I make will usually be wrong, and (b) if I conclude my deliberations without making a judgment, then \\(p\\) is usually true. If we also assume JSE, then it follows there is no way for me to end deliberation. If I make a judgment, I will have to retract it because of (a). But if I think of ending deliberation, then because of (b) I’ll have excellent evidence that \\(p\\), and it would be irrational to ignore this evidence. (Nicholas Silins (2005) has used the idea that failing to make a judgment can be irrational in a number of places, and those arguments motivated this example.)\nThis is puzzling, but not obviously false. It is plausible that there are some epistemic dilemmas, where any position an agent takes is going to be irrational. (By that, I mean it is at least as plausible that there are epistemic dilemmas as that there are moral dilemmas, and I think the plausibility of moral dilemmas is reasonably high.) That a case like the one I’ve described in the previous paragraph is a dilemma is perhaps odd, but no reason to reject the theory.\nThe real problem, I think, for the ratifiability proposal is that there are cases where unratifiable judgments are clearly preferable to ratifiable judgments. Assume that I’m a reasonably good judge of what’s likely to happen in baseball games, but I’m a little over-confident. And I know I’m over-confident. So the rational credence, given some evidence, is usually a little closer to \\(\\frac{1}{2}\\) than I admit. At risk of being arbitrarily precise, let’s say that if \\(p\\) concerns a baseball game, and my credence in \\(p\\) is \\(x\\), the rational credence in \\(p\\), call it \\(y\\), for someone with no other information than this is given by:\n\\[y = x + \\frac{sin(2\\pi x)}{50}\\]\nTo give you a graphical sense of how that looks, the dark line in this graph is \\(y\\), and the lighter diagonal line is \\(y = x\\).\n\nimage\nNote that the two lines intersect at three points: \\((0, 0), (\\frac{1}{2}, \\frac{1}{2})\\) and \\((1, 1)\\). So if my credence in \\(p\\) is either 0, \\(\\frac{1}{2}\\) or 1, then my judgment is ratifiable. Otherwise, it is not. So the ratifiability constraint says that for any \\(p\\) about a baseball game, my credence in \\(p\\) should be either 0, \\(\\frac{1}{2}\\) or 1. But that’s crazy. It’s easy to imagine that I know (a) that in a particular game, the home team is much stronger than the away team, (b) that the stronger team usually, but far from always, wins baseball games, and (c) I’m systematically a little over-confident about my judgments about baseball games, in the way just described. In such a case, my credence that the home team will win should be high, but less than 1. That’s just what the ratificationist denies is possible.\nThis kind of case proves that it isn’t always rational to have ratifiable credences. It would take us too far afield to discuss this in detail, but it is interesting to think about the comparison between the kind of case I just discussed, and the objections to backwards induction reasoning in decision problems that have been made by Pettit and Sugden (1989), and by Stalnaker (1996, 1998, 1999). The backwards induction reasoning they criticise is, I think, a development of the idea that decisions should be ratifiable. And the clearest examples of when that reasoning fails concern cases where there is a unique ratifiable decision, and it is guaranteed to be one of the worst possible outcomes. The example I described in the last few paragraphs has, quite intentionally, a similar structure.\nThe upshot of all this is that I think these regress arguments work. They aren’t, I think, directly an argument against EW. What they are is an argument against the most promising way the EW theorist has for arguing that the table I started with misstates \\(S\\)’s epistemic situation. Given that the regress argument against JSE works though, I don’t see any way of rescuing EW from this argument.\n\n\nChristensen, David. 2007. “Epistemology of Disagreement: The Good News.” Philosophical Review 116 (2): 187–217. https://doi.org/10.1215/00318108-2006-035.\n\n\n———. 2011. “Disagreement, Question-Begging and Epistemic Self-Criticism.” Philosophers’ Imprint 11 (6): 1–22. http://hdl.handle.net/2027/spo.3521354.0011.006.\n\n\nEgan, Andy, and Adam Elga. 2005. “I Can’t Believe I’m Stupid.” Philosophical Perspectives 19 (1): 77–93. https://doi.org/10.1111/j.1520-8583.2005.00054.x.\n\n\nElga, Adam. 2007. “Reflection and Disagreement.” Noûs 41 (3): 478–502. https://doi.org/10.1111/j.1468-0068.2007.00656.x.\n\n\n———. 2010. “How to Disagree about How to Disagree.” In Disagreement, edited by Ted Warfield and Richard Feldman, 175–87. Oxford: Oxford University Press.\n\n\nFeldman, Richard. 2005. “Respecting the Evidence.” Philosophical Perspectives 19 (1): 95–119. https://doi.org/10.1111/j.1520-8583.2005.00055.x.\n\n\n———. 2006. “Epistemological Puzzles about Disagreement.” In Epistemology Futures, edited by Stephen Cade Hetherington, 216–26. Oxford: Oxford University Press.\n\n\nJeffrey, Richard C. 1983. The Logic of Decision. 2nd ed. Chicago: University of Chicago Press.\n\n\nKelly, Thomas. 2005. “The Epistemic Significance of Disagreement.” Oxford Studies in Epistemology 1: 167–96.\n\n\n———. 2010. “Peer Disagreement and Higher Order Evidence.” In Disagreement, edited by Ted Warfield and Richard Feldman, 111–74. Oxford: Oxford University Press.\n\n\nPettit, Philip, and Robert Sugden. 1989. “The Backward Induction Paradox.” Journal of Philosophy 86 (4): 169–82. https://doi.org/10.2307/2026960.\n\n\nSilins, Nicholas. 2005. “Deception and Evidence.” Philosophical Perspectives 19: 375–404. https://doi.org/10.1111/j.1520-8583.2005.00066.x.\n\n\nStalnaker, Robert. 1996. “Knowledge, Belief and Counterfactual Reasoning in Games.” Economics and Philosophy 12: 133–63. https://doi.org/10.1017/S0266267100004132.\n\n\n———. 1998. “Belief Revision in Games: Forward and Backward Induction.” Mathematical Social Sciences 36 (1): 31–56. https://doi.org/10.1016/S0165-4896(98)00007-9.\n\n\n———. 1999. “Extensive and Strategic Forms: Games and Models for Games.” Research in Economics 53 (3): 293–319. https://doi.org/10.1006/reec.1999.0200.\n\n\nWeatherson, Brian. 2007. “The Bayesian and the Dogmatist.” Proceedings of the Aristotelian Society 107: 169–85. https://doi.org/10.1111/j.1467-9264.2007.00217.x.\n\n\nWedgwood, Ralph. 2007. The Nature of Normativity. Oxford: Oxford University Press.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nThough if EW is correct, shouldn’t the scholarly journals be full of just this information?↩︎\nThis is obviously somewhat of an idealisation, since there won’t usually be a unique precise rational response to the evidence. But I don’t think this idealisation hurts the argument to follow. I should note that the evidence here excludes their statements of their credences, so I really mean the evidence that they brought to bear on the debate over whether \\(p\\).↩︎\nAdded in 2010: I still think there’s a dilemma here for EW, but I’m less convinced than I used to be that MJ is correct.↩︎\nBy ex ante and ex post I mean before and after we learn about \\(S\\) and \\(T\\)’s use of \\(E\\) to make a judgment about \\(p\\). I think that should change how reliable we take \\(S\\) and \\(T\\) to be, and that this should matter to what use, if any, we put their judgments, but it is crucial to EW that we ignore this evidence. Or, at least, it is crucial to EW that \\(S\\) and \\(T\\) ignore this evidence.↩︎\nMy explanation is that evidence screens any judgments made on the basis of that evidence, in the sense of screening to be described below.↩︎\nIn general I’m sceptical of always treating evidence probabilistically. Some of my reasons for scepticism are in Weatherson (2007).↩︎\nBranden Fitelson pointed out to me that the probabilistic version entails one extra condition, namely that \\(\\neg B \\wedge C\\) is no worse evidence for \\(A\\) than \\(C\\) is. But I think that extra condition is irrelevant to disagreement debates, so I’m leaving it out.\n\n↩︎\n",
    "preview": "posts/2021-03-04-disagreements-philosophical-and-otherwise/disagreement.jpg",
    "last_modified": "2021-03-04T15:46:38-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-the-role-of-naturalness-in-lewiss-theory-of-meaning/",
    "title": "The Role of Naturalness in Lewis's Theory of Meaning",
    "description": "Many writers have held that in his later work, David Lewis adopted a theory of predicate meaning such that the meaning of a predicate is the most natural property that is (mostly) consistent with the way the predicate is used. That orthodox interpretation is shared by both supporters and critics of Lewis's theory of meaning, but it has recently been strongly criticised by Wolfgang Schwarz. In this paper, I accept many of Schwarz's criticisms of the orthodox interpretation, and add some more. But I also argue that the orthodox interpretation has a grain of truth in it, and seeing that helps us appreciate the strength of Lewis's late theory of meaning.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2013-05-20",
    "categories": [
      "David Lewis",
      "language",
      "metaphysics",
      "history of analytic"
    ],
    "contents": "\n\nContents\nHow Naturalness Enters The Theory of Meaning\nTextual Evidence about Sentence Meaning\nTextual Evidence and Naturalness and Rationality\nWord Meaning and Naturalness\nAn Argument for the Orthodox Interpretation\nIndeterminacy and Radically Deviant Interpretations\nWhat is the Use of a Predicate?\nFrom Theory to Applied Semantics\n\nIt is sometimes claimed (e.g., by Sider (2001a, 2001b; Stalnaker 2004; Williams 2007; Weatherson 2003)) that David Lewis’s theory of predicate meaning assigns a central role to naturalness.1 Some of the people who claim this also say that the theory they attribute to Lewis is true. The authors I have mentioned aren’t as explicit as each other about exactly which theory they are attributing to Lewis, but the rough intuitive idea is that the meaning of a predicate is the most natural property that is more-or-less consistent with the usage of the predicate. Call this kind of interpretation the ‘orthodox’ interpretation of Lewis.2 Recently Wolfgang Schwarz (2009, 209ff) has argued that the orthodox interpretation is a misinterpretation, and actually naturalness plays a much smaller role in Lewis’s theory of meaning than is standardly assumed.3 Simplifying a lot, one key strand in Schwarz’s interpretation is that naturalness plays no role in the theory of meaning in Lewis (1969, 1975), since Lewis hadn’t formulated the concept yet, and Lewis didn’t abandon that theory of meaning, since he never announced he was abandoning it, so naturalness doesn’t play anything like the role orthodoxy assigns to it.\n\nPublished in Journal for the History of Analytic Philosophy, volume 1, number 10.\nIn this article I attempt to steer a middle ground between these two positions. I’m going to defend the following parcel of theses. These are all exegetical claims, but I’m also interested in defending most of the theses that I ultimately attribute to Lewis, so getting clear on just what Lewis meant is of more than historical interest.\nNaturalness matters to Lewis’s (post-1983) theory of sentence meaning only insofar as it matters to his theory of rationality, and the theory of rationality matters to the (pre- and post-1983) theory of meaning.\nNaturalness might play a slightly larger role in Lewis’s theory of word meaning, but it isn’t nearly as significant as the orthodox view suggests.\nWhen we work through Lewis’s theory of word and sentence meaning, we see that the orthodox interpretation assigns to Lewis a theory that isn’t his theory of meaning, but is by his lights a useful heuristic.\nAn even better heuristic than ‘meaning = use plus naturalness’ would be ‘meaning = predication plus naturalness,’ but even this would be a fallible heuristic, not a theory.\nWhen correctly interpreted, Lewis’s theory is invulnerable to the challenges put forward in Williams (2007).\nI’m going to start by saying a little about the many roles naturalness plays in Lewis’s philosophy, and about his big picture views on thought and meaning. Then I’ll offer a number of arguments against the orthodox interpretation of Lewis’s theory of sentence meaning. After that, I’ll turn to Lewis’s theory of word meaning, where it is harder to be quite clear about just what the theory is, and how much it might have changed once natural properties were added to the metaphysics. An appendix discusses some interpretative questions that arise if we are sceptical that any one division of properties can do all the work that Lewis has the natural/non-natural division do.\nHow Naturalness Enters The Theory of Meaning\nMost of the core elements of David Lewis’s philosophy were present, at least in outline, from his earliest work. The big exception is the theory of natural properties introduced in Lewis (1983). As he says in that paper, he had previously believed that “set theory applied to possibilia is all the theory of properties that anyone could ever need” (Lewis 1983, 377n). Once he introduces this new concept of naturalness, Lewis puts it to all sorts of work throughout his philosophy. I’m rather sceptical that there is any one feature of properties that can do all the varied jobs Lewis wants naturalness to do, but the grounds for, and consequences of, this scepticism are a little orthogonal to the main theme of this paper, so I’ve set it aside.\nAs the orthodox interpretation stresses, Lewis has naturalness do some work in this theory of content. That he does think there’s a connection between naturalness and content is undeniable from the most casual reading of his post-1983 work. But just how they are connected is less obvious. To spell out these connections, let’s start with three Lewisian themes.\nFacts about linguistic meaning are to be explained in terms of facts about minds. In particular, to speak a language \\(\\mathcal{L}\\) is to have a convention of being truthful and trusting in \\(\\mathcal{L}\\) (Lewis 1969, 1975). And to have such a convention is a matter of having certain beliefs and desires. So mental content is considerably prior to linguistic content in a Lewisian theory. Moreover, Lewis’s theory of linguistic content is, in the first instance, a theory of sentence meaning, not a theory of word meaning.4\nThe principle of charity plays a central role in Lewis’s theory of mental content Lewis (1974, 1994). To a first approximation, a creature believes that \\(p\\) iff the best interpretation of the creature’s behavioural dispositions includes the attribution of the belief that \\(p\\) to the creature. And, ceteris paribus, it is better to interpret a creature so that it is more rather than less rational. It will be pretty important for what follows that Lewis adopts a principle of charity that highlights rationality, not truth. It is also important to Lewis that we don’t just interpret the individual creature, but creatures of a kind (Lewis 1980). I’m not going to focus on the social externalist features of Lewis’s theory of mental states, but I think they assist the broader story I want to tell.\nLewis’s theory of mental content has it that mental contents are (what most of us would call) properties, not (what most of us would call) propositions (Lewis 1979). So a theory of natural properties can easily play a role in the theory of mental content, since mental contents are properties. If you think mental contents are propositions, the connection between naturalness and mental content will be more indirect. Just how indirect it is will depend on what your theory of propositions is. But if mental contents are Lewisian propositions, the connection may be very indirect indeed. After all, propositions that we might pick out with sentences containing words that denote very unnatural properties, such as All emeroses are gred, might be intuitively very natural.\nNow let’s see why we might end up with naturalness in the theory of meaning. An agent has certain dispositions. For instance, after seeing a bunch of green emeralds, and no non-green emeralds, in a large and diverse range of environments, she has a disposition to say “All emeralds are green.” In virtue of what is she speaking a language in which “green” means green, and not grue? (Note that when I use “grue,” I mean a property that only differs from greenness among objects which it is easy to tell that neither our agent, nor any of her interlocutors, could possibly be acquainted with at the time she makes the utterance in question.)\nLet’s say that \\(\\mathcal{L}_1\\) is English, i.e., a language in which “green” means green, and \\(\\mathcal{L}_2\\) a language which is similar to \\(\\mathcal{L}_1\\) except that “green” means grue. Our question is, what makes it the case that the agent is speaking \\(\\mathcal{L}_1\\) and not \\(\\mathcal{L}_2\\)? That is, what makes it the case that the agent has adopted the convention of being truthful and trusting in \\(\\mathcal{L}_1\\), and not the convention or being truthful and trusting in \\(\\mathcal{L}_2\\)?\nWe assumed that the agent has seen a lot of emeralds which are both green and grue. To a first approximation, it is more charitable to attribute to the agent the belief that all emeralds are green than the belief that all emeralds are grue because greenness is more natural than gruesomeness. As Lewis says, “The principles of charity will impute a bias towards believing things are green rather than grue” (1983, 375). And for Lewis, charity requires imputing more reasonable interpretations. But why is it more charitable to attribute beliefs about greenness to beliefs about grueness? I think it is because we need more evidence to rationally form a belief that some class of things are all grue than we need to form a belief that everything in that class is green. And that’s because, ceteris paribus, we need more evidence to rationally form a belief that all \\(F\\)s are \\(G\\)s than that all \\(F\\)s are \\(H\\)s when \\(G\\) is less natural than \\(H\\). The agent has, we might assume, sufficient evidence to rationally believe that all emeralds are green, but not sufficient evidence to believe that all emeralds are grue.\nSo the first two Lewisian themes notes above, the reduction of linguistic meaning to mental content, and the centrality of a rationality-based principle of charity, push us towards thinking that naturalness is closely connected to mental content and hence to linguistic meaning. And it has pushed us towards thinking that if naturalness is connected to meaning, it is via this connection I’ve posited between naturalness and rational belief. Note that Lewis doesn’t ever endorse anything like that general a connection, but I suspect he had something like this in mind when he wrote the sentence I quoted in the previous paragraph. We’ll come back to this interpretative question at some length below.\nBut the argument I offered was a bit quick, because I ignored the third Lewisian theme: beliefs are relations to properties, not propositions. On Lewis’s theory, to believe that all emeralds are green is to self-ascribe the property of being in a world where all emeralds are green. So if a certain body of evidence makes it possible for the agent to rationally believe that all emeralds are green, but not for her to believe that all emeralds are grue, and that’s because rationality is constitutively connected to naturalness, then that must be because the first of the following properties is more natural than the second:\nBeing in a world where all emeralds are green\nBeing in a world where all emeralds are grue\nThat could still be true, though it is notable how far removed we are from the intuitions that motivate the distinctions between more and less natural properties. It’s not like there is some sense, intuitively, in which things that have the first property form a more unified class than things that have the second property.\nSo it’s plausible that naturalness is connected to mental content, at least as long as naturalness is connected to rational belief. And since mental content is connected to linguistic content, we’re now in the vicinity of the orthodox interpretation. But I don’t think the orthodox interpretation can be right. I’ll give four reasons for this, starting with the textual evidence for and against it.\nTextual Evidence about Sentence Meaning\nThere is some prima facie textual evidence for the orthodox interpretation. But looking more careful at the context of these texts not just undermines the support the text gives to the orthodox interpretation, but actually tells against it. (This part of the paper is indebted even more than the rest to Wolfgang Schwarz’s work, and could be easily skipped by those familiar with that work.)\nI’ll focus on the last seven pages of “New Work for a Theory of Universals.” This is the part of “New Work” that uses the notion of naturalness, as introduced in the paper, to respond to Putnam’s model-theoretic arguments for massive indeterminacy of meaning. Lewis actually responds to Putnam twice over. First, he responds to Putnam directly, by showing how adding naturalness to a use-based theory of sentence meaning avoids the ‘just more theory’ objection that’s central to Putnam’s argument. And when Lewis describes this direct response, he says things that sound a lot like the orthodox interpretation.\n\nI would instead propose that the saving constraint concerns the referent - not the referrer, and not the causal channels between the two. It takes two to make a reference, and we will not find the constraint if we look for it always on the wrong side of the relationship. Reference consists in part of what we do in language or thought when we refer, but in part it consists in eligibility of the referent. And this eligibility to be referred to is a matter of natural properties. (Lewis 1983, 371)\n\nBut after this direct response is finished, Lewis notes that he has conceded quite a lot to Putnam in making the response.\n\nYou might well protest that Putnam’s problem is misconceived, wherefore no need has been demonstrated for resources to solve it. … Where are the communicative intentions and the mutual expectations that seem to have so much to do with what we mean? In fact, where is thought? …I think the point is well taken, but I think it doesn’t matter. If the problem of intentionality is rightly posed there will still be a threat of radical indeterminacy, there will still be a need for saving constraints, there will still be a remedy analogous to Merrill’s suggested answer to Putnam, and there will still be a need for natural properties. (Lewis 1983, 373)\n\nI noted earlier that Schwarz makes much of a similar passage in “Putnam’s Paradox,” and I think he is right to do so. Here’s a crucial quote from that paper.\n\nI shall acquiesce in Putnam’s linguistic turn: I shall discuss the semantic interpretation of language rather than the assignment of content to attitudes, thus ignoring the possibility that the latter settles the former. It would be better, I think, to start with the attitudes and go on to language. But I think that would relocate, rather than avoid, the problem; wherefore I may as well discuss it on Putnam’s own terms. (Lewis 1984, 222)\n\nThat passage ends with a footnote where he says the final section of “New Work” contains a version of how the ‘relocated’ problem would be solved. So let’s turn back to that. The following long portmanteau quote from pages 373 to 375 captures, I think, the heart of my interpretation.\n\nThe problem of assigning content to functionally characterised states is to be solved by means of constraining principles. Foremost among these are principles of fit. …A state typically caused by round things before the eyes is a good candidate for interpretation as the visual experience of confronting something round; and its typical impact on the states interpreted as systems of belief ought to be interpreted as the exogenous addition of a belief that one is confronting something round, with whatever adjustment that addition calls for. …Call two worlds equivalent iff they are alike in respect of the subject’s evidence and behaviour, and note that any decent world is equivalent inter alia to horrendously counterinductive worlds and to worlds where everything unobserved by the subject is horrendously nasty. …We can interchange equivalent worlds ad lib and preserve fit. So, given any fitting and reasonable interpretation, we can transform it into an equally fitting perverse interpretation by swapping equivalent worlds around …If we rely on principles of fit to do the whole job, we can expect radical indeterminacy of interpretation. We need further constraints, of the sort called principles of (sophisticated) charity, or of ‘humanity.’ [A footnote here refers to \"Radical Interpretation\".] Such principles call for interpretations according to which the subject has attitudes that we would deem reasonable for one who has lived the life that he has lived. (Unlike principles of crude charity, they call for imputations of error if he has lived under deceptive conditions.) These principles select among conflicting interpretations that equally well conform to the principles of fit. They impose apriori – albeit defeasible - presumptions about what sorts of things are apt to be believed and desired …It is here that we need natural properties. The principles of charity will impute a bias toward believing that things are green rather than grue …In short, they will impute eligible content …They will impute other things as well, but it is the imputed eligibility that matters to us at present. (Lewis 1983, 373–75, my emphasis)\n\nI think that does a reasonably clear job of supporting the interpretation I set out in the introduction over the orthodox interpretation. Naturalness matters to linguistic meaning all right. But the chain of influence is very long and indirect. Naturalness constrains what is reasonable, reasonableness constrains charitable interpretations, charitable interpretations constrain mental content, and mental content constrains linguistic content. Without naturalness at the first step, we get excessive indeterminacy of content. With it, the Putnamian problems are solved. But there’s no reason to think naturalness has any more direct role to play at any level in the theory of linguistic content.\nIn short, Lewis changed what he thought about rationality when he adopted the theory of natural properties. Since rationality was a part of his theory of mental content, and mental content determines linguistic content, this change had downstream consequences for what he said about linguistic content. But there wasn’t any other way his theory of linguistic content changed, nor, contra orthodoxy, any direct link between naturalness and predicate meaning.\nMoreover, when we look at the closest thing to a worked example in Lewis (1983), we don’t get any motivation for the orthodox interpretation. Here’s the example he uses, which concerns mental content. Let \\(f\\) be any mapping from worlds to worlds such that the agent has the same evidence and behaviour in \\(w\\) and \\(f(w)\\). Extend \\(f\\) to a mapping from sets of worlds to sets of worlds in the following (standard) way: \\(f(S) = \\{f(w): w \\in S\\}\\). Then the agent’s behaviour will be rationalised by her evidence just as much if she has credence function \\(C\\) and value function \\(V\\), as if she has credence function \\(C^\\prime\\) and value function \\(V^\\prime\\), where \\(C^\\prime(f(S)) = C(S)\\), and \\(V^\\prime(f(S)) = V(S)\\). To relate this back to the familiar Goodmanian puzzle, let \\(f\\) map any world where all emeralds are green to nearest world where all emeralds are grue, and vice versa, and map any other world to itself. Then the above argument will say that the agent’s behaviour is rationalised by her evidence just as much as if her credences are \\(C\\) as if they are \\(C^\\prime\\). That is, her behaviour is rationalised by her evidence just as much if she gives very high credence to all emeralds being green as to all emeralds being grue. So understanding charity merely as rationalizing behaviour leaves us without a way to say that the agent believes unobserved emeralds are green and not grue.\nLewis’s solution is to say that charity requires more than that. In particular, it requires that we assign natural rather than unnatural beliefs to agents where that is possible. I’ve argued above that this makes perfect sense if we connect naturalness with rationality. The crucial thing to note here is that this all happens a long time before we can set out the way that a sentence is used, since the way a sentence is used on Lewis’s theory of linguistic content includes the beliefs that are formed on hearing it. So the discussion in “New Work” suggests that naturalness matters for content, but not in a way that can be easily factorised out. And that’s exactly what I think is the best way to understand Lewis’s theory.\nTextual Evidence and Naturalness and Rationality\nA major part of my argument above was that naturalness affected Lewis’s theory of rationality. In particular, once he had naturalness to work with, he seemed to think that it was more rational to project natural rather than unnatural properties. The textual evidence for this is, I’ll admit, fragmentary. But it is fairly widespread. Let’s start with a quote we’ve already seen.\n\nThe principles of charity will impute a bias toward believing that things are green rather than grue (Lewis 1983, 375)\n\nAs noted above, I assume this isn’t a special feature of green and grue, but rather that there is a general principle in favour of projecting natural properties. But it would be good to have more evidence for that.\nLewis returns to the example of the believer in grue emeralds a few times. Here is one version of the story in Plurality.\n\nWe think that some sorts of belief and desire \\(\\dots\\) would be unreasonable in a strong sense \\(\\dots\\) utterly unintelligible and nonsensical. Think of the man who, for no special reason, expects unexamined emeralds to be grue. \\(\\dots\\) What makes the perversely twisted assignment of content incorrect, however well it fits the subject’s behaviour, is exactly that it assigns ineligible, unreasonable content when a more eligible assignment would have fit behaviour equally well. (Lewis 1986, 38–39)\n\nAnd a little later, when replying to Kaplan’s paradox, he says,\n\nGiven a fitting assignment, we can scramble it into an equally fitting but perverse alternative assignment. Therefore a theory of content needs a second part: as well as principles of fit, we need ‘principles of humanity,’ which create a presumption in favour of some sorts of content and against others. (Lewis 1986, 107)\n\nHe returns to this point again in “Reduction of Mind.”\n\n[Folk psychology] sets presumptive limits on what our contents of belief and desire can be. Self-ascribed properties may be ‘far from fundamental,’ I said – but not too far. Especially gruesome gerrymanders are prima facie ineligible to be contents of belief and desire. In short, folk psychology says that we make sense. It credits us with a modicum of rationality in our acting, believing and desiring. (Lewis 1994, 320 in reprint)\n\nThe running thread through these last three quotes is that our theory of mental content rules out gruesome assignments, and it does this because assigning rationality is constitutive of correctly interpreting. This can only work if naturalness is connected to rationality. I’ve attributed a stronger claim to Lewis, that not only is naturalness connected to rationality, but that the connection goes through projection.5\nOne piece of evidence for that is that Lewis says, in “Meaning Without Use” that Kripkenstein’s challenge was “formerly Goodman’s challenge” (Lewis 1992, 109). He goes on to say that the solution to this challenge (or should that be ‘these challenges’) involves “carrying more baggage of primitive distinctions or ontological commitments than some of us might have hoped” (Lewis 1992, 110). A footnote on that sentence cites “New Work,” in case it isn’t obvious that the baggage here is the distinction between natural and unnatural properties. So somehow, Lewis thinks that natural properties help solve Goodman’s puzzle. I think that the simplest such solution is the right one to attribute to Lewis; natural properties are prima facie more eligible to be projected.\nA referee noted that this passage is a little odd; it appears to simply conflate a meta-semantical paradox with an epistemological paradox. But I think that just shows how much, for Lewis, meta-semantical questions are epistemological questions. Words get their meanings in virtue of our conventions. Our conventions consist of our beliefs and desires. And facts about rationality are, in part, constitutive of what we believe and desire.\nFinally, consider the way in which the papers on natural properties are introduced in Papers in Metaphysics and Epistemology. Lewis says that “I had been persuaded by Goodman and others that all properties were equal: it was hopeless to try to distinguish ‘natural’ properties from gruesomely gerrymandered, disjunctive properties.” (Lewis 1999, 1–2) A footnote refers to Fact, Fiction and Forecast. Of course, the point of “New Work” is that Lewis abandons this, explicitly Goodmanian, view. Now that he had learned property egalitarianism from Goodman of course doesn’t show that once he became a property inegalitarian, he applied this to Goodman’s own paradox. But it does seem striking that the only citation of an egalitarian view is of Fact, Fiction and Forecast. I take that to be some, inconclusive, evidence that Lewis did indeed think natural properties were related to Goodman’s paradox.\nUltimately, it seems the textual evidence is this. There are many different occasions where Lewis makes clear there is a connection between naturalness and rationality, and in particular, between naturalness and the kind of rationality that is relevant to content assignment. There are hints that this connection goes via naturalness playing a role in solving Goodman’s paradox. Notably, there is no other obvious way in which naturalness could connect to rationality. At least, I can neither think of another connection, nor see any evidence for another connection in the Lewis corpus. So I conclude, a little tentatively, that Lewis thought natural properties had a role to play in solving Goodman’s paradox.\nWord Meaning and Naturalness\nIn “Languages and Language,” Lewis doesn’t say that human linguistic practices merely determine truth conditions for the spoken sentences. That is, our linguistic practices don’t merely determine which language, in Lewis’s sense, we speak. They also determine, to some extent, a grammar, which specifies the truth conditional contribution of the various parts of the sentence. The grammar determines the “fine structure of meaning” (Lewis 1975, 177) of a sentence or phrase.\nIn comments on an earlier draft of this paper, an anonymous referee stressed that naturalness could enter directly into a theory of meaning once we stopped focussing on sentence meaning, and started looking on word meaning. I don’t mean to say the referee was endorsing any particular role for naturalness in the theory of word meaning. But the point that we need to say more about the Lewisian approach to word meaning before we conclude that naturalness is only indirectly related to meaning is right. And I’m grateful for the encouragement to discuss it further.\nLewis has a short discussion of grammars in “Languages and Language,” and another in “Radical Interpretation.” It’s worth looking at both of these in turn. I’ll take “Languages and Language” first, since even though it has a slightly later publication date, in the respects we’re discussing here it closely resembles the theory in Convention.\nOn pages 177-8 of that paper, Lewis notes three ways in which there may be indeterminacy in the grammar.\nA subject’s behavioural dispositions and anatomy might underdetermine their beliefs and desires.\nThe beliefs and desires might underdetermine the truth conditions of their language.\nThe truth conditions of the language might underdetermine the meanings of the individual words.\nWhile Lewis does not think the second is actually a source of indeterminacy, he does think that the third is.\n\nMy present discussion has been directed at the middle step \\(\\dots\\) I have said \\(\\dots\\) that the beliefs and desires of the subject and his fellows are such as to comprise a fully determinate convention of truthfulness and trust in some definite language. \\(\\dots\\) I am inclined to share in Quine’s doubts about the determinacy of the third step. (Lewis 1975, 178)\n\nLewis gives reasons for this inclination a few paragraphs earlier. He says that while we can say what it is for a community to speak one language rather than another, we can’t say what it is for a community to speak one grammar rather than another. He says that we don’t have any objective measures for evaluating grammars. And he says Quine’s examples of indeterminacy of reference show that languages can have multiple good grammars, even if these disagree radically about the meaning of some constituents.\nNotably, Lewis doesn’t take to show that there is anything wrong with the notion of word meaning. He says it would be “absurd” (177) to conclude that. His conclusion here is more one of modesty rather than philosophical scepticism. We don’t know how to extend the theory of sentence meaning he offers to a theory of word meaning, so we should do what we can without talking about word meaning.\nThe approach in “Radical Interpretation” has a bit more of a hint for how to restore semantic determinacy. The subject matter of that paper is how to solve for the mental and linguistic contents of a speaker, called Karl, given the physical facts about them. Lewis uses M for “a specification, in our language, of the meanings of expressions of Karl’s language.” (Lewis 1974, 333) He lists a number of constraints on a solution, including early versions of his principles of constitutive rationality. But the most notable constraint, from our perspective, is this:\n\nThe Principle of Generativity constrains M: M should assign truth conditions to the sentences of Karl’s language in a way that is at least finitely specifiable, and preferably also reasonably uniform and simple. (Lewis 1974, 339)\n\nThere’s something very odd about this. Lewis, in 1974, didn’t have a theory of what made an assignment simple. He needed his theory of natural properties to do that. Or, at least, once he had the theory of natural properties, it did all the work he ever wanted out of an account of simplicity.\nBe that as it may, it does suggest that Lewis did think that simplicity of assignments could be used as a way of cutting down the third kind of semantic indeterminacy discussed in “Languages and Language.” He doesn’t think it would generate a fully determinate interpretation of Karl’s language.\n\nIt seems hopeless to deny, in the face of such examples as those in [Quine’s “Ontological Relativity,” pp. 30-39], that the truth conditions of full sentences in M do not sutfice to determine the rest of M: the parsings and the meanings of the constituents of sentences. At least, that is so unless there is something more than our Principle of Generativity to constrain this auxiliary syntactic and semantic apparatus. (Lewis 1974, 342–43)\n\nIt’s notable that some of the examples Quine gives in “Ontological Relativity” are not cases where the alternative meanings are by any measure equally natural. This positive allusion to Quine’s examples suggests a link to this comment in “Languages and Language”\n\nWe should regard with suspicion any method that purports to settle objectively whether, in some tribe, “gavagai” is true of temporally continuant rabbits or time-slices thereof. You can give their language a good grammar of either kind—and that’s that. (Lewis 1975, 177)\n\nNote that he doesn’t say ‘equally’ good. And note also how this contrasts with the attitude he takes towards the prospects of indeterminacy in sentence meaning. I earlier quoted him saying that part of the point of “Languages and Language” was to show how the second type of indeterminacy didn’t arise. He ends “Radical Interpretation” with this ‘credo.’\n\nCould indeterminacy of beliefs, desires, and truth conditions also arise because two different solutions both fit all the constraints perfectly? Here is the place to hold the line. This sort of indeterminacy has not been shown by convincing examples, and neither could it be shown–to me–by proof. Credo: if ever you prove to me that all the constraints we have yet found could permit two perfect solutions, differing otherwise than in the auxiliary apparatus of M, then you will have proved that we have not yet found all the constraints. (Lewis 1974, 343)\n\nSo that’s where things stood before 1983. Lewis thought he had a theory that eliminated, or at least minimised, indeterminacy at the level of truth conditions. But he didn’t think his theory eliminated indeterminacy, even quite radical indeterminacy, in word meanings. And he didn’t seem bothered by this aspect of the theory; indeed, he thought Quine’s arguments showed that we shouldn’t eliminate this kind of indeterminacy.\nThis attitude towards Quinean arguments for indeterminacy is obviously a striking contrast to the forcefulness, and rapidity, with which he responded to Putnam’s arguments for indeterminacy. That shouldn’t be too surprising once we attend to Lewis’s threefold distinction between kinds of indeterminacy. Quine was arguing that indeterminacy of the third kind was rampant. Putnam was arguing that indeterminacy of the second kind was rampant. And, as Lewis announced in “Radical Interpretation,” he wasn’t going to believe any such argument.\nStill, we might wonder whether the resources he brought to bear in responding to Putnam also help respond to Quine. Or, perhaps more importantly for exegetical reasons, we might wonder whether Lewis thought they were useful in responding to Quine. The evidence from “New Work” seems to suggest a negative answer to the latter question. Lewis never says that one of the things you can do with the distinction between natural and unnatural properties is respond to arguments for Quinean indeterminacy. And that’s despite the fact that “New Work” has a very survey-like feel; the bulk of the paper is a long list of philosophical work that a theory of universals can do.\nIn “Putnam’s Paradox” there is a brief footnote on Quine’s arguments for indeterminacy. It reads\n\nIt is not clear how much indeterminacy might be expected to remain. For instance, what of Quine’s famous example? His rabbit-stages, undetached rabbit parts, and rabbit-fusion seem only a little, if any, less eligible than rabbits themselves. (Lewis 1984, 228n)\n\nAs I’ve stressed repeatedly, following Schwarz, taking the disclaimers at the start of “Putnam’s Paradox” seriously means that we have to be careful in interpreting what Lewis says about how words acquire determinate meaning in that paper. But even before we adjust for the disclaimers, this is hardly a ringing rejection of Quine’s indeterminacy arguments. The contrast to Lewis’s attitude towards Putnam’s arguments is striking. Since it is the very same contrast that we saw in both “Languages and Language” and “Radical Interpretation,” I think it is fair to assume that he continued to think Quine’s arguments were considerably stronger than Putnam’s.\nBut there is, perhaps, a change of view in “Meaning Without Use.” Here’s the problem Lewis addresses at the end of that paper. Let \\(\\mathcal{L}_1\\) once again be English as we currently understand it, and let \\(\\mathcal{L}_3\\) be just like English, except that it doesn’t assign any truth conditions to sentences over a thousand words long.6 Do our actual linguistic practices manifest a convention of trust in \\(\\mathcal{L}_1\\), or trust in \\(\\mathcal{L}_3\\)? Lewis argues that it is more like a convention of trust in \\(\\mathcal{L}_3\\). If someone utters a very long sentence, we expect some kind of performance error, at best. We don’t, in general, believe what they say. So the theory of “Languages and Language” seems to predict that these long sentences have no truth conditions. But that’s wrong, so the theory must be corrected.\nLewis’s correction appeals, it seems, to natural properties in fixing a grammar. He says that linguistic practice determines truth conditions for a fragment of the language that is widely used. Those truth conditions determine meanings of words. This determination requires natural properties; without them the Quinean problems multiply indefinitely. We then use those word meanings to determine the meaning of unused sentences. A long footnote suggests that the procedure might not be restricted to unused sentences. As long as there is a large enough fragment in which there are conventions of truthfulness and trust, we can extrapolate from that to other parts of the language that are used.\nThis is a marked deviation from anything Lewis had said until then. From the earliest writings, he had stressed a step-by-step approach to content determination. Behavioural dispositions plus physical and biological constraints determine mental content; mental content determines sentence meaning; and sentence meaning determines word meaning. In “Meaning Without Use,” it seemed the last two steps were being somewhat merged.\nBut we shouldn’t overstate how much the third step was allowed to encroach on the second. Lewis does think we need to rule out ‘bent’ grammars, which don’t assign any truth conditions to sentences over a thousand words long, or which give sentences different meanings to what we’d expect if the word ‘cabbage’ appears forty times. But he doesn’t think we need to rule out any ‘straight’ grammar, which includes “any grammar that any linguist would actually propose.” (Lewis 1992, 109)\nSo Lewis’s focus here is to rule out unnatural compositional rules, not unnatural assignments of content to individual words. The reference to linguists here might be useful. Linguists tend to spend much more time on compositional rules than they do on the contents on individual predicates. Notably, Quine didn’t argue for indeterminacy by positing indeterminacy in the compositional rules of the language; his non-standard interpretations all share a standard syntax. If we posit that Lewis thought that there was little syntactic indeterminacy in the language, like there is little indeterminacy at the level of truth conditions of sentences, we can tell a story that doesn’t involve too many unsignalled changes of view. Here’s how I would tell that story in some more detail.\nLewis’s early view, expressed clearly in “Radical Interpretation” and “Languages and Language,” and not retracted before, I think, 1992, has the following parts:\nConventions of truthfulness and trust determine (very sharply) truth conditions for sentences in a speaker’s language.\nAny reasonably good grammar, i.e., assignment of word meanings and compositional rules, that is consistent with the truth conditions is not determinately wrong. There is potentially substantial indeterminacy in the meaning of any given word, because there are many reasonably good grammars consistent with the truth conditions.\nAfter 1983, ‘simplicity’ was understood in terms of naturalness, but otherwise the story doesn’t change a lot.\nThe later view, which goes by somewhat more quickly in “Meaning Without Use,” has the following parts:\nConventions of truthfulness and trust in (the bulk of) the used fragment of the language determine truth conditions for that fragment.\nNaturalness considerations determine the compositional rules for the language by extrapolation from that grammar.\nWord meanings are determined, so far as they are determinate, by the truth conditions for sentences, plus the compositional rules.\nTruth conditions for sentences outside the used fragment are determined by the word meanings and the compositional rules.\nNeither of these views look much like the orthodox view. Remember that the orthodox view has it that considerations of naturalness can be used to resolve debates in metaphysics. That’s certainly the use that Sider (2001a) makes of the orthodox view. But on the early view, simplicity considerations only come in after the truth conditions for every sentence have been determined, and hence so that all debates are settled. And on the later view, simplicity considerations primarily are used to settle truth conditions for unused, or at least unusual, sentences.\nNow if you thought the salient fragment in point 1 of the later view was small, and if you thought naturalness had a major role to play in step 3 of the later view, you would get back to something like the orthodox view. But I don’t see the textual evidence for either of those positions. Lewis says that “the used fragment is large and varied.” (Lewis 1992, 110) It doesn’t look like he is positing wholesale changes to his view on the determination of truth conditions. He is positing some changes; the last two pages of the paper are clearly marked as deviations from his earlier position. But both the examples he uses and the rhetoric around them suggests that the bulk of the changes happen at point 2. Naturalness considerations constraint the syntax of a language much more tightly than they constrain the assignment of meaning to a given word. In sum, at no point in the evolution of his views did Lewis seem to endorse the orthodox interpretation, even as a theory of word meaning.\nAn Argument for the Orthodox Interpretation\nSo far I’ve argued that there is no solid textual support for the orthodox interpretation. My rival interpretation relied on there being a connection between naturalness and induction, and as we’ve just seen, there is some textual evidence for this. But perhaps there is a more indirect way to motivate the orthodox interpretation of Lewis. The orthodox interpretation attributes to Lewis a theory that is quite attractive as a theory of semantic determinacy and indeterminacy. Call that theory the U&N Theory, short for the Use plus Naturalness theory of meaning. Since Lewis was clearly looking for such a theory when he discussed naturalness in the context of his theory of content, it is reasonably charitable to attribute the U&N Theory to him, as the orthodox interpretation does.\nMy response to this will be in three parts. First, I’ll argue in this section that my rival interpretation attributes to Lewis a theory of semantic determinacy and indeterminacy that does just as well at capturing the facts Lewis wanted a theory to capture, so there’s no charity based reason to attribute the U&N Theory to him (And, as we saw in the previous section, there’s no direct textual reason to attribute it to him either.) Second, the U&N Theory is subject to the criticisms in Williams (2007), while the theory I attribute to Lewis is not. Third, the U part of the U&N Theory is hopelessly vague; it isn’t clear how to say what ‘use’ is on a Lewisian theory that makes it suitable to add to naturalness to deliver meanings. Either use is so thick that naturalness is unneeded, or it is so thin that naturalness won’t be sufficient to set meaning. So actually it isn’t particularly charitable to attribute this theory to him.\nStill, let’s start with the attractions of the U&N Theory. On the one hand, agents are inclined to say “All emeralds are green” both in situations where they’ve seen a lot of green emeralds (and no non-green ones) and in situations where they’ve seen a lot of grue emeralds (and no non-grue ones). That’s because, of course, those are exactly the same situations. So at first glance, it doesn’t look like the way in which “green” is used will determine whether it means green or grue. On the other hand, once we add a requirement that terms have a relatively natural meaning, we do get this to fall out as a result. Moreover we can even see how this falls out of a recognisably Lewisian approach to meaning.\nConsider again our agent who says “All emeralds are green” after seeing a lot of emeralds that are both green and grue. And remember that for her to speak a language, she must typically conform to conventions of truthfulness and trust in that language. Now if the agent was speaking \\(\\mathcal{L}_2\\), she would have to think that she’s doing an OK job of being truthful in \\(\\mathcal{L}_2\\) by saying “All emeralds are green.” But that would be crazy. Why should she think that all emeralds are grue given her evidence base? To attribute to her that belief would be to gratuitously attribute irrational beliefs to her. And on Lewis’s picture, gratuitous attributions of irrationality are false. So the agent doesn’t have that belief. So she’s not speaking \\(\\mathcal{L}_2\\).\nThings are even clearer from the perspective of hearers. A hearer of “All emeralds are green” would be completely crazy to come to believe that all emeralds are grue. The hearer knows, after all, that the speaker has no acquaintance with the emeralds that would have to be blue for all emeralds to be grue. So the hearer knows that this utterance could not be sufficient evidence to believe that all emeralds are grue. Yet if she speaks \\(\\mathcal{L}_2\\), she is disposed to believe that all emeralds are grue on hearing “All emeralds are green.” She isn’t irrational, or at least we shouldn’t assign irrationality to her so quickly, so she doesn’t speak \\(\\mathcal{L}_2\\).\nSo it looks like in this one case at least, we have a case where use plus naturalness gives us the right theory. Agents are disposed to use “green” to describe emeralds that are green/grue. But the fact that greenness is more natural than gruesomeness makes it more appropriate to attribute to them a convention according to which “All emeralds are green” means that all emeralds are green and not that all emeralds are grue.\nBut more carefully, what we should say is that the U&N Theory gives us the right result in this case. It doesn’t follow that it will work in all cases, or anything like it. And it doesn’t follow that it works for the right reasons. As we’ll see, neither of those claims are true. In fact, just re-reading the last three paragraphs should undermine the second claim. Because we just saw a derivation that the agents are not speaking \\(\\mathcal{L}_2\\), that didn’t even appeal to the U&N Theory. Rather, that derivation simply used the theory of meaning in Convention and the theory of mental content in “Radical Interpretation.” It’s true that the latter theory assigns a special role to rationality, and the theory of rationality we used has, among other things, a role for natural properties, but that is very different to the idea that naturalness feeds directly into the theory of meaning in the way the orthodox interpretation says. As I said at the start, I think the best interpretation of Lewis is that he changed his theory of rationality in 1983, but that’s the only change to his theory of meaning.\nPut another way, these reflections on “green” and “grue” are consistent with the view that the U&N Theory is a false theory, but a useful heuristic. It’s a useful heuristic because it agrees with the true Lewisian theory in core cases, and is much easier to apply. That’s exactly what I think the U&N Theory is, both as a matter of fact, and as a matter of Lewis interpretation.\nIndeterminacy and Radically Deviant Interpretations\nIf the U&N Theory is a heuristic not a theory, we should expect that it will break down in extreme cases. That’s exactly what we see in the cases discussed in Williams (2007). Those cases highlight the fact that a Lewisian theorist needs to be careful that we don’t end up concluding that normal people, such as the agent in our example who says “All emeralds are green,” speak \\(\\mathcal{L}_4\\). \\(\\mathcal{L}_4\\) is a language in which all sentences express claims about a particular mathematical model (essentially a Henkin model of the sentence the agent accepts), and it is set up in such a way that ordinary English sentences come out true, and about very natural parts of the model. On the U&N Theory, it could easily turn out that ordinary speakers are speaking \\(\\mathcal{L}_4\\), since the assigned meanings are so natural. We can see this isn’t a consequence of Lewis’s theory by working through the case from first principles. I have two arguments here, the first of them relying on some slightly contentious claims about the epistemology of mathematics, the second less contentious.\nAssume, for reductio, that ordinary speakers are speaking \\(\\mathcal{L}_4\\). So, for instance, when O’Leary says “The beer is in the fridge,” what he says is that a certain complicated mathematical model has a certain property. (And indeed it has that property.) Now this won’t be a particularly rational thing for O’Leary to say unless he knows more mathematics than ordinary folks like him ordinarily do. So if O’Leary has adopted a convention of truthfulness and trust in \\(\\mathcal{L}_4\\), then uttering “The beer is in the fridge” would be irrational, even if he is standing in front of the open fridge, looking at the beer. That’s a gratuitous assignment of irrationality, and gratuitous assignments of irrationality are false, so O’Leary doesn’t speak \\(\\mathcal{L}_4\\).\nPerhaps that is too quick. After all, the mathematical claim that \\(\\mathcal{L}_4\\) associates with “The beer is in the fridge” is a necessary truth. And Lewis’s theory of content is intentional, not hyper-intentional. So O’Leary does know it is true. (And when he is standing in front of the fridge, there’s even a sense that he knows that “The beer is in the fridge” expresses a truth, if \\(\\mathcal{L}_4\\) is really his language.) I think that’s probably not the right sense of “rational,” and I’m not altogether sure how much hostility to hyper-intensionalism we should attribute to Lewis. But so as to avoid these questions, it’s easier to consider a different argument that focusses attention on O’Leary’s audience.\nWhen O’Leary says “The beer is in the fridge,” Daniels hears him, and then walks to the fridge. Why does Daniels make such a walk? Well, he wants beer, and believes it is in the fridge. That looks like a nice rational explanation. But why does he believe the beer is in the fridge? I say it’s because he’s (rationally) adopted a convention of truthfulness and trust in \\(\\mathcal{L}_1\\), and so he rationally comes to believe the beer is in the fridge when O’Leary says “The beer is in the fridge.” On the assumption that O’Leary and Daniels speak \\(\\mathcal{L}_4\\), none of this story goes through. But we must have some rational explanation of why O’Leary’s statement makes Daniels walk to the fridge. So O’Leary and Daniels must not be speaking \\(\\mathcal{L}_4\\).\nMichael Morreau pointed out (when I presented this talk at CSMN) that the preceding argument may be too quick. Perhaps there is a way of rationalising Daniels’s actions upon hearing O’Leary’s words consistent with the idea that they both speak \\(\\mathcal{L}_4\\). Perhaps, for instance, Daniels’s walking to the fridge constitutes saying something in a complicated sign language, and that thing is the rational reply to what O’Leary said. If this kind of response works, and I have no reason to think it won’t, the solution is to increase the costs to Daniels of performing such a reply. For instance, not too long ago I heard Mayor Bloomberg say “Lower Manhattan is being evacuated because of the impending hurricane,” and I (and my family) packed up and evacuated from Lower Manhattan. Even if one could find an interpretation of our actions in evacuating that made them constitute the assertion of a sensible reply to Bloomberg’s mathematical assertion in \\(\\mathcal{L}_4\\), it would be irrational to think I made such an assertion. Evacuating ahead of a storm with an infant is not fun - if it was that hard to make mathematical assertions, I wouldn’t make them! And I certainly wouldn’t make them in reply to someone who wouldn’t even see my gestures. So I think at least some of the actions that are rationalised by testimony, interpreted as sentences of \\(\\mathcal{L}_1\\), are not rationalised by testimony, interpreted as \\(\\mathcal{L}_4\\). By the kind of appeal to the principle of charity we have used a lot already, that means that \\(\\mathcal{L}_4\\) is not the language most people speak.\nThe central point here is that when we are ruling out particularly deviant interpretations of some speakers, we have to make heavy use of the requirement that the interpretation of their shared language rationalises what they do. In part that means it must rationalise why they utter the strings that they do in fact utter. And when we’re considering this, we should remember the role of naturalness in a theory of rationality. But it also means that it must rationalise why people respond to various strings with non-linguistic actions, such as walking to the fridge, or evacuating Lower Manhattan. Naturalness has less of a role to play here, but the Lewisian theory still gets the right answers provided we apply it carefully. Since the Lewisian theory gets the right answers, and the U&N Theory gets the wrong answers, it follows that the U&N Theory isn’t Lewis’s theory, and so orthodoxy is wrong.\nWhat is the Use of a Predicate?\nWe concluded the last section with an argument that Lewis isn’t vulnerable to the claim that his theory assigns complicated mathematical claims as the meanings of ordinary English sentences. That interpretation, we argued, is inconsistent with the way those sentences are used. In particular, it is inconsistent with the way that hearers use sentences to guide their actions.\nSo far so good, we might think. But notice how much has been packed into the notion of use to get us this far. In identifying the use O’Leary makes of “The beer is in the fridge,” we have to say a lot about O’Leary’s beliefs and desires. And in identifying the use Daniels makes of it, we primarily talk about the sentence’s effects on Daniels’s beliefs and desires. That is, just saying how the sentence is used requires saying a lot about mental states of speakers. And that will often require appealing to constitutive rationality; we say that Daniels’s beliefs about the fridge changed because we need to rationalise his fridge-directed behaviour.\nAnd this should all make us suspicious about the prospects for identifying meaning (in a Lewisian theory) with use plus naturalness. The argument above that naturalness mattered to meaning relied on the idea that naturalness matters because it affects which states are rational, and hence which states are actualised. A belief that all emeralds are grue is unnatural, so it is hard to hold. And since it is hard to hold, it is hard to think one is conforming to a convention of truthfulness in a language if one utters sentences that mean, in that language, that all emeralds are grue. That’s why it is wrong, ceteris paribus, to interpret people as speaking about grueness.\nBut now consider what happened when we were talking about Daniels and O’Leary. Even to say how they were using the sentence “The beer is in the fridge,” we had to say what they believed before and after the sentence was uttered. In other words, their mental states were constitutive of the way the sentence was used. Now add in the extra premise, argued for above, that naturalness matters to Lewis’s theory of linguistic content because, and only because, it matters to his theory of mental content. (And it only matters to mental content because it matters to the principle of charity that Lewis uses.) If mental states, and their changes, are part of how the sentences are used, it will be rather misleading to say that meaning is determined by use plus naturalness. A better thing to say is that meaning is determined by use, and that some key parts of use, i.e., mental states of speakers and hearers, are determined in part by naturalness.\nSo I’m sceptical of the U&N Theory. We can put the argument of the last few paragraphs as a dilemma. There are richer and thinner ways of identifying the use to which a sentence is put. A thin way might, for instance, just focus on the observable state of the part of the physical world in which the sentence is uttered. A rich way might include include, inter alia, the use that is made of the sentence in the management of belief and the generation of rational action. If we adopt the thin way of thinking about use, then adding naturalness won’t be enough to say what makes it the case that O’Leary and Daniels are speaking \\(\\mathcal{L}_1\\) rather than \\(\\mathcal{L}_4\\). If we adopt the rich way of thinking about use, then the role that naturalness plays in the theory of meaning has been incorporated into the metaphysics of use. Neither way makes the U&N Theory true while assigning naturalness an independent role. This dilemma isn’t just an argument that we shouldn’t attribute the U&N Theory to Lewis; it is an argument against anyone adopting that theory.\nFrom Theory to Applied Semantics\nSo far we’ve argued that Lewis’s semantic theory did not look a lot like the orthodox interpretation. It’s true that he thought the way a sentence was used was of primary importance in determining its meaning. And it’s true that he thought naturalness mattered to meaning. But that wasn’t because naturalness came in to resolve the indeterminacy left in a use-based theory of meaning. Rather, it was because naturalness was in a part of the theory of mental content, and specifying the mental states of speakers and hearers is part of specifying how the sentence is used.\nBut note that these considerations apply primarily to investigations at a very high level of generality, such as when we’re trying to solve the problems described in “Radical Interpretation.” They don’t apply to investigations into applied semantics. Let’s say we are trying to figure out what O’Leary and Daniels mean by “green.” And assume that we are taking for granted that they are speaking a language which is, in most respects, like English. This is hardly unusual in ordinary work in applied semantics. If we are writing a paper on the semantics of colour terms, a paper like, say, “Naming the Colours,” we don’t concern ourselves with the possibility that every sentence in the language refers to some complicated mathematical claim or other.\nNow given those assumptions, we can identify a moderately thin notion of use. We know that O’Leary uses “green” to describe things that are, by appearance, both green and grue. We also know that when O’Leary makes such a description, Daniels expects the object will be both green and grue. So focus on a notion of use such that the use of a predicate just is a function of which objects speakers will typically apply the predicate to, and which properties hearers take those objects to have once they hear the predication. If we wanted to be more precise, we could call this notion of ‘use’ simply predication. When we are doing applied semantics, especially when we are trying to figure out the meaning of predicates, we typically know which objects a speaker is disposed to predicate a predicate of, and that’s the salient feature of use. (This is why I said the most accurate heuristic would be meaning is predication plus naturalness; predication is the bit of use we care about in this context.)\nThis identification of use wouldn’t make any sense if we were engaged in theorising at a much more abstract level. If we are doing radical interpretation, then we have to take non-semantic inputs, and solve simultaneously for the values of the subject term and the predicate term in a (simple) sentence. But when we are just doing applied semantics, and working just on the meaning of a term like “green” in a well-functioning language, we can presuppose facts about the denotation of the subject term in sentences like S is green, and presuppose facts about what is the subject and what is the predicate in that sentence, and then we can look at which properties hearers come to associate with that very object on hearing that sentence.\nNow that we have a notion of use that’s distinct from naturalness, we can ask whether it is plausible that predicate meaning is use (in that sense) plus naturalness. And, quite plausibly, the answer is yes. The arguments in Sider (2001a) and Weatherson (2003) in favour of this theory look like, at the very least, good arguments that the theory does the right job in resolving Kripkensteinian problems. The theory is immune to objections based on radical re-interpretations of the language, as in Williams (2007), because those will be inconsistent with the use so defined. And the theory fits nicely into Lewis’s broader theory of meaning, i.e., his metasemantics, which is in turn well motivated. So I think there are good reasons to hold that when we’re doing applied semantics, the U&N Theory delivers the right verdicts, and delivers them for Lewisian reasons. That’s the heart of what’s true about the U&N Theory, even if it isn’t a fully general theory of meaning.\n\n\nBays, Timothy. 2007. “The Problem with Charlie: Some Remarks on Putnam, Lewis and Williams.” Philosophical Review 116 (3): 401–25. https://doi.org/10.1215/00318108-2007-003.\n\n\nHawthorne, John. 2007. “Craziness and Metasemantics.” Philosophical Review 116 (3): 427–40. https://doi.org/10.1215/00318108-2007-004.\n\n\nHolton, Richard. 2003. “David Lewis’s Philosophy of Language.” Mind and Language 18 (3): 286–95. https://doi.org/10.1111/1468-0017.00228.\n\n\nLewis, David. 1969. Convention: A Philosophical Study. Cambridge: Harvard University Press.\n\n\n———. 1974. “Radical Interpretation.” Synthese 27 (3-4): 331–44. https://doi.org/10.1007/bf00484599.\n\n\n———. 1975. “Languages and Language.” In Minnesota Studies in the Philosophy of Science, 7:3–35. Minneapolis: University of Minnesota Press.\n\n\n———. 1979. “Attitudes de Dicto and de Se.” Philosophical Review 88 (4): 513–43. https://doi.org/10.2307/2184646.\n\n\n———. 1980. “Mad Pain and Martian Pain.” In Readings in the Philosophy of Psychology, edited by Ned Block, I:216–32. Cambridge: Harvard University Press.\n\n\n———. 1983. “New Work for a Theory of Universals.” Australasian Journal of Philosophy 61 (4): 343–77. https://doi.org/10.1080/00048408312341131.\n\n\n———. 1984. “Putnam’s Paradox.” Australasian Journal of Philosophy 62 (3): 221–36. https://doi.org/10.1080/00048408412340013.\n\n\n———. 1986. On the Plurality of Worlds. Oxford: Blackwell Publishers.\n\n\n———. 1992. “Meaning Without Use: Reply to Hawthorne.” Australasian Journal of Philosophy 70 (1): 106–10. https://doi.org/10.1080/00048408112340093.\n\n\n———. 1994. “Reduction of Mind.” In A Companion to the Philosophy of Mind, edited by Samuel Guttenplan, 412–31. Oxford: Blackwell. https://doi.org/10.1017/CBO9780511625343.019.\n\n\n———. 1999. Papers in Metaphysics and Epistemology. Cambridge: Cambridge University Press.\n\n\nSchwarz, Wolfgang. 2006. “Lewisian Meaning Without Naturalness.”\n\n\n———. 2009. David Lewis: Metaphysik Und Analyse. Paderborn: Mentis-Verlag.\n\n\nSider, Theodore. 2001a. “Criteria of Personal Identity and the Limits of Conceptual Analysis.” Philosophical Perspectives 15: 189–209. https://doi.org/10.1111/0029-4624.35.s15.10.\n\n\n———. 2001b. Four-Dimensionalism. Oxford: Oxford University Press.\n\n\n———. 2012. Writing the Book of the World. Oxford: Oxford University Press.\n\n\nStalnaker, Robert. 2004. “Lewis on Intentionality.” Australasian Journal of Philosophy 82 (1): 199–212. https://doi.org/10.1080/713659796.\n\n\nWeatherson, Brian. 2003. “What Good Are Counterexamples?” Philosophical Studies 115 (1): 1–31. https://doi.org/10.1023/A:1024961917413.\n\n\n———. 2010. “Vagueness as Indeterminacy.” In Cuts and Clouds: Vaguenesss, Its Nature and Its Logic, edited by Richard Dietz and Sebastiano Moruzzi, 77–90. Oxford: Oxford University Press.\n\n\nWilliams, J. Robert G. 2007. “Eligibility and Inscrutability.” Philosophical Review 116: 361–99. https://doi.org/10.1215/00318108-2007-002.\n\n\nHolton (2003) is more nuanced, but does tell a similar story in the context of discussing Lewis’s account of (potential) semantic indeterminacy. Weatherson (2010) follows Holton in this respect.↩︎\nAs some further evidence for how orthodox the ‘orthodox’ interpretation is, note that Williams (2007) is a prize winning essay published with two commentaries in the Philosophical Review. That paper takes the orthodox interpretation as its starting point, and neither of the commentaries (Bays (2007) and Hawthorne (2007)) criticise this starting point.↩︎\nSchwarz (2006) develops his criticism of orthodoxy in more detail, and in English, but it is as yet unpublished.↩︎\nThese points are stressed by Wolfgang Schwarz (2006, 2009). He also notes that in “Putnam’s Paradox” Lewis explicitly sets these parts of his theory aside so he can discuss Putnam’s arguments on grounds most favourable to Putnam. As Schwarz says, this should make us suspicious of the central role “Putnam’s Paradox” plays in defences of the orthodox interpretation. We will return to this point in the section on textual evidence for and against orthodoxy.\nA referee notes, correctly, that the phrase ‘in the first instance’ is doing a lot of work here. That’s right; we’ll return in much more detail below to Lewisian theories of word meaning, and what role naturalness plays in them.↩︎\nThe view I’m attributing to Lewis is endorsed by one prominent supporter of the orthodox interpretation, namely Ted Sider. See his (2012, 35ff).↩︎\nIf you think sentences with a thousand words are too easy to understand for the argument of this paragraph, make the threshold higher; as long as the threshold is finite, it won’t affect the argument.\n\n↩︎\n",
    "preview": "posts/2021-01-05-the-role-of-naturalness-in-lewiss-theory-of-meaning/natural.jpg",
    "last_modified": "2021-02-05T15:25:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-05-margins-and-errors/",
    "title": "Margins and Errors",
    "description": "Timothy Williamson has argued that cases involving fallible measurement show that knowledge comes apart from justified true belief in ways quite distinct from the familiar ‘double luck’ cases. I start by describing some assumptions that are necessary to generate Williamson's conclusion, and arguing that these assumptions are well justified. I then argue that the existence of these cases poses problems for theorists who suppose that knowledge comes apart from justified true belief only in a well defined class of cases. I end with some general discussion of what we can know on the basis of imperfect measuring devices.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2013-04-25",
    "categories": [
      "epistemology"
    ],
    "contents": "\n\nContents\nMeasurement, Justification and Knowledge\nThe Class of Gettier Cases is Disjunctive\nThere is No Solution to the Gettier Problem\nWhat Can We Learn from Fallible Machines?\n\nRecently, Timothy Williamson (2013) has argued that considerations about margins of errors can generate a new class of cases where agents have justified true beliefs without knowledge. I think this is a great argument, and it has a number of interesting philosophical conclusions. In this note I’m going to go over the assumptions of Williamson’s argument. I’m going to argue that the assumptions which generate the justification without knowledge are true. I’m then going to go over some of the recent arguments in epistemology that are refuted by Williamson’s work. And I’m going to end with an admittedly inconclusive discussion of what we can know when using an imperfect measuring device.\nPublished in Inquiry 56: 63-76.\nPicture by docoverachiever via Creative Commons.\nMeasurement, Justification and Knowledge\nWilliamson’s core example involves detecting the angle of a pointer on a wheel by eyesight. For various reasons, I find it easier to think about a slightly different example: measuring a quantity using a digital measurement device. This change has some costs relative to Williamson’s version – for one thing, if we are measuring a quantity it might seem that the margin of error is related to the quantity measured. If I eyeball how many stories tall a building is, my margin of error is 0 if the building is 1-2 stories tall, and over 10 if the building is as tall as the World Trade Center. But this problem is not as pressing for digital devices, which are often very unreliable for small quantities. And, at least relative to my preferences, the familiarity of quantities makes up for the loss of symmetry properties involved in angular measurement.\nTo make things explicit, I’ll imagine the agent \\(S\\) is using a digital scale. The scale has a margin of error \\(m\\). That means that if the reading, i.e., the apparent mass is \\(a\\), then the agent is justified in believing that the mass is in \\([a-m, a+m]\\). We will assume that \\(a\\) and \\(m\\) are luminous; i.e., the agent knows their values, and knows she knows them, and so on. This is a relatively harmless idealisation for \\(a\\); it is pretty clear what a digital scale reads.1 It is a somewhat less plausible assumption for \\(m\\). But we’ll assume that \\(S\\) has been very diligent about calibrating her scale, and that the calibration has been recently and skillfully carried out, so in practice \\(m\\) can be assessed very accurately.\nWe’ll make three further assumptions about \\(m\\) that strike me as plausible, but which may I guess be challenged. I need to be a bit careful with terminology to set out the first one. I’ll use \\(V\\) and \\(v\\) as variables that both pick out the true value of the mass. The difference is that \\(v\\) picks it out rigidly, while \\(V\\) picks out the value of the mass in any world under consideration. Think of \\(V\\) as shorthand for the mass of the object and \\(v\\) as shorthand for the actual mass of the object. (More carefully, \\(V\\) is a random variable, while \\(v\\) is a standard, rigid, variable.) Our first assumption then is that \\(m\\) is also related to what the agent can know. In particular, we’ll assume that if the reading \\(a\\) equals \\(v\\), then the agent can know that \\(V \\in [a-m, a+m]\\), and can’t know anything stronger than that. That is, the margin of error for justification equals, in the best case, the margin of error for knowledge. The second is that the scale has a readout that is finer than \\(m\\). This is usually the case; the last digit on a digital scale is often not significant. The final assumption is that it is metaphysically possible that the scale has an error on an occasion that is greater than \\(m\\). This is a kind of fallibilism assumption – saying that the margin of error is \\(m\\) does not mean there is anything incoherent about talking about cases where the error on an occasion is greater than \\(m\\).\nThis error term will do a lot of work in what follows, so I’ll use \\(e\\) to be the error of the measurement, i.e., \\(|a-v|\\). For ease of exposition, I’ll assume that \\(a \\geq v\\), i.e., that any error is on the high side. But this is entirely dispensible, and just lets me drop some disjunctions later on.\nNow we are in a position to state Williamson’s argument. Assume that on a particular occasion, \\(0 < e < m\\). Perhaps \\(v = 830, m =10\\) and \\(a = 832\\), so \\(e = 2\\). Williamson appears to make the following two assumptions.2\nThe agent is justified in believing what they would know if appearances matched reality, i.e., if \\(V\\) equalled \\(a\\).\nThe agent cannot come to know something about \\(V\\) on the basis of a suboptimal measurement that they could not also know on the basis of an optimal measurement.\nI’m assuming here that the optimal measurement displays the correct mass. I don’t assume the actual measurement is wrong. That would require saying something implausible about the semantic content of the display. It’s not obvious that the display has a content that could be true or false, and if it does have such a content it might be true. (For instance, the content might be that the object on the scale has a mass near to \\(a\\), or that with a high probability it has a mass near to \\(a\\), and both of those things are true.) But the optimal measurement would be to have \\(a = v\\), and in this sense the measurement is suboptimal.\nThe argument then is pretty quick. From the first assumption, we get that the agent is justified in believing that \\(V \\in [a - m, a + m]\\). Assume then that the agent forms this justified belief. This belief is incompatible with \\(V \\in [v - m, a - m)\\). But if \\(a\\) equalled \\(v\\), then the agent wouldn’t be in a position to rule out that \\(V \\in [v - m, a - m)\\). So by premise 2 she can’t knowledgeably rule it out on the basis of a mismeasurement. So her belief that \\(V \\geq a - m\\) cannot be knowledge. So this justified true belief is not knowledge.\nIf you prefer doing this with numbers, here’s the way the example works using the numbers above. The mass of the object is 830. So if the reading was correct, the agent would know just that the mass is between 820 and 840. The reading is 832. So she’s justified in believing, and we’ll assume she does believe, that the mass is between 822 and 842. That belief is incompatible with the mass being 821. But by premise 2 she can’t know the mass is greater than 821. So the belief doesn’t amount to knowledge, despite being justified and, crucially, true. After all, 830 is between 822 and 842, so her belief that the mass is in this range is true. So simple reflections on the workings on measuring devices let us generate cases of justified true beliefs that are not knowledge.\nI’ll end this section with a couple of objections and replies.\nObjection: The argument that the agent can’t know that \\(V \\in [a - m, a + m]\\) is also an argument that the argument can’t justifiably believe that \\(V \\in [a - m, a + m]\\). After all, why should it be possible to get justification from a suboptimal measurement when it isn’t possible to get the same justification from an optimal measurement?\nReply: It is possible to have justification to believe an outright falsehood. It is widely believed that you can have justification even when none of your evidential sources are even approximately accurate (Cohen 1984). And even most reliabilists will say that you can have false justified beliefs if you use a belief forming method that is normally reliable, but which badly misfires on this occasion. In such cases we clearly get justification to believe something from a mismeasurement that we wouldn’t get from a correct measurement. So the objection is based on a mistaken view of justification.\nObjection: Premise 2 fails in cases using random sampling. Here’s an illustration. An experimenter wants to know what percentage of \\(F\\)s are \\(G\\). She designs a survey to ask people whether they are \\(G\\). The survey is well designed; everyone gives the correct answer about themselves. And she designs a process for randomly sampling the \\(F\\)s to get a good random selection of 500. It’s an excellent process; every \\(F\\) had an equal chance of being selected, and the sample fairly represents the different demographically significant subgroups of the \\(F\\)s. But by the normal processes of random variation, her group contains slightly more \\(G\\)s than the average. In her survey, 28% of people said (truly!) that they were \\(G\\), while only 26% of \\(F\\)s are \\(G\\)s. Assuming a margin of error in such a study of 4%, it seems plausible to say she knows that between 25 and 32% of \\(F\\)s are \\(G\\)s. But that’s not something she could have known the survey had come back correctly reporting that 26% of \\(F\\)s are \\(G\\)s.\nReply: I think the core problem with this argument comes in the last sentence. A random survey isn’t, in the first instance, a measurement of a population. It’s a measurement of those surveyed, from which we draw extrapolations about the population. In that sense, the only measurement in the imagined example was as good as it could be; 28% of surveyed people are in fact \\(G\\). So the survey was correct, and it is fine to conclude that we can in fact know that between 24 and 32 percent of \\(F\\)s are \\(G\\)s.\nThere are independent reasons for thinking this is the right way to talk about the case. If a genuine measuring device, like a scale, is off by a small amount, we regard that as a reason for tinkering with the device, and trying to make it more accurate. That’s one respect in which the measurement is suboptimal, even if it is correct within the margin of error. This reason to tinker with the scale is a reason that often will be outweighed. Perhaps it is technologically infeasible to make the machine more accurate. More commonly, the only way to guarantee greater accuracy would be more cost and hassle than it is worth. But it remains a reason. The fact that this experiment came out with a deviation between the sample and the population is not a reason to think that it could have been run in a better way, or that there is some reason to improve the survey. That’s just how random sampling goes. If it were a genuine measurement of the population, the deviation between the ‘measurement’ and what is being measured would be a reason to do things differently. There isn’t any such reason, so the sample is not truly a measurement.\nSo I don’t think this objection works, and I think the general principle that you can’t get extra knowledge from a suboptimal measurement is right. But note also that we don’t need this general principle to suggest that there will be cases of justified true belief without knowledge in the cases of measurement. Consider a special case where \\(e\\) is just less than \\(m\\). For concreteness, say \\(a = v + 0.95m\\), so \\(e = 0.95m\\). Now assume that whatever is justifiedly truly believed in this case is known, so \\(S\\) knows that \\(V \\in [a - m, a + m]\\). That is, \\(S\\) knows that \\(V \\in [v - 0.05m, a + m]\\).\nWe don’t need any principles about measurement to show this is false; safety considerations will suffice. Williamson (2000) says that a belief that \\(p\\) is safe only if \\(p\\) is true in all nearby worlds. But given how close \\(v\\) is to the edge of the range \\([v - 0.05m, a + m]\\). Rival conceptions of safety don’t help much more than this. The most prominent of these, suggested by Sainsbury (1995), says that a belief is safe only if the method that produced it doesn’t produce a false belief in any nearby world. But if the scale was off by \\(0.95m\\), it could have been off by \\(1.05m\\), so that condition fails too.\nI don’t want the last two paragraphs to leave too concessive an impression. I think the objection fails because it relies on a misconception of the notion of measurement. But I think that even if the objection works, we can get a safety based argument that some measurement cases will produce justified true beliefs without knowledge. And that will matter for the argument of the next two sections.\nThe Class of Gettier Cases is Disjunctive\nThere’s an unfortunate terminological confusion surrounding gaps between knowledge and justification. Some philosophers use the phrase ‘Gettier case’ to describe any case of a justified true belief that isn’t knowledge. Others use it to describe just cases that look like the cases in Gettier (1963), i.e., cases of true belief derived from justified false belief. I don’t particularly have strong views on whether either of these uses is better, but I do think it is important to keep them apart.\nI’ll illustrate the importance of this by discussing a recent argument due to Jeremy Fantl and Matthew McGrath (Fantl and McGrath 2009 Ch. 4). I’ve previously discussed this argument (Weatherson 2011), but I don’t think I quite got to the heart of why I don’t like the kind of reasoning they are using.\nThe argument concerns an agent, call her \\(T\\), who has the following unfortunate combination of features. She is very confident that \\(p\\). And with good reason; her evidence strongly supports \\(p\\). For normal reasoning, she takes \\(p\\) for granted. That is, she doesn’t distinguish between \\(\\varphi\\) is best given \\(p\\), and that \\(\\varphi\\) is simply best. And that’s right too, given the strong evidence that \\(p\\). But she’s not crazy. Were she to think that she was facing a bet on extreme odds concerning \\(p\\), she would cease taking \\(p\\) for granted, and revert to trying to maximise expected value given the high probability that \\(p\\). But she doesn’t think any such bet is salient, so her disposition to retreat from \\(p\\) to Probably p has not been triggered. So far, all is going well. I’m inclined to say that this is enough to say that \\(T\\) justifiedly believes that \\(p\\). She believes that \\(p\\) in virtue of the fact that she takes \\(p\\) for granted in actual reasoning.3 She’s disposed to stop doing so in some circumstances, but until that disposition is triggered, she has the belief. And this is the right way to act given her evidence, so her belief is justified. So far, so good.\nUnfortunately, \\(T\\) really does face a bet on long odds about \\(p\\). She knows she has to choose between \\(\\varphi\\) and \\(\\psi\\). And she knows that \\(\\varphi\\) will produce the better outcome iff \\(p\\). But she thinks the amount she’ll gain by choosing \\(\\psi\\) if \\(\\neg p\\) is roughly the same as the amount she’ll gain by choosing \\(\\varphi\\) if \\(p\\). That’s wrong, and her evidence clearly shows it is wrong. If \\(p\\) is false, then \\(\\varphi\\) will be much worse than \\(\\psi\\). In fact, the potential loss here is so great that \\(\\psi\\) has the greater expected value given the correct evidential probability of \\(p\\). I think that means she doesn’t know that \\(p\\). Someone who knows that \\(p\\) can ignore \\(\\neg p\\) possibilities in practical reasoning. And someone who could ignore \\(\\neg p\\) possibilities in practical reasoning would choose \\(\\varphi\\) over \\(\\psi\\), since it is better if \\(p\\). But \\(T\\) isn’t in a position to make that choice, so she doesn’t know that \\(p\\).\n(I’ve said here that \\(T\\) is wrong about the costs of choosing \\(\\varphi\\) if \\(p\\), and her evidence shows she is wrong. In fact I think she doesn’t know \\(p\\) if either of those conditions obtain. But here I only want to use the weaker claim that she doesn’t know \\(p\\) if both conditions obtain.)\nFantl and McGrath agree about the knowledge claim, but disagree about the justified belief claim. They argue as follows (this is my version of the ‘Subtraction Argument’ from page 97 of their book).\n\\(T\\) is justfied in choosing \\(\\varphi\\) iff she knows that \\(p\\).\nWhether \\(T\\)’s belief that \\(p\\) is true is irrelevant to whether she is justified in choosing \\(\\varphi\\).\nWhether \\(T\\)’s belief that \\(p\\) is ‘Gettiered’ is irrelevant to whether she is justified in choosing \\(\\varphi\\).\nKnowledge is true, justified, UnGettiered belief.\nSo \\(T\\) is justfied in choosing \\(\\varphi\\) iff she is justified in believing that \\(p\\).\n\\(T\\) is not justified in choosing \\(\\varphi\\).\nSo \\(T\\) is not justified in believing that \\(p\\).\nI think this argument is only plausible if we equivocate on what it is for a belief to be ‘Gettiered.’\nAssume first that ‘Gettiered’ means ‘derived from a false intermediate step.’ Then premise 4 is false, as Williamson’s example shows. \\(S\\) has a justified true belief that is neither knowledge nor derived from a false premise.\nAssume then that ‘Gettiered’ simply means that the true belief is justified without being known. In that case we have no reason to accept premise 3. After all, the class of true justified beliefs that are not knowledge is pretty open ended. Before reading Williamson, we may not have thought that this class included the beliefs of agents using measuring devices that were functioning properly but imperfectly. But it does. Prior to the end of epistemology, we simply don’t know what other kind of beliefs might be in this class. There’s no way to survey all the ways for justification to be insufficient for knowledge, and see if all of them are irrelevant to the justification for action. I think one way a justified belief can fall short of knowledge is if it is tied up with false beliefs about the stakes of bets. It’s hard to say that that is irrelevant to the justification of action.\nIt is by now reasonably well known that logical subtraction is a very messy and complicated business. See, for instance, Humberstone (2000) for a clear discussion of the complications. In general, unless it is analytic that \\(F\\)s are \\(G\\)s and \\(H\\)s, for some antecedently understood \\(G\\) and \\(H\\), there’s nothing interesting to say about the class of things that are \\(G\\) but not \\(F\\). It will just be a disjunctive shambles. The same is true for knowledge and justification. The class of true beliefs that are justified but not known is messy and disjunctive. We shouldn’t expect to have any neat way of overviewing it. That in part means we can’t say much interesting about it as a class, contra premise 3 in the above argument. It also means the prospects for ‘solving the Gettier problem’ are weak. We’ll turn to that issue next.\nThere is No Solution to the Gettier Problem\nThe kind of example that Edmund Gettier (1963) gives to refute the justified true belief theory of knowledge has what Linda Zagzebski (2009, 117) aptly calls a “double luck” structure. In Gettier’s original cases, there’s some bad luck that leads to a justified belief being false. But then there’s some good luck that leads to an inference from that being true. As was quickly realised in the literature, the good and bad luck doesn’t need to apply to separate inferential steps. It might be that the one belief that would have been false due to bad luck also ends up being true due to good luck.\nThis has led to a little industry, especially in the virtue epistemology section of the market, of attempts to “solve the Gettier problem” by adding an anti-luck condition to justification, truth and belief and hoping that the result is something like an analysis of knowledge. As Zagzebski (1994) showed, this can’t be an independent condition on knowledge. If it doesn’t entail truth, then we will be able to recreate the Gettier cases. But maybe a ‘fourth’ condition that entails truth (and perhaps belief) will suffice. Let’s quickly review some of these proposals.\nSo Zagzebski (1996) suggested that the condition is that the belief be true because justified. John Greco (2010) says that the extra condition is that the beliefs be “intellectually creditable.” That is, the primary that the subject ended up with a true belief is that it was the result of her reliable cognitive faculties. Ernest Sosa (2007) said that knowledge is belief that is true because it manifests intellectual competence. John Turri (2011) says that knowledge is belief the truth of which is a manifestation of the agent’s intellectual competence.\nIt should be pretty clear that no such proposal can work if what I’ve said in earlier sections is remotely right. Assume again that \\(v = 830, a = 832\\) and \\(m = 10\\). The agent believes that \\(V \\in [822, 842]\\). This belief is, we’ve said, justified and true. Does it satisfy these extra conditions?\nMy short answer is that it does. My longer answer is that it does if any belief derived from the use of a measuring device does, and since some beliefs derived from the use of measuring devices amount to knowledge, the epistemologists are committed to the belief satisfying the extra condition. Let’s go through those arguments in turn.\nIn our story, \\(S\\) demonstrates a range of intellectual competencies. She uses a well-functioning measuring device. It is the right kind of device for the purpose she is using. By hypothesis, she has had the machine carefully checked, and knows exactly the accuracy of the machine. She doesn’t form any belief that is too precise to be justified by the machine. And she ends up with a true belief precisely because she has so many competencies.\nNote that if we change the story so \\(a\\) is closer to \\(v + m\\), the case that the belief is true in virtue of \\(S\\) being so competent becomes even stronger. Change the case so that \\(a = 839\\), and she forms the true belief that \\(V \\in [829, 849]\\). Now if \\(S\\) had not been so competent, she may have formed a belief with a tighter range, since she could easily have guessed that the margin of error of the machine is smaller. So in this case the truth of the belief is very clearly due to her competence. But as we noted at the end of section 1, in the cases where \\(a\\) is near \\(v + m\\), the argument that we have justified true belief without knowledge is particularly strong. Just when the gap between justification and knowledge gets most pronounced, the competence based approach to knowledge starts to issue the strongest verdicts in favour of knowledge.\nBut maybe this is all a mistake. After all, the object doesn’t have the mass it has because of \\(S\\)’s intellectual competence. The truth of any claim about its mass is not because of \\(S\\)’s competence, or a manifestation of that competence. So maybe these epistemologists get the correct verdict that \\(S\\) does not know that \\(V \\in [a - m, a + m]\\)?\nNot so quick. Even had \\(a\\) equalled \\(v\\), all these claims would have been true. And in that case, \\(S\\) would have known that \\(V\\) was within \\(m\\) of the measurement. What is needed for these epistemological theories to be right is that there can be a sense that a belief that \\(p\\) can be true in virtue of some cause \\(C\\) without \\(C\\) being a cause of \\(p\\). I’m inclined to agree with the virtue epistemologists that such a sense can be given. (I think it helps to give up on content essentialism for this project, as suggested by David (2002) and endorsed in@Weatherson2004-WEALMT.) But I don’t think it will help. There’s no real way in which a belief is true because of competencies, or in which the truth of a belief manifests competence, in the good case where \\(a = v\\), but not in the bad cases, where \\(a\\) is in \\((0, m)\\). These proposals might help with ‘double luck’ cases, but there is more to the space between justification and knowledge than those cases. Of course, I think the space in question includes some cases involving false beliefs about the practical significance of \\(p\\), but I don’t expect everyone to agree with that. Happily, the Williamsonian cases should be less controversial.\nWhat Can We Learn from Fallible Machines?\nMy presentation of Williamson’s argument in section 1 abstracted away from several features of his presentation. In particular, I didn’t make any positive assumption about what the agent can know when they find out that the machine reads \\(a\\). Williamson makes a suggestion, though he offers it more as the most internalist friendly suggestion than the most likely correct hypothesis.\nThe suggestion, which I’ll call the Circular Reading Centred hypothesis, is that the most the agent can know is that \\(V \\in [a - (e + m), a + (e + m)]\\). That is, the agent can know that \\(V\\) is in a region centred on \\(a\\), the ‘radius’ of which is the margin of error \\(m\\), plus the error on this occasion \\(e\\). This is actually a quite attractive suggestion, though not the only suggestion we could make. Let’s look through some other options and see how well they work.\nWe said above that the agent can’t know more from a mismeasurement than they can know from an accurate measurement. And we said that given an accurate measurement, the most they can know is that \\(V \\in [v - m, v + m]\\). So here’s one very restrictive suggestion: if \\(a \\in [v - m, v + m]\\), then the agent can know that \\(V \\in [v - m, v + m]\\). But we can easily rule that out on the basis of considerations about justification. The strongest proposition the agent is justified in believing is that \\(V \\in [a - m, a + m]\\). If the agent could know that \\(V \\in [v - m, v + m]\\), then she could know that \\(V \\notin (v + m, a + m]\\), even though she isn’t justified in believing this. This is absurd, so that proposal is wrong.\nWe now have two principles on the table: \\(S\\) can’t know anything by a mismeasurement that she knows on the basis of a correct measurement, and that she can only know things she’s justified in believing. The first principle implies that for all \\(x \\in [v - m, v + m]\\), that \\(V = x\\) is epistemically possible. The second implies that for all \\(x \\in [a -m, a + m]\\), that \\(V = x\\) is epistemically possible. Our next proposal is that the epistemic possibilities, given a reading of \\(a\\), are just that \\(V \\in [v - m, v + m] \\cup [a - m, a + m]\\).\nBut this is fairly clearly absurd too. Assume that \\(a > v + 2m\\). This is unlikely, but as we said above not impossible. Now consider the hypothesis that \\(V \\in (v + m, a - m)\\). On the current hypothesis, this would be ruled out. That is, she would know it doesn’t obtain. But this seems bizarre. There are epistemic possibilities all around it, but somehow she’s ruled out this little gap, and done so on the basis of a horrifically bad measurement.\nThis suggests two other approaches that are consistent with the two principles, and which do not have such an odd result. I’ll list them alongside the proposal we mentioned earlier.\nCircular Appearance Centred\nThe strongest proposition the agent can know is that \\(V \\in [a - (e + m), a + (e + m)]\\).\n\nCircular Reality Centred\nThe strongest proposition the agent can know is that \\(V \\in [v - (e + m), v + (e + m)]\\).\n\nElliptical\nThe strongest proposition the agent can know is that \\(V \\in [v - m, a + m]\\).\n\nThe last proposal is called Elliptical because it in effect says that there are two foci for the range of epistemic possibilities. The agent can’t rule out anything within \\(m\\) of the true value, or anything within \\(m\\) of the apparent value, or anything between those.\nActually we can motivate the name even more by considering a slight generalisation of the puzzle that we started with. Assume that \\(R\\) is trying to determine the location of an object in a two-dimensional array. As before, she has a digital measuring device, perhaps a GPS locator trained on the object in question. And she knows that margin of error of the device is \\(m\\). The object is actually located at \\(\\langle x_v, y_v \\rangle\\), and the device says it is at \\(\\langle x_a, y_a \\rangle\\). So the epistemic possibilities, by the reasoning given above, should include the circles with radius \\(m\\) centred on \\(\\langle x_v, y_v \\rangle\\) and \\(\\langle x_a, y_a \\rangle\\). Call these circles \\(C_v\\) and \\(C_a\\). Unless \\(\\langle x_v, y_v \\rangle= \\langle x_a, y_a \\rangle\\), the union of these circles will not be convex. If the distance between \\(\\langle x_v, y_v \\rangle\\) and \\(\\langle x_a, y_a \\rangle\\) is greater than \\(2m\\), the union won’t even be connected. So just as we ‘filled in’ the gap in the one-dimensional case, the natural thing to say is that any point in the convex hull of \\(C_v\\) and \\(C_a\\) is an epistemic possibility.\nBut now see what happens if we say those are all of the epistemic possibilities, i.e., that the agent knows that the true value lies in the convex hull of the two circles. Here’s what it might look like.\nNow consider the line from \\(\\langle x_v, y_v \\rangle\\) to \\(\\langle x_a, y_a \\rangle\\). No matter how bad the measurement is, the convex hull of the two circles \\(C_v\\) and \\(C_a\\) will include no points more than distance \\(m\\) from the line between \\(\\langle x_v, y_v \\rangle\\) to \\(\\langle x_a, y_a \\rangle\\). That is, the agent can know something surprisingly precise about how close \\(V\\) is to a particular line, even on the basis of a catastrophically bad measurement.\nThere are some circumstances where this wouldn’t be counterintuitive. Assume that \\(x_v = x_a\\), while \\(y_v\\) and \\(y_a\\) are very very different. And assume further that \\(\\langle x_a, y_a \\rangle\\) is calculated by using two very different procedures for the \\(x\\) and \\(y\\) coordinates. (Much as sailors used to use very different procedures to calculate longitude and latitude.) Then the fact that one process failed badly doesn’t, I think, show that we can’t get fairly precise knowledge from the other process.\nBut that’s not the general case. If the machine determines \\(\\langle x_a, y_a \\rangle\\) by a more holistic process, then a failure on one dimension should imply that we get less knowledge on other dimensions, since it makes it considerably flukier that we got even one dimension right. So I think the space of epistemic possibilities, in a case involving this kind of errant measurement, must be greater than the convex hull of \\(C_v\\) and \\(C_a\\).\nFortunately, there are a couple of natural generalisations of the elliptical proposal that avoid this complication. One of them says that the space of epistemic possibilities forms an ellipse. In particular, it is the set of all points such that the sum of the distance from that point to \\(\\langle x_v, y_v \\rangle\\) and the distance from that point to \\(\\langle x_a, y_a \\rangle\\) is less than or equal to \\(2m + e\\), where \\(e\\) again is the distance between the measured and actual value. As you can quickly verify, that includes all points on the line from \\(\\langle x_v, y_v \\rangle\\) to \\(\\langle x_a, y_a \\rangle\\), plus an extension of length \\(m\\) beyond in each direction. But it doesn’t just contain the straight path between \\(C_v\\) and \\(C_a\\); it ‘bulges’ in the middle. And the considerations above suggest that is what should happen.\nThe other alternative is to drop the idea that the space of possibilities should be elliptical, and have another circular proposal. In particular, we say that the space of possibilities is the circle whose centre is halfway between \\(\\langle x_v, y_v \\rangle\\) and \\(\\langle x_a, y_a \\rangle\\), and whose radius is \\(m + \\nicefrac{e}{2}\\). Again, that will include all points on the line from \\(\\langle x_v, y_v \\rangle\\) to \\(\\langle x_a, y_a \\rangle\\), plus an extension of length \\(m\\) beyond in each direction. But it will include a much larger space in the middle.\nI think both of these are somewhat plausible proposals, though the second suffers from a slightly weaker version of the objection I’m about to mount to the Circular Reality Centred proposal. But they do share one weakness that I think counts somewhat against them. It’s easy enough to see what the weakness is in the one-dimensional case, so let’s return to it for the time being, and remember we’re assuming that \\(a > v\\).\nConsider a case where \\(e\\) is rather large, much larger than \\(m\\). This affects how far below \\(v\\) we have to go in order to reach possibilities that are ruled out by the measurement. But it doesn’t affect how far above \\(v\\) we have to go in order to reach such possibilities. Indeed, no matter how bad \\(e\\) is, we can be absolutely certain that we know \\(V < a + 2m\\), or that we know that \\(V > a - 2m\\). That seems a little odd; if the measurement is so badly mistaken, it seems wrong that it can give us such a fine verdict, at least in one direction.\nI don’t think that’s a conclusive objection. Well, I don’t think many of the considerations I’ve listed here are conclusive, but this seems even weaker. But it is a reason to look away from the elliptical proposal and back towards the circular proposals that we started with.\nIf we just look at first order knowledge claims, it is hard to feel much of an intuitive pull towards one or other of the alternatives. Perhaps safety based considerations favour the Reality Centred over the Appearance Centred version, but I don’t think the salient safety consideration is that strong.\nIf we look at iterated knowledge claims, however, there is a big problem with the Reality Centred approach. The intuition here is clearer if we use numerical examples, so I’ll work through a case with numbers first, then do the general version next.\nAssume, as above, that \\(v = 830, a = 834\\) and \\(m = 10\\). So we have a pretty decent measurement here. On the Reality Centred proposal, the strongest thing that \\(S\\) can know is that \\(V \\in [816, 844]\\). So it is an epistemic possibility that \\(V = 816\\). Assume that that’s the actual possibility. Then the measurement is rather bad; the new value for \\(e\\) is 18. Were \\(V\\) to equal 816, while \\(a\\) equalled 834, then on the Reality Centred approach, the epistemic possibilities would be a circle of radius \\(e+m\\), i.e., 28, around the actual value, i.e., 816. So the strongest thing the agent could know is that \\(V \\in [788, 844]\\). On the other hand, if \\(V\\) were 844, the strongest thing the agent could know is that \\(V \\in [824, 864]\\). Putting those together, the strongest thing the agent can know that she knows is that \\(V \\in [788, 864]\\). That’s a very large range already. Similar calculations show that the strongest thing the agent can know that she knows that she knows is that \\(V \\in [732, 904]\\).\nNow I’ll grant that intuitions about second and third order knowledge are not always maximally sharp. But I think it is very implausible that a relatively accurate measurement like this could lead to such radical ignorance in the second and third orders of knowledge. So I think the Reality Centred approach can’t be right.\nThe general form the case is as follows. The strongest thing the agent can know is that \\(V \\in [v - (e + m), a + m]\\). The strongest thing she can know that she knows is that \\(V \\in [v - 3(e + m), a + 3m]\\). And the strongest thing she can know that she knows that she knows is that \\(V \\in [v - 7(e + m), a + 7m]\\). In general, we have exponential growth of the possibilities as we add one extra order of knowledge. That seems absurd to me, so the Reality Centred approach is wrong.\nNote that this isn’t a problem with the Appearance Centred approach. The first-order epistemic possibilities are that \\(V \\in [a - (e + m), a + e + m]\\). If \\(V\\) is at the extremes of this range, then \\(e\\) will be rather large. For example, if \\(V\\) were equal to \\(a + e + m\\), then the new error would be \\(e + m\\), since the measured value is still \\(a\\). So the range of possibilities would be that \\(V \\in [a - ((e + m) + m), a + ((e + m) + m)]\\). Somewhat surprisingly, those would also be the possibilities if \\(V\\) were equal to \\(a - (e + m)\\), since the only feature of \\(V\\) that affects the epistemic possibilities for \\(V\\) is its distance from \\(a\\). So for all \\(S\\) knows that she knows, \\(V\\) could be anything in \\([a - (e + 2m), a + (e + 2m)]\\). Similar reasoning shows that for all \\(V\\) knows that she knows that she knows, \\(V\\) could be anything in \\([a - (e + 3m), a + (e + 3m)]\\). In general, \\(V\\) has \\(n\\)’th order knowledge that \\(V\\) is in \\([a - (e + nm), a + (e + nm)]\\). This linear growth in the size of the range of epistemic possibilities is more plausible than the exponential growth on the Reality Centred approach.\nSo all things considered, I think the Circular Appearance Centred approach is the right one, as Williamson suggests. Any simple alternative seems to have rather counterintuitive consequences.\n\n\nCohen, Stewart. 1984. “Justification and Truth.” Philosophical Studies 46 (3): 279–95. https://doi.org/10.1007/bf00372907.\n\n\nDavid, Marian. 2002. “Content Essentialism.” Acta Analytica 17: 103–14. https://doi.org/10.1007/bf03177510.\n\n\nFantl, Jeremy, and Matthew McGrath. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nGettier, Edmund L. 1963. “Is Justified True Belief Knowledge?” Analysis 23 (6): 121–23. https://doi.org/10.2307/3326922.\n\n\nGreco, John. 2010. Achieving Knowledge. Cambridge: Cambridge University Press.\n\n\nHumberstone, Lloyd. 2000. “Parts and Partitions.” Theoria 66 (1): 41–82. https://doi.org/10.1111/j.1755-2567.2000.tb01144.x.\n\n\nRoss, Jacob, and Mark Schroeder. 2014. “Belief, Credence, and Pragmatic Encroachment.” Philosophy and Phenomenological Research 88 (2): 259–88. https://doi.org/10.1111/j.1933-1592.2011.00552.x.\n\n\nSainsbury, Mark. 1995. “Vagueness, Ignorance and Margin for Error.” British Journal for the Philosophy of Science 46: 589–601. https://doi.org/10.1093/bjps/46.4.589.\n\n\nSosa, Ernest. 2007. A Virtue Epistemology: Apt Belief and Reflective Knowledge. Oxford: Oxford University Press.\n\n\nTurri, John. 2011. “Manifest Failure: The Gettier Problem Solved.” Philosophers’ Imprint 11 (8): 1–11. http://hdl.handle.net/2027/spo.3521354.0011.008.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\n———. 2011. “Knowledge, Bets and Interests.” In Forthcoming Volume on Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, 75–103. Oxford: Oxford University Press.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\n———. 2013. “Gettier Cases in Epistemic Logic.” Inquiry 56 (1): 1–14. https://doi.org/10.1080/0020174X.2013.775010.\n\n\nZagzebski, Linda. 1994. “The Inescapability of Gettier Problems.” The Philosophical Quarterly 44 (174): 65–73. https://doi.org/10.2307/2220147.\n\n\n———. 1996. Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge. Cambridge: Cambridge University Press.\n\n\n———. 2009. On Epistemology. Belmont, CA.: Wadsworth.\n\n\nThis isn’t always true. If a scale flickers between reading 832g and 833g, it takes a bit of skill to determine what the reading is. But we’ll assume it is clear in this case. On an analogue scale, the luminosity assumption is rather implausible, since it is possible to eyeball with less than perfect accuracy how far between one marker and the next the pointer is.↩︎\nI’m not actually sure whether Williamson makes the first, or thinks it is the kind of thing anyone who thinks justification is prior to knowledge should make.↩︎\nThere are some circumlocutions here because I’m being careful to be sensitive to the points raised in Ross and Schroeder (2014) about the relationship between belief and reasoning. I think there’s less distance between the view they put forward and the view I defended in Weatherson (2005) than they do, but this is a subtle matter, and for this paper’s purposes I want to go along with Ross and Schroeder’s picture of belief.\n\n↩︎\n",
    "preview": "posts/2021-03-05-margins-and-errors/measurement.jpg",
    "last_modified": "2021-03-12T14:18:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-04-ross-on-sleeping-beauty/",
    "title": "Ross on Sleeping Beauty",
    "description": "In two excellent recent papers, Jacob Ross has argued that the standard arguments for the ‘thirder’ answer to the Sleeping Beauty puzzle lead to violations of countable additivity. The problem is that most arguments for that answer generalise in awkward ways when he looks at the whole class of what he calls Sleeping Beauty problems. In this note I develop a new argument for the thirder answer that doesn't generalise in this way.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2013-03-01",
    "categories": [
      "epistemology",
      "games and decisions",
      "notes"
    ],
    "contents": "\nIn two excellent recent papers, (Ross 2010) and (Ross 2012), Jacob Ross has argued that the standard arguments for the \\(\\frac{1}{3}\\) answer to the Sleeping Beauty puzzle lead to violations of countable additivity. The problem is that most arguments for the \\(\\frac{1}{3}\\) answer generalise in awkward ways when he look at the whole class of what he calls Sleeping Beauty problems.\nPublished in Philosophical Studies 155: 503-512.\nPicture via The Met.\n\nLet us define a Sleeping Beauty problem as any problem in which a fully rational agent, Beauty, will undergo one or more mutually indistinguishable awakenings and in which the number of awakenings she will undergo is determined by the outcome of a random process. Let \\(S\\) be a partition of alternative hypotheses concerning the outcome of this process. Beauty knows the objective chance of each hypothesis in \\(S\\), and she also knows how many times she will awaken conditional on each of these hypotheses, but she has no other relevant information. The problem is to determine how her credence should be divided among the hypotheses in \\(S\\) when she first awakens. The original Sleeping Beauty problem is the instance of this class of problems in which the random process that determines how many times Beauty awakens is a fair coin toss and in which Beauty awakens once given Heads and twice given Tails. (Ross 2010, 413–14)\n\nThe ‘Generalised Thirder Principle’ says\n\nIn any Sleeping Beauty problem, defined by a partition \\(S\\), upon first awakening, Beauty’s credence in any given hypothesis in \\(S\\) should be proportional to the product of its objective chance and the number of times Beauty awakens if this hypothesis is true. (Ross 2010, 414)\n\nRoss shows that this Principle leads to violations of countable additivity, and argues (convincingly to my mind) that this is a serious problem for the Principle. In Ross (2010) he argues that many arguments for the \\(\\frac{1}{3}\\) answer imply the Generalised Thirder Principle, and in Ross (2012) he argues that the same is true of the argument for the \\(\\frac{1}{3}\\) answer in Weatherson (2011). He suggests that we have some inductive reason to think that all arguments for the \\(\\frac{1}{3}\\) answer will imply the Generalised Thirder Principle. In reply I want to make three points.\nFirst, Ross offers a much more careful presentation and analysis of the argument for the \\(\\frac{1}{3}\\) answer than appears in Weatherson (2011), and that analysis does really suggest that the argument overgenerates. But second, the careful setting out suggests that the argument fails by its own standards; it commits a fallacy of equivocation. So I don’t think it’s as much inductive evidence for Ross’s general conclusion about all arguments for the \\(\\frac{1}{3}\\) answer as he suggests. And third, the inductive conclusion Ross draws is not true. There are arguments for the \\(\\frac{1}{3}\\) answer that don’t imply the Generalised Thirder principle. Below I outline two ways to fix the argument in Weatherson (2011) so that it becomes such an argument.\nIt will be worthwhile having a minimal example of where the Generalised Thirder principle goes beyond the arguments for the \\(\\frac{1}{3}\\) answer, so consider the following minimal version.\n\nThree-Day Sleeping Beauty\nA coin will be flipped. If it comes up Heads, Beauty will wake up Monday, and go back to sleep until Thursday. If it comes up Tails, Beauty will wake Monday, then have her memories of that waking erased, then wake again Tuesday, and have her memories of that waking erased, then wake up Wednesday, and go back to sleep until Thursday. The different possible wakings will be indistinguishable.1 When she wakes on Monday, what should her credence be that the coin landed Heads?\n\nThe Generalised Thirder says that it should be \\(\\frac{1}{4}\\). One of our aims here will be to come up with an argument for the \\(\\frac{1}{3}\\) answer in the original puzzle that doesn’t imply the answer to the Three-Day Sleeping Beauty puzzle is \\(\\frac{1}{4}\\).\nReturning to the original puzzle, Let Cr\\(_1\\) be Beauty’s credence function when she wakes on Monday. Let \\(M\\) be the proposition Beauty would express on waking with “Today is Monday.” And let \\(H\\) be the proposition that the coin landed heads. The \\(\\frac{1}{3}\\) answer entails that the following claims are all true.\nCr\\(_1\\)\\((M \\wedge H)\\) = Cr\\(_1\\)\\((M \\wedge \\neg H)\\)\nCr\\(_1\\)\\((\\neg M \\wedge \\neg H)\\)\nCr\\(_1\\)\\((M \\wedge \\neg H)\\) = Cr\\(_1\\)\\((\\neg M \\wedge \\neg H)\\)\nThe usual argument for the \\(\\frac{1}{3}\\) answer argues for (1) and (2)Cr, and then derives (3). And the arguments for (2) Cr typically generalise into arguments for the Generalised Thirder Principle, and hence to violations of countable additivity.\nTo get a bit better feel for what’s going on with these principles, consider one interesting sub-class of Ross’s large category of Sleeping Beauty problems. These are problems where there are \\(n\\) chance hypotheses, and for any \\(i\\) such that \\(1 \\leq i \\leq n\\), Beauty is woken \\(i\\) times if chance hypothesis \\(h_i\\) is true. When she wakes up, we can let the proposition \\(p_{ji}\\), where \\(1 \\leq j \\leq i \\leq n\\), be the proposition that this is the \\(i\\)’th day, and chance hypothesis \\(h_j\\) is true. The following table, where the chance hypotheses are on the rows, and days are on the columns, represents the possibilities as they strike Beauty.\n\n\nDay 1\nDay 2\n\\(\\dots\\)\nDay \\(n\\)\n\\(h_1\\)\n\\(p_{11}\\)\n\n\n\n\\(h_2\\)\n\\(p_{12}\\)\n\\(p_{22}\\)\n\n\n…\n\n\n\n\n\\(h_n\\)\n\\(p_{1n}\\)\n\\(p_{2n}\\)\n\\(\\dots\\)\n\\(p_{nn}\\)\n\nThe usual argument for the \\(\\frac{1}{3}\\) answer to Sleeping Beauty includes a ‘vertical’ argument and a ‘horizontal’ argument. The ‘vertical’ argument attempts to show that \\(\\text{Cr}_1(p_{1i} | p_{11} \\vee \\dots \\vee p_{1n}) = Ch(h_i)\\). The ‘horizontal’ argument attempts to show that \\(\\text{Cr}_1(p_{ji} | p_{i1} \\vee \\dots \\vee p_{ii}) = \\frac{1}{i}\\). Or, at least, it attempts to show that those claims are true for the special case where \\(n = 2\\). but as Ross shows, the arguments offered seem to work in the \\(n = 2\\) case iff they work in the general case. And these vertical and horizontal arguments together do imply the Generalised Thirder Principle.\nThe argument in Weatherson (2011) took a different tack. It argued for (1) and (3), and derived (2) Cr. In terms of the table above, the idea was to replace the ‘horizontal’ argument, and indeed to reject its conclusion in the general case, with a ‘diagonal’ argument, which showed that \\(\\text{Cr}_1(p_{ii} | p_{11} \\vee \\dots \\vee p_{nn}) = Ch(h_i)\\). If the vertical and diagonal arguments worked, and didn’t overgeneralise, then they wouldn’t entail a solution for cases like Three-Day Sleeping Beauty, or about any case from the above class where \\(n > 2\\).\nIf we drop the restriction to Sleeping Beauty problems where Beauty is woken \\(i\\) times if chance hypothesis \\(h_i\\) is true, and return to Ross’s more general class, the arguments in Weatherson (2011) were meant to prove the following two claims, and not a lot more.\nFirst Day\nIn any Sleeping Beauty problem where Beauty wakes at least one time in every chance hypothesis, and exactly one time in at least one of them, when she wakes the first time, her conditional credences in each hypothesis in \\(S\\), conditional on this actually being the first waking, equals the objective chance of each such hypothesis.\n\nLast Day\nIn any Sleeping Beauty problem where Beauty wakes at least one time in every chance hypothesis,, when she wakes the first time, her conditional credences in each hypothesis in \\(S\\), conditional on this actually being the last waking, equals the objective chance of each such hypothesis.\n\nSince First Day entails (1) , and Last Day entails (3), these principles entail the \\(\\frac{1}{3}\\) answer. But they don’t settle what to say about the Three-Day Sleeping Beauty example. If Cr\\(_3\\) is Beauty’s credences when she wakes for the first time in that example, these principles are consistent with Beauty having the following credal distribution. \\[\\begin{aligned}\n\\text{Cr}_3(\\text{Today is Monday and Heads}) &= \\frac{1}{5} \\\\\n\\text{Cr}_3(\\text{Today is Monday and Tails}) &= \\frac{1}{5} \\\\\n\\text{Cr}_3(\\text{Today is Tuesday and Tails}) &= \\frac{2}{5} \\\\\n\\text{Cr}_3(\\text{Today is Wednesday and Tails}) &= \\frac{1}{5} \\end{aligned}\\] But those credences are incompatible with the Generalised Thirder Principle, so First Day and Last Day do not entail that principle.\nRoss (2012) argues that this isn’t right, and that the motivation for First Day offered in Weatherson (2011) in fact does lead to the Generalised Thirder Principle on its own. I think that’s true in a sense; the argument provides just as much support for the Generalised Thirder Principle as it does for First Day. But that’s because it is a bad argument, and doesn’t support First Day. We’ll see why that’s true shortly. But since it is true, a new argument is needed for (1) , one that supports First Day, but not the Generalised Thirder principle.\nMost discussions of Sleeping Beauty assume that the contents of propositional attitudes are sets of centered worlds. Following Stalnaker (2008), the argument in Weatherson (2011) tried to get by with propositions simply being sets of worlds. The key idea was that the worlds themselves would be fine-grained enough that thoughts like Hesperus is Phosophorus, or Today is Monday would be contingently true if true at all. Very roughly, we associate singular terms with something like Fregean senses.2 When Beauty wonders whether today is Monday, she isn’t wondering about whether an instance of the law of identity is true. She has no more interest in that law than does the first gentleman of Europe. She is wondering, in effect, whether two senses, TODAY and MONDAY, have the same referent.\nThis way of looking at things implies that credal dynamics need to be complicated. Sometimes our credences change because we acquire more information, and we react accordingly. But sometimes our credences change because we acquire new senses, and we can think new thoughts. That will become crucial in what follows.\nThere is a quick argument for Last Day. Consider what happens after Beauty wakes up at the end of the puzzle, and is told that the game is over. (That is, on Wednesday in the original puzzle, or on Thursday in the three-day variant.) Plausibly, her credence in \\(H\\) should be back to \\(\\frac{1}{2}\\), or at least it is hard to see a good argument why it should be anything else.3 She can also think back to her last waking, and think to herself If H, that was on Monday, and if \\(\\neg\\)H, that was on Tuesday. (She’ll replace Tuesday with Wednesday in the three-day variant.) Call this conditional \\(C\\). Now think back to that last waking. At that time she is wondering various thoughts about the waking she is currently undergoing, a waking she will describe as “this waking.” When she wakes on Wednesday, and thinks about “that waking,” it is plausible she is thinking the very same kind of Fregean thought. She is thinking about the same thing, and thinking about it in the same kind of way. If that’s right, then the big difference between her credences after the puzzle ends and her credences on the last waking are just that she comes to learn \\(C\\). So on the last waking, her credence in \\(H\\) conditional on \\(C\\) should be \\(\\frac{1}{2}\\), since when she learns \\(C\\) and nothing else, her credence in \\(H\\) becomes \\(\\frac{1}{2}\\). And that entails (3), and similar reasoning generalises to all Sleeping Beauty problems to entail Last Day.4\nThe argument in Weatherson (2011) for (1) , and for First Day, involved a rather baroque variant of the example involving time travelers. And, as we’ve noted already, it also involves some fallacious equivocation. Before we get to that, it is worth noting two other arguments for the same conclusion, neither of which generalise to the Generalised Thirder Principle. Both arguments have contentious premises, but they are somewhat independent, so I hope that presenting both arguments will increase the number of people who agree with FirstDay.\nThe first argument is an argument from the Principal Principle. Consider the version of the Sleeping Beauty puzzle where the coin is tossed after Beauty goes back to sleep on Monday. (We didn’t say so far when the coin is tossed, and it was consistent with everything we said that it is Monday night.) And assume that Beauty knows that the coin is tossed Monday. So when she wakes on Monday, she knows that the chance of \\(H\\) is \\(\\frac{1}{2}\\) if it is Monday. That is, \\(\\text{Cr}_1(Ch(H) = \\frac{1}{2} | M) = 1\\). The Principal Principle says that unless Beauty has inadmissible evidence, \\(\\text{Cr}_1(H | Cr(H) = \\frac{1}{2}) = \\frac{1}{2}\\). And plausibly she doesn’t have any inadmissible evidence conditional on it being Monday. Putting these two together, we get \\(\\text{Cr}_1(H | M) = \\frac{1}{2}\\). And that kind of reasoning generalises to support First Day.\nThe argument here is similar to the original argument given for (1) in Elga (2000). Elga imagines a variant of the example where Beauty is told, sometime after she wakes up, that it is Monday, and argues that after that she should have credence \\(\\frac{1}{2}\\) in \\(H\\), and derives (1) from that. Halpern (2004) objects to this argument on the grounds that the possibility of Beauty being told what day it is undermines the indistinguishability of the wakings, and this undermines Elga’s own argument for (2) Cr. I’m not sure Halpern is right, but in any case, this argument doesn’t rely on any possibility of Beauty being told what day it is. I imagine that some people will object to the claim that Beauty has no inadmissible evidence. But it is hard to see what she knows which is inadmissible, at least conditional on it being Monday. She knows that if it is Monday, the truth of \\(H\\) rests on a chance event that is yet to take place, and from which she is causally isolated. That looks to me like knowledge that she has no inadmissible evidence.\nThe second argument is a version of the the Technicolor Beauty argument in Titlebaum (2008). It relies on a variant of the example that drops the idea that the wakings are indistinguishable in a strong sense. I’ll set up first what the idea behind the argument is, and then set out how it works. Assume that Beauty can think about \\(M\\) on Sunday. Follow Ross in using Cr\\(_0\\) for Beauty’s credences on Sunday. The same Principal Principle style argument we used above suggests that \\(\\text{Cr}_0(H | M) = \\frac{1}{2}\\). Indeed in this case the argument is even stronger, since everyone agrees that on Sunday, Beauty has no inadmissible evidence. But nothing happens to surprise Beauty between Sunday and Monday, so \\(\\text{Cr}_1(H | M) = \\frac{1}{2}\\) should be \\(\\frac{1}{2}\\) as well, and we derive (1) from there.\nThere are a few problems with this argument. For one thing, the ‘no surprise’ premise goes by very quickly. More importantly, Beauty can’t actually have \\(M\\) thoughts on Sunday. She can think to herself that Monday is Monday, or at least she could if she cared to think about the law of identity. But that’s not the same thought as \\(M\\). Remember, the guiding idea here is that contents are Fregean; it isn’t easy to have the same thought as someone who thinks This is Monday. Something dramatic needs to happen to let Beauty have such a thought on Sunday, when she isn’t in a position to make the same kind of demonstration as she is on Monday.\nHere’s one way the dramatic thing might happen. Change the example so that the wakings Beauty undergoes are not phenomenally indistinguishable. In fact, Beauty is told on Sunday that each waking will be in a brightly coloured room, and the colours will be different each day. As it happens, the room that will be used for Monday is red, though Beauty doesn’t know that. Let \\(RE\\) be the proposition that one of Beauty’s wakings will be in a red room, and \\(RM\\) be the proposition that she wakes Monday in a red room. Now clearly \\(RE\\), on its own, is inadmissible evidence in the sense that Cr\\(_0\\)\\((H | RE)\\) need not be \\(\\frac{1}{2}\\). After all, \\(RE\\) is probabilistic evidence that Beauty has more than one waking, since the more wakings she has, the more chance there is that one of them will be in a red room. On the other hand, \\(RM\\) does not look like inadmissible evidence. She has to wake up in some colour room or other on Monday; learning it is red doesn’t change anything. So Cr\\(_0\\)\\((H | RM)\\) should be \\(\\frac{1}{2}\\). And that’s true even though \\(RM\\) obviously entails \\(RE\\).\nNow she wakes on Monday, and the room is red. What follows? Well, she now knows \\(RE\\). And she can identify her current waking with the red waking she imagined (or at least could have imagined) on Sunday. So it is at least arguable that when on Monday she considers the thought This waking is on Monday, that’s the very same thought she considers on Sunday by saying to herself The waking in a red room is on Monday. Making that last claim more plausible would require offering a more detailed theory of mental content than I have the space (or ability) to do here. For now I’m just going to take as a premise that there’s a workable theory of mental content that types contents more finely than does a purely referential theory, but on which it is nevertheless the case that Beauty’s demonstrative thought on Monday has the same content as her descriptive thought (about the waking in the red room) does on Sunday.\nHere is one way of thinking about that claim about content that may make it more plausible. (This idea is derived from the arguments in Jeshion (2002).) Imagine that on Sunday Beauty names the waking in a red room. She calls it ‘Bluey.’ She knows that ‘Bluey’ might not refer. That is, she knows that Bluey, like Vulcan and Sherlock Holmes, might not exist. But she nevertheless entertains detailed thoughts about Bluey. She wonders if Bluey will be on Monday, whether she’ll be happy when Bluey happens, and so on. Now she wakes up on Monday, and sees the red walls. She says to herself, “This is Bluey.” From that point on, it seems that she’d express the same thought with This waking is \\(\\varphi\\) and Bluey is \\(\\varphi\\), and it seems she’s express the same thought with Bluey is \\(\\varphi\\) and The waking in a red room is \\(\\varphi\\). By appeal to transitivity of identity, and substituting a particular value for \\(\\varphi\\), we get that she expresses the same thought by saying This waking is on Monday as by saying The waking in a red room is on Monday.\nIf that claim about content is right, then all that happens on Monday is that Beauty learns that \\(RE\\) is true. She doesn’t acquire the ability to think new thoughts, or to make fresh divisions in possibility space, the way that she does in the standard version of the puzzle. In the standard version of the puzzle, the demonstrative thought she considers on Monday, the one she would express by saying This is Monday, is not equivalent to anything she can think on Sunday. So when she wakes, she not only acquires some evidence, she acquires a new cognitive capacity. That doesn’t happen here, which makes the calculations easier.\nIn particular, it lets us appeal to the following key fact. If \\(E_2\\) entails \\(E_1\\), and a particular update only involves conditionalising one’s prior credences, then learning \\(E_1\\) doesn’t change the conditional credence of anything given \\(E_2\\). That’s a consequence of the following theorem. Let \\(\\Pr\\) be any probability function, and let \\(\\Pr^+\\) be the result of conditionalising that function on \\(E_1\\). Then \\(\\Pr(H | E_2) = \\Pr^+(H | E_2)\\). So if Beauty only learns \\(RE\\), that doesn’t change the conditional credence of anything given \\(RM\\). In particular, it doesn’t change the conditional credence of \\(H\\) given \\(RM\\). So Cr\\(_1\\)\\((H | RM)\\) = \\(\\frac{1}{2}\\). And since \\(M\\) and \\(RM\\) are trivially equivalent, since Beauty can see the room is red, it follows that Cr\\(_1\\)\\((H | M)\\) = \\(\\frac{1}{2}\\), as required.\nI suspect the main objection to this argument will be that adding the room colours makes a substantial change to the problem. The fact that \\(\\frac{1}{3}\\) is the correct answer in this technicolour version of Sleeping Beauty, says the objector, is no reason to think that it is also the correct answer in the version where the wakings are phenomenally indistinguishable. But I think the objector will have a hard time making the case that phenomenal indistinguishability is epistemically significant unless they want to defend what Williamson (2007) calls the ‘phenomenal conception of evidence.’ As has been pointed out in prior work on Sleeping Beauty, the different wakings are not evidentially equivalent; when she wakes up and sees that this waking is happening, that’s a piece of evidence she gets in some but not all wakings. (This point is made in Weintraub (2004) and Stalnaker (2008).) It might well be argued that this demonstrative evidence is in a sense symmetric; although she gets evidence that is different in some sense on the different wakings, the force of that evidence is the same. But that’s still true in the technicolour version of the problem as well. So I think, contra the objector, that this is a good argument for (1) .\nBut neither of those were the argument offered in Weatherson (2011). That argument involved a rather baroque modification to the puzzle. A time traveller films Beauty waking on Monday and travels back to Sunday to show Beauty the film. After she sees the film, and can think about the waking it depicts, the traveller tells her that he took it on Monday. He then wipes Beauty’s memories of this telling, but not of the showing of the film. The argument then proceeds as follows.\nOn Sunday, she can think about her Monday waking in the same way as she thinks about it on Monday when she wakes, thanks to the time traveller’s film.\nOn Sunday, the rational credence in \\(H\\) is \\(\\frac{1}{2}\\).\nIf premise 1 is true, then on Monday, after she wakes, the only difference between her epistemic state then and her epistemic state after being told that the film was of Monday is that she no longer knows \\(M\\).\nIf the only difference between two epistemic states is that in the first, an agent knows \\(M\\) and in the second she does not, then the rational credence of \\(H\\) given \\(M\\) in the second state equals the rational credence of \\(H\\) in the first state.\nAnd from that \\(\\text{Cr}_1(H | M) =\\)\\(\\frac{1}{2}\\) was claimed to follow. Now there wasn’t much of an argument for the first premise offered, and it might well be thought objectionable. It certainly relies on a liberal conception of sameness of ‘ways of thinking.’ But let’s set that aside, because there is a much bigger problem with the argument. It hopelessly equivocates on the phrase ‘On Sunday.’ Let’s distinguish the following four times that are all on Sunday.\n\\(t_1\\) is before the time traveller turns up.\n\\(t_2\\) is immediately after the time traveller shows Beauty the film.\n\\(t_3\\) is immediately after the time traveller tells Beauty that the film is of Monday.\n\\(t_4\\) is after the time traveller wipes Beauty’s memories of that telling, but not of the showing of the film.\nNow let’s consider the first two premises in Weatherson’s argument. The first premise is clearly false if ‘On Sunday’ refers to \\(t_1\\), but arguably true if it refers to \\(t_2, t_3\\) or \\(t_4\\). The second premise is clearly true if ‘On Sunday’ refers to \\(t_1\\), but much less plausible if it refers to any later time. If it refers to \\(t_2\\) or \\(t_4\\), it is arguably equivalent to the \\(\\frac{1}{2}\\) answer to the original Sleeping Beauty problem, which makes it pretty useless in an argument for the \\(\\frac{1}{3}\\) answer! If it refers to \\(t_3\\), it is much too close to what we’re trying to prove in arguing for FirstDay, so it is still argumentatively useless.\nRoss argues that the style of argument we’ve been considering would, if it showed that (1) , and indeed more generally supported First Day, would show much more. In fact, it would show that for any two chance hypotheses \\(h_i\\) and \\(h_j\\), and any \\(k\\) such that \\(h_j\\) is consistent with at least \\(k\\) wakings, that the following is true. (I’ll again use \\(p_{kj}\\) to mean that this is the \\(k\\)’th waking and \\(h_j\\) is true.)\n\\[\\text{Cr}_1(p_{1i} | (p_{1i} \\vee p_{kj})) = \\frac{Ch(h_i)}{Ch(h_i) + Ch(h_j)}\\]\nAnd from that we can derive the Generalised Thirder Principle, and hence countable additivity violations. That wasn’t what was intended; the argument was only designed to work for the special case where \\(k = 1\\), i.e., First Day. Now I think Ross is right in the following sense; there’s just as good an argument in Weatherson (2011) for the above equation as there is for First Day. But that argument is no good for the reasons described above. Let’s see where the same equivocation comes into Ross’s telling of the story. I’ve changed Ross’s notation a fair bit into notation I find easier to work with. Hopefully I haven’t lost anything in the process. I’m also going to focus on the special case of Three-Day Sleeping Beauty, where \\(h_1\\) is the coin lands heads and \\(h_2\\) is the coin lands tails, and on the case where \\(k = 2\\). So what we’re really going to look at is whether there’s an argument in Three-Day Sleeping Beauty for this equation.\n\\[\\text{Cr}_3(p_{11} | (p_{11} \\vee p_{22})) = \\frac{Ch(h_1)}{Ch(h_1) + Ch(h_2)} = \\frac{1}{2}\\]\nNow as noted above, I think that this equation need not hold in Three-Day Sleeping Beauty; in the model I gave for it earlier, \\(\\text{Cr}_3(p_{11}) = \\frac{1}{5}\\), and \\(\\text{Cr}_3(p_{22}) = \\frac{2}{5}\\), so \\(\\text{Cr}_3(p_{11} | (p_{11} \\vee p_{22})) =\\) \\(\\frac{1}{3}\\). But let’s see how Ross derives the \\(\\frac{1}{2}\\) answer.\nAgain there is a time-traveller who shows Beauty a film. But this isn’t necessarily a film of the first waking; the time traveller films the first waking if \\(h_1\\), and the second waking if \\(h_2\\). After seeing the film, Beauty is told this. So on Sunday, says Ross, Beauty’s credences should satisfy these constraints. (I’m following Ross in using Cr\\(_0\\) for the Sunday credences.) \\[\\begin{aligned}\n\\text{Cr}_0(h_1) &= \\text{Cr}_0(h_1 \\wedge p_{11}) \\\\\n\\text{Cr}_0(h_1) &= Ch(h_1)\\end{aligned}\\]\nThere are other premises used, but these will be the crucial ones. They’re crucial because there’s no good reason to think that there’s any time Sunday when Beauty’s credences should satisfy both these equations. Before she sees the film, she can’t even think about propositions like \\(p_{11}\\), since she can’t have singular thoughts about the waking it depicts. After she sees the film, there is no reason to think that her credences should align with chances. Causal contact with a time traveller who brings information that may well be about a time after the chance event, evidence whose existence may depend on the outcome of the chance event, is pretty much paradigmatically inadmissible evidence for the purposes of the Principal Principle. So after the film, there is no reason to think \\(\\text{Cr}_0(h_1) = Ch(h_1)\\).\nI should stress that Ross doesn’t endorse the equivocating premises here; he merely attributes them to Weatherson (2011), and fairly so. But I think once we see the equivocation we can see there is no fear the kind of argument used in Weatherson (2011) will lead to the Generalised Thirder Principle. That argument is too flawed to lead to anything. But there are plenty of other arguments for First Day, such as the two arguments offered here. And both of those arguments rely on distinctive features of the first day. Most notably, they rely on the fact that for all we say in the setup of the problem, the first waking is before the chance event. So there’s no reason to think they will have the problematic consequences that Ross finds in the argument for First Day in Weatherson (2011).\n\n\n\nElga, Adam. 2000. “Self-Locating Belief and the Sleeping Beauty Problem.” Analysis 60 (4): 143–47. https://doi.org/10.1093/analys/60.2.143.\n\n\nHalpern, Joseph. 2004. “Sleeping Beauty Reconsidered: Conditioning and Reflection in Asynchronous Systems.” In Oxford Studies in Epistemology, 1:111–42. Oxford: Oxford University Press.\n\n\nJeshion, Robin. 2002. “Acquiantanceless de Re Belief’.” In Meaning and Truth: Investigations in Philosophical Semantics, edited by Joseph Keim Campbell, Michael O’Rourke, and David Shier, 53–74. New York: Seven Bridges Press.\n\n\nRoss, Jacob. 2010. “Sleeping Beauty, Countable Additivity, and Rational Dilemmas.” Philosophical Review 119 (4): 411–47. https://doi.org/10.1215/00318108-2010-010.\n\n\n———. 2012. “All Roads Lead to Violations of Countable Additivity.” Philosophical Studies 161 (3): 381–90. https://doi.org/10.1007/s11098-011-9744-z.\n\n\nStalnaker, Robert. 2008. Our Knowledge of the Internal World. Oxford: Oxford University Press.\n\n\nTitlebaum, Michael. 2008. “The Relevance of Self-Locating Beliefs.” Philosophical Review 117 (4): 555–605. https://doi.org/10.1215/00318108-2008-016.\n\n\nWeatherson, Brian. 2011. “Stalnaker on Sleeping Beauty.” Philosophical Studies 155 (3): 445–56. https://doi.org/10.1007/s11098-010-9613-1.\n\n\nWeintraub, Ruth. 2004. “Sleeping Beauty: A Simple Solution.” Analysis 64 (1): 8–10. https://doi.org/10.1093/analys/64.1.8.\n\n\nWilliamson, Timothy. 2007. The Philosophy of Philosophy. Blackwell.\n\n\nI have some concerns about what ‘indistinguishable’ means in this context. We’ll come back to that issue a lot in what follows.↩︎\nIn Weatherson (2011) these are described as haeceitties, but this is misleading at best. It is crucial that ‘Hesperus’ and ‘Phosphorus’ have different associations, but intuitively they have the same haecceity. Thinking of the associations as being with something like senses is better.↩︎\nOne might be tempted by a seemingly stronger argument. For instance, one could argue that on Wednesday, Beauty knows that the chance of \\(H\\) was \\(\\frac{1}{2}\\), and she has no inadmissible evidence, so by the Principal Principle her credence in \\(H\\) should be \\(\\frac{1}{2}\\). But it isn’t clear that she has no inadmissible evidence; perhaps the evidence she would express by thinking back to her last waking and saying That waking happened is inadmissible. Or one might be tempted by a Reflection Principle based argument against any alternative credence. But such arguments seem to lead to odd results in general around Sleeping Beauty. It’s best, I think, to stick with the clear intuition that on Wednesday her credence in \\(H\\) should be \\(\\frac{1}{2}\\).↩︎\nStrictly speaking, all we’ve really shown is that Beauty’s credences on the last day she wakes should satisfy Last Day. So if the focus of the puzzle is on her credences on the first day, all we’ve strictly speaking shown is that if \\(H\\), then Last Day is true. I think it is plausible that Last Day should be independent of how the coin lands, but I admit that I don’t have an argument against someone who wants to dispute this.↩︎\n",
    "preview": "posts/2021-02-04-ross-on-sleeping-beauty/eros.jpg",
    "last_modified": "2021-03-08T16:13:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-knowledge-bets-and-interests/",
    "title": "Knowledge, Bets and Interests",
    "description": "This paper argues that the interest-relativity of knowledge cannot be explained by the interest-relativity of belief. The discussion starts with an argument that knowledge plays a key pair of roles in decision theory. It is then argued that knowledge cannot play that role unless knowledge is interest-relative. The theory of the interest-relativity of belief is reviewed and revised. That theory can explain some of the cases that are used to suggest knowledge is interest-relative. But it can’t explain some cases involving ignorance, or mistake, about the odds at which a bet is offered. The paper ends with an argument that these cases require positing interest-relative defeaters, which affect whether an agent knows something without affecting whether she believes it, or is justified in believing it.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2012-07-26",
    "categories": [
      "epistemology",
      "interest-relativity",
      "games and decisions"
    ],
    "contents": "\n\nContents\nThe Interest-Relativity of Knowledge\nThe Struction of Decision Problems\nFrom Decision Theory to Interest-Relativity\n\nThe Interest-Relativity of Belief\nInterests and Functional Roles\nTwo Caveats\n\nInterest-Relative Defeaters\nThe Coraline Example\nWhat Coraline Knows and What She Believes\nStakes as Defeaters\nFantl and McGrath on Interest-Relativity\n\n\nWhen you pick up a volume like this one, which describes itself as being about ‘knowledge ascriptions,’ you probably expect to find it full of papers on epistemology, broadly construed. And you’d probably expect many of those papers to concern themselves with cases where the interests of various parties (ascribers, subjects of the ascriptions, etc.) change radically, and this affects the truth values of various ascriptions. And, at least in this paper, your expectations will be clearly met.\nPublished in Knowledge Ascriptions, edited by Jessica Brown and Mikkel Gerken, OUP, 75-103.\nPicture by Phillip Ritz via Creative Commons.\nBut here’s an interesting contrast. If you’d picked up a volume of papers on ‘belief ascriptions,’ you’d expect to find a radically different menu of writers and subjects. You’d expect to find a lot of concern about names and demonstratives, and about how they can be used by people not entirely certain about their denotation. More generally, you’d expect to find less epistemology, and much more mind and language. I haven’t read all the companion papers to mine in this volume, but I bet you won’t find much of that here.\nThis is perhaps unfortunate, since belief ascriptions and knowledge ascriptions raise at least some similar issues. Consider a kind of contextualism about belief ascriptions, which holds that (L) can be truly uttered in some contexts, but not in others, depending on just what aspects of Lois Lane’s psychology are relevant in the conversation.1\nLois Lane believes that Clark Kent is vulnerable to kryptonite.\nWe could imagine a theorist who says that whether (L) can be uttered truly depends on whether it matters to the conversation that Lois Lane might not recognise Clark Kent when he’s wearing his Superman uniform. And, this theorist might continue, this isn’t because ‘Clark Kent’ is a context-sensitive expression; it is rather because ‘believes’ is context-sensitive. Such a theorist will also, presumably, say that whether (K) can be uttered truly is context-sensitive.\nLois Lane knows that Clark Kent is vulnerable to kryptonite.\nAnd so, our theorist is a kind of contextualist about knowledge ascriptions. But they might agree with approximately none of the motivations for contextualism about knowledge ascriptions put forward by Cohen (1988), DeRose (1995) or Lewis (1996). Rather, they are a contextualist about knowledge ascriptions solely because they are contextualist about belief ascriptions like (L).\nCall the position I’ve just described doxastic contextualism about knowledge ascriptions. It’s a kind of contextualism all right; it says that (K) is context sensitive, and not merely because of the context-sensitivity of any term in the ‘that’-clause. But it explains the contextualism solely in terms of the contextualism of belief ascriptions. The more familiar kind of contextualism about knowledge ascriptions we’ll call non-doxastic contextualism. Note that the way we’re classifying theories, a view that holds that (K) is context-sensitive both because (L) is context-sensitive and because Cohen et al are correct is a version of non-doxastic contextualism. The label ‘non-doxastic’ is being used to mean that the contextualism isn’t solely doxastic, rather than as denying contextualism about belief ascriptions.\nWe can make the same kind of division among interest-relative invariantist, or IRI, theories of knowledge ascriptions. Any kind of IRI will say that there are sentences of the form S knows that p whose truth depends on the interests, in some sense, of \\(S\\). But we can divide IRI theories up the same way that we divide up contextualist theories.\nDoxastic IRI\nKnowledge ascriptions are interest-relative, but their interest-relativity traces solely to the interest-relativity of the corresponding belief ascriptions.\n\nNon-Doxastic IRI\nKnowledge ascriptions are interest-relative, and their interest-relativity goes beyond the interest-relativity of the corresponding belief ascriptions.\n\nAgain, a theory that holds both that belief ascriptions are interest-relative, and that some of the interest-relativity of knowledge ascriptions is not explained by the interest-relativity of belief ascriptions, will count as a version of non-doxastic IRI. I’m going to defend a view from this class here.\nIn my Weatherson (2005) I tried to motivate Doxastic IRI. It isn’t completely trivial to map my view onto the existing views in the literature, but the idea was to renounce contextualism and all its empty promises, and endorse a position that’s usually known as ‘strict invariantism’ about these classes of statements:\n\\(S\\) is justified in having credence \\(x\\) in \\(p\\);\nIf \\(S\\) believes that \\(p\\), she knows that \\(p\\);\nwhile holding that the interests of S are relevant to the truth of statements from these classes:\n\\(S\\) believes that \\(p\\);\n\\(S\\) justifiably believes that \\(p\\);\n\\(S\\) knows that \\(p\\).\nBut I didn’t argue for all of that. What I argued for was Doxastic IRI about ascriptions of justified belief, and I hinted that the same arguments would generalise to knowledge ascriptions. I now think those hints were mistaken, and want to defend Non-Doxastic IRI about knowledge ascriptions.2 My change of heart has been prompted by cases like those Jason Stanley (2005) calls ‘Ignorant High Stakes’ cases.3 But to see why these cases matter, it will help to start with why I think some kind of IRI must be true.\nHere’s the plan of attack. In section 1, I’m going to argue that knowledge plays an important role in decision theory. In particular, I’ll argue (a) that it is legitimate to write something onto a decision table iff the decision maker knows it to be true, and (b) it is legitimate to leave a possible state of the world off a decision table iff the decision maker knows it not to obtain. I’ll go on to argue that this, plus some very plausible extra assumptions about the rationality of certain possible choices, implies that knowledge is interest-relative. In section 2 I’ll summarise and extend the argument from Weatherson (2005) that belief is interest-relative. People who are especially interested in the epistemology rather than the theory of belief may skip this. But I think this material is important; most of the examples of interest-relative knowledge in the literature can be explained by the interest-relativity of belief. I used to think all such cases could be explained. Section 3 describes why I no longer think that. Reflections on cases like the Coraline example suggests that there are coherence constraints on knowledge that go beyond the coherence constraints on justified true belief. The scope of these constraints is, I’ll argue, interest-relative. So knowledge, unlike belief or justified belief, has interest-relative defeaters. That’s inconsistent with Doxastic IRI, so Doxastic IRI is false.\nThe Interest-Relativity of Knowledge\nThe Struction of Decision Problems\nProfessor Dec is teaching introductory decision theory to her undergraduate class. She is trying to introduce the notion of a dominant choice. So she introduces the following problem, with two states, \\(S_1\\) and \\(S_2\\), and two choices, \\(C_1\\) and \\(C_2\\), as is normal for introductory problems.\n\n\n\\(S_1\\)\n\\(S_2\\)\n\\(C_1\\)\n-$200\n$1000\n\\(C_2\\)\n-$100\n$1500\n\nShe’s hoping that the students will see that \\(C_1\\) and \\(C_2\\) are bets, but \\(C_2\\) is clearly the better bet. If \\(S_1\\) is actual, then both bets lose, but \\(C_2\\) loses less money. If \\(S_2\\) is actual, then both bets win, but \\(C_2\\) wins more. So \\(C_2\\) is better. That analysis is clearly wrong if the state is causally dependent on the choice, and controversial if the states are evidentially dependent on the choices. But Professor Dec has not given any reason for the students to think that the states are dependent on the choices in either way, and in fact the students don’t worry about that kind of dependence.\nThat doesn’t mean, however, that the students all adopt the analysis that Professor Dec wants them to. One student, Stu, is particularly unwilling to accept that \\(C_2\\) is better than \\(C_1\\). He thinks, on the basis of his experience, that when more than $1000 is on the line, people aren’t as reliable about paying out on bets. So while \\(C_1\\) is guaranteed to deliver $1000 if \\(S_2\\), if the agent bets on \\(C_2\\), she might face some difficulty in collecting on her money.\nGiven the context, i.e., that they are in an undergraduate decision theory class, it seems that Stu has misunderstood the question that Professor Dec intended to ask. But it is a little harder than it first seems to specify just exactly what Stu’s mistake is. It isn’t that he thinks Professor Dec has misdescribed the situation. It isn’t that he thinks the agent won’t collect $1500 if she chooses \\(C_2\\) and is in \\(S_2\\). He just thinks that she might not be able to collect it, so the expected payout might really be a little less than $1500.\nBut Stu is not the only problem that Professor Dec has. She also has trouble convincing Dom of the argument. He thinks there should be a third state added, \\(S_3\\). In \\(S_3\\), there is a vengeful God who is about to end the world, and take everyone who chose \\(C_1\\) to heaven, while sending everyone who chose \\(C_2\\) to hell. Since heaven is better than hell, \\(C_2\\) does not dominate \\(C_1\\); it is worse in \\(S_3\\). If decision theory is to be useful, we must say something about why we can leave states like \\(S_3\\) off the decision table.\nSo in order to teach decision theory, Professor Dec has to answer two questions.4\nWhat makes it legitimate to write something on the decision table, such as the ‘$1500’ we write in the bottom right cell of Dec’s table?\nWhat makes it legitimate to leave something off a decision table, such as leaving Dom’s state \\(S_3\\) off the table?\nLet’s start with a simpler problem that helps with both questions. Alice is out of town on a holiday, and she faces the following decision choice concerning what to do with a token in her hand.\n\nChoice\nOutcome\nPut token on table\nWin $1000\nPut token in pocket\nWin nothing\n\nThis looks easy, especially if we’ve taken Professor Dec’s class. Putting the token on the table dominates putting the token in her pocket. It returns $1000, versus no gain. So she should put the token on the table.\nI’ve left Alice’s story fairly schematic; let’s fill in some of the details. Alice is on holiday at a casino. It’s a fair casino; the probabilities of the outcomes of each of the games is just what you’d expect. And Alice knows this. The table she’s standing at is a roulette table. The token is a chip from the casino worth $1000. Putting the token on the table means placing a bet. As it turns out, it means placing a bet on the roulette wheel landing on 28. If that bet wins she gets her token back and another token of the same value. There are many other bets she could make, but Alice has decided not to make all but one of them. Since her birthday is the 28\\(^{\\text{th}}\\), she is tempted to put a bet on 28; that’s the only bet she is considering. If she makes this bet, the objective chance of her winning is \\(\\frac{1}{38}\\), and she knows this. As a matter of fact she will win, but she doesn’t know this. (This is why the description in the table I presented above is truthful, though frightfully misleading.) As you can see, the odds on this bet are terrible. She should have a chance of winning around \\(\\frac{1}{2}\\) to justify placing this bet.5 So the above table, which makes it look like placing the bet is the dominant, and hence rational, option, is misleading.\nJust how is the table misleading though? It isn’t because what is says is false. If Alice puts the token on the table she wins $1000; and if she doesn’t, she stays where she is. It isn’t, or isn’t just, that Alice doesn’t believe the table reflects what will happen if she places the bet. As it turns out, Alice is smart, so she doesn’t form beliefs about chance events like roulette wheels. But even if she did, that wouldn’t change how misleading the table is. The table suggests that it is rational for Alice to put the token on the table. In fact, that is irrational. And it would still be irrational if Alice believes, irrationally, that the wheel will land on 28.\nA better suggestion is that the table is misleading because Alice doesn’t know that it accurately depicts the choice she faced. If she did know that these were the outcomes to putting the token on the table versus in her pocket, it seems it would be rational for her to put it on the table. If we take it as tacit in a presentation of a decision problem that the agent knows that the table accurately depicts the outcomes of various choices in different states, then we can tell a plausible story about what the miscommunication between Professor Dec and her students was. Stu was assuming that if the agent wins $1500, she might not be able to easily collect. That is, he was assuming that the agent does not know that she’ll get $1500 if she chooses \\(C_2\\) and is in state \\(S_2\\). Professor Dec, if she’s anything like other decision theory professors, will have assumed that the agent did know exactly that. And the miscommunication between Professor Dec and Dom also concerns knowledge. When Dec wrote that table up, she was saying that the agent knew that \\(S_1\\) or \\(S_2\\) obtained. And when she says it is best to take dominating options, she means that it is best to take options that one knows to have better outcomes. So here are the answers to Stu and Dom’s challenges.\nIt is legitimate to write something on the decision table, such as the ‘$1500’ we write in the bottom right cell of Dec’s table, iff the decision maker knows it to be true.\nIt is legitimate to leave something off a decision table, such as leaving Dom’s state \\(S_3\\) off the table, iff the decision maker knows it not to obtain.\nPerhaps those answers are not correct, but what we can clearly see by reflecting on these cases is that the standard presentation of a decision problem presupposes not just that the table states what will happen, but the agent stands in some special doxastic relationship to the information explicitly on the table (such as that Alice will get $1500 if \\(C_2\\) and \\(S_2\\)) and implied by where the table ends (such as that \\(S_3\\) will not happen). Could that relationship be weaker than knowledge? It’s true that it is hard to come up with clear counterexamples to the suggestion that the relationship is merely justified true belief. But I think it is somewhat implausible to hold that the standard presentation of an example merely presupposes that the agent has a justified true belief that the table is correct, and does not in addition know that the table is correct.\nMy reasons for thinking this are similar to one of the reasons Timothy Williamson (Williamson 2000 Ch. 9) gives for doubting that one’s evidence is all that one justifiably truly believes. To put the point in Lewisian terms, it seems that knowledge is a much more natural relation than justified true belief. And when ascribing contents, especially contents of tacitly held beliefs, we should strongly prefer to ascribe more rather than less natural contents.6\nSo the ‘special doxastic relationship’ is not weaker than knowledge. Could it be stronger? Could it be, for example, that the relationship is certainty, or some kind of iterated knowledge? Plausibly in some game-theoretic settings it is stronger – it involves not just knowing that the table is accurate, but knowing that the other player knows the table is accurate. In some cases, the standard treatment of games will require positing even more iterations of knowledge. For convenience, it is sometimes explicitly stated that iterations continue indefinitely, so each party knows the table is correct, and knows each party knows this, and knows each party knows that, and knows each party knows that, and so on. An early example of this in philosophy is in the work by David Lewis (1969) on convention. But it is usually acknowledged (again in a tradition extending back at least to Lewis) that only the first few iterations are actually needed in any problem, and it seems a mistake to attribute more iterations than are actually used in deriving solutions to any particular game.\nThe reason that would be a mistake is that we want game theory, and decision theory, to be applicable to real-life situations. There is very little that we know, and know that we know, and know we know we know, and so on indefinitely (Williamson 2000 Ch. 4). There is, perhaps, even less that we are certain of. If we only could say that a person is making a particular decision when they stand in these very strong relationships to the parameters of the decision table, then people will almost never be making the kinds of decision we study in decision theory. Since decision theory and game theory are not meant to be that impractical, I conclude that the ‘special doxastic relationship’ cannot be that strong. It could be that in some games, the special relationship will involve a few iterations of knowledge, but in decision problems, where the epistemic states of others are irrelevant, even that is unnecessary, and simple knowledge seems sufficient.\nIt might be argued here that we shouldn’t expect to apply decision theory directly to real-life problems, but only to idealised versions of them, so it would be acceptable to, for instance, require that the things we put in the table are, say, things that have probability exactly 1. In real life, virtually nothing has probability 1. In an idealisation, many things do. But to argue this way seems to involve using ‘idealisation’ in an unnatural sense. There is a sense in which, whenever we treat something with non-maximal probability as simply given in a decision problem that we’re ignoring, or abstracting away from, some complication. But we aren’t idealising. On the contrary, we’re modelling the agent as if they were irrationally certain in some things which are merely very very probable.\nSo it’s better to say that any application of decision theory to a real-life problem will involve ignoring certain (counterfactual) logical or metaphysical possibilities in which the decision table is not actually true. But not any old abstraction will do. We can’t ignore just anything, at least not if we want a good model. Which abstractions are acceptable? The response I’ve offered to Dom’s challenge suggests an answer to this: we can abstract away from any possibility in which something the agent actually knows is false. I don’t have a knock-down argument that this is the best of all possible abstractions, but nor do I know of any alternative answer to the question which abstractions are acceptable which is nearly as plausible.\nWe might be tempted to say that we can abstract away from anything such that the difference between its probability and 1 doesn’t make a difference to the ultimate answer to the decision problem. More carefully, the idea would be that we can have the decision table represent that \\(p\\) iff \\(p\\) is true and treating \\(\\Pr(p)\\) as 1 rather than its actual value doesn’t change what the agent should do. I think this is the most plausible story one could tell about decision tables if one didn’t like the knowledge first story that I tell. But I also don’t think it works, because of cases like the following.\nLuc is lucky; he’s in a casino where they are offering better than fair odds on roulette. Although the chance of winning any bet is , if Luc bets $10, and his bet wins, he will win $400. (That’s the only bet on offer.) Luc, like Alice, is considering betting on 28. As it turns out, 28 won’t come up, although since this is a fair roulette wheel, Luc doesn’t know this. Luc, like most agents, has a declining marginal utility for money. He currently has $1,000, and for any amount of money \\(x\\), Luc gets utility \\(u(x) = x^{\\frac{1}{2}}\\) out of having \\(x\\). So Luc’s current utility (from money) is, roughly, 31.622. If he bets and loses, his utility will be, roughly, 31.464. And if he bets and wins, his utility will be, roughly, 37.417. So he stands to gain about 5.794, and to lose about 0.159. So he stands to gain about 36.5 as much as he stands to lose. Since the odds of winning are less than \\(\\frac{1}{36.5}\\), his expected utility goes down if he takes the bet, so he shouldn’t take it. Of course, if the probability of losing was 1, and not merely \\(\\frac{37}{38}\\), he shouldn’t take the bet too. Does that mean it is acceptable, in presenting Luc’s decision problem, to leave off the table any possibility of him winning, since he won’t win, and setting the probability of losing to 1 rather than \\(\\frac{37}{38}\\) doesn’t change the decision he should make? Of course not; that would horribly misstate the situation Luc finds himself in. It would misrepresent how sensitive Luc’s choice is to his utility function, and to the size of the stakes. If Luc’s utility function was \\(u(x) = x^{\\frac{3}{4}}\\), then he should take the bet. If his utility function is unchanged, but the bet was $1 against $40, rather than $10 against $400, he should take the bet. Leaving off the possibility of winning hides these facts, and badly misrepresents Luc’s situation.\nI’ve argued that the states we can ‘leave off’ a decision table are the states that the agent knows not to obtain. The argument is largely by elimination. If we can only leave off things that have probability 1, then decision theory would be useless; but it isn’t. If we say we can leave off things if setting their probability at 1 is an accepable idealisation, we need a theory of acceptable idealisations. If this is to be a rival to my theory, the idealisation had better not be it’s acceptable to treat anything known as having probability 1. But the most natural alternative idealisation badly misrepresents Luc’s case. If we say that what can be left off is not what’s known not to obtain, but what is, say, justifiably truly believed not to obtain, we need an argument for why people would naturally use such an unnatural standard. This doesn’t even purport to be a conclusive argument, but these considerations point me towards thinking that knowledge determines what we can leave off.\nI also cheated a little in making this argument. When I described Alice in the casino, I made a few explicit comments about her information states. And every time, I said that she knew various propositions. It seemed plausible at the time that this is enough to think those propositions should be incorporated into the table we use to represent her decision. That’s some evidence against the idea that more than knowledge, perhaps iterated knowledge or certainty, is needed before we add propositions to the decision table.\nFrom Decision Theory to Interest-Relativity\nThis way of thinking about decision problems offers a new perspective on the issue of whether we should always be prepared to bet on what we know.7 To focus intuitions, let’s take a concrete case. Barry is sitting in his apartment one evening when he hears a musician performing in the park outside. The musician, call her Beth, is one of Barry’s favourite musicians, so the music is familiar to Barry. Barry is excited that Beth is performing in his neighbourhood, and he decides to hurry out to see the show. As he prepares to leave, a genie appears an offers him a bet.8 If he takes the bet, and the musician is Beth, then the genie will give Barry ten dollars. On the other hand, if the musician is not Beth, he will be tortured in the fires of hell for a millenium. Let’s put Barry’s options in table form.\n\n\nMusician is Beth\nMusician is not Beth\nTake Bet\nWin $10\n1000 years of torture\nDecline Bet\nStatus quo\nStatus quo\n\nIntuitively, it is extremely irrational for Barry to take the bet. People do make mistakes about identifying musicians, even very familiar musicians, by the strains of music that drift up from a park. It’s not worth risking a millenium of torture for $10.\nBut it also seems that we’ve misstated the table. Before the genie showed up, it seemed clear that Barry knew that the musician was Beth. That was why he went out to see her perform. (If you don’t think this is true, make the sounds from the park clearer, or make it that Barry had some prior evidence that Beth was performing which the sounds from the park remind him of. It shouldn’t be too hard to come up with an evidential base such that (a) in normal circumstances we’d say Barry knew who was performing, but (b) he shouldn’t take this genie’s bet.) Now our decision tables should reflect the knowledge of the agent making the decision. If Barry knows that the musician is Beth, then the second column is one he knows will not obtain. So let’s write the table in the standard form.\n\n\nMusician is Beth\n\nTake Bet\nWin $10\n\nDecline Bet\nStatus quo\n\n\nAnd it is clear what Barry’s decision should be in this situation. Taking the bet dominates declining it, and Barry should take dominating options.\nWhat has happened? It is incredibly clear that Barry should decline the bet, yet here we have an argument that he should take the bet. If you accept that the bet should be declined, then it seems to me that there are three options available.\nBarry never knew that the musician was Beth.\nBarry did know that the musician was Beth, but this knowledge was destroyed by the genie’s offer of the bet.\nStates of the world that are known not to obtain should still be represented in decision problems, so taking the bet is not a dominating option.\nThe first option is basically a form of scepticism. If the take-away message from the above discussion is that Barry doesn’t know the musician is Beth, we can mount a similar argument to show that he knows next to nothing.9 And the third option would send us back into the problems about interpreting and applying decision theory that we spent the first few pages trying to get out of.\nSo it seems that the best solution here, or perhaps the least bad solution, is to accept that knowledge is interest-relative. Barry did know that the musician was Beth, but the genie’s offer destroyed that knowledge. When Barry was unconcerned with bets at extremely long odds on whether the musician is Beth, he knows Beth is the musician. Now that he is interested in those bets, he doesn’t know that.10\nThe argument here bears more than a passing resemblance to the arguments in favour of interest-relativity that are made by Hawthorne, Stanley, and Fantl and McGrath. But I think the focus on decision theory shows how we can get to interest-relativity with very weak premises.11 In particular, the only premises I’ve used to derive an interest-relative conclusion are:\nBefore the genie showed up, Barry knew the musician was Beth.\nIt’s rationally permissible, in cases like Barry’s, to take dominating options.\nIt’s always right to model decision problems by including what the agent knows in the ‘framework.’ That is, our decision tables should include what the agent knows about the payoffs in different states, and leave off any state the agent knows not to obtain.\nIt is rationally impermissible for Barry to take the genie’s offered bet.\nThe second premise there is much weaker than the principles linking knowledge and action defended in previous arguments for interest-relativity. It isn’t the claim that one can always act on what one knows, or that one can only act on what one knows, or that knowledge always (or only) provides reason to act. It’s just the claim that in one very specific type of situation, in particular when one has to make a relatively simple bet, which affects nobody but the person making the bet, it’s rationally permissible to take a dominating option. In conjunction with the third premise, it entails that in those kind of cases, the fact that one knows taking the bet will lead to a better outcome suffices for making acceptance of the bet rationally permissible. It doesn’t say anything about what else might or might not make acceptance rationally permissible. It doesn’t say anything about what suffices for rationally permissibility in other kinds of cases, such as cases where someone else’s interests are at stake, or where taking the bet might violate a deontological constraint, or any other way in which real-life choices differ from the simplest decision problems.12 It doesn’t say anything about any other kind of permissibility, e.g., moral permissibility. But it doesn’t need to, because we’re only in the business of proving that there is some interest-relativity to knowledge, and an assumption about practical rationality in some range of cases suffices to prove that.13\nThe case of Barry and Beth also bears some relationship to one of the kinds of case that have motivated contextualism about knowledge. Indeed, it has been widely noted in the literature on interest-relativity that interest-relativity can explain away many of the puzzles that motivate contextualism. And there are difficulties that face any contextualist theory (Weatherson 2006). So I prefer an invariantist form of interest-relativity about knowledge. That is, my view is a form of interest-relative-invariantism, or IRI.14\nNow everything I’ve said here leaves it open whether the interest-relativity of knowledge is a natural and intuitive theory, or whether it is a somewhat unhappy concession to difficulties that the case of Barry and Beth raise. I think the former is correct, and interest-relativity is fairly plausible on its own merits, but it would be consistent with my broader conclusions to say that in fact the interest-relative theory of knowledge is very implausible and counterintuitive. If we said that, we could still justify the interest-relative theory by noting that we have on our hands here a paradoxical situation, and any option will be somewhat implausible. This consideration has a bearing on how we should think about the role of intuitions about cases, or principles, in arguments that knowledge is interest-relative. Several critics of the view have argued that the view is counter-intuitive, or that it doesn’t accord with the reactions of non-expert judges.15 In a companion paper, “Defending Interest-Relative Invariantism,” I note that those arguments usually misconstrue what the consequences of interest-relative theories of knowledge are. But even if they don’t, I don’t think there’s any quick argument that if interest-relativity is counter-intuitive, it is false. After all, the only alternatives that seem to be open here are very counter-intuitive.\nFinally, it’s worth noting that if Barry is rational, he’ll stop (fully) believing that the musician is Beth once the genie makes the offer. Assuming the genie allows this, it would be very natural for Barry to try to acquire more information about the singer. He might walk over to the window to see if he can see who is performing in the park. So this case leaves it open whether the interest-relativity of knowledge can be explained fully by the interest-relativity of belief. I used to think it could be; I no longer think that. To see why this is so, it’s worth rehearsing how the interest-relative theory of belief runs.\nThe Interest-Relativity of Belief\nInterests and Functional Roles\nThe previous section was largely devoted to proving an existential claim: there is some interest-relativity to knowledge. Or, if you prefer, it proved a negative claim: the best theory of knowledge is not interest-neutral. But this negative conclusion invites a philosophical challenge: what is the best explanation of the interest-relativity of knowledge? My answer is in two parts. Part of the interest-relativity of knowledge comes from the interest-relativity of belief, and part of it comes from the fact that interests generate certain kinds of doxastic defeaters. It’s the second part, the part that is new to this paper, that makes the theory a version of non-doxastic IRI.\nHere’s my theory of belief. \\(S\\) believes that \\(p\\) iff conditionalising on \\(p\\) doesn’t change \\(S\\)’s answer to any relevant question. I’m using ‘relevance’ here in a non-technical sense; I say a lot more about how to cash out the notion in my (2005). The key thing to note is that relevance is interest-relative, so the theory of belief is interest-relative. There is a bit more to say about what kind of questions are important for this definition of belief. In part because I’ve changed my mind a little bit on this since the earlier paper, I’ll spend a bit more time on it. The following four kinds of questions are the most important.\nHow probable is \\(q\\)?\nIs \\(q\\) or \\(r\\) more probable?\nHow good an idea is it to do \\(\\phi\\)?\nIs it better to do \\(\\phi\\) or \\(\\psi\\)?\nThe theory of belief says that someone who believes that \\(p\\)doesn’t change their answer to any of these questions upon conditionalising on \\(p\\). Putting this formally, and making the restriction to relevant questions explicit, we get the following theorems of our theory of belief.16\nBAP\nFor all relevant \\(q, x\\), if \\(p\\) is believed then \\(\\Pr(q) = x\\) iff \\(\\Pr(q | p) = x\\).\n\nBCP\nFor all relevant \\(q, r\\), if \\(p\\) is believed then \\(\\Pr(q) \\geq \\Pr(r)\\) iff \\(\\Pr(q | p) \\geq \\Pr(r | p)\\).\n\nBAU\nFor all relevant \\(\\phi, x\\), if \\(p\\) is believed then \\(U(\\phi) = x\\) iff \\(U(\\phi | p) = x\\).\n\nBCU\nFor all relevant \\(\\phi, \\psi\\), if \\(p\\) is believed then \\(U(\\phi) \\geq U(\\psi)\\) iff \\(U(\\phi | p) \\geq U(\\psi | p)\\).\n\nIn the earlier paper I focussed on BAU and BCU. But BAP and BCP are important as well. Indeed, focussing on them lets us derive a nice result.\nCharlie is trying to figure out exactly what the probability of \\(p\\) is. That is, for any \\(x \\in [0, 1]\\), whether \\(\\Pr(p) = x\\) is a relevant question. Now Charlie is well aware that \\(\\Pr(p | p) = 1\\). So unless \\(\\Pr(p) = 1\\), Charlie will give a different answer to the questions How probable is p? and Given p, how probable is p?. So unless Charlie holds that \\(\\Pr(p)\\) is 1, she won’t count as believing that \\(p\\). One consequence of this is that Charlie can’t reason, “The probability of \\(p\\) is exactly 0.978, so \\(p\\).” That’s all to the good, since that looks like bad reasoning. And it looks like bad reasoning even though in some circumstances Charlie can rationally believe propositions that she (rationally) gives credence 0.978 to. Indeed, in some circumstances she can rationally believe something in virtue of it being 0.978 probable.\nThat’s because the reasoning in the previous paragraph assumes that every question of the form Is the probability of p equal to x? is relevant. In practice, fewer questions than that will be relevant. Let’s say that the only questions relevant to Charlie are of the form What is the probability of \\(p\\) to one decimal place?. And assume that no other questions become relevant in the course of her inquiry into this question.17 Charlie decides that to the first decimal place, \\(\\Pr(p) = 1.0\\), i.e., \\(\\Pr(p) > 0.95\\). That is compatible with simply believing that \\(p\\). And that seems right; if for practical purposes, the probability of \\(p\\) is indistinguishable from 1, then the agent is confident enough in \\(p\\) to believe it.\nSo there are some nice features of this theory of belief. Indeed, there are several reasons to believe it. It is, I have argued, the best functionalist account of belief. I’m not going to argue for functionalism about the mind, since the argument would take at least a book. (The book in question might look a lot like Braddon-Mitchell and Jackson (2007).) But I do think functionalism is true, and so the best functionalist theory of belief is the best theory of belief.\nThe argument for this theory of belief in my (2005) rested heavily on the flaws of rival theories. We can see those flaws by looking at a tension that any theory of the relationship between belief and credence must overcome. Each of the following three principles seems to be plausible.\nIf \\(S\\) has a greater credence in \\(p\\) than in \\(q\\), and she believes \\(q\\), then she believes \\(p\\) as well; and if her credences in both \\(p\\) and \\(q\\) are rational, and her belief in \\(q\\) is rational, then so is her belief in \\(p\\).\nIf \\(S\\) rationally believes \\(p\\) and rationally believes \\(q\\), then it is open to her to rationally believe \\(p \\wedge q\\) without changing her credences.\n\\(S\\) can rationally believe \\(p\\) while having credence of less than 1 in \\(p\\).\nBut these three principles, together with some principles that are genuinely uncontroversial, entail an absurd result. By 3, there is some \\(p\\) such that Cr\\((p) = x < 1\\), and \\(p\\) is believed. (Cr is the function from any proposition to our agent’s credence in that propositions.) Let \\(S\\) know that a particular fair lottery has \\(l\\) tickets, where \\(l > \\frac{1}{1-x}\\). The uncontroversial principle we’ll use is that in such a case \\(S\\)’s credence that any given ticket will lose should be \\(\\frac{l-1}{l}\\). Since \\(\\frac{l-1}{l} > x\\), it follows by 1 that \\(S\\) believes of each ticket that it will lose. Since her credences are rational, these beliefs are rational. By repeated applications of 2 then, the agent can rationally believe that each ticket will lose. But she rationally gives credence 0 to the proposition that each ticket will lose. So by 1 she can rationally believe any proposition in which her credence is greater than 0. This is absurd.18\nI won’t repeat all the gory details here, but one of the consequences of the discussion in Weatherson (2005) was that we could hold on to 3, and onto restricted versions of 1 and 2. In particular, if we restricted 1 and 2 to relevant propositions (in some sense) they became true, although the unrestricted version is false. A key part of the argument of the earlier paper was that this was a better option than the more commonly taken option of holding on to unrestricted versions of 1 and 3, at the cost of abandoning 2 even in clear cases. But one might wonder why I’m holding so tightly on to 3. After all, there is a functionalist argument that 3 is false.\nA key functional role of credences is that if an agent has credence \\(x\\) in \\(p\\) she should be prepared to buy a bet that returns 1 util if \\(p\\), and 0 utils otherwise, iff the price is no greater than \\(p\\) utils. A key functional role of belief is that if an agent believes \\(p\\), and recognises that \\(\\phi\\) is the best thing to do given \\(p\\), then she’ll do \\(\\phi\\). Given \\(p\\), it’s worth paying any price up to 1 util for a bet that pays 1 util if \\(p\\). So believing \\(p\\) seems to mean being in a functional state that is like having credence 1 in \\(p\\).\nBut this argument isn’t quite right. If we spell out more carefully what the functional roles of credence and belief are, a loophole emerges in the argument that belief implies credence 1. The interest-relative theory of belief turns out to exploit that loophole. What’s the difference, in functional terms, between having credence \\(x\\) in \\(p\\), and having credence \\(x + \\varepsilon\\) in \\(p\\)? Well, think again about the bet that pays 1 util if \\(p\\), and 0 utils otherwise. And imagine that bet is offered for \\(x + \\frac{\\varepsilon}{2}\\) utils. The person whose credence is \\(x\\) will decline the offer; the person whose credence is \\(x + \\varepsilon\\) will accept it. Now it will usually be that no such bet is on offer.19 No matter; as long as one agent is disposed to accept the offer, and the other agent is not, that suffices for a difference in credence.\nThe upshot of that is that differences in credences might be, indeed usually will be, constituted by differences in dispositions concerning how to act in choice situations far removed from actuality. I’m not usually in a position of having to accept or decline a chance to buy a bet for 0.9932 utils that the local coffee shop is currently open. Yet whether I would accept or decline such a bet matters to whether my credence that the coffee shop is open is 0.9931 or 0.9933. This isn’t a problem with the standard picture of how credences work. It’s just an observation that the high level of detail embedded in the picture relies on taking the constituents of mental states to involve many dispositions.\nOne of the crucial features of the theory of belief I’m defending is that what an agent believes is in general insensitive to such abtruse dispositions, although it is very sensitive to dispositions about practical matters. It’s true that if I believe that \\(p\\), and I’m rational enough, I’ll act as if \\(p\\) is true. Is it also true that if I believe \\(p\\), I’m disposed to act as if \\(p\\) is true no matter what choices are placed in front of me? The theory being defended here says no, and that seems plausible. As we say in the case of Barry and Beth, Barry can believe that \\(p\\), but be disposed to lose that belief rather than act on it if odd choices, like that presented by the genie, emerge.\nThis suggests the key difference between belief and credence 1. For a rational agent, a credence of 1 in \\(p\\) means that the agent is disposed to answer a wide range of questions the same way she would answer that question conditional on \\(p\\). That follows from the fact that these four principles are trivial theorems of the orthodox theory of expected utility.20\nC1AP\nFor all \\(q, x\\), if \\(\\Pr(p) = 1\\) then \\(\\Pr(q) = x\\) iff \\(\\Pr(q | p) = x\\).\n\nC1CP\nFor all \\(q, r\\), if \\(\\Pr(p) = 1\\) then \\(\\Pr(q) \\geq \\Pr(r)\\) iff \\(\\Pr(q | p) \\geq \\Pr(r | p)\\).\n\nC1AU\nFor all \\(\\phi, x\\), if \\(\\Pr(p) = 1\\) then \\(U(\\phi) = x\\) iff \\(U(\\phi | p) = x\\).\n\nC1CP\nFor all \\(\\phi, \\psi\\), if \\(\\Pr(p) = 1\\) then \\(U(\\phi) \\geq U(\\psi)\\) iff \\(U(\\phi | p) \\geq U(\\psi | p)\\).\n\nThose look a lot like the theorems of the theory of belief that we discussed above. But note that these claims are unrestricted, whereas in the theory of belief, we restricted attention to relevant actions, propositions, utilities and probabilities. That turns out to be the difference between belief and credence 1. Since that difference is interest-relative, belief is interest-relative.\nI used to think that that was all the interest-relativity we needed in epistemology. Now I don’t, for reasons that I’ll go through in section three. (Readers who care more about the theory of knowledge than the theory of belief may want to skip ahead to that section.) But first I want to clean up some loose ends in the acount of belief.\nTwo Caveats\nThe theory sketched so far seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and it handles difficult cases, like that of Charlie, nicely. But it needs to be supplemented and clarified a little to handle some other difficult cases. In this section I’m going to supplement the theory a little to handle what I call ‘impractical propositions,’ and say a little about morally loaded action.\nJones has a false geographic belief: he believes that Los Angeles is west of Reno.21 This isn’t because he’s ever thought about the question. Rather, he’s just disposed to say “Of course” if someone asks, “Is Los Angeles west of Reno?” That disposition has never been triggered, because no one’s ever bothered to ask him this. Call the proposition that Los Angeles is west of Reno \\(p\\).\nThe theory given so far will get the right result here: Jones does believe that \\(p\\). But it gets the right answer for an odd reason. Jones, it turns out, has very little interest in American geography right now. He’s a schoolboy in St Andrews, Scotland, getting ready for school and worried about missing his schoolbus. There’s no inquiry he’s currently engaged in for which \\(p\\) is even close to relevant. So conditionalising on \\(p\\) doesn’t change the answer to any inquiry he’s engaged in, but that would be true no matter what his credence in \\(p\\) is.\nThere’s an immediate problem here. Jones believes \\(p\\), since conditionalising on \\(p\\) doesn’t change the answer to any relevant inquiry. But for the very same reason, conditionalising on \\(\\neg p\\) doesn’t change the answer to any relevant inquiry. It seems our theory has the bizarre result that Jones believes \\(\\neg p\\) as well. That is both wrong and unfair. We end up attributing inconsistent beliefs to Jones simply because he’s a harried schoolboy who isn’t currently concerned with the finer points of geography of the American southwest.\nHere’s a way out of this problem in four relatively easy steps.22 First, we say that which questions are relevant questions is not just relative to the agent’s interests, but also relevant to the proposition being considered. A question may be relevant relative to \\(p\\), but not relative to \\(q\\). Second, we say that relative to \\(p\\), the question of whether \\(p\\) is more probable than \\(\\neg p\\) is a relevant question. Third, we infer from that that an agent only believes \\(p\\) if their credence in \\(p\\) is greater than their credence in \\(\\neg p\\), i.e., if their credence in \\(p\\) is greater than \\(\\frac{1}{2}\\). Finally, we say that when the issue is whether the subject believes that \\(p\\), the question of whether \\(p\\) is more probable than \\(\\neg p\\) is not only relevant on its own, but it stays being a relevant question conditional on any \\(q\\) that is relevant to the subject. In the earlier paper (Weatherson 2005) I argue that this solves the problem raised by impractical propositions in a smooth and principled way.\nThat’s the first caveat. The second is one that isn’t discussed in the earlier paper. If the agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. And within orthodox decision theory, it is easy enough to talk about, and reason about, conditional utilities. That’s important, because conditional utilities play an important role in the theory of belief offered at the start of this section. But if the agent faces moral constraints on her decision, it isn’t always so easy to think about conditional utilities.\nWhen agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is best to supplement orthodox decision theory with an extra assumption. The assumption is, roughly, that for choices that may harm others, expected value is absolute value. It’s easiest to see what this means using a simple case of three-way choice. The kind of example I’m considering here has been used for (slightly) different purposes by Frank Jackson (1991).\nThe agent has to do \\(\\varphi\\) or \\(\\psi\\). Failure to do either of these will lead to disaster, and is clearly unacceptable. Either \\(\\varphi\\) or \\(\\psi\\) will avert the disaster, but one of them will be moderately harmful and the other one will not. The agent has time before the disaster to find out which of \\(\\varphi\\) and \\(\\psi\\) is harmful and which is not for a nominal cost. Right now, her credence that \\(\\varphi\\) is the harmful one is, quite reasonably, \\(\\frac{1}{2}\\). So the agent has three choices:\nDo \\(\\varphi\\);\nDo \\(\\psi\\); or\nWait and find out which one is not harmful, and do it.\nWe’ll assume that other choices, like letting the disaster happen, or finding out which one is harmful and doing it, are simply out of consideration. In any case, they are clearly dominated options, so the agent shouldn’t do them. Let \\(p\\) be the propostion that \\(\\varphi\\) is the harmful one. Then if we assume the harm in question has a disutility of 10, and the disutility of waiting to act until we know which is the harmful one is 1, the values of the possible outcomes are as follows:\n\n\n\\(p\\)\n\\(\\neg p\\)\nDo \\(\\varphi\\)\n-10\n0\nDo \\(\\psi\\)\n0\n-10\nFind out which is harmful\n-1\n-1\n\nGiven that \\(Pr(p) = \\frac{1}{2}\\), it’s easy to compute that the expected value of doing either \\(\\varphi\\) or \\(\\psi\\) is -5, while the expected value of finding out which is harmful is -1, so the agent should find out which thing is to be done before acting. So far most consequentialists would agree, and so probably would most non-consequentialists for most ways of fleshing out the abstract example I’ve described.23\nBut most consequentialists would also say something else about the example that I think is not exactly true. Just focus on the column in the table above where \\(p\\) is true. In that column, the highest value, 0, is alongside the action Do \\(\\psi\\). So you might think that conditional on \\(p\\), the agent should do \\(\\psi\\). That is, you might think the conditional expected value of doing \\(\\psi\\), conditional on \\(p\\) being true, is 0, and that’s higher than the conditional expected value of any other act, conditional on \\(p\\). If you thought that, you’d certainly be in agreement with the orthodox decision-theoretic treatment of this problem.\nIn the abstract statement of the situation above, I said that one of the options would be harmful, but I didn’t say who it would be harmful to. I think this matters. I think what I called the orthodox treatment of the situation is correct when the harm accrues to the person making the decision. But when the harm accrues to another person, particularly when it accrues to a person that the agent has a duty of care towards, then I think the orthodox treatment isn’t quite right.\nMy reasons for this go back to Jackson’s original discussion of the puzzle. Let the agent be a doctor, the actions \\(\\varphi\\) and \\(\\psi\\) be her prescribing different medication to a patient, and the harm a severe allergic reaction that the patient will have to one of the medications. Assume that she can run a test that will tell her which medication the patient is allergic to, but the test will take a day. Assume that the patient will die in a month without either medication; that’s the disaster that must be averted. And assume that the patient is is some discomfort that either medication would relieve; that’s the small cost of finding out which medication is risk. Assume finally that there is no chance the patient will die in the day it takes to run the test, so the cost of running the test is really nominal.\nA good doctor in that situation will find out which medication the patient is allergic to before ascribing either medicine. It would be reckless to ascribe a medicine that is unnecessary and that the patient might be allergic to. It is worse than reckless if the patient is actually allergic to the medicine prescribed, and the doctor harms the patient. But even if she’s lucky and prescribes the ‘right’ medication, the recklessness remains. It was still, it seems, the wrong thing for her to do.\nAll of that is in Jackson’s discussion of the case, though I’m not sure he’d agree with the way I’m about the incorporate these ideas into the formal decision theory. Even under the assumption that \\(p\\), prescribing \\(\\psi\\) is still wrong, because it is reckless. That should be incorporated into the values we ascribe to different actions in different circumstances. The way I do it is to associate the value of each action, in each circumstance, with its actual expected value. So the decision table for the doctor’s decision looks something like this.\n\n\n\\(p\\)\n\\(\\neg p\\)\nDo \\(\\varphi\\)\n-5\n-5\nDo \\(\\psi\\)\n-5\n-5\nFind out which is harmful\n-1\n-1\n\nIn fact, the doctor is making a decision under certainty. She knows that the value of prescribing either medicine is -5, and the value of running the tests is -1, so she should run the tests.\nIn general, when an agent has a duty to maximise the expected value of some quantity \\(Q\\), then the value that goes into the agent’s decision table in a cell is not the value of \\(Q\\) in the world-action pair the agent represents. Rather, it’s the expected value of \\(Q\\) given that world-action pair. In situations like this one where the relevant facts (e.g., which medicine the patient is allergic to) don’t affect the evidence the agent has, the decision is a decision under certainty. This is all as things should be. When you have obligations that are drawn in terms of the expected value of a variable, the actual values of that variable cease to be directly relevant to the decision problem.\nSimilar morals carry across to theories that offer a smaller role to expected utility in determining moral value. In particular, it’s often true that decisions where it is uncertain what course of action will produce the best outcome might still, in the morally salient sense, be decisions under certainty. That’s because the uncertainty doesn’t impact how we should weight the different possible outcomes, as in orthodox utility theory, but how we should value them. That’s roughly what I think is going on in cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.24\n\nA student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn’t started as the surgeon is consulting the patient’s notes. The student is puzzled and asks one of the nurses what’s going on:\nStudent: I don’t understand. Why is she looking at the patient’s records? She was in clinic with the patient this morning. Doesn’t she even know which kidney it is?\nNurse: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn’t operate before checking the patient’s records. (Brown 2008, 1144–45)\n\nIt is tempting, but for reasons I’ve been going through here mistaken, to represent the surgeon’s choice as follows. Let Left mean the left kidney is diseased, and Right mean the right kidney is diseased.\n\n\nLeft\nRight\nRemove left kidney\n\\(1\\)\n\\(-1\\)\nRemove right kidney\n\\(-1\\)\n\\(1\\)\nCheck notes\n\\(1-\\varepsilon\\)\n\\(1-\\varepsilon\\)\n\nHere \\(\\varepsilon\\) is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she’s in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.\nBut that reasoning assumes that the surgeon does not have any epistemic obligations over and above her duty to maximise expected utility. And that’s very implausible. It’s totally implausible on a non-consequentialist moral theory. A non-consequentialist may think that some people have just the same obligations that the consequentialist says they have – legislators are frequently mentioned as an example – but surely they wouldn’t think surgeons are in this category. And even a consequentialist who thinks that surgeons have special obligations in terms of their institutional role should think that the surgeon’s obligations go above and beyond the obligation every agent has to maximise expected utility.\nIt’s not clear exactly what the obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed. Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities.\nHowever we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the ‘cost’ of violating this obligation, call it \\(\\delta\\) is greater than the cost of checking the notes. So here is the decision table the surgeon faces.\n\n\nLeft\nRight\nRemove left kidney\n\\(1-\\delta\\)\n\\(-1-\\delta\\)\nRemove right kidney\n\\(-1-\\delta\\)\n\\(1-\\delta\\)\nCheck notes\n\\(1-\\varepsilon\\)\n\\(1-\\varepsilon\\)\n\nAnd it isn’t surprising, or a problem for an interest-relative theory of knowledge or belief, that the surgeon should check the notes, even if she believes and knows that the left kidney is the diseased one.\nInterest-Relative Defeaters\nAs I said at the top, I’ve changed my view from Doxastic IRI to Non-Doxastic IRI. The change of heart is occasioned by cases like the following, where the agent is mistaken, and hence ignorant, about the odds at which she is offered a bet on \\(p\\). In fact the odds are much longer than she thinks. Relative to what she stands to win, the stakes are too high.\nThe Coraline Example\nThe problem for Doxastic IRI arises because of cases like that of Coraline. Here’s what we’re going to stipulate about Coraline.\nShe knows that \\(p\\) and \\(q\\) are independent, so her credence in any conjunction where one conjunct is a member of \\(\\{p, \\neg p\\}\\) and the other is a member of \\(\\{q, \\neg q\\}\\) will be the product of her credences in the conjuncts.\nHer credence in \\(p\\) is 0.99, just as the evidence supports.\nHer credence in \\(q\\) is also 0.99. This is unfortunate, since the rational credence in \\(q\\) given her evidence is 0.01.\nThe only relevant question for her which is sensitive to \\(p\\) is whether to take or decline a bet with the following payoff structure.25 (Assume that the marginal utility of money is close enough to constant that expected dollar returns correlate more or less precisely with expected utility returns.)\n\n\n\\(p \\wedge q\\)\n\\(p \\wedge \\neg q\\)\n\\(\\neg p\\)\nTake bet\n100\n1\n1000\nDecline bet\n0\n0\n0\n\nAs can be easily computed, the expected utility of taking the bet given her credences is positive, it is just over $89. And Coraline takes the bet. She doesn’t compute the expected utility, but she is sensitive to it.26 That is, had the expected utility given her credences been close to 0, she would have not acted until she made a computation. But from her perspective this looks like basically a free $100, so she takes it. Happily, this all turns out well enough, since \\(p\\) is true. But it was a dumb thing to do. The expected utility of taking the bet given her evidence is negative, it is a little under -$8. So she isn’t warranted, given her evidence, in taking the bet.\nWhat Coraline Knows and What She Believes\nAssume, for reductio, that Coraline knows that \\(p\\). Then the choice she faces looks like this.\n\n\n\\(q\\)\n\\(\\neg q\\)\nTake bet\n100\n1\nDecline bet\n0\n0\n\nSince taking the bet dominates declining the bet, she should take the bet if this is the correct representation of her situation. She shouldn’t take the bet, so by modus tollens, that can’t be the correct representation of her situation. If she knew \\(p\\), that would be the correct representation of her situation. So, again by modus tollens, she doesn’t know \\(p\\).\nNow let’s consider three possible explanations of why she doesn’t know that \\(p\\).\nShe doesn’t have enough evidence to know that \\(p\\), independent of the practical stakes.\nIn virtue of the practical stakes, she doesn’t believe that \\(p\\);\nIn virtue of the practical stakes, she doesn’t justifiably believe that \\(p\\), although she does actually believe it.\nIn virtue of the practical stakes, she doesn’t know that \\(p\\), although she does justifiably believe it.\nI think option 1 is implausibly sceptical, at least if applied to all cases like Coraline’s. I’ve said that the probability of \\(p\\) is 0.99, but it should be clear that all that matters to generating a case like this is that \\(p\\) is not completely certain. Unless knowledge requires certainty, we’ll be able to generate Coraline-like cases where there is sufficient evidence for knowledge. So that’s ruled out.\nOption 2 is basically what the Doxastic IRI theorist has to say. If Coraline has enough evidence to know \\(p\\), but doesn’t know \\(p\\) due to practical stakes, then the Doxastic IRI theorist is committed to saying that the practical stakes block belief in \\(p\\). That’s the Doxastic IRI position; stakes matter to knowledge because they matter to belief.\nBut that’s also an implausible description of Coraline’s situation. She is very confident that \\(p\\). Her confidence is grounded in the evidence in the right way. She is insensitive in her actual deliberations to the difference between her evidence for \\(p\\) and evidence that guarantees \\(p\\). She would become sensitive to that difference if someone offered her a bet that she knew was a 1000-to-1 bet on \\(p\\), but she doesn’t know that’s what is on offer. In short, there is no difference between her unconditional attitudes, and her attitudes conditional on \\(p\\), when it comes to any live question. That’s enough, I think, for belief. So she believes that \\(p\\). And that’s bad news for the Doxastic IRI theorist; since it means here that stakes matter to knowledge without mattering to belief. I conclude, reluctantly, that Doxastic IRI is false.\nStakes as Defeaters\nThat still leaves two options remaining, what I’ve called options 3 and 4 above. Option 3, if suitably generalised, says that knowledge is practically sensitive because the justification condition on belief is practically sensitive. Option 4 says that practical considerations impact knowledge directly. As I read them, Jeremy Fantl and Matthew McGrath defend a version of Option 3. In the next and last subsection, I’ll argue against that position. But first I want to sketch what a position like option 4 would look like.\nKnowledge, unlike justification, requires a certain amount of internal coherence among mental states. Consider the following story from David Lewis:\n\nI speak from experience as the repository of a mildly inconsistent corpus. I used to think that Nassau Street ran roughly east-west; that the railroad nearby ran roughly north-south; and that the two were roughly parallel. (Lewis 1982, 436)\n\nI think in that case that Lewis doesn’t know that Nassau Street runs roughly east-west. (From here on, call the proposition that Nassau Street runs roughly east-west \\(N\\).) If his belief that it does was acquired and sustained in a suitably reliable way, then he may well have a justified belief that \\(N\\). But the lack of coherence with the rest of his cognitive system, I think, defeats any claim to knowledge he has.\nCoherence isn’t just a requirement on belief; other states can cohere or be incoherent. Assume Lewis corrects the incoherence in his beliefs, and drops the belief that Nassau Street the railway are roughly parallel. Still, if Lewis believed that \\(N\\), preferred doing \\(\\varphi\\) to doing \\(\\psi\\) conditional on \\(N\\), but actually preferred doing \\(\\psi\\) to doing \\(\\varphi\\), his cognitive system would also be in tension. That tension could, I think, be sufficient to defeat a claim to know that \\(N\\).\nAnd it isn’t just a requirement on actual states; it can be a requirement on rational states. Assume Lewis believed that \\(N\\), preferred doing \\(\\varphi\\) to doing \\(\\psi\\) conditional on \\(N\\), and preferred doing \\(\\varphi\\) to doing \\(\\psi\\), but should have preferred doing \\(\\psi\\) to doing \\(\\varphi\\) given his interests. Then I think the fact that the last preference is irrational, plus the fact that were it corrected there would be incoherence in his cognitive states defeats the claim to know that \\(N\\).\nA concrete example of this helps make clear why such a view is attractive, and why it faces difficulties. Assume there is a bet that wins $2 if \\(N\\), and loses $10 if not. Let \\(\\varphi\\) be taking that bet, and \\(\\psi\\) be declining it. Assume Lewis shouldn’t take that bet; he doesnt have enough evidence to do so. Then he clearly doesn’t know that \\(N\\). If he knew that \\(N\\), \\(\\varphi\\) would dominate \\(\\psi\\), and hence be rational. But it isn’t, so \\(N\\) isn’t known. And that’s true whether Lewis’s preferences between \\(\\varphi\\) and \\(\\psi\\) are rational or irrational.\nAttentive readers will see where this is going. Change the bet so it wins a penny if \\(N\\), and loses $1,000 if not. Unless Lewis’s evidence that \\(N\\) is incredibly strong, he shouldn’t take the bet. So, by the same reasoning, he doesn’t know that \\(N\\). And we’re back saying that knowledge requires incredibly strong evidence. The solution, I say, is to put a pragmatic restriction on the kinds of incoherence that matter to knowledge. Incoherence with respect to irrelevant questions, such as whether to bet on \\(N\\) at extremely long odds, doesn’t matter for knowledge. Incoherence (or coherence obtained only through irrationality) does. The reason, I think, that Non-Doxastic IRI is true is that this coherence-based defeater is sensitive to practical interests.\nThe string of cases about Lewis and \\(N\\) has ended up close to the Coraline example. We already concluded that Coraline didn’t know \\(p\\). Now we have a story about why - belief that \\(p\\) doesn’t cohere sufficiently well with what she should believe, namely that it would be wrong to take the bet. If all that is correct, just one question remains: does this coherence-based defeater also defeat Coraline’s claim to have a justified belief that \\(p\\)? I say it does not, for three reasons.\nFirst, her attitude towards \\(p\\) tracks the evidence perfectly. She is making no mistakes with respect to \\(p\\). She is making a mistake with respect to \\(q\\), but not with respect to \\(p\\). So her attitude towards \\(p\\), i.e. belief, is justified.\nSecond, talking about beliefs and talking about credences are simply two ways of modelling the very same things, namely minds. If the agent both has a credence 0.99 in \\(p\\), and believes that \\(p\\), these are not two different states. Rather, there is one state of the agent, and two different ways of modelling it. So it is implausible to apply different valuations to the state depending on which modelling tools we choose to use. That is, it’s implausible to say that while we’re modelling the agent with credences, the state is justified, but when we change tools, and start using beliefs, the state is unjustified. Given this outlook on beliefs and credences, it is natural to say that her belief is justified. Natural, but not compulsory, for reasons Jeremy Fantl pointed out to me.27 We don’t want a metaphysics on which persons and philosophers are separate entities. Yet we can say that someone is a good person but a bad philosopher. Normative statuses can differ depending on which property of a thing we are considering. That suggests it is at least coherent to say that one and the same state is a good credence but a bad belief. But while this may be coherent, I don’t think it is well motivated, and it is natural to have the evaluations go together.\nThird, we don’t need to say that Coraline’s belief in \\(p\\) is unjustified in order to preserve other nice theories, in the way that we do need to say that she doesn’t know \\(p\\) in order to preserve a nice account of how we understand decision tables. It’s this last point that I think Fantl and McGrath, who say that the belief is unjustified, would reject. So let’s conclude with a look at their arguments.\nFantl and McGrath on Interest-Relativity\nFantl and McGrath’s argue for the principle (JJ), which entails that Coraline is not justified in believing \\(p\\).\n(JJ)\nIf you are justified in believing that \\(p\\), then \\(p\\) is warranted enough to justify you in \\(\\varphi\\)-ing, for any \\(\\varphi\\). (Fantl and McGrath 2009, 99)\n\nIn practice, what this means is that there can’t be a salient \\(p, \\varphi\\) such that:\nThe agent is justified in believing \\(p\\);\nThe agent is not warranted in doing \\(\\varphi\\); but\nIf the agent had more evidence for \\(p\\), and nothing else, the agent would be be warranted in doing \\(\\varphi\\).\nThat is, once you’ve got enough evidence, or warrant, for justified belief in \\(p\\), then you’ve got enough evidence for \\(p\\) as matters for any decision you face. This seems intuitive, and Fantl and McGrath back up its intuitiveness with some nicely drawn examples. But I think it is false, and the Coraline example shows it is false. Coraline isn’t justified in taking the bet, and is justified in believing \\(p\\), but more evidence for \\(p\\) would suffice for taking the bet. So Coraline’s case shows that (JJ) is false. But there are a number of possible objections to that position. I’ll spend the rest of this section, and this paper, going over them.28\nObjection: The following argument shows that Coraline is not in fact justified in believing that \\(p\\).\n\\(p\\) entails that Coraline should take the bet, and Coraline knows this.\nIf \\(p\\) entails something, and Coraline knows this, and she justifiably believes \\(p\\), she is in a position to justifiably believe the thing entailed.\nCoraline is not in a position to justifiably believe that she should take the bet.\nSo, Coraline does not justifiably believe that \\(p\\)\nReply: The problem here is that premise 1 is false. What’s true is that \\(p\\) entails that Coraline will be better off taking the bet than declining it. But it doesn’t follow that she should take the bet. Indeed, it isn’t actually true that she should take the bet, even though \\(p\\) is actually true. Not just is the entailment claim false, the world of the example is a counterinstance to it.\nIt might be controversial to use this very case to reject premise 1. But the falsity of premise 1 should be clear on independent grounds. What \\(p\\) entails is that Coraline will be best off by taking the bet. But there are lots of things that will make me better off that I shouldn’t do. Imagine I’m standing by a roulette wheel, and the thing that will make me best off is betting heavily on the number than will actually come up. It doesn’t follow that I should do that. Indeed, I should not do it. I shouldn’t place any bets at all, since all the bets have a highly negative expected return.\nIn short, all \\(p\\) entails is that taking the bet will have the best consequences. Only a very crude kind of consequentialism would identify what I should do with what will have the best returns, and that crude consequentialism isn’t true. So \\(p\\) doesn’t entail that Coraline should take the bet. So premise 1 is false.\nObjection: Even though \\(p\\) doesn’t entail that Coraline should take the bet, it does provide inductive support for her taking the bet. So if she could justifiably believe \\(p\\), she could justifiably (but non-deductively) infer that she should take the bet. Since she can’t justifiably infer that, she isn’t justified in taking the bet.\nReply: The inductive inference here looks weak. One way to make the inductive inference work would be to deduce from \\(p\\) that taking the bet will have the best outcomes, and infer from that that the bet should be taken. But the last step doesn’t even look like a reliable ampliative inference. The usual situation is that the best outcome comes from taking an ex ante unjustifiable risk.\nIt may seem better to use \\(p\\) combined with the fact that conditional on \\(p\\), taking the bet has the highest expected utility. But actually that’s still not much of a reason to take the bet. Think again about cases, completely normal cases, where the action with the best outcome is an ex ante unjustifiable risk. Call that action \\(\\varphi\\), and let \\(B \\varphi\\) be the proposition that \\(\\varphi\\) has the best outcome. Then \\(B \\varphi\\) is true, and conditional on \\(B \\varphi\\), \\(\\varphi\\) has an excellent expected return. But doing \\(\\varphi\\) is still running a dumb risk. Since these kinds of cases are normal, it seems it will very often be the case that this form of inference leads from truth to falsity. So it’s not a reliable inductive inference.\nObjection: In the example, Coraline isn’t just in a position to justifiably believe \\(p\\), she is in a position to know that she justifiably believes it. And from the fact that she justifiably believes \\(p\\), and the fact that if \\(p\\), then taking the bet has the best option, she can infer that she should take the bet.\nReply: It’s possible at this point that we get to a dialectical impasse. I think this inference is non-deductive, because I think the example we’re discussing here is one where the premises are true and the conclusion false. Presumably someone who doesn’t like the example will think that it is a good deductive inference.\nHaving said that, the more complicated example at the end of Weatherson (2005) was designed to raise the same problem without the consequence that if \\(p\\) is true, the bet is sure to return a positive amount. In that example, conditionalising on \\(p\\) means the bet has a positive expected return, but still possibly a negative return. But in that case (JJ) still failed. If accepting there are cases where an agent justifiably believes \\(p\\), and hence justifiably believes taking the bet will return the best outcome, and knows all this, but still can’t rationally bet on \\(p\\) is too much to accept, that more complicated example might be more persuasive. Otherwise, I concede that someone who believes (JJ) and thinks rational agents can use it in their reasoning will not think that a particular case is a counterexample to (JJ).\nObjection:If Coraline were ideal, then she wouldn’t believe \\(p\\). That’s because if she were ideal, she would have a lower credence in \\(q\\), and if that were the case, her credence in \\(p\\) would have to be much higher (close to 0.999) in order to count as a belief. So her belief is not justified.\nReply: The premise here, that if Coraline were ideal she would not believe that \\(p\\), is true. The conclusion, that she is not justified in believing \\(p\\), does not follow. It’s always a mistake to identify what should be done with what is done in ideal circumstances. This is something that has long been known in economics. The locus classicus of the view that this is a mistake is Lipsey and Lancaster (1956). A similar point has been made in ethics in papers such as Watson (1977) and Kennett and Smith (1996a, 1996b). And it has been extended to epistemology by Williamson (1998).\nAll of these discussions have a common structure. It is first observed that the ideal is both \\(F\\) and \\(G\\). It is then stipulated that whatever happens, the thing being created (either a social system, an action, or a cognitive state) will not be \\(F\\). It is then argued that given the stipulation, the thing being created should not be \\(G\\). That is not just the claim that we shouldn’t aim to make the thing be \\(G\\). It is, rather, that in many cases being \\(G\\) is not the best way to be, given that \\(F\\)-ness will not be achieved. Lipsey and Lancaster argue that (in an admittedly idealised model) that it is actually quite unusual for \\(G\\) to be best given that the system being created will not be \\(F\\).\nIt’s not too hard to come up with examples that fit this structure. Following (Williamson 2000, 209), we might note that I’m justified in believing that there are no ideal cognitive agents, although were I ideal I would not believe this. Or imagine a student taking a ten question mathematics exam who has no idea how to answer the last question. She knows an ideal student would correctly answer an even number of questions, but that’s no reason for her to throw out her good answer to question nine. In general, once we have stipulated one departure from the ideal, there’s no reason to assign any positive status to other similarities to the idea. In particular, given that Coraline has an irrational view towards \\(q\\), she won’t perfectly match up with the ideal, so there’s no reason it’s good to agree with the ideal in other respects, such as not believing \\(p\\).\nStepping back a bit, there’s a reason the interest-relative theory says that the ideal and justification come apart right here. On the interest-relative theory, like on any pragmatic theory of mental states, the identification of mental states is a somewhat holistic matter. Something is a belief in virtue of its position in a much broader network. But the evaluation of belief is (relatively) atomistic. That’s why Coraline is justified in believing \\(p\\), although if she were wiser she would not believe it. If she were wiser, i.e., if she had the right attitude towards \\(q\\), the very same credence in \\(p\\) would not count as a belief. Whether her state counts as a belief, that is, depends on wide-ranging features of her cognitive system. But whether the state is justified depends on more local factors, and in local respects she is doing everything right.\nObjection: If Coraline is justified in believing \\(p\\), then Coraline can use \\(p\\) as a premise in practical reasoning. If Coraline can use \\(p\\) as a premise in practical reasoning, and \\(p\\) is true, and her belief in \\(p\\) is not Gettiered, then she knows \\(p\\). By hypothesis, her belief is true, and her belief is not Gettiered. So she should know \\(p\\). But she doesn’t know \\(p\\). So by several steps of modus tollens, she isn’t justified in believing \\(p\\).29\nReply: This objection this one turns on an equivocation over the neologism ‘Gettiered.’ Some epistemologists use this to simply mean that a belief is justified and true without constituting knowledge. By that standard, the third sentence is false. Or, at least, we haven’t been given any reason to think that it is true. Given everything else that’s said, the third sentence is a raw assertion that Coraline knows that \\(p\\), and I don’t think we should accept that.\nThe other way epistemologists sometimes use the term is to pick out justified true beliefs that fail to be knowledge for the reasons that the beliefs in the original examples from Gettier (1963) fail to be knowledge. That is, it picks out a property that beliefs have when they are derived from a false lemma, or whatever similar property is held to be doing the work in the original Gettier examples. Now on this reading, Coraline’s belief that \\(p\\) is not Gettiered. But it doesn’t follow that it is known. There’s no reason, once we’ve given up on the JTB theory of knowledge, to think that whatever goes wrong in Gettier’s examples is the only way for a justified true belief to fall short of knowledge. It could be that there’s a practical defeater, as in this case. So the second sentence of the objection is false, and the objection again fails.\nOnce we have an expansive theory of defeaters, as I’ve adopted here, it becomes problematic to describe the case in the language Fantl and McGrath use. They focus a lot on whether agents like Coraline have ‘knowledge-level justification’ for \\(p\\), which is defined as “justification strong enough so that shortcomings in your strength of justification stand in the way of your knowing.” (Fantl and McGrath 2009, 97). An important part of their argument is that an agent is justified in believing \\(p\\) iff they have knowledge-level justification for \\(p\\). I haven’t addressed this argument, so I’m not really addressing the case on their terms.\nWell, does Coraline have knowledge-level justification for \\(p\\)? I’m not sure, because I’m not sure I grasp this concept. Compare the agent in Harman’s dead dictator case (Harman 1973, 75). Does she have knowledge-level justification that the dictator is dead? In one sense yes; it is the existence of misleading news sources that stops her knowing. In another sense no; she doesn’t know, but if she had better evidence (e.g., seeing the death happen) she would know. I want to say the same thing about Coraline, and that makes it hard to translate the Coraline case into Fantl and McGrath’s terminology.\n\n\nBlome-Tillmann, Michael. 2009. “Contextualism, Subject-Sensitive Invariantism, and the Interaction of ‘Knowledge’-Ascriptions with Modal and Temporal Operators.” Philosophy and Phenomenological Research 79 (2): 315–31. https://doi.org/10.1111/j.1933-1592.2009.00280.x.\n\n\nBraddon-Mitchell, David, and Frank Jackson. 2007. The Philosophy of Mind and Cognition, Second Edition. Malden, MA: Blackwell.\n\n\nBrown, Jessica. 2008. “Knowledge and Practical Reason.” Philosophy Compass 3 (6): 1135–52. https://doi.org/10.1111/j.1747-9991.2008.00176.x.\n\n\nCohen, Stewart. 1988. “How to Be a Fallibilist.” Philosophical Perspectives 2: 91–123. https://doi.org/10.2307/2214070.\n\n\nDeRose, Keith. 1995. “Solving the Skeptical Problem.” Philosophical Review 104 (1): 1–52. https://doi.org/10.2307/2186011.\n\n\nFantl, Jeremy, and Matthew McGrath. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nFeltz, Adam, and Chris Zarpentine. 2010. “Do You Know More When It Matters Less?” Philosophical Psychology 23 (5): 683–706. https://doi.org/10.1080/09515089.2010.514572.\n\n\nGettier, Edmund L. 1963. “Is Justified True Belief Knowledge?” Analysis 23 (6): 121–23. https://doi.org/10.2307/3326922.\n\n\nHammond, Peter J. 1988. “Consequentialist Foundations for Expected Utility.” Theory and Decision 25: 25–78. https://doi.org/10.1007/BF00129168.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHawthorne, John, and Jason Stanley. 2008. “Knowledge and Action.” Journal of Philosophy 105 (10): 571–90. https://doi.org/10.5840/jphil20081051022.\n\n\nIchikawa, Jonathan. 2009. “Explaining Away Intuitions.” Studia Philosophica Estonica 22 (2): 94–116. https://doi.org/10.12697/spe.2009.2.2.06.\n\n\nJackson, Frank. 1991. “Decision Theoretic Consequentialism and the Nearest and Dearest Objection.” Ethics 101 (3): 461–82. https://doi.org/10.1086/293312.\n\n\nKennett, Jeanette, and Michael Smith. 1996a. “Frog and Toad Lose Control.” Analysis 56 (2): 63–73. https://doi.org/10.1111/j.0003-2638.1996.00063.x.\n\n\n———. 1996b. “Philosophy and Commonsense: The Case of Weakness of Will.” In The Place of Philosophy in the Study of Mind, edited by Michaelis Michael and John O’Leary-Hawthorne, 141–57. Norwell, MA: Kluwer. https://doi.org/10.1017/CBO9780511606977.005.\n\n\nLewis, David. 1969. Convention: A Philosophical Study. Cambridge: Harvard University Press.\n\n\n———. 1982. “Logic for Equivocators.” Noûs 16 (3): 431–41. https://doi.org/10.1017/cbo9780511625237.009.\n\n\n———. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nLipsey, R. G., and Kelvin Lancaster. 1956. “The General Theory of Second Best.” Review of Economic Studies 24 (1): 11–32. https://doi.org/10.2307/2296233.\n\n\nRunyon, Damon. 1992. Guys & Dolls: The Stories of Damon Runyon. New York: Penguin.\n\n\nStalnaker, Robert. 2008. Our Knowledge of the Internal World. Oxford: Oxford University Press.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nSturgeon, Scott. 2008. “Reason and the Grain of Belief.” Noûs 42 (1): 139–65. https://doi.org/10.1111/j.1468-0068.2007.00676.x.\n\n\nWatson, Gary. 1977. “Skepticism about Weakness of Will.” Philosophical Review 86 (3): 316–39. https://doi.org/10.2307/2183785.\n\n\nWeatherson, Brian. 2003. “What Good Are Counterexamples?” Philosophical Studies 115 (1): 1–31. https://doi.org/10.1023/A:1024961917413.\n\n\n———. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\n———. 2006. “Questioning Contextualism.” In Epistemology Futures, edited by Stephen Cade Hetherington, 133–47. Oxford: Oxford University Press.\n\n\nWilliamson, Timothy. 1998. “Conditionalizing on Knowledge.” British Journal for the Philosophy of Science 49 (1): 89–121. https://doi.org/10.1093/bjps/49.1.89.\n\n\n———. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nThe reflections in the next few paragraphs are inspired by some comments by Stalnaker in his (2008), though I don’t want to suggest the theory I’ll discuss is actually Stalnaker’s.↩︎\nWhether Doxastic or Non-Doxastic IRI is true about justified belief ascriptions turns on some tricky questions about what to say when a subject’s credences are nearly, but not exactly appropriate given her evidence. Space considerations prevent a full discussion of those cases here. Whether I can hold onto the strict invariantism about claims about justified credences depends, I now think, on whether an interest-neutral account of evidence can be given. Discussions with Tom Donaldson and Jason Stanley have left me less convinced than I was in 2005 that this is possible, but this is far too big a question to resolve here.↩︎\nI mean here the case of Coraline, to be discussed in section 3 below. Several people have remarked in conversation that Coraline doesn’t look to them like a case of Ignorant High Stakes. This isn’t surprising; Coraline is better described as being mistaken than ignorant, and she’s mistaken about odds not stakes. If they’re right, that probably means my argument for Non-Doxastic IRI is less like Stanley’s, and hence more original, than I think it is. So I don’t feel like pressing the point! But I do want to note that I thought the Coraline example was a variation on a theme Stanley originated.↩︎\nIf we are convinced that the right decision is the one that maximises expected utility, there is a sense in which these questions collapse. For the expected utility theorist, we can solve Dom’s question by making sure the states are logically exhaustive, and making the ‘payouts’ in each state be expected payouts. But the theory that the correct decision is the one that maximises expected utility, while plausibly true, is controversial. It shouldn’t be assumed when we are investigating the semantics of decision tables.↩︎\nAssuming Alice’s utility curve for money curves downwards, she should be looking for a slightly higher chance of winning than \\(\\frac{1}{2}\\) to place the bet, but that level of detail isn’t relevant to the story we’re telling here.↩︎\nI’m here retracting some things I said a few years ago in a paper on philosophical methodology (Weatherson 2003). There I argued that identifying knowledge with justified true belief would give us a theory on which knowledge was more natural than a theory on which we didn’t identify knowledge with any other epistemic property. I now think that is wrong for a couple of reasons. First, although it’s true (as I say in the earlier paper) that knowledge can’t be primitive or perfectly natural, this doesn’t make it less natural than justification, which is also far from a fundamental feature of reality. Indeed, given how usual it is for languages to have a simple representation of knowledge, we have some evidence that it is very natural for a term from a special science. Second, I think in the earlier paper I didn’t fully appreciate the point (there attributed to Peter Klein) that the Gettier cases show that the property of being a justified true belief is not particularly natural. In general, when \\(F\\) and \\(G\\) are somewhat natural properties, then so is the property of being \\(F \\wedge G\\). But there are exceptions, especially in cases where these are properties that a whole can have in virtue of a part having the property. In those cases, a whole that has an \\(F\\) part and a \\(G\\) part will be \\(F \\wedge G\\), but this won’t reflect any distinctive property of the whole. And one of the things the Gettier cases show is that the properties of being justified and being true, as applied to belief, fit this pattern.\nNote that even if you think that philosophers are generally too quick to move from instinctive reactions to the Gettier case to abandoning the justified true belief theory of knowledge, this point holds up. What is important here is that on sufficient reflection, the Gettier cases show that some justified true beliefs are not knowledge, and that the cases in question also show that being a justified true belief is not a particularly natural or unified property. So the point I’ve been making in the last this footnote is independent of the point I wanted to stress in “What Good are Counterexamples?” namely, that philosophers in some areas (especially epistemology) are insufficiently reformist in their attitude towards our intuitive reactions to cases.↩︎\nThis issue is of course central to the plotline in Hawthorne (2004).↩︎\nAssume, perhaps implausibly, that the sudden appearance of the genie is evidentially irrelevant to the proposition that the musician is Beth. The reasons this may be implausible are related to the arguments in (Runyon 1992, 14–15). Thanks here to Jeremy Fantl.↩︎\nThe idea that interest-relativity is a way of fending off scepticism is a very prominent theme in Fantl and McGrath (2009).↩︎\nOn the version of IRI I’m defending, Barry is free to be interested in whatever he likes. If he started wondering about whether it would be rational to take such a bet, he loses the knowledge that Beth is the musician, even if there is no genie and the bet isn’t offered. The existence of the genie’s offer makes the bet a practical interest; merely wondering about the genie’s offer makes the bet a cognitive interest. But both kinds of interests are relevant to knowledge.↩︎\nAs they make clear in their (2008), Hawthorne and Stanley are interested in defending relatively strong premises linking knowledge and action independently of the argument for the interest-relativity of knowledge. What I’m doing here is showing how that conclusion does not rest on anything nearly as strong as the principles they believe, and so there is plenty of space to disagree with their general principles, but accept interest-relativity. The strategy here isn’t a million miles from the point noted in Fantl and McGrath (2009, 72n14) when they note that much weaker premises than the ones they endorse imply a failure of ‘purism.’↩︎\nI have more to say about those cases in section 2.2.↩︎\nAlso note that I’m not taking as a premise any claim about what Barry knows after the bet is offered. A lot of work on interest-relativity has used such premises, or premises about related intuitions. This seems like a misuse of the method of cases to me. That’s not because we should never use intuitions about cases, just that these cases are too hard to think that snap judgments about them are particularly reliable. In general, we can know a lot about cases by quickly reflecting on them. Similarly, we know a lot about which shelves are level and which are uneven by visual inspection, i.e., ‘eyeballing.’ But when different eyeballs disagree, it’s time to bring in other tools. That’s the approach of this paper. I don’t have a story about why the various eyeballs disagree about cases like Barry’s; that seems like a task best undertaken by a psychologist not a philosopher (Ichikawa 2009).↩︎\nThis is obviously not a full argument against contextualism; that would require a much longer paper than this.↩︎\nSee, for instance, Blome-Tillmann (2009), or Feltz and Zarpentine (2010).↩︎\nIn the last two lines, I use \\(U(\\phi)\\) to denote the expected utility of \\(\\phi\\), and \\(U(\\phi | p)\\) to denote the expected utility of \\(\\phi\\) conditional on \\(p\\). It’s often easier to write this as simply \\(U(\\phi \\wedge p)\\), since the utility of \\(\\phi\\) conditional on \\(p\\) just is the utility of doing \\(\\phi\\) in a world where \\(p\\) is true. That is, it is the utility of \\(\\phi \\wedge p\\) being realised. But we get a nicer symmetry between the probabilistic principles and the utility principles if we use the explictly conditional notation for each.↩︎\nThis is probably somewhat unrealistic. It’s hard to think about whether \\(\\Pr(p)\\) is closer to 0.7 or 0.8 without raising to salience questions about, for example, what the second decimal place in \\(\\Pr(p)\\) is. This is worth bearing in mind when coming up with intuitions about the cases in this paragraph.↩︎\nSee Sturgeon (2008) for discussion of a similar puzzle for anyone trying to tell a unified story of belief and credence.↩︎\nThere are exceptions, especially in cases where \\(p\\) concerns something significant to financial markets, and the agent trades financial products. If you work through the theory that I’m about to lay out, one consequence is that such agents should have very few unconditional beliefs about financially-sensitive information, just higher and lower credences. I think that’s actually quite a nice outcome, but I’m not going to rely on that in the argument for the view.↩︎\nThe presentation in this section, as in the earlier paper, assumes at least a weak form of consequentialism in the sense of Hammond (1988). This was arguably a weakness of the earlier paper. We’ll return to the issue of what happens in cases where the agent doesn’t, and perhaps shouldn’t, maximise expected utility, at the end of the section.↩︎\nI’m borrowing this example from Fred Dretske, who uses it to make some interesting points about dispositional belief.↩︎\nThe recipe here is similar to that given in Weatherson (2005), but the motivation is streamlined. Thanks to Jacob Ross for helpful suggestions here.↩︎\nSome consequentialists say that what the agent should do depends on whether \\(p\\) is true. If \\(p\\) is true, she should do \\(\\psi\\), and if \\(p\\) is false she should do \\(\\varphi\\). As we’ll see, I have reasons for thinking this is rather radically wrong.↩︎\nThe target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. Since my theories are close enough, at least in consequences, to Hawthorne and Stanley’s, it is important to note how my theory handles the case.↩︎\nI’m more interested in the abstract structure of the case than in whether any real-life situation is modelled by just this structure. But it might be worth noting the rough kind of situation where this kind of situation can arise. So let’s say Coraline has a particular bank account that is uninsured, but which currently paying 10% interest, and she is deciding whether to deposit another $1000 in it. Then \\(p\\) is the proposition that the bank will not collapse, and she’ll get her money back, and \\(q\\) is the proposition that the interest will stay at 10%. To make the model exact, we have to also assume that if the interest rate on her account doesn’t stay at 10%, it falls to 0.1%. And we have to assume that the interest rate and the bank’s collapse are probabilistically independent. Neither of these are at all realistic, but a realistic case would simply be more complicated, and the complications would obscure the philosophically interesting point.↩︎\nIf she did compute the expected utility, then one of the things that would be salient for her is the expected utility of the bet. And the expected utility of the bet is different to its expected utility given \\(p\\). So if that expected utility is salient, she doesn’t believe \\(p\\). And it’s going to be important to what follows that she does believe \\(p\\).↩︎\nThe following isn’t Fantl’s example, but I think it makes much the same point as the examples he suggested.↩︎\nThanks here to a long blog comments thread with Jeremy Fantl and Matthew McGrath for making me formulate these points much more carefully. The original thread is at http://tar.weatherson.org/2010/03/31/do-justified-beliefs-justify-action/.↩︎\nCompare the ‘subtraction argument’ on page 99 of Fantl and McGrath (2009).\n\n↩︎\n",
    "preview": "posts/2021-03-04-knowledge-bets-and-interests/sully.jpg",
    "last_modified": "2021-03-04T11:07:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-in-defense-of-a-kripkean-dogma/",
    "title": "In Defense of a Kripkean Dogma",
    "description": "A reply to some empirical arguments against Kripkean meta-semantics.",
    "author": [
      {
        "name": "Jonathan Jenkins Ichikawa",
        "url": "http://jichikawa.net"
      },
      {
        "name": "Ishani Maitra",
        "url": "https://lsa.umich.edu/philosophy/people/faculty/imaitra.html"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2012-07-01",
    "categories": [
      "methodology",
      "language",
      "notes"
    ],
    "contents": "\n\nContents\nExperiments and Reference\nGödel’s Role in Naming and Necessity\nReference in Philosophy\n\nIn “Against Arguments from Reference” (Mallon et al. 2009), Ron Mallon, Edouard Machery, Shaun Nichols, and Stephen Stich (hereafter, MMNS) argue that recent experiments concerning reference undermine various philosophical arguments that presuppose the correctness of the causal-historical theory of reference. We will argue three things in reply. First, the experiments in question—concerning Kripke’s Gödel/Schmidt example—don’t really speak to the dispute between descriptivism and the causal-historical theory; though the two theories are empirically testable, we need to look at quite different data than MMNS do to decide between them. Second, the Gödel/Schmidt example plays a different, and much smaller, role in Kripke’s argument for the causal-historical theory than MMNS assume. Finally, and relatedly, even if Kripke is wrong about the Gödel/Schmidt example—indeed, even if the causal-historical theory is not the correct theory of names for some human languages—that does not, contrary to MMNS’s claim, undermine uses of the causal-historical theory in philosophical research projects.\n\nPublished in Philosophy and Phenomenological Research 85: 56-68.\nExperiments and Reference\nMMNS start with some by now famous experiments concerning reference and mistaken identity. The one they focus on, and which we’ll focus on too, is a variant of Kripke’s Gödel/Schmidt example. Here is the question they gave to subjects.\n\nSuppose that John has learned in college that Gödel is the man who proved an important mathematical theorem, called the incompleteness of arithmetic. John is quite good at mathematics and he can give an accurate statement of the incompleteness theorem, which he attributes to Gödel as the discoverer. But this is the only thing that he has heard about Gödel. Now suppose that Gödel was not the author of this theorem. A man called “Schmidt” whose body was found in Vienna under mysterious circumstances many years ago, actually did the work in question. His friend Gödel somehow got hold of the manuscript and claimed credit for the work, which was thereafter attributed to Gödel. Thus he has been known as the man who proved the incompleteness of arithmetic. Most people who have heard the name ‘Gödel’ are like John; the claim that Gödel discovered the incompleteness theorem is the only thing they have ever heard about Gödel. When John uses the name ‘Gödel,’ is he talking about:\nthe person who really discovered the incompleteness of arithmetic? or\nthe person who got hold of the manuscript and claimed credit for the work? (MMNS 2009: 341)\n\nThe striking result is that while a majority of American subjects answer (B), consistently with Kripke’s causal-historical theory of names, the majority of Chinese subjects answer (A).1 To the extent that Kripke’s theory is motivated by the universality of intuitions in favour of his theory in cases like this one, Kripke’s theory is undermined.\nThere are now a number of challenges to this argument in the literature. Before developing our own challenge, we’ll briefly note five extant ones, which all strike us as at least approximately correct.\nKripke’s theory is a theory of semantic reference. When asked who John is talking about, it is natural that many subjects will take this to be a question about speaker reference. And nothing in Kripke’s theory denies that John might refer to the person who proved the incompleteness of arithmetic, even if his word refers to someone else. (Ludwig 2007; Deutsch 2009)\nKripke’s argument relies on the fact that ’\nGödel’ refers to Gödel, not to the universality or otherwise of intuitions about what it refers to. That some experimental subjects don’t appreciate this fact doesn’t make it any less of a fact. (Deutsch 2009)\nIf the subjects genuinely were descriptivists, it isn’t clear how they could make sense of the vignette, since the name ‘Gödel’ is frequently used in the vignette itself to refer to the causal origin of that name, not to the prover of the incompleteness or arithmetic.\n2\nOn a related point, Martí doesn’t mention this, but subjects who aren’t descriptivists should also object to the vignette, since in the story John doesn’t learn Gödel proved the incompleteness of arithmetic, at least not if ‘learn’ is factive. (Martı́ 2009)\nThe experiment asks subjects for their judgments about a metalinguistic, and hence somewhat theoretical, question about the mechanics of reference. It’s better practice to observe how people actually refer, rather than asking them what they think about reference. (Martı́ 2009; Devitt 2011)\nIntuitions about the Gödel/Schmidt case play at best a limited role in Kripke’s broader arguments, so experimental data undermining their regularity do not cast serious doubt on Kripke’s theory of reference. (Devitt 2011)\nWe think challenges (1)-(3) work. Something like (4) should work too, although it requires some qualification. Consider, for instance, what happens in syntax. It’s true, of course, that we don’t go around asking ordinary speakers whether they think Lectures on Government and Binding was an advance over Aspects. Or, if we did, we wouldn’t think it had much evidential value. But that’s not because ordinary speaker judgments are irrelevant to syntax. On the contrary, judgments about whether particular strings constitute well-formed sentences are an important part of our evidence.3 But they are not our only evidence, or even our primary evidence; we also use corpus data about which words and phrases are actually used, and many syntacticians take such usage evidence to trump evidence from metasemantic intuitions.4 Even when we do seek such intuitive answers, perhaps because there isn’t enough corpus data to settle the usage issue, the questions might be about cases that are quite different to the cases we primarily care about. So we might ask a lot about speakers’ judgments concerning questions even if we care primarily about the syntax of declarative sentences.\nIf what Kripke (1980) says in Naming and Necessity (hereafter, NN) is right, then we should expect something similar in the case of reference. Kripke anticipates that some people will disagree with him about some of the examples, and offers a few replies. (Our discussion here largely draws on footnote 36 of NN.) Part of his reply is a version of point 1 above; those disagreements may well be over speaker reference, not semantic reference. That reply is correct; it’s hard for us to hear a question about who someone is talking about as anything but a question about speaker reference. He goes on to note that his theory makes empirical predictions about how names are used.\n\nIf I mistake Jones for Smith, I may refer (in an appropriate sense) to Jones when I say that Smith is raking the leaves … Similarly, if I erroneously think that Aristotle wrote such-and-such passage, I may perhaps sometimes use ‘Aristotle’ to refer to the actual author of the passage … In both cases, I will withdraw my original statement, and my original use of the name, if apprised of the facts. (NN 86n)\n\nThis seems entirely right. There’s some sense in which John, in MMNS’s vignette, is referring to\nGödel and some sense in which he’s referring to Schmidt. Just thinking about the particular utterance he makes using ‘Gödel’ won’t help much in teasing apart speaker reference and semantic reference. What we should look to are patterns of—or if they’re not available, intuitions about—withdrawals of statements containing disputed names. To use the example Kripke gives here, consider a speaker who (a) associates with the name ‘Aristotle’ only the description ‘the author of The Republic,’ (b) truly believes that a particular passage in The Republic contains a quantifier scope fallacy, and (c) is a descriptivist. She might say “Aristotle commits a quantifier scope fallacy in this passage.” When she’s informed that the passage was written by Plato, she’ll no longer utter those very words, but she’ll still insist that the sentence she uttered was literally true. That’s because she’ll claim that in that sentence ‘Aristotle’ just referred to the author of the passage, and that person did commit a quantifier scope fallacy. A non-descriptivist will take back the claim expressed, though she might insist that what she intended to say was true.\nSo to show that subjects in different parts of the world really have descriptivist intuitions about the Gödel/Schmidt case, we might ask about whether they think John should withdraw, or clarify, his earlier statements if apprised of the facts. Or we might ask whether they would withdraw, or clarify, similar statements they had made if apprised of the facts. Or, even better, we might test whether in practice people in different parts of the world really do withdraw their prior claims at different rates when apprised of the facts about a Gödel/Schmidt case. Kripke is right that given descriptivism, a speaker shouldn’t feel obliged to withdraw the original statement when apprised of the facts, but given the causal-historical theory, they should. So there are experiments that we could run which would discriminate between descriptivist and causal-historical approaches, but we don’t think the actual experiment MMNS run does so.\nIn its broad terms, we agree with Devitt’s challenge (5), although we understand the role of the Gödel/Schmidt case rather differently than he does. We turn now to this question.\nGödel’s Role in Naming and Necessity\nIn the first section we argued that the experimental data MMNS offer do not show that the correct account of the Gödel/Schmidt example is different in different dialects. In this section we want to argue that there’s very little one could show about the Gödel/Schmidt example that would bear on the broader question of what the correct theory of reference is. To see this, let’s review where the Gödel/Schmidt example comes up in Naming and Necessity.\nIn the first lecture, Kripke argues, via the modal argument, that names can’t be synonymous with descriptions. The reason is that in modal contexts, substituting a name for an individuating description alters truth values. So a pure descriptivism that treats names and descriptions as synonymous is off the table. What’s left, thinks Kripke, is what Soames calls “weak descriptivism” (Soames 2003 Volume II, 356). This is the view that although names are not synonymous with descriptions, and do not abbreviate descriptions, they do have their reference fixed by descriptions.\nHere is the way Kripke introduces the picture that he is attacking.\n\nThe picture is this. I want to name an object. I think of some way of describing it uniquely and then I go through, so to speak, a sort of mental ceremony: By ‘Cicero’ I shall mean the man who denounced Cataline … [M]y intentions are given by first, giving some condition which uniquely determines an object, then using a certain word as a name for the object determined by this condition. (NN 79)\n\nThe Gödel/Schmidt example, or at least the version of it that MMNS discuss, comes up in Kripke’s attack on one of the consequences of this picture of naming. (A variant on the example, where no one proves the incompleteness of arithmetic, is used to attack another consequence of the theory.) So the role of the Gödel/Schmidt example is to undermine this picture of names and naming.\nBut note that it is far from the only attack on this picture. Indeed, it is not even the first attack. Kripke’s first argument is that for most names, most users of the name cannot give an individuating description of the bearer of the name. In fact, those users cannot even give a description of the bearer that is individuating by their own lights. The best they can do for ‘Cicero’ is ‘a Roman orator’ and the best they can do for ‘Feynman’ is ‘a famous physicist.’ (NN 81) But it isn’t that these users think that there was only one Roman orator, or that there is only one famous physicist. It’s just that they don’t know any more about the bearers of these names they possess. The important point here is that Kripke starts with some examples where the best description a speaker can associate with a name is a description that isn’t individuating even by the speakers’ own lights. And he thinks that descriptivists can’t explain how names work in these cases.\nNow perhaps we’ll get new experimental evidence that even in these cases, some experimental subjects have descriptivist intuitions. Some people might intuit that if a speaker does not know of any property that distinguishes Feynman from Gell-Mann, their name ‘Feynman’ is indeterminate in reference between Feynman from Gell-Mann. We’re not sure what such an experiment would tell us about the metaphysics of reference, but maybe someone could try undermining Kripke’s argument this way. But that’s not what MMNS found; their experiments don’t bear on what Kripke says about ‘Feynman,’ and hence don’t bear on his primary argument against weak descriptivism.\nSome philosophers will hold that although the picture Kripke describes here, i.e., weak descriptivism, can’t be right in general for Feynman/Gell-Mann reasons, it could be true in some special cases. We agree. So does Kripke. The very next sentence after the passage quoted above says, “Now there may be some cases in which we actually do this.” (NN 79) And he proceeds to describe three real life cases (concerning ‘Hesperus,’ ‘Jack the Ripper’ and ‘Neptune’) where the picture is plausibly correct. But he thinks these cases are rare. In particular, we shouldn’t think that the existence of an individuating description is sufficient reason to believe that we are in such a case. That, at last, is the point of the Gödel/Schmidt example. His conclusion from that example is that weak descriptivism isn’t correct even in those special cases of names where the speaker possesses a description that she takes to be individuating.5\nMichael Devitt (2011) also argues that MMNS exaggerate the importance of the Gödel/Schmidt case. He identifies a number of Kripke’s other arguments (including the Feynman one we mention) that he takes to be more central, and, like us, he argues that MMNS’s results do not cast doubt on these arguments. We agree, noting only two points of difference. First, as suggested above, although the Gödel/Schmidt case is not the only or the most central motivation for Kripke’s theory of reference, we do think that it plays a distinctive role, compared with that of, for instance, the Feynman case. It refutes even the weak version of weak descriptivism according to which, in the special case in which subjects do possess individuating descriptions, those descriptions determine reference. We think the Gödel/Schmidt case (together with the Peano/Dedekind case) form the basis of the only argument in Naming and Necessity against this weak weak descriptivism. (On a closely related point, we, unlike Devitt, take the Gödel/Schmidt case to be addressing a quantitative question about how common descriptive names are, not the qualitative question about whether the causal-historical theory is true at all; we’ll expand on this point below.) Second, Devitt expresses some scepticism about the Gödel/Schmidt judgment on the grounds that the relevant case is somewhat ‘fanciful’—actual cases, Devitt suggests, are better to be trusted. While there is surely some truth in the suggestion that intuitions about esoteric and complicated cases can be less trustworthy than those about everyday ones, we see little reason for concern in this instance; the Gödel case does not describe a scenario we should expect to find trouble thinking about.\nOur reconstruction of the structure of Kripke’s argument should make it clear how unimportant the Gödel/Schmidt example is to the broader theoretical questions. If Kripke were wrong about the Gödel/Schmidt case, that would at most show that there are a few more descriptive names than we thought there were. But since the existence of some descriptive names is consistent with the causal-historical theory of reference, the existence of a few more is too. All the Gödel/Schmidt example is used for in Naming and Necessity is to show that the number of descriptive names in English is not just small, it is very small. But the truth of the causal-historical theory of reference doesn’t turn on whether there are few descriptive names, or very few descriptive names.\nOnce we see that the Gödel/Schmidt example concerns a quantitative question (are descriptive names rare or very rare?) rather than a qualitative question (is the causal-historical theory correct?), we can see some limitations of the experiment MMNS rely on. The case that MMNS describes to their subjects has several distinctive features, and it isn’t clear that we’d be justified in drawing conclusions from it about cases that lack those features. Here is one such feature. The subject of the vignette (John) acquires the name ‘Gödel’ at the same time as he acquires an individuating description of Gödel. Suppose it turned out that, in some dialects at least, that would be sufficient for the name to be a descriptive name; i.e., for it to be a name whose reference is fixed by a description somehow attached to that name. If this conjecture is true, then descriptive names are a little more common than Kripke thinks they are, but not a lot more common. Now we don’t actually think this conjecture is true. And for the reasons given in section 1 we don’t think this experiment is evidence for it. What we do think is that (a) it’s hard to see how studying reactions to cases like the Gödel/Schmidt example could show more than that some such claim about the prevalence of descriptive names is true, and (b) such claims are not inconsistent with the causal-historical theory.\nWe’ve argued that even if Kripke is wrong about the Gödel/Schmidt example, that doesn’t undermine the arguments for the main conclusions of Naming and Necessity. A natural inference from this is that experiments about the Gödel/Schmidt example can’t undermine those conclusions. We think the natural inference is correct. A referee has suggested that this is too quick. After all, if we have experimental evidence that Kripke is wrong about the Gödel/Schmidt case, we might have some grounds for suspicion about the other cases that Kripke uses in the arguments for more central conclusions. That is, if MMNS are right about the Gödel/Schmidt case, that doesn’t give us a deductive argument against the other anti-descriptivist moves, but it might give us an inductive argument against them. This is an important worry, but we think it can be adequately responded to.\nThe first thing to note is that it would be foolish to fall back to a general scepticism about human judgment just because people disagree in their intuitive reactions to some tricky cases. This point is well argued by Timothy Williamson in his (2007 Ch. 6). If there’s a worry here, it must be because the evidence about the Gödel/Schmidt example supports a more modest generalisation about judgments about cases, but that generalisation is nevertheless strong enough to undermine Kripke’s other arguments. We doubt such a generalisation exists.\nIt can’t be that the experiments about the Gödel/Schmidt example show that intuitive judgments about reference are systematically mistaken. Most of our intuitions in this field are surely correct. For instance, our intuitions that ‘Kripke’ refers to Kripke and not Obama, and that ‘Obama’ refers to Obama and not Kripke, are correct. (And experiments like the ones MMNS ran don’t give us any reason at all to doubt that.) And we could produce many more examples like that. At most, the experiments can show us that there are spots of inaccuracy in a larger pool of correct judgments.\nIt might be argued that we should be sceptical of intuitions about reference in counterfactual cases. The correct judgments cited in the previous paragraph are all about real cases, but the Gödel/Schmidt example is not a real case. Now we don’t think that the experiments do undermine all intuitions about reference in counterfactual cases, but even if they did, that wouldn’t affect the Kripkean argument. That’s because the central argument against descriptivism at the start of Lecture II involves real cases. The heavy lifting is done by cases where speakers don’t think they have an individuating description to go along with names they use (e.g., ‘Feynman’ and ‘Gell-Mann’), or they believe they have an individuating description, but that description involves some kind of circularity (e.g., ‘Einstein,’ ‘Cicero’). It seems to us that these cases are much more like the cases where we know people have accurate intuitions about reference (e.g., ‘Obama’ refers to Obama), than they are like cases where there is some dispute about their accuracy (e.g., ‘Gödel’ would refer to Gödel even if Schmidt had proved the incompleteness of arithmetic). So there’s no reason to doubt the intuitions that underlie these central Kripkean arguments. And so there’s no reason from these experiments to doubt the anti-descriptivist conclusions Kripke draws from them.\nReference in Philosophy\nIf the data about the Gödel/Schmidt example don’t undermine the causal-historical theory of reference, then presumably they don’t undermine philosophical uses of that theory. But we think MMNS overstate the role that theories of reference play in philosophical theorising, and we’ll end by saying something about this.\nOne simple reaction to MMNS’s argument is to say that at most they show that the causal-historical theory of reference is not true of some dialects. But, a philosopher might say, they are not writing in such a dialect, and the causal-historical theory is true of their dialect. And that’s all they needed for their argument. MMNS anticipate this objection, and reply to it in section 3.3 of their paper. The reply is, in essence, that such a picture would make a mess of communication. If we posit dialectical variation to explain different reactions to the Gödel/Schmidt example, and to other examples, then we cannot know what dialect someone is speaking without knowing how they respond to these examples. And plainly we don’t need to quiz people in detail about philosophical examples in order to communicate with them.\nWe offer three replies.\nFirst, at least one of us is on record raising in principle suspicions about this kind of argument Maitra (2007). The take-home message from that paper is that communication is a lot easier than many theorists have supposed, and requires much less pre-communicative agreement. It seems to us that the reply MMNS offer here is susceptible to the arguments in that paper, but for reasons of space we won’t rehearse those arguments in detail.\nSecond, it’s one thing to think that variation in reference between dialects leads to communication breakdown, it’s another thing altogether to think that variation in meta-semantics leads to such breakdown. A little fable helps make this clear. In some parts of Melbourne, ‘Gödel’ refers to Gödel because of the causal chains between the users of the name and the great mathematician. In other parts, ‘Gödel’ refers to Gödel because the speakers use it as a descriptive name, associated with the description ‘the man who proved the incompleteness of arithmetic.’ Kevin doesn’t know which area he is in when he sees a plaque over a door saying “Gödel lived here.” It seems to us that Kevin can understand the sign completely without knowing how ‘Gödel’ got its reference. Indeed, he even knows what proposition the sign expresses. So meta-semantic variation between dialects need not lead to communicative failure, even when hearers don’t know which dialect is being used.\nThird, if MMNS’s argument succeeds, it seems to us that it shows descriptivist theories, including the weak weak descriptivism that Kripke is arguing against with the Gödel/Schmidt example, are doomed. (The arguments in this paragraph are not original. Similar arguments are used frequently in, e.g., Fodor and Lepore (1992).) It’s a platitude that different people know different things. Barring a miracle, that means different people will associate different descriptions with different names. If there is widespread use of descriptive names, that means there will be widespread differences in which descriptions are associated with which names. And that will produce at least as much communicative difficulty as having some people be causal-historical theorists and some people be descriptivists. In short, if MMNS’s argument against ‘referential pluralism’ is sound, there is an equally sound argument against descriptivism. And note that this argument doesn’t rely on any thought experiments about particular cases. It doesn’t even rely on thought experiments about names like ‘Einstein,’ where there isn’t any evidence that Kripke is wrong about how those names work.\nDialectically, the situation is this. MMNS have offered an argument from the possibility of communicating under conditions of ignorance about one’s interlocutor’s knowledge. Similar arguments have been offered against descriptivism. If such arguments are successful, then descriptivism is false, and there’s no problem with philosophers making arguments from the falsity of descriptivism. If such arguments are unsuccessful, then MMNS haven’t shown that it is wrong for philosophers to assume that the causal-historical theory is the right theory for their dialect, even if some other people are descriptivists. And, as MMNS concede, as long as the philosophers themselves speak a causal-historical theory dialect, the uses of the causal-historical theory in philosophy seem appropriate. The only way this argument could fail is if MMNS’s argument from the possibility of communicating under conditions of ignorance about one’s interlocutor’s knowledge is stronger than the analogous arguments against descriptivism. But we see no reason to believe that is so. If anything, it seems like a weaker argument, because of the considerations arising from our fable about Kevin and the ‘Gödel lived here’ sign.\nSo we don’t think MMNS have a good reply to the philosopher who insists that they only need the causal-historical theory to be true of their dialect. But in fact we think that philosophers rarely even assume that much.\nLet’s consider one of the examples that they cite: Richard Boyd’s use of the causal-historical theory of reference in developing and defending his version of “Cornell Realism” in his (1988). Here’s one way one could try and argue for moral realism from the causal-historical theory.\nThe causal-historical theory of reference is the correct theory of reference for all words in all dialects (or at least our dialect).\nSo, it is the correct theory for ‘good.’\nBut that’s not Boyd’s actual argument. And that’s a good thing, because the first premise is implausible. Someone defending it has to explain descriptive names like ‘Neptune,’ logical terms like ‘and,’ empty predicates like ‘witch,’ and so on. And Boyd’s not in that business. His argument is subtler. Boyd uses the causal-historical theory for two purposes. First, he uses the development of a naturalistically acceptable theory of reference as part of a long list of developments in post-positivist philosophy that collectively constitute a “distinctively realist conception of the central issues in the philosophy of science” (Boyd 1988, 188). Second, he uses the causal-historical theory of reference, as it applies to natural kind terms, as part of a story about how we can know a lot about kinds that are not always easily observable (Boyd 1988, 195–96). By analogy, he suggests that we should be optimistic that a naturalistically acceptable moral theory exists, and that it is consistent with us having a lot of moral knowledge.\nOnce we look at the details of Boyd’s argument, we see that it is an argument that duelling intuitions about the Gödel/Schmidt example simply can’t touch. In part that’s because Boyd cares primarily about natural kind terms, not names. But more importantly it is because, as we noted in section 2, the only point that’s at issue by the time Kripke raises the Gödel/Schmidt example is the number of descriptive names. Just looking at the arguments Kripke raises before that example gives us more than enough evidence to use in the kind of argument Boyd is making.\nIt would take us far beyond the length of a short reply to go through every philosophical use of the causal-historical theory that MMNS purport to refute in this much detail. But we think that the kind of response we’ve used here will frequently work. That is, we think few, if any, of the arguments they attack use the parts of the causal-historical theory that Kripke is defending with the Gödel/Schmidt example, and so even if that example fails, it wouldn’t undermine those theories.\n\n\n\nBoyd, Richard. 1988. “How to Be a Moral Realist.” In Essays in Moral Realism, edited by Geoffrey Sayre-McCord, 181–228. Ithaca: Cornell University Press.\n\n\nDeutsch, Max. 2009. “Experimental Philosophy and the Theory of Reference.” Mind and Language 24 (4): 445–66. https://doi.org/10.1111/j.1468-0017.2009.01370.x.\n\n\nDevitt, Michael. 2011. “Experimental Semantics.” Philosophy and Phenomenological Research 82 (2): 418–35. https://doi.org/ppr201182222.\n\n\nFodor, Jerry A., and Ernest Lepore. 1992. Holism: A Shopper’s Guide. Cambridge: Blackwell.\n\n\nKripke, Saul. 1980. Naming and Necessity. Cambridge: Harvard University Press.\n\n\nLudwig, Kirk. 2007. “The Epistemology of Thought Experiments: First Person Versus Third Person Approaches.” Midwest Studies in Philosophy 31 (1): 128–59. https://doi.org/10.1111/j.1475-4975.2007.00160.x.\n\n\nMaitra, Ishani. 2007. “How and Why to Be a Moderate Contextualist.” In Context Sensitivity and Semantic Minimalism: New Essays on Semantics and Pragmatics, edited by Gerhard Preyer and Georg Peter, 111–32. Oxford: Oxford University Press.\n\n\nMallon, Ron, Eduoard Machery, Shaun Nichols, and Stephen Stich. 2009. “Against Arguments from Reference.” Philosophy and Phenomenological Research 79 (2): 332–56. https://doi.org/10.1111/j.1933-1592.2009.00281.x.\n\n\nMartı́, Genoveva. 2009. “Against Semantic Multi-Culturalism.” Analysis 69 (1): 42–48. https://doi.org/10.1093/analys/ann007.\n\n\nSoames, Scott. 2003. Philosophical Analysis in the Twentieth Century. Princeton: Princeton University Press.\n\n\nWilliamson, Timothy. 2007. The Philosophy of Philosophy. Blackwell.\n\n\nNote that a causal descriptivist about names will also say that the correct answer to this question is (B). So the experiment isn’t really testing descriptivism as such versus Kripke’s causal-historical theory, but some particular versions of descriptivism against Kripke’s theory. These versions of descriptivism say that names refer to the satisfiers of (generally non-linguistic) descriptions that the name’s user associates with the name. One such version is ‘famous deeds’ descriptivism, and the descriptions MMNS use are typically famous deeds; nevertheless, that seems inessential to their experiments. When we use ‘descriptivism’ in this paper, we’ll mean any such version of descriptivism. Thanks here to an anonymous referee.↩︎\nThis objection relies on an empirical assumption that may be questionable. It assumes that the subject of the experiment associates the same description with ‘Gödel’ as John does. A subject who (a) is a descriptivist and (b) associates with the name ‘Gödel’ the description ‘the man who proved the compatibility of time travel and general relativity,’ can also make sense of the vignette, contra Martí. So perhaps the objection could be resisted. But we think this empirical assumption is actually fairly plausible. Unless the experimental subjects were being picked from a very biased sample, the number of subjects who are familiar with Gödel’s work on closed time-like curves is presumably vanishingly small! We’re grateful here to an anonymous referee.↩︎\nThis point suggests Martí’s criticism of MMNS as stated overshoots. She wants to dismiss arguments from metalinguistic intuitions altogether. But intuitions about well-formedness are metalinguistic intuitions, and they are a key part of the syntactician’s toolkit. Martí concedes something like this point, but claims that the cases are not on a par, because syntax concerns a normative issue and reference does not. We’re quite suspicious that there’s such a striking distinction between the kind of subject-matter studied by syntacticians and semanticists. Devitt’s version of this point is more modest and does not obviously commit to this exaggeration.↩︎\nHere’s one example where testing intuitions and examining the corpus may lead to different answers. Many people think, perhaps because they’ve picked up something from a bad style guide, that the sentence ‘Whenever someone came into Bill’s shop, he greeted them with a smile,’ contains one or two syntactic errors. (It uses a possessive as the antecedent of a pronoun, and it uses ‘them’ as a bound singular variable.) Even if most subjects in a survey said such a sentence was not a well-formed sentence of English, corpus data could be used to show that it is. Certainly the existence of a survey showing that users in, say, Scotland and New Jersey give different answers when asked about whether the sentence is grammatical would not show that there’s a syntactic difference between the dialects spoken in Scotland and New Jersey. You’d also want to see how the sentences are used.↩︎\nThe Gödel/Schmidt example is also distinctive in another way, in that the description in question actually applies to the referent of the name, and indeed speakers actually know this. But the flow of the text around the example (especially on page 84) suggests Kripke intends the example to make the same point as is made by other examples, such as the Peano/Dedekind case (in which the possessed description doesn’t actually apply to the referent of the name). So this is probably not crucial to the point the example makes. We’ll return below to the issue of just what this example shows. The key point is that the more distinctive the example is, the less that would follow if Kripke were wrong about the example; he might only be wrong about examples with just those distinctive features.↩︎\n",
    "preview": "posts/2021-01-05-in-defense-of-a-kripkean-dogma/godel.jpg",
    "last_modified": "2021-02-05T15:24:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-induction-and-supposition/",
    "title": "Induction and Supposition",
    "description": "An argument that we should not treat rules of inductive inference in ordinary life as being anything like the inference rules in natural deduction systems.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2012-05-01",
    "categories": [
      "epistemology",
      "induction",
      "scepticism"
    ],
    "contents": "\nHere’s a fairly quick argument that there is contingent a priori knowledge. Assume there are some ampliative inference rules. Since the alternative appears to be inductive scepticism, this seems like a safe enough assumption. Such a rule will, since it is ampliative, licence some particular inference From \\(A\\) infer \\(B\\) where \\(A\\) does not entail \\(B\\). That’s just what it is for the rule to be ampliative. Now run that rule inside suppositional reasoning. In particular, first assume \\(A\\), then via this rule infer \\(B\\). Now do a step of \\(\\rightarrow\\)-introduction, inferring \\(A \\rightarrow B\\) and discharging the assumption \\(A\\). Since \\(A\\) does not entail \\(B\\), this will be contingent, and since it rests on a sound inference with no (undischarged) assumptions, it is a priori knowledge.\n\nPublished in The Reasoner 6: 78-80.\nThis argument is hardly new; John Hawthorne (2002) suggested a similar argument ten years ago. But it is a quick argument for a striking conclusion, and deserves close scrutiny. I’m going to argue that it fails because it falsely assumes that we can treat rules of ampliative inference like rules in a natural deduction system, and hence as rules that we can apply inside the scope of a supposition. That assumption has recently been defended by Stewart Cohen (2010) and Sinan Dogramaci (2010), but I’m going to argue, using a construction similar to one found in Dogramaci, that it leads to absurdity given other plausible premises.\nHere’s the main argument. If any ampliative inference is justified, I think the following rule, called ‘IR,’ is justified, since this is a very weak form of an inductive inference.\nIR\nFrom There are infinitely many Fs, and at most one is not G and x is F infer x is G unless there is some \\(H\\) such that it is provable from the undischarged assumptions that x is F and H and There are finitely many things that are both F and H, and one of them is not G.\n\nNote that the rule doesn’t say that merely one \\(F \\wedge \\neg G\\) has been observed; it requires that just one such thing exists. So this seems like a very plausible inference; it really is just making an inference within a known distribution, not outside it. And it is explicitly qualified to deal with defeaters. And yet even this rule, when applied inside the scope of suppositions, can lead to absurdity.\nIn the following proof, we’ll let \\(N\\) be the predicate ‘is a natural number,’ and \\(P\\) be the predicate ‘is the predecessor of,’ and I’ll appeal to the fact that there are infinitely many natural numbers, and each number has at most one predecessor. I’ll use a version of the proof system in E. J. Lemmon’s Beginning Logic, but it should be easy to transform the proof into any other proof system.\n\\[\\begin{aligned}\n1 && (1) && &Na && \\text{assumption} \\\\\n2 && (2) && &Nb && \\text{assumption} \\\\\n1, 2 && (3) && &\\neg Pab && \\text{(1), (2), IR} \\\\\n1 && (4)  && &Nb \\rightarrow \\neg Pab && \\text{(2), (3), CP} \\\\\n1 && (5)  && &\\forall y (Ny \\rightarrow \\neg Pay) && \\text{(4), UI} \\\\\n && (6)  && &Na \\rightarrow \\forall y (Ny \\rightarrow \\neg Pay) && \\text{(1), (5), CP} \\\\\n && (7) && &\\forall x (Nx \\rightarrow \\forall y (Ny \\rightarrow \\neg Pxy)) && \\text{(6), UI} \\\\\n && (8) && &N2 \\rightarrow \\forall y (Ny \\rightarrow \\neg P2y) && \\text{(7), UE}\\end{aligned}\\] So we get the absurd result that if 2 is a number (which it is!), then it is the predecessor of no number. But that’s absurd, since obviously 3 is a number and 2 is the predecessor of it. Note that at step 3 we use rule IR with \\(F\\) being the predicate is a natural number, \\(G\\) being the predicate does not have a as a predecessor, and \\(b\\) being \\(x\\).\nWhat could have gone wrong? I think the problem is using IR in the context of a suppositional proof, as we’ve done here. But let’s check if there is another guilty suspect.\nIf the problem is Conditional Proof (CP in Lemmon’s system), then that’s about as bad for the proof in the first paragraph that there are contingent a priori truths as if the problem is IR. Since we’re interested in whether that proof works, we won’t investigate this option further. In any case, if \\(\\rightarrow\\) is material implication, that rule seems unobjectionable. A referee suggested that if we’ve used an ampliative rule earlier, then \\(\\rightarrow\\) should be weaker than material implication, and under that interpretation (5) through (8) may be plausible. I think that claim is basically right, but note that if we do this the argument for contingent a priori knowledge with which I started will fail, since the contingency of \\(A \\supset B\\) will not imply the contingency of \\(A \\rightarrow B\\) if \\(\\rightarrow\\) is weaker than \\(\\supset\\).\nIt is hard to imagine that Universal Elimination (UE) is the problem. In any case, line (7) is obviously bad anyway, so something must have gone wrong in the proof before that.\nPerhaps the problem is with Universal Introduction (UI); this is what Dogramaci suggests. One objection he offers is that although we can prove every instance of the universal quantifier, inferring the universal version creates an undue aggregation of risks. Even if line (4) is very probable, and it would still be probable if \\(a\\) were replaced with \\(c\\), \\(d\\) or any other name, it doesn’t follow that the universal at line (5) is very probable. But I think this is to confuse defeasible reasoning with probabilistic reasoning. The only way to implement this restriction on making inferences that aggregate risk would be to prevent us making any inference where the conclusion was less probable than the premises. That will rule out uses of \\(\\forall\\)-introduction as at (5). But it will also rule \\(\\wedge\\)-introduction, and indeed any other inference with more than one input step. To impose such a restriction would be to cripple natural deduction.\nAnother objection he offers (UI) is simply that it is the least plausible, or least intuitive, of the rules used here. But in fact (UI) is extremely intuitive. If we can prove every instance of a schema, we should be able to prove its universal closure. On the other hand, allowing ampliative rules to be used inside the scope of a supposition allows a quick proof of contingent a priori knowledge, as shown in the first paragraph. Now maybe there is such knowledge, but its existence is hardly intuitive.\nSo I conclude the weakest link in the argument is step (3). Although IR is a good rule, it can’t be used inside the scope of a supposition. And since IR is about as weak an inductive rule as we can imagine, I conclude that ampliative inference rules can’t in general be used inside the scope of suppositions.\nThe general lesson here is that, as was made clear many years ago by Gilbert Harman (1986) is that there is a difference between rules of inference and rules of implication. The quick proof that there’s contingent a priori knowledge uses a rule of inference as if it is a rule of implication. Not respecting this distinction between inference and implication leads to disaster, as we’ve shown here, and should be shunned.\n\n\n\nCohen, Stewart. 2010. “Bootstrapping, Defeasible Reasoning and a Priori Justification.” Philosophical Perspectives 24 (1): 141–59. https://doi.org/10.1111/j.1520-8583.2010.00188.x.\n\n\nDogramaci, Sinan. 2010. “Knowledge of Validity.” Noûs 44 (3): 403–32. https://doi.org/0.1111/j.1468-0068.2010.00746.x.\n\n\nHarman, Gilbert. 1986. Change in View. Cambridge, MA: Bradford.\n\n\nHawthorne, John. 2002. “Deeply Contingent a Priori Knowledge.” Philosophy and Phenomenological Research 65 (2): 247–69. https://doi.org/10.1111/j.1933-1592.2002.tb00201.x.\n\n\n\n\n",
    "preview": "posts/2021-03-04-induction-and-supposition/kent.jpg",
    "last_modified": "2021-03-04T10:55:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-explanation-idealisation-and-the-goldilocks-problem/",
    "title": "Explanation, Idealisation and the Goldilocks Problem",
    "description": "A contribution to a symposium on Michael Strevens's book Depth.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2012-03-21",
    "categories": [
      "explanation",
      "philosophy of economics",
      "book symposium",
      "on books"
    ],
    "contents": "\n\nContents\nThe Goldilocks Problem\nIdealisations in Explanation\nThe Kairetic Theory of Explanation\nEquilibrium Explanations in Economics\nPossible Responses\n\nMichael Strevens’s book Depth is a great achievement.1 To say anything interesting, useful and true about explanation requires taking on fundamental issues in the metaphysics and epistemology of science. So this book not only tells us a lot about scientific explanation, it has a lot to say about causation, lawhood, probability and the relation between the physical and the special sciences. It should be read by anyone interested in any of those questions, which includes presumably the vast majority of readers of this journal.\nPublished in Philosophy and Phenomenological Research, 84(2): 461-473.\nImage from Creative Commons.\nOne of its many virtues is that it lets us see more clearly what questions about explanation, causation, lawhood and so on need answering, and frames those questions in perspicuous ways. I’m going to focus on one of these questions, what I’ll call the Goldilocks problem. As it turns out, I’m not going to agree with all the details of Strevens’s answer to this problem, though I suspect that something like his answer is right. At least, I hope something like his answer is right; if it isn’t, I’m not sure where else we can look.\nThe Goldilocks Problem\nSam has engaged in some unhealthy activity, and is now profusely vomiting in the bathroom. Here are three things that are true of the buildup to this unfortunate turn of events.\nSam either ate a carton of raw eggs, or drank a bottle of vodka.\nSam ate a carton of raw eggs.\nSam ate a carton of raw eggs that were bought at midday.\nAll three of these claims are interesting things to know about the buildup to the vomiting. But intuitively, or at least according to my intuitions, (2) is the best explanation of the lot. That’s because intuitively, (1) is too weak, and (3) is too strong, while (2) is just right.\nLet’s assume for now these intuitions are correct. We then have the puzzle of explaining why explanations of moderate strength, like (2), are strictly better than either weaker explanations, like (1), or stronger explanations, like (3). Put another way, we have to explain what makes (2) ‘just right.’ Call this the Goldilocks problem.2\nIf the Goldilocks problem was merely a matter of first-pass intuitions, then perhaps the right way to solve it would be to explain why we have quirky intuitons about explanations. But I think we can see that it turns on deeper features than that.\nOn the one hand, we want explanations, particularly of single events, to locate those events in the causal structure of the world. That’s why we’re pushed towards saying that (3) is the bext explanation of Sam’s current activity. Indeed, in his defence of a causal theory of explanation, David Lewis (1986) says that (3) is really the best explanation, though we might prefer to use, or to offer, (2) for pragmatic reasons.\nOn the other hand, we want explanations that unify disparate phenomena. If we see that an event is just one instance of the right kind of pattern, it feels more explicable. That pushes us towards explanations that encompass more and more actual and possible outcomes. This pushes us away from (3) as an explanation, and towards (2), but also away from (2) and towards (1). After all, if we accepted (1) as the best explanation for what’s going on, we would have an explanation that encompasses even more events.3\nWe can also get pushed towards (1) as being the ideal explanation by considering ways in which (2) is a better explanation than (3). There is a sense in which some of the information in (3) is redundant. No matter when Sam bought the eggs, the vomiting would have resulted given that they were eaten. Here is one principle we might draw from that. If \\(E^\\prime\\) is logically weaker than \\(E\\), and the outcome \\(O\\) would have happened even if \\(E^\\prime\\) had happened but \\(E\\) had not, then \\(E^\\prime\\) is a better explanation than \\(E\\). This will get the right result that (2) is a better explanation than (3). But it will get the wrong result that (1) is a better explanation than (2).\nTo some extent, the observations of the last three paragraphs point to a solution to the Goldilocks problem. There are virtues that (2) has over (3), in not being too specific, and over (1), in being specific enough for the task at hand. But as a moment’s reflection will show, attempting to turn these ideas into a theory is not exactly trivial. It’s much too easy to come up with principles that end up implying that (2) has all the vices of (1) and (3), and is really worse than each, rather than better. (The attempt to use counterfactuals to give a sufficient condition for superiority of explanation in the last paragraph is illustrative of how we might end up theorising this way.) Having a theory of explanation that avoids these traps is both desirable, and difficult.\nIdealisations in Explanation\nMuch more familiar than the Goldilocks problem is the problem of accounting for the role of idealisations in explanation. Explanations seem, after all, factive. The sentence p because q just entails both \\(p\\) and \\(q\\). And yet explanations involving idealisations seem to be false. Here’s an illustrative example.\nOn a busy suburban corner, there are four gas stations.4 Although the price for which they offer gas fluctuates a lot from day to day, the four usually have the same price, even to the nearest tenth of a cent. Why might that be? One might suspect collusion, but we’ll stipulate that this is a real free market, and the stations are actually competing, not colluding. Another might be that the stations are using ‘cost-plus’ pricing. But in fact, given the many and varied ways in which the stations (or their corporate parents) have used derivatives to hedge their costs, the four actually face very different input costs. And in any case, a ‘cost-plus’ theory can’t explain the fluctuation of prices.\nThe real explanation is relatively simple. If any station charges a higher price than its rivals, then no one will come to that station. And that’s something the station desparately wants to avoid. So no station charges a higher price than the others. And that means they all charge the same price.\nNow why, might we ask, is it that if any station charges a higher price than its rivals, then no one will come to that station? There’s a simple explanation here too. First, customers know the prices at each of the four stations, or at least if they don’t the cost of getting those prices is zero. Second, the customers are each utility-maximisers who prefer having more money to less. And third, the goods that the stations are offering are perfect substitutes. Those three premises entail that a station with a higher price than the others will have zero customers.\nBut just wait! Precisely none of those three premises are perfectly true. There is some cost in figuring out the prices at each. If there weren’t, we couldn’t explain why stations put up such big signs advertising their prices. The point of those signs is to reduce the cost of acquiring price information. And, as philosophers of economics never tire of pointing out, customers aren’t perfect utility maximisers. And, finally, the goods aren’t perfect substitues. The stations might have different queue lengths, or reputations for quality, or associations with firms that pollute the Gulf of Mexico, and so on.5\nStrevens has a nice story to tell here about what we should say about the explanations like the one I just offered. When the explainer says that, for instance, the cost of acquiring price information is zero, we should interpret them charitably, and loosely. We should apply the same principles as we apply when interpreting someone’s claim that Brazil is triangular. The truth-conditional content of the claim is not that the cost of acquiring price information is precisely zero. Rather, it is that the cost is in a not-too-large range that includes zero. How large is ‘not-too-large?’ That depends on what the person is trying to explain? If they are trying to explain the size of gas station signage, it will be a small range; if they are trying to explain the dynamics of gas station pricing, it will be somewhat larger.\nStrevens’s theory here is hermeneutic, not revolutionary. He doesn’t say that we should replace the explanations that economists give, which are full of freely available information, perfectly substitutable goods, utility maximising agents and so on, with explanations that involve low cost information, highly substitutable goods, and agents who usually choose high utility outcomes. Rather, he is saying that the explanations those economists give already involve low cost (but not necessarily free) information, highly (but not necessarily perfectly) substitutable goods, and so on. This seems entirely right to me. Well known results showing the limitations of human rationality simply don’t undermine the stories like the one I told explaining the correlation between prices at nearby gas stations, even though a cursory glance at those explanations might appear to involve appeal to perfectly rational buyers.\nNow what happens when we interpret an explanation as saying not that some value is zero, but that it is near zero? Well, we get an instance of the Goldilocks problem back. We could imagine an explanation of the gas station prices that includes the exact value of the cost of acquiring information about each station’s price. That explanation would be more precise than the explanation that merely says the cost of acquiring price information is low. But despite that increase in precision, it would be a worse explanation, and it would be worse for just the same reason that (3) is a worse explanation than (2). (Of course, we haven’t yet said just what that reason is!)\nSo puzzles about idealisations in explanation reduce, given Strevens’s nice hermeneutic suggestion, to the Goldilocks problem. That raises the interest in solving the Goldilocks problem, so let’s turn to Strevens’s own solution to it.\nThe Kairetic Theory of Explanation\nI’m going to have to simplify a lot in sketching Strevens’s theory of explanation, but I hope the following offers a not-too-inaccurate picture. For Strevens, explanations of individual events are causal models. (Explanations of regularities are basically explanations of the events that make up the regularity.) A causal model is a valid argument, whose premises are all true, and whose conclusion is the event to be explained, such that the conclusion can be derived from the premises using (more-or-less) nothing but modus ponens, with every such step, from \\(C\\) and \\(C \\rightarrow E\\) to \\(E\\), being such that in reality \\(C\\) caused \\(E\\). When an argument has this property, Strevens says that the premises causally entail the conclusion. In practice, these models typically have three (kinds of) premises: a specification of initial conditions, a law (or set of laws) linking those conditions to the eventual result, and a ‘no defeaters’ condition, since the laws in question will usually not guarantee any outcome.6\nThere will usually be many such explanations. For instance, we could start with either (1), (2) or (3), add an appropriate law and a no defeaters condition, and causally derive that Sam is nauseous. Strevens then puts two extra conditions on causal models, one of which provides a ranking of explanations, the other of which is a necessary condition for an explanation being satisfactory.\nThe ranking condition is that the weaker the set of initial conditions are, the better the explanation is. If we weaken the initial conditions, but can still causally derive the explanandum, then the stronger set of initial conditions contained redundant information and better explanations excise redundant information. The condition that some information is necessary for the causal entailment to go through is what Strevens calls ‘the kairetic condition’ on explanatory relevance, and that in turn is why the theory is called a kairetic theory of explanation.\nOnce we loosen the specification of the initial conditions, a range of different possible causal pathways are compatible with the argument being a causal entailment. The necessary condition Strevens adds is that these possible pathways must be coherent. And he defines cohesion as “dynamic continguity” (105). That is, if we situate all the possible causal chains in a possible space, an argument satisfies the cohesion condition if the set of chains consistent with the argument’s premises causally entailing the conclusion form a contiguous set.\nNote that contiguity is not that closely related to a similarity condition. The set of all possible causal pathways is perfectly contiguous, although its members are severely dissimilar. On the other hand, some small sets of causal pathways are not contiguous. So consider (4) and (5) below. Arguably the set of worlds in which (4) is true is not contiguous – there is a disconnect between the worlds where Suzy throws and the worlds where Billy throws – while the set of worlds in which (5) is true is contiguous.\nEither Billy or Suzy threw a brick at the window at exactly \\(2\\pi\\) mph.\nSuzy threw a brick at the window at between 5 and 30 mph.\nAlthough the worlds where Suzy throws hard are very dissimilar from the worlds where Suzy throws softly, there is a chain of worlds connecting the two. And each member of the chain is very similar to the next member. That suffices for contiguity.\nIt’s important to what follows that Strevens takes contiguity here to be physical contiguity. That is, two worlds (or causal pathways) are contiguous iff they are contiguous from the perspective of fundamental physics. Contiguity is not meant to be something defined in terms of explanations, and nor is it meant to be contiguity in terms of properties of interest to the special sciences. This will be important for what follows.\nWe’re now in a position to see Strevens’s solution to the Goldilocks problem. The detail about when Sam bought the eggs is irrelevant to the conclusion that Sam is nauseous. As long as the eggs were bought, and eaten, Sam’s nausea will exist. Indeed, its existence will be guaranteed by a causal law, given the appropriate ‘no defeaters’ condition. So the kairetic condition says we improve the explanation of Sam’s nausea by dropping the time at which the eggs were bought.7 Now if we start with (1), there will still be a causal law that lets us derive Sam’s nausea. But the space of causal pathways consistent with the argument we generate will not be contiguous. It will contain the worlds where the eggs cause nausea, and the worlds where the vodka causes nausea, and nothing in between. So it isn’t an eligible explanation. So the kairetic account predicts, correctly, that the best explanation of Sam’s nausea starts with (2). QED.\nEquilibrium Explanations in Economics\nBut there’s a difficulty looming for this nice theory. It isn’t at all clear how we’re going to generalise it to cover explanations in the social sciences. It’s perhaps easiest to see this if we look at an example. This example is originally from Hendricks and Porter (1988), though much of my discussion of it leans heavily on the exposition in Sutton (2000, 47–56).\nThe fact to be explained concerns the amount that oil exploration firms pay for, and eventually earn from, licences to drill in various tracts of the Gulf of Mexico. At various times, the government opens up the rights to drill on new tracts of sea bed. Firms are allowed to make a single bid for the rights to these tracts, and the highest bid wins. Some firms that bid have, prior to the opening of the new tract, drilling rights to some adjacent tract, and some do not. Having drilling rights to an adjacted tract is useful, because oil deposits tend not to follow the sharp lines on government surveyors’ maps. If you have already been working on an area adjacent to the one being auctioned, you have a pretty good idea how much oil that tract contains. If you don’t, then you have to make a guess based on more general features of that region of the Gulf. The stylised fact to be explained is that firms that bid on tracts adjacent to their existing tracts made a large profit, on average, while firms that bid on non-adjacent tracts made no net profit. (In fact they averaged a small loss, but the amount is close enough to zero that it’s worth treating their net returns as zero.) Why might this be?\nThe explanation that Hendricks and Porter offer starts with the following game, from Wilson (1967). Assume that two players, \\(A\\) and \\(B\\), are bidding on a good of some value in \\([0, 1]\\). \\(A\\) knows exactly how valuable the good is - call this value \\(x\\). \\(B\\) has no idea how valuable the good is; her credences about its possible value are distributed evenly over \\([0, 1]\\). Both \\(A\\) and \\(B\\) know these facts about each other. What should each of them do?\nStandard game theory has an answer. The game has a single Nash equilibrium. \\(A\\) bids \\(\\frac{x}{2}\\), and \\(B\\) plays a mixed strategy, randomly choosing a bid from \\([0, \\frac{1}{2}]\\). If each of them play these strategies, then \\(A\\) has an expected return of \\(\\frac{x^2}{2}\\), and \\(B\\) has an expected return of 0. Moreover, given each of them is playing those strategies, the other party cannot do better by changing their strategy. (That’s just what it means for the strategies to form a Nash equilibrium.)\nNow Hendricks and Porter go on to suggest that the drilling rights auctions are more or less like these games, with \\(A\\)’s role being filled by the firm with an adjacent tract, and \\(B\\)’s role by the firm with no adjacent tract.8 If we apply that model, we get plausible results for how much profit the two kinds of firms should earn, including a nice story about why the non-adjacent firms earn no profit. Indeed, we even get a satisfactory (at least to economists) story about why firms without adjacent tracts continue to bid even though they earn no net profit by doing so. If they didn’t bid, then firms with adjacent tracts could win the bidding by bidding a penny, and then it would be valuable to bid. In other words, the only equilibrium solution requires them to bid, even though they get no gain from it.\nThere is obviously a bit of work to do to show that this game provides a good model of Gulf of Mexico auctions. For one thing, we have to show that we can treat the auction as having effectively two players. Hendricks and Porter suggest that the behaviour of firms with adjacent tracts is sufficiently cooperative that this is a legitimate idealisation. There are other idealisations too, all of which I think can be fit nicely into Strevens’s kairetic story. We have to treat the firms with adjacent tracts as knowing the value of the tract, when really they’ll only know the approximate value. But it is plausible that treating their ignorance as being zero-valued, i.e., treating their knowledge as being perfect, makes no difference to what we need to explain. Similarly, it is not really true that the other firms have no idea how valuable the tracts are. But their knowledge levels are close enough to being represented by a flat probability distribution over the possible values of the tract that it doesn’t make a difference to this story to model their knowledge more precisely.\n(There is an interesting technical point here. Strevens focusses on cases where the idealisations involve giving some variable a “zero, infinite or some other extreme or default value” (318). In social sciences, one useful ‘default’ value is that the variable is represented by a flat probability function over some interval. This will rarely be exactly right; whether we interpret the probability function metaphysically or epistemically the ‘right’ function will presumably have some bumps or kinks in it. But it is an acceptable idealisation.)\nSo far so good. We started with an interesting set of facts, we found a nice mathematical model that has the facts as a consequence, and we argued (or at least hinted at how one could argue) that the deviation between the model and the facts was irrelevant to the outcome to be explained. So we’ve arguably fit a widely accepted economic explanation into the kairetic framework.\nBut once we start looking at the details, some problems start to emerge. Remember that on the kairetic account, explanations must be causal derivations. It doesn’t look at first like we’ve got any causation in the economic explanation. But I think that’s wrong. After all, there’s a reason why the two types of firms bid they way they do. The structure of the auction, along with other facts, causes them to make these bids. It isn’t something you’ll see highlighted in Hendricks and Porter, but it’s arguable their story is a causal story.\nThe problem is the ‘other facts’ you need to cite to complete this causal explanation. Those don’t seem to be sufficiently ‘cohesive’ for Strevens’s story to hold up. What we know is that if the actors follow equilibrium strategies, then we’ll get the results that are actually observed. But why should we think that actors will do just that? There are several possible reasons; too many reasons it might seem for the kairetic theory to work.\nPossibly the actors are perfectly rational, and perfectly rational beings play Nash equilibrium strategies.9 Possibly the actors are worried about their strategies leaking out, and are maximising expected utility relative to that assumption.10 Possibly there are a number of actors playing other strategies, but they don’t tend to survive economically, and so the statistics are dominated by firms that do survive, and the survivors generally play equilibrium strategies.11 Possibly the firms are run by a lot of game theorists, and “game theory is an excellent way of predicting the behaviour of professional game theorists.”12 More likely, some combination of these four reasons, and even some others, is causally relevant to the establishment and maintenance of this equilibrium.\nAnd that is something that’s hard to fit into the kairetic framework. We can show how the background facts about the case (i.e., the risks and rewards facing the competing oil firms), and a general causal law (i.e., that firms tend to end up playing equilibrium strategies) entail the conclusions that various firms bid on newly released tracts despite having zero expected profit. The problem is that many distinct causal pathways are compatible with this loosely described causal structure, and these pathways are not ‘cohesive.’ So the kairetic theory of explanation predicts that the explanation offered in Hendricks and Porter (1988) is not a good explanation of the observed behaviour in the auctions. That should worry anyone who either finds it intuitively plausible that it is a good explanation, or thinks that we should defer somewhat to the salient experts on what is a good explanation.\nPossible Responses\nI think these equilibrium explanations are a challenge to Strevens’s solution to the Goldilocks problem, and I think that’s a problem given the importance of solving the Goldilocks problem to the broader aims of the kairetic theory of explanation. But there are a number of ways Strevens could respond to this challenge. Indeed, we can see three responses already made in Depth. So I’ll end by noting why I don’t think those three responses work.\nFirst, it is true that some equilibrium explanations are cohesive in Strevens’s sense. Strevens discusses an example proposed by Elliot Sober (1983). Here is how Strevens describes the case.\n\nConsider a ball released at the inside lip of a basin. The ball rolls down into, then back and forth inside, the basin, eventually coming to rest at its lowest point. This will happen no matter what the ball’s release point. … Sober claims, quite rightly, that an equilibrium explanation … is the best explanation of the ball’s final resting place. (267-8)\n\nNow there are many ways in which the ball might have reached its equilibrium state. But note that these ways are all fairly similar to one another. The ways are, collectively, cohesive in just the sense needed for the kairetic theory.13 But this is surely an accident of the example. The ways in which agents reach a game-theoretic equilibrium are very different from one another, which makes that case rather unlike the case of a ball descending to the bottom of a basin. In short, while some equilibrium explanations will be suitably cohesive many, perhaps even most, will not.\nSecond, sometimes we don’t want to fully explain why \\(p\\) is true, but merely why \\(p\\) is true rather than \\(q\\), or why \\(p\\) is true given that \\(r\\) is true. In these cases, Strevens says that we exploit ‘explanatory frameworks’ (149) which fix certain facts as given for the purposes of explanation. So we might take \\(p \\vee q\\), or \\(r\\), to simply be fixed background facts; part of the framework relative to which explanations are made. When a proposition is part of the framework, its presence in the derivation of the intended outcome does not contribute to incohesiveness (163). So if we say that, for instance, the fact that games like the tract auction end up at equilibrium is part of the framework, then the orthodox explanation of, say, why firms bid despite a zero expected profit, can work. In short, the story about why firms bid is incohesive, but the story about why firms bid given that firms play equilibrium strategies is cohensive, and it is the latter that economists are trying to explain.14\nNow perhaps that’s true of what some economists are doing some of the time. But it seems too defeatist to me. Part of the appeal of game theoretic explanations is that they are supposed to explain why we get to, and stay at, equilibrium. I don’t think a practicing economist would say that they are merely presupposing that players in a game reach equilibrium, as opposed to offering a theory where that fact falls out as a nice explanandum. It’s true that economists do leave some things in the framework. They generally assume that economic actors are agents, while leaving the story about how agency might be physically realised to other disciplines. But it seems wrong to me to say that all the facts about how equilibrium is established and preserved are simply framework questions.\nFinally, Strevens notes that we can often refer to causal processes in explanations without being able to fully describe them. If someone asks why the temperature in this room stays so even while the temperature in other rooms fluctuates, I can explain the stability by saying that a thermostat regulates the temperature. Now at first this might look like a very incohesive explanation. There are many ways that a thermostat might work, and they don’t form anything like a coherent set. But perhaps that’s the wrong way to take my explanation. We could take the explanation as referring to the particular thermostat that is present, and the particular way in which it regulates the temperature. That explanation will be very cohesive; indeed, the real worry is that it is too precise. Of course, I might not be able to describe the process by which the thermostat regulates temperature. But this is no barrier to my being able to refer to it, any more than ignorance of chemistry is a barrier to my being able to refer to H\\(_2\\)O.\nCould this help with the tract auction we are discussing? At first glance it seems like it might. Perhaps the explanation can simply refer to the means by which a particular firm ends up playing an equilibrium strategy, even if it cannot describe that means. But the second glance is more troubling. Remember that what we’re trying to explain here is an average, not a particular firm’s behaviour. And it is meant to be consistent with the explanations that different firms get to equilibrium in very different ways. So we can’t really just refer to those different methods; we can only describe what they have in common. And that leaves us back with an incohesive explanation. Indeed, Strevens notes this point in a similar context when he says that “in aggregative and regularity explanation … there is a real risk” that we won’t pick out a cohesive causal mechanism. (154)\nSo I’m left thinking that we need somehow to supplement the story Strevens offers to make it plausible as an account of explanation in the special sciences. The kind of equilibrium explanations game theorists offer of economic outcomes are at least sometimes good explanations. But what makes them good is not the cohesiveness of their underlying physical mechanisms. It is, at least intuitively, the cohesiveness of the explanations from the perspective of the special science in question. If that intuition is right, we theorists still have work to do in characterising this notion of cohesiveness.\n\n\n\nFriedman, Michael. 1974. “Explanation and Scientific Understanding.” Journal of Philosophy 71 (1): 5–19. https://doi.org/10.2307/2024924.\n\n\nHahn, Frank. 1996. “Rerum Cognoscere Causas.” Economics and Philosophy 12 (2): 183–95. https://doi.org/10.1017/S0266267100004156.\n\n\nHendricks, Kenneth, and Robert H. Porter. 1988. “An Empirical Study of an Auction with Asymmetric Information.” The American Economic Review 78 (5): 865–83.\n\n\nKitcher, Philip. 1989. “Explanatory Unification and the Causal Structure of the World.” In Scientific Explanation, edited by Philip Kitcher and Wesley Salmon, 13:410–505. Minnesota Studies in Philosophy of Science. Minneapolis: University of Minnesota Press.\n\n\nLewis, David. 1986. “Causal Explanation.” In Philosophical Papers, II:214–40. Oxford: Oxford University Press.\n\n\nSober, Elliot. 1983. “Equilibrium Explanation.” Philosophical Studies 43 (2): 201–10. https://doi.org/10.1007/BF00372383.\n\n\nStalnaker, Robert. 1996. “Knowledge, Belief and Counterfactual Reasoning in Games.” Economics and Philosophy 12: 133–63. https://doi.org/10.1017/S0266267100004132.\n\n\n———. 1998. “Belief Revision in Games: Forward and Backward Induction.” Mathematical Social Sciences 36 (1): 31–56. https://doi.org/10.1016/S0165-4896(98)00007-9.\n\n\n———. 1999. “Extensive and Strategic Forms: Games and Models for Games.” Research in Economics 53 (3): 293–319. https://doi.org/10.1006/reec.1999.0200.\n\n\nStrevens, Michael. 2008. Depth: An Account of Scientific Explanations. Cambridge, MA: Harvard University Press.\n\n\nSutton, John. 2000. Marshall’s Tendencies: What Can Economists Know? Cambridge, MA: MIT Press.\n\n\nWilson, Robert B. 1967. “Competitive Bidding with Asymmetric Information.” Management Science 13 (11): 816–20. https://doi.org/10.1287/mnsc.13.11.816.\n\n\nAll page references, unless otherwise noted, are to Strevens (2008).↩︎\nStrevens calls the problem of how to explain why (2) is a better explanation than (1) ‘the disjunction problem.’ Given that the problem arises in the context of a theory that aims to explain why (2) is better than (3), I think the disjunction problem and the Goldilocks problem are not particularly distinct.↩︎\nFor more on explanation as unification, see Friedman (1974) and, especially, Kitcher (1989).↩︎\n‘Petrol stations’ if that fits your dialect better.↩︎\nGiven the last point, we’d expect that after the BP disaster in the Gulf of Mexico, stations weren’t too worried about being undercut on price by a nearby BP station.↩︎\nFor instance, the gravitational law says that there is a downward force on my coffee cup, but it doesn’t guarantee that it moves downwards. And, indeed, there are currently sufficiently many forces acting on it that it remains suspended 80 feet above the ground. The ‘no defeaters’ clause is intended to rule out such mischief.↩︎\nOf course, if we wanted to explain the time of Sam’s nausea, and not just its existence, the extra details in (3) might matter.↩︎\nThere are a lot of technical details I’m suppressing here, many of which Hendricks and Porter take into account, and many of which they rightly suppress. Sutton characterises Hendricks and Porter’s model as having considerably fewer added complications to the simple game Wilson develops than Hendricks and Porter themselves do. For instance, Sutton suppreses, while Hendricks and Porter explicitly consider, the possible efficiency gains derivable from owning adjacent tracts, but this only makes a small difference to the final result. On the kairetic theory of explanation, these simplifications actually improve the explanation considerably.↩︎\nThe normative claim here, that perfectly rational beings play Nash equilibrium strategies, seems implausible to me for reasons similar to those set out by Stalnaker (1996, 1998, 1999).↩︎\nNote that maximising expected utility does not entail playing equilibrium strategies without some extra assumption about strategies leaking, since a mixed strategy can be part of a unique equilibrium, but can never be uniquely utility maximising.↩︎\nPhilosophers tend to overstate how much economists rely on rationality assumptions. One of the attractions of game-theoretic explanations is that they don’t require all the agents to be perfectly rational. After all, game-theoretic explanations work well in evolutionary biology, and the players there are certainly not perfectly rational. For more on this point, and especially on how much work economists do to weaken rationality postulates, see Hahn (1996).↩︎\nThe quote is from a blog post by Daniel Davies on October 8, 2010. See http://d-squareddigest.blogspot.com/2010/10/on-not-being-obliged-to-vote-for.html.↩︎\nActually, this sentence isn’t as obviously true as it seems. Strevens’s discussion of the case brings out some unexpected difficulties in accomodating Sober’s claim in the kairetic theory. But this doesn’t affect my point, which is that the case is relatively easy for the kairetic theory to accommodate.↩︎\nThis might be reading too much into Strevens’s discussion. What he says about a related example is that the existence of communicative channels within firms is part of the ‘framework’ when making economic explanations. I don’t know whether he would extend this story to cover all means by which firms get to equilibrium.↩︎\n",
    "preview": "posts/2021-01-03-explanation-idealisation-and-the-goldilocks-problem/goldilocks.jpg",
    "last_modified": "2021-02-04T15:22:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-games-and-the-reason-knowledge-principle/",
    "title": "Games and the Reason-Knowledge Principle",
    "description": "A potential counterexample to Hawthorne and Stanley's Reason-Knowledge Principle",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2012-01-01",
    "categories": [
      "epistemology",
      "interest-relativity"
    ],
    "contents": "\nJohn Hawthorne and Jason Stanley (2008), defend what they call the “Reason-Knowledge Principle.”\n\nWhere one’s choice is \\(p\\)-dependent, it is appropriate to treat the proposition that \\(p\\) as a reason for acting iff you know that \\(p\\). (578)\n\nThere have been many attempts in the literature to show that this leads to implausible actions. As Jonathan Ichikawa (2012) shows, most of these attempts rest on further, and arguably false, assumptions about the connection between reasons and action. Relatedly, most of those responses concern the role of knowledge and reasons in decision-making. I’ll argue that we can formulate a sharper problem for the principle if we focus on game-playing, and say exactly which extra assumptions we are making.\nPublished in The Reasoner 6: 6-7.\nPicture by Khánh Hmoong via Creative Commons.\nThe Reason-Knowledge Principle should have the following implications, at least for cases where \\(S\\)’s aim is to produce the best outcome.\nIf \\(S\\) knows that \\(\\varphi\\) and \\(\\psi\\) will produce the same outcome, and \\(S\\) must choose \\(\\varphi\\) or \\(\\psi\\), then it is rationally permissible for \\(S\\) to choose \\(\\psi\\).\nIf \\(S\\) knows that \\(\\varphi\\) and \\(\\psi\\) will produce the same outcome if \\(p\\), and \\(\\varphi\\) will produce a better outcome if \\(\\neg p\\), then it is rationally permissible for \\(S\\) to choose \\(\\psi\\) iff she knows \\(p\\).\nThe point of (1) is that \\(S\\) can use her knowledge that \\(\\varphi\\) and \\(\\psi\\) will produce the same outcome to justify making an arbitrary choice between \\(\\varphi\\) and \\(\\psi\\). And the point of (2) is that the Reason-Knowledge Principle suggests only knowledge that \\(p\\) could justify ignoring the fact that \\(\\psi\\) does worse than \\(\\varphi\\) if \\(\\neg p\\).\nDefine a symmetric game as having these features:\nThe game is purely co-operative; each player gets the same payoffs;\nEach player knows nothing about the other save that it is common knowledge the players are rational, and hence know what each other’s rational requirements are;\nEach player has the same moves available; and,\nThe payoffs are a function of just which moves are made, not of who makes them.\nAssume \\(A\\) and \\(B\\) are playing a symmetric game, and it is common knowledge which symmetric game they are playing. Then the following premise seems hard to dispute:\nIt is rationally required for \\(A\\) to play \\(\\varphi\\) iff \\(A\\) knows \\(B\\) will play \\(\\varphi\\).\nWhat makes (3) so compelling is that we can derive it from (4), (5) and (6).\n\\(A\\) knows that \\(B\\) will play \\(\\varphi\\) iff \\(A\\) knows that any rational player will play \\(\\varphi\\).\nIf \\(A\\) knows any rational player will play \\(\\varphi\\), then \\(A\\) is rationally required to play \\(\\varphi\\).\nIf \\(A\\) is rationally required to play \\(\\varphi\\), then \\(A\\) knows that any rational player will play \\(\\varphi\\).\nWe get (4) from the fact that \\(A\\) knows nothing about \\(B\\) save that she is rational. We get (5) by the factivity of knowledge. And we get (6) by the requirement that the players are rational, and hence know what rationality requires of each player. And these three together entail (3). So (3) is true, and (1) and (2) are entailed by the Reason-Knowledge Principle. Unfortunately, (1), (2) and (3) are inconsistent, as we’ll now show.\nInformally, in this game \\(A\\) and \\(B\\) must each play either a green or red card. I will capitalise \\(A\\)’s moves, i.e., \\(A\\) can play GREEN or RED, and italicise \\(B\\)’s moves, i.e., \\(B\\) can play green or red. If two green cards, or one green card and one red card are played, each player gets $1. If two red cards are played, each gets nothing. Each cares just about their own wealth, so getting $1 is worth 1 util. All of this is common knowledge. More formally, here is the game table, with \\(A\\) on the row and \\(B\\) on the column.\n\n\ngreen\nred\nGREEN\n1, 1\n1, 1\nRED\n1, 1\n0, 0\n\nAssume \\(A\\) knows \\(B\\) will play green. By (3), it is rationally required that \\(A\\) plays GREEN. But \\(A\\) can use this knowledge of \\(B\\) to deduce that GREEN and RED have the same payoff. So by (1), it is rationally permissible to play RED. Contradiction.\nNow assume \\(A\\) does not know \\(B\\) will play green. By (3), it is not a rational requirement that \\(A\\) plays GREEN. But \\(A\\) knows that GREEN does better than RED unless \\(B\\) plays green. And since she doesn’t know \\(B\\) plays green, by (2), she’s required to play GREEN. Contradiction.\nSo either assuming that \\(A\\) does or does not know that it is rationally required for \\(B\\) to play green leads to a contradiction given (1), (2) and (3). So these three premises are inconsistent. Since (3) is true, that means (1) or (2) is false. And since the Reason-Knowledge principle entails those two premises, one of which is false, the Reason-Knowledge Principle is false.\nI’m not entirely sure which of (1) and (2) is false; both of them do feel plausible. I suspect the problem is (1). Assume \\(A\\) deduces from premises she believes that rational players will play a green card. Perhaps she agrees with Robert Stalnaker (1998) that rationality requires avoiding weakly dominated options. Then she knows it doesn’t matter to her outcome whether she plays GREEN or RED; she will get $1 either way. But if she plays RED, she is incoherent; she is doing something she thinks no rational player does. And perhaps this incoherence is a bad thing in itself. Niko Kolodny (2005) argues that incoherence is not bad in itself; Jacob Ross (2012) argues that it is. The suggestion that (1) is the false premise favours Ross’s view over Kolodny’s. But this conclusion is very speculative; the main thing I wanted to note was the problem this game raises for the Reason-Knowledge Principle.\n\n\n\nHawthorne, John, and Jason Stanley. 2008. “Knowledge and Action.” Journal of Philosophy 105 (10): 571–90. https://doi.org/10.5840/jphil20081051022.\n\n\nIchikawa, Jonathan. 2012. “Experimentalist Pressure Against Traditional Methodology.” Philosophical Psychology 25 (5): 743–65. https://doi.org/10.1080/09515089.2011.625118.\n\n\nKolodny, Niko. 2005. “Why Be Rational?” Mind 114 (455): 509–63. https://doi.org/10.1093/mind/fzi509.\n\n\nRoss, Jacob. 2012. “All Roads Lead to Violations of Countable Additivity.” Philosophical Studies 161 (3): 381–90. https://doi.org/10.1007/s11098-011-9744-z.\n\n\nStalnaker, Robert. 1998. “Belief Revision in Games: Forward and Backward Induction.” Mathematical Social Sciences 36 (1): 31–56. https://doi.org/10.1016/S0165-4896(98)00007-9.\n\n\n\n\n",
    "preview": "posts/2021-03-04-games-and-the-reason-knowledge-principle/redgreen.jpg",
    "last_modified": "2021-03-04T11:15:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-the-temporal-generality-problem/",
    "title": "The Temporal Generality Problem",
    "description": "The traditional generality problem for process reliabilism concerns the difficulty in identifying each belief forming process with a particular kind of process. Thatidentification is necessary since individual belief forming processes are typically of many kinds, and those kinds may vary in reliability. I raise a new kind of generality problem, one which turns on the difficulty of identifying beliefs with processes by which they were formed. This problem arises because individual beliefs may be the culmination of overlapping processes of distinct lengths, and these processes may differ in reliability. I illustrate the force of this problem with a discussion of recent work on the bootstrapping problem.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2012-01-01",
    "categories": [
      "epistemology",
      "scepticism",
      "notes"
    ],
    "contents": "\n\nContents\nTwo Kinds of Generality Problem\nCan the Problems be Solved Simultaneously?\nGenerality and Bootstrapping\n\nTwo Kinds of Generality Problem\nThe generality problem is a well-known problem for process reliabilist theories of justification.1 Here’s how the problem usually gets started. In the first instance, token processes of belief formation are not themselves reliable or unreliable. Rather, it is types of processes of belief formation that are reliable or unreliable. But any token process is an instance of many different types. And these types may differ in reliability.\n\nPublished in Logos and Episteme 3: 117-122.\nFor instance, imagine I read in the satirical newspaper The Onion that Barack Obama is the president. On this basis, I come to believe that Barack Obama as president. The process I have used to form this belief is an instance of each of these types.\n\nComing to believe that Barack Obama is the president;\nBelieving something because it was written in The Onion; and\nBelieving something because it was written in a newspaper.\n\nThe first type of process is very reliable, at least in 2012. The second is highly unreliable, and the third is very reliable. So should we say that the token process I used was reliable or unreliable? More generally, is there a principled way to map token processes to types of process in a way that lets us systematically say whether a particular process is reliable or not? Critics of reliabilism argue that there is not.\nAs I said, this problem has been around for quite a while, but I don’t think the full force of the problem has been appreciated. Reliabilism is a theory about whether a belief is justified or unjustified. But to determine whether the belief is justified, we step back from the belief itself in two respects. First, we look not to the belief, but to the token process of belief formation from which it results. Second, we look not just to that process, but to kinds of processes of which it is an instant. When carrying this out, we need to make the following two mappings.\n\nBelief \\(\\rightarrow\\) Token process of belief formation;\nToken process of belief formation \\(\\rightarrow\\) type of process of belief formation\n\nThe traditional point of the generality problem is that the second of these mappings is one-many, not one-one. Each token process is associated with many, many types of processes. But what hasn’t been sufficiently appreciated is that the first mapping is one-many as well. And this generates a new, and potentially harder, form of the generality problem.\nThat the first mapping is one-many isn’t because of any special properties of beliefs. Typically, an event is the conclusion of more than one process. Imagine that I travel from Michigan to New York to see a friend. I conclude this journey by walking to the friend’s apartment. With the last step I take, I conclude several processes. These include:\n\nWalking from the subway station to the apartment;\nTravelling by public transit from the airport to the apartment; and\nTravelling from Michigan to my friend’s apartment.\n\nIt is possible that one of these is a quite reliable process, while the others are not. If I am good at navigating the Manhattan street grid by foot, but poor at making it to the airport on time, then process one will be a highly reliable process, while process three will not. So should we say that my arrival at my friend’s apartment was the result of a reliable process or not? The best reply to that question is to point out that it is ill formed. Given that I made it to the nearest subway station, I used a reliable process to traverse the last few blocks. But the longer process I used was not as reliable.\nThis raises a conceptual worry for process reliabilist theories. If there is no such thing as the reliability of a conclusion, but only the reliability of a process of getting from one or other starting point to that conclusion, then it seems that in identifying the justifiedness of a belief with the reliability of the process used to generate it, we commit a kind of category mistake. Note that this problem would persist even if we had a one-one mapping from token processes to epistemologically relevant types of processes that would let us solve the traditional form of the generality problem. We would still need a way of saying which of the many processes which terminate in a belief is the epistemologically relevant one. I don’t think there’s any reason to think there is a good answer to this question. I call this the Temporal Generality Problem, because the different processes that culminate in a belief are typically of different durations.\nCan the Problems be Solved Simultaneously?\nI’ve argued in the previous section that in theory the Temporal Generality Problem is distinct from the traditional version of the generality problem. But one might think that in practice a solution to the latter will solve problems to do with the former. Consider the following three step process.\n\nI hear an astrologer say that Napoleon Bonaparte will win the 2013 US Presidential election.\nI form the belief that Napoleon Bonaparte will win the 2013 US Presidential election.\nI deduce that there will be a US Presidential election in 2013.\n\nThe process by which I got from 2 to 3 is, on the face of it, highly reliable. Assuming that I’m a mostly sensible person, coming to believe obvious logical consequences of my prior beliefs is a highly reliable process. Yet clearly the process that runs from 1 to 3, i.e., the process of believing obvious logical consequences of the contents of astrological predictions, is not a reliable process. So, one might ask, is the resultant belief justified, because it is formed by the reliable process that runs from 2 to 3, or unjustified, because it is formed by the unreliable process that runs from 1 to 3?\nClearly, this is a false dilemma. The salient kind of process I’m using between 2 and 3 is not believe obvious logical consequences of a belief, but believe obvious logical consequences of a belief formed by an unreliable process. Once we identify the kind of process used at the last stage correctly, we can see that the unreliability of the whole process causes the process used at the last stage to be unreliable.\nWe might even get cases that go the other way. There are plenty of occasions in science where scientists use mathematical techniques which cannot be made rigorous, and idealisations that cannot easily be replaced with approximations, or with any other statement known to be true.2 If we looked at such a step in isolation, we would possibly think that it is an unreliable step, even though it is part of a longer, reliable process. But the fact that it is part of a reliable process matters. In particular, it matters to the way we identify the step the scientist is using with a larger kind of inferential processes. That kind won’t involve, for instance, all instances of reasoning from false premises, or of reasoning with incoherent mathematical models. Rather, it will just include the kind of reasoning that is licenced by the norms of the science that the scientist is participating in, and that kind might be a very reliable kind of process.\nBut there is one very special case where I think this kind of solution to the Temporal Generality Problem will not work. It concerns the way in which a reliabilist will try and solve the bootstrapping problem, as developed by Stewart Cohen (2002) and Jonathan Vogel (2000). We’ll turn next to that problem.\nGenerality and Bootstrapping\nHilary Kornblith (2009) has proposed that looking at processes of longer duration generates a reliabilist solution to the bootstrapping problem. I’m going to argue that Kornblith’s solution, which I agree is the kind of thing a reliabilist should say, in fact shows that the Temporal Generality Problem is a distinct kind of generality problem, and perhaps a much harder problem than the traditional generality problem.\nLet’s start with a very abstract version of the problem. Assume device \\(D\\) is highly reliable, and \\(S\\) trusts device \\(D\\) without antecedently knowing that it is reliable. Then the following sequence of events take place.\n\nAt \\(t_0\\), \\(S\\) sees that device \\(D\\) says that \\(p\\).\nAt \\(t_1\\), \\(S\\) forms the belief that \\(D\\) says at \\(t_0\\) that \\(p\\)on the basis of this perception.3\nAt \\(t_2\\), \\(S\\) forms the belief that \\(p\\), on the basis that the machine says so.\nAt \\(t_3\\), \\(S\\) forms the belief that the machine is accurate at \\(t_0\\), on the basis of her last two beliefs.\n\nWhat should a reliabilist say about all this? Well, the process that runs from \\(t_0\\) to \\(t_1\\), the process of believing machine readings are as they appear, looks pretty reliable, so the belief formed at \\(t_1\\) looks pretty reliable. And the process that runs from \\(t_1\\) to \\(t_2\\), i.e., the process of believing that things are as machine \\(D\\) says they are, also looks pretty reliable, so that belief looks pretty reliable. And the process that runs from \\(t_2\\) to \\(t_3\\), i.e., the process of drawing obvious logical consequences from beliefs formed by reliable processes, also looks pretty reliable. It’s true that at \\(t_2\\), \\(S\\) doesn’t know she’s using a reliable process. And hence at \\(t_3\\), \\(S\\) doesn’t know that this is the kind of process that she’s using. But none of this should matter to an externalist like the reliabilist, since they think what matters is actual reliability, not known reliability.\nBut there are two problems lurking in the vicinity. First, many people think that it is very bizarre that \\(S\\) can form a justified belief that \\(D\\) is accurate at \\(t_0\\) on the basis of simply looking at \\(D\\). That’s the intuition behind the bootstrapping problem. Second, the case looks like an instance of the Temporal Generality Problem. The two problems are related. Kornblith’s solution to the bootstrapping problem is to insist that the process used is in fact unreliable. What he means to draw our attention to is that the process which runs from \\(t_0\\) to \\(t_3\\) is unreliable. And he’s right. That looks like a process of determining whether a machine is accurate by simply looking at the machine and trusting it. Of course, there are several other ways we could classify the process used, but Kornblith argues that this is the best classification, and I think he’s right. And if he is right, then we have part of a solution to the bootstrapping problem.\nBut if Kornblith is right, then we pretty clearly also have a nasty instance of the Temporal Generality Problem. Because now it looks like a chain of three reliable processes, those that run from \\(t_0\\) to \\(t_1\\), from \\(t_1\\) to \\(t_2\\), and from \\(t_2\\) to \\(t_3\\), collectively form an unreliable process. The belief that is formed at \\(t_3\\) is the culmination of two processes; a reliable one that runs from \\(t_2\\) to \\(t_3\\), and an unreliable one that runs from \\(t_0\\) to \\(t_3\\). If a belief is justified iff it is the outcome of a reliable process, and unjustified iff it is the outcome of an unreliable process, then the belief is both justified and unjustified, which is a contradiction.\nHow could the reliabilist escape this problem? I can see only two ways out. One is to say that the process that runs from \\(t_0\\) to \\(t_3\\) is in fact a reliable process. But that’s to fall back into the bootstrapping problem. And in any case, it seems absurd, since that process really does look like a process of determining whether a machine is reliable by simply looking at it. The other is to say that the process that runs from \\(t_2\\) to \\(t_3\\) is unreliable. To do that, we’d need to come up with a natural kind of process which is unreliable, and which this process instantiates. This does not look easy. I’m not going to insist this couldn’t be done, but I’ll end by noting three challenges that stand in the way of getting it done, and which seem pretty formidible.\nFirst, if we say the process that runs from \\(t_2\\) to \\(t_3\\) is unreliable, then we are putting general restrictions on how we can obtain knowledge by deductive inference. As John Hawthorne (2005) argues, any such restrictions will be hard to motivate.\nSecond, the restrictions will have to be fairly sweeping to cover the range of conclusions that, intuitively, cannot be drawn through this kind of reasoning. Imagine a variant on the above example where at \\(t_3\\), \\(S\\) concludes that either \\(D\\) is accurate at \\(t_0\\) or it will snow tomorrow. That’s entailed, obviously, by what she knows at \\(t_2\\). And yet the process of getting from \\(t_0\\) to that conclusion seems unreliable. So we can’t simply say that what’s ruled out are cases where the agent draws a conclusion that is simply about \\(D\\).\nThird, the classification of the process that runs from \\(t_2\\) to \\(t_3\\) must not merely fail to be ad hoc, it must plausibly be the most natural classification available. And yet it seems there is one very natural classification that is not available, namely the classification of the process as an instance of deduction from known premises, or from premises arrived at by highly reliable processes.\nSo the challenge this problem raises for reliabilism is substantial. I don’t mean to say it is a knock-down drawn-out refutation; philosophical arguments rarely are. But it does add a new dimension to the generality problem, and as we’ve seen in the last few paragraphs, put some new constraints on solutions to the old version of the generality problem.\n\n\nCohen, Stewart. 2002. “Basic Knowledge and the Problem of Easy Knowledge.” Philosophy and Phenomenological Research 65 (2): 309–29. https://doi.org/10.1111/j.1933-1592.2002.tb00204.x.\n\n\nConee, Earl, and Richard Feldman. 1998. “The Generality Problem for Reliabilism.” Philosophical Studies 89 (1): 1–29. https://doi.org/10.1023/A:1004243308503.\n\n\nDavey, Kevin. 2003. “Is Mathematical Rigor Necessary in Physics?” British Journal for the Philosophy of Science 54 (3): 439–63. https://doi.org/10.1093/bjps/54.3.439.\n\n\n———. 2011. “Idealizations and Contextualism in Physics.” Philosophy of Science 78 (1): 16–38. https://doi.org/10.1086/658093.\n\n\nFeldman, Richard. 1985. “Reliability and Justification.” Monist 68 (2): 159–74. https://doi.org/10.5840/monist198568226.\n\n\nGoldman, Alvin. 1979. “What Is Justified Belief.” In Justification and Knowledge, edited by George Pappas, 1–23. Dordrecht: Reidel.\n\n\nHawthorne, John. 2005. “The Case for Closure.” In Contemporary Debates in Epistemology, edited by Matthias Steup and Ernest Sosa, 26–43. Malden, MA: Blackwell.\n\n\nKornblith, Hilary. 2009. “A Reliabilist Solution to the Problem of Promiscuous Bootstrapping.” Analysis 69 (2): 263–67. https://doi.org/10.1093/analys/anp012.\n\n\nVogel, Jonathan. 2000. “Reliabilism Leveled.” Journal of Philosophy 97 (11): 602–23. https://doi.org/10.2307/2678454.\n\n\nOn process reliabilism, see (Goldman 1979). On the generality problem, see (Feldman 1985; Conee and Feldman 1998)↩︎\nOn non-rigorous techniques, see (Davey 2003); on idealisations, see (Davey 2011).↩︎\nOn some theories of perception, it might be that \\(t_0 = t_1\\), since perception involves belief formation. I don’t mean to rule those theories out; the notation here is meant to be consistent with the hypothesis that \\(t_0 = t_1\\).\n\n↩︎\n",
    "preview": "posts/2021-03-04-the-temporal-generality-problem/clocktower.jpg",
    "last_modified": "2021-03-04T10:42:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-05-dogmatism-probability-and-logical-uncertainty/",
    "title": "Dogmatism, Probability and Logical Uncertainty",
    "description": "Many epistemologists hold that an agent can come to justifiably believe that p is true by seeing that it appears that p is true, without having any antecedent reason to believe that visual impressions are generally reliable. Certain reliabilists think this, at least if the agent’s vision is generally reliable. And it is a central tenet of dogmatism (as described by Pryor (2000) and Pryor (2004)) that this is possible. Against these positions it has been argued (e.g. by Cohen (2005) and White (2006)) that this violates some principles from probabilistic learning theory. To see the problem, let’s note what the dogmatist thinks we can learn by paying attention to how things appear. (The reliabilist says the same things, but we’ll focus on the dogmatist.)",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      },
      {
        "name": "David Jehle",
        "url": {}
      }
    ],
    "date": "2012-01-01",
    "categories": [
      "epistemology"
    ],
    "contents": "\n\nContents\nIntuitionistic Probability\nLogical Uncertainty\nAppendix: Proofs\n\nMany epistemologists hold that an agent can come to justifiably believe that \\(p\\) is true by seeing that it appears that \\(p\\) is true, without having any antecedent reason to believe that visual impressions are generally reliable. Certain reliabilists think this, at least if the agent’s vision is generally reliable. And it is a central tenet of dogmatism (as described by Pryor (2000) and Pryor (2004)) that this is possible. Against these positions it has been argued (e.g. by Cohen (2005) and White (2006)) that this violates some principles from probabilistic learning theory. To see the problem, let’s note what the dogmatist thinks we can learn by paying attention to how things appear. (The reliabilist says the same things, but we’ll focus on the dogmatist.)\nPublished in New Waves in Philosophical Logic, eedited by Greg Restall and Gillian Russell, Palgrave, 95-111.\nPicture by Bravehardt via Creative Commons.\nSuppose an agent receives an appearance that \\(p\\), and comes to believe that \\(p\\). Letting Ap be the proposition that it appears to the agent that \\(p\\), and \\(\\rightarrow\\) be the material conditional, we can say that the agent learns that \\(p\\), and hence is in a position to infer \\(Ap \\rightarrow p\\), once they receive the evidence Ap.1 This is surprising, because we can prove the following.2\n\nTheorem 1\nIf \\(Pr\\) is a classical probability function, then\\(Pr(Ap \\rightarrow p | Ap) \\leq Pr(Ap \\rightarrow p)\\).\n\n(All the theorems are proved in the appendix.) We can restate Theorem 1 in the following way, using classically equvalent formulations of the material conditional.\n\nTheorem 2\nIf \\(Pr\\) is a classical probability function, then\n\\(Pr(\\neg(Ap \\wedge \\neg p) | Ap) \\leq Pr(\\neg(Ap \\wedge \\neg p))\\); and\n\\(Pr(\\neg Ap \\vee p | Ap) \\leq Pr(\\neg Ap \\vee p)\\).\n\nAnd that’s a problem for the dogmatist if we make the standard Bayesian assumption that some evidence \\(E\\) is only evidence for hypothesis \\(H\\) if \\(Pr(H | E) > Pr(H)\\). For here we have cases where the evidence the agent receives does not raise the probability of \\(Ap \\rightarrow p\\), \\(\\neg(Ap \\wedge \\neg p)\\) or \\(\\neg Ap \\vee p\\), so the agent has not received any evidence for them, but getting this evidence takes them from not having a reason to believe these propositions to having a reason to get them.\nIn this paper, we offer a novel response for the dogmatist. The proof of Theorem 1 makes crucial use of the logical equivalence between \\(Ap \\rightarrow p\\) and \\(((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap)\\). These propositions are equivalent in classical logic, but they are not equivalent in intuitionistic logic. Exploiting this non-equivalence, we derive two claims. In Section 1 we show that Theorems 1 and 2 fail in intuitionistic probability theory. In Section 2 we consider how an agent who is unsure whether classical or intuitionistic logic is correct should apportion their credences. We conclude that for such an agent, theorems analogous to Theorems 1 and 2 fail even if the agent thinks it extremely unlikely that intuitionistic logic is the correct logic. The upshot is that if it is rationally permissible to be even a little unsure whether classical or intuitionistic logic is correct, it is possible that getting evidence that \\(Ap\\) raises the rational credibility of \\(Ap \\rightarrow p\\), \\(\\neg(Ap \\wedge \\neg p)\\) and \\(\\neg Ap \\vee p\\).\nIntuitionistic Probability\nIn Weatherson (2003), the notion of a \\(\\vdash\\)-probability function, where \\(\\vdash\\) is an entailment relation, is introduced. For any \\(\\vdash\\), a \\(\\vdash\\)-probability function is a function \\(Pr\\) from sentences in the language of \\(\\vdash\\) to \\([0, 1]\\) satisfying the following four constraints.3\n(P0)\n\\(Pr(p) = 0\\) if \\(p\\) is a \\(\\vdash\\)-antithesis, i.e. iff for any \\(X, p \\vdash X\\).\n\n(P1)\n\\(Pr(p) = 1\\) if \\(p\\) is a \\(\\vdash\\)-thesis, i.e. iff for any \\(X, X \\vdash p\\).\n\n(P2)\nIf \\(p \\vdash q\\) then \\(Pr(p) \\leq Pr(q)\\).\n\n(P3)\n\\(Pr(p) + Pr(q) = Pr(p \\vee q) + Pr(p \\wedge q)\\).\n\nWe’ll use \\(\\vdash_{CL}\\) to denote the classical entailment relation, and \\(\\vdash_{IL}\\) to denote the intuitionist entailment relation. Then what we usually take to be probability functions are \\(\\vdash_{CL}\\)-probability functions. And intuitionist probability functions are \\(\\vdash_{IL}\\)-probability functions.\nIn what follows we’ll make frequent appeal to three obvious consequences of these axioms, consequences which are useful enough to deserve their own names. Hopefully these are obvious enough to pass without proof.4\n(P1\\(^*\\))\n\\(0 \\leq Pr(p) \\leq 1\\).\n\n(P2\\(^*\\))\nIf \\(p \\dashv \\vdash q\\) then \\(Pr(p) = Pr(q)\\).\n\n(P3\\(^*\\))\nIf \\(p \\wedge q\\) is a \\(\\vdash\\)-antithesis, then \\(Pr(p) + Pr(q) = Pr(p \\vee q)\\).\n\n\\(\\vdash\\)-probability functions obviously concern unconditional probability, but we can easily extend them into conditional \\(\\vdash\\)-probability functions by adding the following axioms.5\n(P4)\nIf \\(r\\) is not a \\(\\vdash\\)-antithesis, then \\(Pr(\\cdot | r)\\) is a \\(\\vdash\\)-probability function; i.e., it satisfies P0-P3.\n\n(P5)\nIf \\(r \\vdash p\\) then \\(Pr(p | r) = 1\\).\n\n(P6)\nIf \\(r\\) is not a \\(\\vdash\\)-antithesis, then \\(Pr(p \\wedge q | r) = Pr(p | q \\wedge r)Pr(q | r)\\).\n\nThere is a simple way to generate \\(\\vdash_{CL}\\) probability functions. Let \\(\\langle W, V\\rangle\\) be a model where \\(W\\) is a finite set of worlds, and \\(V\\) a valuation function defined on them with respect to a (finite) set \\(K\\) of atomic sentences, i.e., a function from \\(K\\) to subsets of \\(W\\). Let \\(L\\) be the smallest set including all members of \\(K\\) such that whenever \\(A\\) and \\(B\\) are in \\(L\\), so are \\(A \\wedge B\\), \\(A \\vee B\\), \\(A \\rightarrow B\\) and \\(\\neg A\\). Extend \\(V\\) to \\(V^*\\), a function from \\(L\\) to subsets of \\(W\\) using the usual recursive definitions of the sentential connectives. (So \\(w \\in V^*(A \\wedge B)\\) iff \\(w \\in V^*(A)\\) and \\(w \\in V^*(B)\\), and so on for the other connectives.) Let \\(m\\) be a measure function defined over subsets of W. Then for any sentence \\(S\\) in \\(L\\), \\(Pr(S)\\) is \\(m(\\{w: w \\in V^*(S)\\})\\). It isn’t too hard to show that Pr is a \\(\\vdash_{CL}\\) probability function.\nThere is a similar way to generate \\(\\vdash_{IL}\\) probability functions. This method uses a simplified version of the semantics for intuitionistic logic in Kripke (1965). Let \\(\\langle W, R, V\\rangle\\) be a model where \\(W\\) is a finite set of worlds, \\(R\\) is a reflexive, transitive relation defined on \\(W\\), and \\(V\\) is a valuation function defined on them with respect to a (finite) set \\(K\\) of atomic sentences. We require that \\(V\\) be closed with respect to \\(R\\), i.e. that if \\(x \\in V(p)\\) and \\(xRy\\), then \\(y \\in V(p)\\). We define \\(L\\) the same way as above, and extend \\(V\\) to \\(V^*\\) (a function from \\(L\\) to subsets of \\(W\\)) using the following definitions.\n\n\\(w \\in V^*(A \\wedge B)\\) iff \\(w \\in V^*(A)\\) and \\(w \\in V^*(B)\\).\\(w \\in V^*(A \\vee B)\\) iff \\(w \\in V^*(A)\\) or \\(w \\in V^*(B)\\).\\(w \\in V^*(A \\rightarrow B)\\) iff for all \\(w^{\\prime}\\) such that \\(wRw^{\\prime}\\) and \\(w^{\\prime}\\in V^*(A), w^{\\prime} \\in V^*(B)\\).\\(w \\in V^*(\\neg A)\\) iff for all \\(w^{\\prime}\\) such that \\(wRw^{\\prime}\\), it is not the case that \\(w^{\\prime} \\in V^*(A)\\).\n\nFinally, we let \\(m\\) be a measure function defined over subsets of \\(W\\). And for any sentence \\(S\\) in \\(L\\), \\(Pr(S)\\) is \\(m(\\{w: w \\in V^*(S)\\})\\). Weatherson (2003) shows that any such \\(Pr\\) is a \\(\\vdash_{IL}\\) probability function.\nTo show that Theorem 1 may fail when \\(Pr\\) is \\(\\vdash_{IL}\\) a probability function, we need a model we’ll call \\(M\\). The valuation function in \\(M\\) is defined with respect to a language where the only atomic propositions are \\(p\\) and \\(Ap\\). \\[\\begin{aligned}\nW &= \\{1, 2, 3\\} \\\\\nR &=  \\{\\langle 1, 1\\rangle , \\langle 2, 2\\rangle , \\langle 3, 3\\rangle , \\langle 1, 2\\rangle , \\langle 1, 3\\rangle \\} \\\\\nV(p) &= \\{2\\} \\\\\nV(Ap) &= \\{2, 3\\}\\end{aligned}\\]\nGraphically, \\(M\\) looks like this.\n\n(70, 50) (35, 5)(-1, 1)30 (35, 5)(1, 1)30 (35,5) (4.8,35.5) (65.2,35.5) (28, 5)\\(1\\) (0,35.5)\\(2\\) (60,35.5)\\(3\\) (7,35.5)\\(Ap, p\\) (67,35.5)\\(Ap\\)\n\nWe’ll now consider a family of measures over \\(m\\). For any \\(x \\in (0, 1)\\), let \\(m_x\\) be the measure function such that \\(m_x(\\{1\\}) = 1 - x, m_x(\\{2\\}) = x\\), and \\(m_x(\\{3\\}) = 0\\). Corresponding to each function \\(m_x\\) is a \\(\\vdash_{IL}\\) probability function we’ll call \\(Pr_x\\). Inspection of the model shows that Theorem 3 is true.\n\nTheorem 3.\nIn \\(M\\), for any \\(x \\in (0, 1)\\),\n\\(Pr_x(Ap \\rightarrow p)\\) = \\(Pr_x((Ap \\rightarrow p) \\wedge Ap) = x\\)\n\\(Pr_x(\\neg Ap \\vee p)\\) = \\(Pr_x((\\neg Ap \\vee p) \\wedge Ap) = x\\)\n\\(Pr_x(\\neg(Ap \\wedge \\neg p))\\) = \\(Pr_x(\\neg(Ap \\wedge \\neg p) \\wedge Ap) = x\\)\n\nAn obvious corollary of Theorem 3 is\n\nTheorem 4.\nFor any \\(x \\in (0, 1)\\),\n\\(1 = Pr_x(Ap \\rightarrow p | Ap) > Pr_x(Ap \\rightarrow p) = x\\)\n\\(1 = Pr_x(\\neg Ap \\vee p | Ap) > Pr_x(\\neg Ap \\vee p) = x\\)\n\\(1 = Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) > Pr_x(\\neg(Ap \\wedge \\neg p)) = x\\)\n\nSo for any \\(x\\), conditionalising on \\(Ap\\) actually raises the probability of \\(Ap \\rightarrow p, \\neg(Ap \\wedge \\neg p)\\) and \\(\\neg Ap \\vee p\\) with respect to \\(Pr_x\\). Indeed, since \\(x\\) could be arbitrarily low, it can raise the probability of each of these three propositions from any arbitrarily low value to 1. So it seems that if we think learning goes by conditionalisation, then receiving evidence \\(Ap\\) could be sufficient grounds to justify belief in these three propositions. Of course, this relies on our being prepared to use the intuitionist probability calculus. For many, this will be considered too steep a price to pay to preserve dogmatism. But in section 2 we’ll show that the dogmatist does not need to insist that intuitionistic logic is the correct logic for modelling uncertainty. All they need to show is that it might be correct, and then they’ll have a response to this argument.\nLogical Uncertainty\nWe’re going to build up to a picture of how to model agents who are rationally uncertain about whether the correct logic is classical or intuitionistic. But let’s start by thinking how an agent who is unsure which of two empirical theories \\(T_1\\) or \\(T_2\\) is correct. We’ll assume that the agent is using the classical probability calculus, and the agent knows which propositions are entailed by each of the two theories. And we’ll also assume that the agent is sure that it’s not the case that each of these theories is false, and the theories are inconsistent, so they can’t both be true.\nThe natural thing then is for the agent to have some credence \\(x\\) in \\(T_1\\), and credence \\(1-x\\) in \\(T_2\\). She will naturally have a picture of what the world is like assuming \\(T_1\\) is correct, and on that picture every proposition entailed by \\(T_1\\) will get probability 1. And she’ll have a picture of what the world is like assuming \\(T_2\\) is correct. Her overall credal state will be a mixture of those two pictures, weighted according to the credibility of \\(T_1\\) and \\(T_2\\).\nIf we’re working with unconditional credences as primitive, then it is easy to mix two probability functions to produce a credal function which is also a probability function. Let \\(Pr_1\\) be the probability function that reflects the agent’s views about how things probably are conditional on \\(T_1\\) being true, and \\(Pr_2\\) the probability function that reflects her views about how things probably are conditional on \\(T_2\\) being true. Then for any \\(p\\), let \\(Cr(p) = xPr_1(p) + (1-x)Pr_2(p)\\), where \\(Cr\\) is the agent’s credence function.\nIt is easy to see that \\(Cr\\) will be a probability function. Indeed, inspecting the axioms P0-P3 makes it obvious that for any \\(\\vdash\\), mixing two \\(\\vdash\\)-probability functions as we’ve just done will always produce a \\(\\vdash\\)-probability function. The axioms just require that probabilities stand in certain equalities and inequalities that are obviously preserved under mixing.\nIt is a little trickier to mix conditional probability functions in an intuitive way, for the reasons set out in Jehle and Fitelson (2009). But in a special case, these difficulties are not overly pressing. Say that a \\(\\vdash\\)-probability function is regular iff for any p, q in its domain, \\(Pr(p | q) = 0\\) iff \\(p \\wedge q\\) is a \\(\\vdash\\)-antitheorem. Then, for any two regular conditional probability functions \\(Pr_1\\) and \\(Pr_2\\) we can create a weighted mixture of the two of them by taking the new unconditional probabilities, i.e. the probabilities of \\(p\\) given \\(T\\), where \\(T\\) is a theorem, to be weighted sums of the unconditional probabilities in \\(Pr_1\\) and \\(Pr_2\\). That is, our new function \\(Pr_3\\) is given by:\n\\[Pr_3(p | T) = xPr_1(p | T) + (1-x)Pr_2(p | T)\\]\nIn the general case, this does not determine exactly which function \\(Pr_3\\) is, since it doesn’t determine the value of \\(Pr_3(p | q)\\) when \\(Pr_1(q | T) = Pr_2(q | T) = 0\\). But since we’re paying attention just to regular functions this doesn’t matter. If the function is regular, then we can just let the familiar ratio account of conditional probability be a genuine definition. So in general we have,\n\\[Pr_3(p | q) = \\frac{Pr_3(p \\wedge q | T)}{Pr_3(q | T)}\\]\nAnd since the numerator is 0 iff \\(q\\) is an anti-theorem, whenever \\(Pr(p | q)\\) is supposed to be defined, i.e. when \\(q\\) is not an anti-theorem, the right hand side will be well defined. As we noted, things get a lot messier when the functions are not regular, but those complications are unnecessary for the story we want to tell.\nNow in the cases we’ve been considering so far, we’ve been assuming that \\(T_1\\) and \\(T_2\\) are empirical theories, and that we could assume classical logic in the background. Given all that, most of what we’ve said in this section has been a fairly orthodox treatment of how to account for a kind of uncertainty. But there’s no reason, we say, why we should restrict \\(T_1\\) and \\(T_2\\) in this way. We could apply just the same techniques when \\(T_1\\) and \\(T_2\\) are theories of entailment.\nWhen \\(T_1\\) is the theory that classical logic is the right logic of entailment, and \\(T_2\\) the theory that intuitionistic logic is the right logic of entailment, then \\(Pr_1\\) and \\(Pr_2\\) should be different kinds of probability functions. In particular, \\(Pr_1\\) should be a \\(\\vdash_{CL}\\)-probability function, and \\(Pr_2\\) should be a \\(\\vdash_{IL}\\)-probability function. That’s because \\(Pr_1\\) represents how things probably are given \\(T_1\\), and given \\(T_1\\), how things probably are is constrained by classical logic. And \\(Pr_2\\) represents how things probably are given \\(T_2\\), and given \\(T_2\\), how things probably are is constrained by intuitionistic logic.\nIf we do all that, we’re pushed towards the thought that the if someone is uncertain whether the right logic is intuitionistic or classical logic, then the right theory of probability for them is intuitionistic probability theory. That’s because of Theorem 5.\n\nTheorem 5 Let \\(Pr_1\\) be a regular conditional \\(\\vdash_{CL}\\)-probability function, and \\(Pr_2\\) be a regular conditional \\(\\vdash_{IL}\\)-probability function that is not a \\(\\vdash_{CL}\\)-probability function. And let \\(Pr_3\\) be defined as in the text. (That is, \\(Pr_3(A) = xPr_1(A) + (1-x)Pr_2(A)\\), and \\(Pr_3(A | B) = \\frac{Pr_3(A \\wedge B)}{Pr_3(B)}\\).) Then \\(Pr_3\\) is a regular conditional \\(\\vdash_{IL}\\)-probability function.\n\nThat’s to say, if the agent is at all unsure whether classical logic or intuitionistic logic is the correct logic, then their credence function should be an intuitionistic probability function.\nOf course, if the agent is very confident that classical logic is the correct logic, then they couldn’t rationally have their credences distributed by any old intuitionistic probability function. After all, there are intuitionistic probability functions such that \\(Pr(p \\vee \\neg p) = 0\\), but an agent whose credence that classical logic is correct is, say, 0.95, could not reasonably have credence 0 in \\(p \\vee \\neg p\\). For our purposes, this matters because we want to show that an agent who is confident, but not certain, that classical logic is correct can nevertheless be a dogmatist. To fill in the argument we need,\n\nTheorem 6 Let \\(x\\) be any real in \\((0, 1)\\). Then there is a probability function \\(Cr\\) that (a) is a coherent credence function for someone whose credence that classical logic is correct is \\(x\\), and (b) satisfies each of the following inequalities: \\[\\begin{aligned}\nPr(Ap \\rightarrow p | Ap) &> Pr(Ap \\rightarrow p) \\\\\nPr(\\neg Ap \\vee p | Ap) &> Pr(\\neg Ap \\vee p) \\\\\nPr(\\neg(Ap \\wedge \\neg p) | Ap) &> Pr(\\neg(Ap \\wedge \\neg p)) \\end{aligned}\\]\n\nThe main idea driving the proof of Theorem 6 which is set out in the appendix, is that if intuitionistic logic is correct, it’s possible that conditionalising on Ap raises the probability of each of these three propositions from arbitrarily low values to 1. So as long as the prior probability of each of the three propositions, conditional on intuitionistic logic being correct, is low enough, it can still be raised by conditionalising on Ap.\nMore centrally, we think Theorem 6 shows that the probabilistic argument against dogmatism is not compelling. The original argument noted that the dogmatist says that we can learn the three propositions in Theorem 6, most importantly \\(Ap \\rightarrow p\\), by getting evidence Ap. And it says this is implausible because conditionalising on Ap lowers the probability of \\(Ap \\rightarrow p\\). But it turns out this is something of an artifact of the very strong classical assumptions that are being made. The argument not only requires the correctness of classical logic, it requires that the appropriate credence the agent should have in classical logic’s being correct is one. And that assumption is, we think, wildly implausible. Even if the agent should be very confident that classical logic is the correct logic, it shouldn’t be a requirement of rationality that she be absolutely certain that it is correct.\nSo we conclude that this argument fails. A dogmatist about perception who is at least minimally open-minded about logic can marry perceptual dogmatism to a probabilistically coherent theory of confirmation.\nThis paper is one more attempt on our behalf to defend dogmatism from a probabilistic challenge. Weatherson (2007) defends dogmatism from the so-called “Bayesian objection.” And Jehle (2009) not only shows that dogmatism can be situated nicely into a probabilistically coherent theory of confirmation, but also that within such a theory, many of the traditional objections to dogmatism are easily rebutted. We look forward to future research on the connections between dogmatism and probability, but we remain skeptical that dogmatism will be undermined solely by probabilistic considerations.\nAppendix: Proofs\n\nTheorem 1\nIf \\(Pr\\) is a classical probability function, then\\(Pr(Ap \\rightarrow p | Ap) \\leq Pr(Ap \\rightarrow p)\\).\n\nProof: Assume \\(Pr\\) is a classical probability function, and \\(\\vdash\\) the classical consequence relation.\n\\[\\begin{aligned}\n1. &Ap \\rightarrow p \\dashv \\vdash ((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap) & \\text{} \\\\\n2. &Pr(Ap \\rightarrow p) = Pr(((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap)) & \\text{1, P2$^*$} \\\\\n3. & Pr ((Ap \\rightarrow p) \\wedge Ap) \\vee ((Ap \\rightarrow p) \\wedge \\neg Ap)) = \\\\&Pr ((Ap \\rightarrow p) \\wedge Ap) + Pr ((Ap \\rightarrow p) \\wedge \\neg Ap)  \n & \\text{P3$^*$}  \\\\\n4. &Pr((Ap \\rightarrow p) \\wedge Ap) = Pr (Ap)Pr(Ap \\rightarrow p|Ap) & \\text{P6} \\\\\n5. &Pr((Ap \\rightarrow p) \\wedge \\neg Ap) = Pr(\\neg Ap)Pr(Ap \\rightarrow p |\\neg Ap) & \\text{P6} \\\\\n6. &Pr(Ap \\rightarrow p) = \\\\&Pr(Ap)Pr(Ap \\rightarrow p|Ap) + Pr (\\neg Ap)Pr(Ap \\rightarrow p |\\neg Ap) & \\text{2, 4, 5} \\\\\n7. &(Ap \\rightarrow p) \\wedge Ap \\dashv \\vdash \\neg Ap & \\text{} \\\\\n8. &Pr((Ap \\rightarrow p) \\wedge Ap) = Pr(\\neg Ap) & \\text{7, P2$^*$} \\\\\n9. &Pr(Ap \\rightarrow p |\\neg Ap) = 1 \\text{ or } Pr(\\neg Ap) = 0 & \\text{8, P6}  \\\\\n10. &Pr(Ap \\rightarrow p | Ap) \\leq 1 & \\text{P4, P5} \\\\\n11. &Pr(Ap \\rightarrow p) \\geq \\\\ &Pr(Ap)Pr(Ap \\rightarrow p|Ap) + Pr (\\neg Ap)Pr(Ap \\rightarrow p |Ap)  & \\text{6, 9, 10} \\\\\n12. &\\vdash Ap \\vee \\neg Ap & \\text{} \\\\\n13. &Pr(Ap \\vee \\neg Ap) = 1 & \\text{12, P1} \\\\\n14. &Pr(Ap) + Pr (\\neg Ap) = 1 & \\text{13, P3$^*$} \\\\\n15. &Pr(Ap \\rightarrow p ) \\geq Pr (Ap \\rightarrow p|Ap) & \\text{11, 14} \\end{aligned}\\] Note (11) is an equality iff (8) is. The only step there that may not be obvious is step 10. The reason it holds is that either \\(Ap\\) is a \\(\\vdash\\)-antitheorem or it isn’t. If it is, then it entails \\(Ap \\rightarrow p\\), so by P5, \\(Pr(Ap \\rightarrow p | Ap) \\leq 1\\). If it is not, then by P1\\(^*\\), \\(Pr(x | Ap) \\leq 1\\) for any \\(x\\), so \\(Pr(Ap \\rightarrow p | Ap) \\leq 1\\).\n\nTheorem 2\nIf \\(Pr\\) is a classical probability function, then\n\\(Pr(\\neg(Ap \\wedge \\neg p) | Ap) \\leq Pr(\\neg(Ap \\wedge \\neg p))\\); and\n\\(Pr(\\neg Ap \\vee p | Ap) \\leq Pr(\\neg Ap \\vee p)\\).\n\nProof: Assume \\(Pr\\) is a classical probability function, and \\(\\vdash\\) the classical consequence relation. \\[\\begin{aligned}\n1. &Ap \\rightarrow p \\dashv  \\vdash \\neg(Ap \\wedge \\neg p) &  \\\\\n2. &Pr(Ap \\rightarrow p) = Pr(\\neg(Ap \\wedge \\neg p)) & 1, P2^* \\\\\n3. &Pr(Ap \\rightarrow p | Ap) = Pr(\\neg(Ap \\wedge \\neg p) | Ap) & 1, P4, P5 \\\\\n4. &Pr(Ap \\rightarrow p ) \\geq Pr (Ap \\rightarrow p|Ap) & \\text{Theorem 1} \\\\\n5. &Pr(\\neg(Ap \\wedge \\neg p) | Ap) \\geq Pr(\\neg(Ap \\wedge \\neg p)) & 2, 3, 4 \\\\\n6. &Ap \\rightarrow p \\dashv  \\vdash \\neg Ap \\vee p &  \\\\\n7. &Pr(Ap \\rightarrow p) = Pr(\\neg Ap \\vee p) & 6, P2^* \\\\\n8. &Pr(Ap \\rightarrow p | Ap) = Pr(\\neg Ap \\vee p | Ap) & 6, P4, P5 \\\\\n9. &Pr(\\neg Ap \\vee p | Ap) \\geq Pr(\\neg Ap \\vee p) & 4, 7, 8\\end{aligned}\\]\nThe only minor complication is with step 3. There are two cases to consider, either \\(Ap\\) is a \\(\\vdash\\)-antitheorem or it isn’t. If it is a \\(\\vdash\\)-antitheorem, then both the LHS and RHS of (3) equal 1, so they are equal. If it is not a \\(\\vdash\\)-antitheorem, then by P4, \\(Pr(\\cdot | Ap)\\) is a probability function. So by P2\\(^*\\), and the fact that \\(Ap \\rightarrow p \\dashv \\vdash \\neg(Ap \\wedge \\neg p)\\), we have that the LHS and RHS are equal.\n\nTheorem 3.\nIn \\(M\\), for any \\(x \\in (0, 1)\\),\n\\(Pr_x(Ap \\rightarrow p)\\) = \\(Pr_x((Ap \\rightarrow p) \\wedge Ap) = x\\)\n\\(Pr_x(\\neg Ap \\vee p)\\) = \\(Pr_x((\\neg Ap \\vee p) \\wedge Ap) = x\\)\n\\(Pr_x(\\neg(Ap \\wedge \\neg p))\\) = \\(Pr_x(\\neg(Ap \\wedge \\neg p) \\wedge Ap) = x\\)\n\nRecall what \\(M\\) looks like.\n\n(70, 50) (35, 5)(-1, 1)30 (35, 5)(1, 1)30 (35,5) (4.8,35.5) (65.2,35.5) (28, 5)\\(1\\) (0,35.5)\\(2\\) (60,35.5)\\(3\\) (7,35.5)\\(Ap, p\\) (67,35.5)\\(Ap\\)\n\nThe only point where \\(Ap \\rightarrow p\\) is true is at 2. Indeed, \\(\\neg(Ap \\rightarrow p)\\) is true at 3, and neither \\(Ap \\rightarrow p\\) nor \\(\\neg(Ap \\rightarrow p)\\) are true at 1. So \\(Pr_x(Ap \\rightarrow p) = m_x(\\{2\\}) = x\\). Since Ap is also true at 2, that’s the only point where \\((Ap \\rightarrow p) \\wedge Ap\\) is true. So it follows that \\(Pr_x((Ap \\rightarrow p) \\wedge Ap) = m_x(\\{2\\}) = x\\).\nSimilar inspection of the model shows that 2 is the only point where \\(\\neg(Ap \\wedge \\neg p)\\) is true, and the only point where \\(\\neg Ap \\vee p\\) is true. And so (b) and (c) follow in just the same way.\nIn slight contrast, \\(Ap\\) is true at two points in the model, 2 and 3. But since \\(m_x(\\{3\\}) = 0\\), it follows that \\(m_x(\\{2, 3\\}) = m_x(\\{2\\}) = x\\). So \\(Pr_x(Ap) = x\\).\n\nTheorem 4.\nFor any \\(x \\in (0, 1)\\),\n\\(1 = Pr_x(Ap \\rightarrow p | Ap) > Pr_x(Ap \\rightarrow p) = x\\)\n\\(1 = Pr_x(\\neg Ap \\vee p | Ap) > Pr_x(\\neg Ap \\vee p) = x\\)\n\\(1 = Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) > Pr_x(\\neg(Ap \\wedge \\neg p)) = x\\)\n\nWe’ll just go through the argument for (a); the other cases are similar. By P6, we know that \\(Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) Pr_x(Ap) = Pr_x((Ap \\rightarrow p) \\wedge Ap)\\). By Theorem 3, we know that \\(Pr_x(Ap) = Pr_x((Ap \\rightarrow p) \\wedge Ap)\\), and that both sides are greater than 0. (Note that the theorem is only said to hold for \\(x > 0\\).) The only way both these equations can hold is if \\(Pr_x(\\neg(Ap \\wedge \\neg p) | Ap) = 1\\). Note also that by hypothesis, \\(x < 1\\), and from this claim (a) follows. The other two cases are completely similar.\n\nTheorem 5 Let \\(Pr_1\\) be a regular conditional \\(\\vdash_{CL}\\)-probability function, and \\(Pr_2\\) be a regular conditional \\(\\vdash_{IL}\\)-probability function that is not a \\(\\vdash_{CL}\\)-probability function. And let \\(Pr_3\\) be defined as in the text. (That is, \\(Pr_3(A) = xPr_1(A) + (1-x)Pr_2(A)\\), and \\(Pr_3(A | B) = \\frac{Pr_3(A \\wedge B)}{Pr_3(B)}\\).) Then \\(Pr_3\\) is a regular conditional \\(\\vdash_{IL}\\)-probability function.\n\nWe first prove that \\(Pr_3\\) satisfies the requirements of an unconditional \\(\\vdash_{IL}\\)-probability function, and then show that it satisfies the requirements of a conditional \\(\\vdash_{IL}\\)-probability function.\nIf \\(p\\) is an \\(\\vdash_{IL}\\)-antithesis, then it is also a \\(\\vdash_{CL}\\)-antithesis. So \\(Pr_1(p) = Pr_2(p) = 0\\). So \\(Pr_3(A) = 0x + 0(1-x) = 0\\), as required for (P0).\nIf \\(p\\) is an \\(\\vdash_{IL}\\)-thesis, then it is also a \\(\\vdash_{CL}\\)-thesis. So \\(Pr_1(p) = Pr_2(p) = 1\\). So \\(Pr_3(p) = x + (1-x) = 1\\), as required for (P1).\nIf \\(p \\vdash_{IL} q\\) then \\(p \\vdash_{CL} q\\). So we have both \\(Pr_1(p) \\leq Pr(q)\\) and \\(Pr_2(p) \\leq Pr_2(q)\\). Since \\(x \\geq 0\\) and \\((1-x) \\geq 0\\), these inequalities imply that \\(xPr_1(p) \\leq xPr(q)\\) and \\((1-x)Pr_2(p) \\leq (1-x)Pr_2(q)\\). Summing these, we get \\(xPr_1(p) + (1-x)Pr_2(p) \\leq xPr_1(q) + (1-x)Pr_2(q)\\). And by the definition of \\(Pr_3\\), that means that \\(Pr_3(p) \\leq Pr_3(q)\\), as required for (P2).\nFinally, we just need to show that \\(Pr_3(p) + Pr_3(q) = Pr_3(p \\vee q) + Pr_3(p \\wedge q)\\), as follows:\n\\[\\begin{aligned}\nPr_3(p) + Pr_3(q) &= xPr_1(p) + (1-x)Pr_2(p) + xPr_1(q) + (1-x)Pr_2(q) \\\\\n &= x(Pr_1(p) + Pr_1(q)) + (1-x)(Pr_2(p) + Pr_2(q)) \\\\\n &= x(Pr_1(p \\vee q) + Pr_1(p \\wedge q)) + (1-x)(Pr_2(p \\vee q) + Pr_2(p \\wedge q)) \\\\\n &= xPr_1(p \\vee q) + (1-x)Pr_2(p \\vee q) + xPr_1(p \\wedge q)) + (1-x)Pr_2(p \\wedge q) \\\\\n &= Pr_3(p \\vee q) + Pr_3(p \\wedge q) \\text{ as required}\\end{aligned}\\]\nNow that we have shown \\(Pr_3\\) is an unconditional \\(\\vdash_{IL}\\)-probability function, we need to show it is a conditional \\(\\vdash_{IL}\\)-probability function, where \\(Pr_3(p | r) =_{df} \\frac{Pr_3(p \\wedge r)}{Pr_3(r)}\\). Remember we are assuming that both \\(Pr_1\\) and \\(Pr_2\\) are regular, from which it clearly follows that \\(Pr_3\\) is regular, so this definition is always in order. (That is, we’re never dividing by zero.) The longest part of showing \\(Pr_3\\) is a conditional \\(\\vdash_{IL}\\)-probability function is showing that it satisfies (P4), which has four parts. We need to show that \\(Pr(\\cdot | r)\\) satisfies (P0)-(P3). Fortunately these are fairly straightforward.\nIf \\(p\\) is an \\(\\vdash_{IL}\\)-antithesis, then so is \\(p \\wedge r\\). So \\(Pr_3(p \\wedge r) = 0\\), so \\(Pr_3(p | r) = 0\\), as required for (P0).\nIf \\(p\\) is an \\(\\vdash_{IL}\\)-thesis, then \\(p \\wedge r \\dashv \\vdash r\\), so \\(Pr_3(p \\wedge r) = Pr_3(r)\\), so \\(Pr_3(p | r) = 1\\), as required for (P1).\nIf \\(p \\vdash_{IL} q\\) then \\(p \\wedge r \\vdash_{IL} q \\wedge r\\). So \\(Pr_3(p \\wedge r) \\leq Pr_3(q \\wedge r)\\). So \\(\\frac{Pr_3(p \\wedge r)}{Pr_3(r)} \\leq \\frac{Pr_3(q \\wedge r)}{Pr_3(r)}\\). That is, \\(Pr_3(p | r) \\leq Pr_3(q | r)\\), as required for (P2).\nFinally, we need to show that \\(Pr_3(p | r) + Pr_3(q | r) = Pr_3(p \\vee q | r) + Pr_3(p \\wedge q | r)\\), as follows, making repeated use of the fact that \\(Pr_3\\) is an unconditional \\(\\vdash_{IL}\\)-probability function, so we can assume it satisfies (P3), and that we can substitute intuitionistic equivalences inside \\(Pr_3\\).\n\\[\\begin{aligned}\nPr_3(p | r) + Pr_3(q | r) &= \\frac{Pr_3(p \\wedge r)}{Pr_3(r)} + \\frac{Pr_3(q \\wedge r)}{Pr_3(r)} \\\\\n&= \\frac{Pr_3(p \\wedge r) + Pr(q \\wedge r)}{Pr_3(r)} \\\\\n&= \\frac{Pr_3((p \\wedge r) \\vee (q \\wedge r)) + Pr_3((p \\wedge r) \\wedge (q \\wedge r))}{Pr_3(r)} \\\\\n&=\\frac{Pr_3(p \\vee q) \\wedge r) + Pr_3((p \\wedge q) \\wedge r)}{Pr_3(r)} \\\\\n&=\\frac{Pr_3(p \\vee q) \\wedge r)}{Pr_3(r)} + \\frac{Pr_3((p \\wedge q) \\wedge r)}{Pr_3(r)} \\\\\n&=Pr_3(p \\vee q | r) + Pr_3(p \\wedge q | r) \\text{ as required}\\end{aligned}\\]\nNow if \\(r \\vdash_{IL} p\\), then \\(r \\wedge p ~_{IL}\\dashv \\vdash_{IL} p\\), so \\(Pr_3(r \\wedge p) = Pr_3(p)\\), so \\(Pr_3(p | r) = 1\\), as required for (P5).\nFinally, we show that \\(Pr_3\\) satisfies (P6).\n\\[\\begin{aligned}\nPr_3(p \\wedge q | r) &= \\frac{Pr_3(p \\wedge q \\wedge r)}{Pr_3(r)} \\\\\n &= \\frac{Pr_3(p \\wedge q \\wedge r)}{Pr_3(q \\wedge r)} \\frac{Pr_3(q \\wedge r)}{Pr_3(r)} \\\\\n &=Pr_3(p | q \\wedge r) Pr_3(q | r) \\text{ as required}\\end{aligned}\\]\n\nTheorem 6 Let \\(x\\) be any real in \\((0, 1)\\). Then there is a probability function \\(Cr\\) that (a) is a coherent credence function for someone whose credence that classical logic is correct is \\(x\\), and (b) satisfies each of the following inequalities: \\[\\begin{aligned}\nPr(Ap \\rightarrow p | Ap) &> Pr(Ap \\rightarrow p) \\\\\nPr(\\neg Ap \\vee p | Ap) &> Pr(\\neg Ap \\vee p) \\\\\nPr(\\neg(Ap \\wedge \\neg p) | Ap) &> Pr(\\neg(Ap \\wedge \\neg p)) \\end{aligned}\\]\n\nWe’ll prove this by constructing the function \\(Pr\\). For the sake of this proof, we’ll assume a very restricted formal language with just two atomic sentences: \\(Ap\\) and \\(p\\). This restriction makes it easier to ensure that the functions are all regular, which as we noted in the main text lets us avoid various complications. The proofs will rely on three probability functions defined using this Kripke tree \\(M\\).\n\n(100, 40) (50, 5)(-3, 2)45 (50, 5)(-1, 2)15 (50, 5)(1, 2)15 (50, 5)(3, 2)45 (50,5) (4.5,35.5) (65.2,35.5) (34.8,35.5) (95.5,35.5) (42, 5)\\(0\\) (0,35.5)\\(1\\) (30,35.5)\\(2\\) (60,35.5)\\(3\\) (90,35.5)\\(4\\) (7,35.5)\\(Ap, p\\) (37,35.5)\\(Ap\\) (67,35.5)\\(p\\)\n\nWe’ve shown on the graph where the atomic sentences true: \\(Ap\\) is true at 1 and 2, and \\(p\\) is true at 1 and 3. So the four terminal nodes represent the four classical possibilities that are definable using just these two atomic sentences. We define two measure functions \\(m_1\\) and \\(m_2\\) over the points in this model as follows:\n\n\n\\(m(\\{0\\})\\)\n\\(m(\\{1\\})\\)\n\\(m(\\{2\\})\\)\n\\(m(\\{3\\})\\)\n\\(m(\\{4\\})\\)\n\\(m_1\\)\n0\n\\(\\frac{x}{2}\\)\n\\(\\frac{1-x}{2}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(m_2\\)\n\\(\\frac{x}{2}\\)\n\\(\\frac{1-x}{4}\\)\n\\(\\frac{1-x}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\nWe’ve just specified the measure of each singleton, but since we’re just dealing with a finite model, that uniquely specifies the measure of any set. We then turn each of these into probability functions in the way described in section 1. That is, for any proposition \\(X\\), and \\(i \\in \\{1, 2\\}\\), \\(Pr_i(X) = m_i(M_X)\\), where \\(M_X\\) is the set of points in \\(M\\) where \\(X\\) is true.\nNote that the terminal nodes in \\(M\\), like the terminal nodes in any Kripke tree, are just classical possibilities. That is, for any sentence, either it or its negation is true at a terminal node. Moreover, any measure over classical possibilities generates a classical probability function. (And vice versa, any classical probability function is generated by a measure over classical possibilities.) That is, for any measure over classical possibilities, the function from propositions to the measure of the set of possibilities at which they are true is a classical probability function. Now \\(m_1\\) isn’t quite a measure over classical possibilities, since strictly speaking \\(m_1(\\{0\\})\\) is defined. But since \\(m_1(\\{0\\}) = 0\\) it is equivalent to a measure only defined over the terminal nodes. So the probability function it generates, i.e., \\(Pr_1\\), is a classical probability function.Of course, with only two atomic sentences, we can also verify by brute force that \\(Pr_1\\) is classical, but it’s a little more helpful to see why this is so. In contrast, \\(Pr_2\\) is not a classical probability function, since \\(Pr_2(p \\vee \\neg p) = 1 - \\frac{x}{2}\\), but it is an intuitionistic probability function.\nSo there could be an agent who satisfies the following four conditions:\nHer credence that classical logic is correct is \\(x\\);\nHer credence that intuitionistic logic is correct is \\(1-x\\);\nConditional on classical logic being correct, she thinks that \\(Pr_1\\) is the right representation of how things probably are; and\nConditional on intuitionistic logic being correct, she thinks that \\(Pr_2\\) is the right representation of how things are.\nSuch an agent’s credences will be given by a \\(\\vdash_{IL}\\)-probability function \\(Pr\\) generated by ‘mixing’ \\(Pr_1\\) and \\(Pr_2\\). For any sentence \\(Y\\) in the domain, her credence in \\(Y\\) will be \\(xPr_1(Y) + (1-x)Pr_2(Y)\\). Rather than working through each proposition, it’s easiest to represent this function by mixing the measures \\(m_1\\) and \\(m_2\\) to get a new measure \\(m\\) on the above Kripke tree. Here’s the measure that \\(m\\) assigns to each node.\n\n\n\\(m(\\{0\\})\\)\n\\(m(\\{1\\})\\)\n\\(m(\\{2\\})\\)\n\\(m(\\{3\\})\\)\n\\(m(\\{4\\})\\)\n\\(m\\)\n\\(\\frac{x(1-x)}{2}\\)\n\\(\\frac{3x^2 - 2x + 1}{4}\\)\n\\(\\frac{1-x^2}{4}\\)\n\\(\\frac{1}{4}\\)\n\\(\\frac{1}{4}\\)\n\nAs usual, this measure \\(m\\) generates a probability function \\(Pr\\). We’ve already argued that \\(Pr\\) is a reasonable function for someone whose credence that classical logic is \\(x\\). We’ll now argue that \\(Pr(Ap \\rightarrow p | Ap) > Pr(Ap \\rightarrow p)\\).\nIt’s easy to see what \\(Pr(Ap \\rightarrow p)\\) is. \\(Ap \\rightarrow p\\) is true at 1, 3 and 4, so\n\\[\\begin{aligned}\nPr(Ap \\rightarrow p) &= m({1}) + m({3}) + m(4) \\\\\n &= \\frac{3x^2 - 2x + 1}{4} + \\frac{1}{4} + \\frac{1}{4} \\\\\n &= \\frac{3x^2 - 2x + 3}{4} \\end{aligned}\\]\nSince \\(Pr\\) is regular, we can use the ratio definition of conditional probability to work out \\(Pr(Ap \\rightarrow p | Ap)\\).\n\\[\\begin{aligned}\nPr(Ap \\rightarrow p | Ap) &= \\frac{Pr((Ap \\rightarrow p) \\wedge Ap)}{Pr(Ap)} \\\\\n &= \\frac{m({1})}{m({1}) + m({2})} \\\\\n &= \\frac{\\frac{3x^2 - 2x + 1}{4}}{\\frac{3x^2 - 2x + 1}{4} + \\frac{1-x^2}{4}} \\\\\n &= \\frac{3x^2 - 2x + 1}{(3x^2 - 2x + 1) + (1-x^2)} \\\\\n &= \\frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\end{aligned}\\]\nPutting all that together, we have\n\\[\\begin{aligned}\n&& Pr(Ap \\rightarrow p | Ap) &> Pr(Ap \\rightarrow p) \\\\\n\\Leftrightarrow &&  \\frac{3x^2 - 2x + 3}{4}  &> \\frac{3x^2 - 2x + 1}{2(x^2 - x + 1)} \\\\\n\\Leftrightarrow && 3x^2 - 2x + 3  &> \\frac{6x^2 - 4x + 2}{x^2 - x + 1} \\\\\n\\Leftrightarrow && (3x^2 - 2x + 3)(x^2 + x + 1)  &> 6x^2 - 4x + 2 \\\\\n\\Leftrightarrow && 3x^4 - 5x^3 + 8x^2 - 5x + 3  &> 6x^2 - 4x + 2 \\\\\n\\Leftrightarrow && 3x^4 - 5x^3 + 2x^2 - x + 1 &> 0 \\\\\n\\Leftrightarrow && (3x^2 + x + 1)(x^2 - 2x + 1) &> 0 \\\\\n\\Leftrightarrow && (3x^2 + x + 1)(x - 1)^2 &> 0\\end{aligned}\\]\nBut it is clear that for any \\(x \\in (0,1)\\), both of the terms of the LHS of the final line are positive, so their product is positive. And that means \\(Pr(Ap \\rightarrow p | Ap) > Pr(Ap \\rightarrow p)\\). So no matter how close \\(x\\) gets to 1, that is, no matter how certain the agent gets that classical logic is correct, as long as \\(x\\) does not reach 1, conditionalising on \\(Ap\\) will raise the probability of \\(Ap \\rightarrow p\\). As we’ve been arguing, as long as there is any doubt about classical logic, even a vanishingly small doubt, there is no probabilistic objection to dogmatism.\nTo finish up, we show that \\(Pr(\\neg Ap \\vee p | Ap) > Pr(\\neg Ap \\vee p)\\) and \\(Pr(\\neg(Ap \\wedge \\neg p) | Ap) > Pr(\\neg(Ap \\wedge \\neg p))\\). To do this, we just need to note that \\(Ap \\rightarrow p\\), \\(\\neg Ap \\vee p\\) and \\(\\neg(Ap \\wedge \\neg p)\\) are true at the same points in the model, so their probabilities, both unconditionally and conditional on \\(Ap\\), will be identical. So from \\(Pr(Ap \\rightarrow p | Ap) > Pr(Ap \\rightarrow p)\\) the other two inequalities follow immediately.\n\n\nCohen, Stewart. 2005. “Why Basic Knowledge Is Easy Knowledge.” Philosophy and Phenomenological Research 70 (2): 417–30. https://doi.org/10.1111/j.1933-1592.2005.tb00536.x.\n\n\nHájek, Alan. 2003. “What Conditional Probability Could Not Be.” Synthese 137 (3): 273–323. https://doi.org/10.1023/B:SYNT.0000004904.91112.16.\n\n\nJehle, David. 2009. “Some Results in Bayesian Confirmation Theory with Applications.” PhD thesis, Cornell University.\n\n\nJehle, David, and Branden Fitelson. 2009. “What Is the ‘Equal Weight View?’.” Episteme 6 (3): 280–93. https://doi.org/10.3366/E1742360009000719.\n\n\nKripke, Saul. 1965. “Semantical Analysis of Intuitionistic Logic.” In Formal Systems and Recursive Functions, edited by Michael Dummett and John Crossley. Amsterdam: North-Holland.\n\n\nPopper, Karl, and David Miller. 1987. “Why Probabilistic Support Is Not Inductive.” Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences 321 (1562): 569–91. https://doi.org/10.1098/rsta.1987.0033.\n\n\nPryor, James. 2000. “The Sceptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\n———. 2004. “What’s Wrong with Moore’s Argument?” Philosophical Issues 14 (1): 349–78. https://doi.org/10.1111/j.1533-6077.2004.00034.x.\n\n\nWeatherson, Brian. 2003. “From Classical to Intuitionistic Probability.” Notre Dame Journal of Formal Logic 44 (2): 111–23. https://doi.org/10.1305/ndjfl/1082637807.\n\n\n———. 2007. “The Bayesian and the Dogmatist.” Proceedings of the Aristotelian Society 107: 169–85. https://doi.org/10.1111/j.1467-9264.2007.00217.x.\n\n\nWhite, Roger. 2006. “Problems for Dogmatism.” Philosophical Studies 131 (3): 525–57. https://doi.org/10.1007/s11098-004-7487-9.\n\n\nWilliams, J. R. G. 2012. “Gradational Accuracy and Non-Classical Semantics.” Review of Symbolic Logic 5 (4): 513–37. https://doi.org/10.1017/S1755020312000214.\n\n\nWe’re assuming here that the agent’s evidence really is Ap, not \\(p\\). That’s a controversial assumption, but it isn’t at issue in this debate.↩︎\nPopper and Miller (1987) prove a stronger result than Theorem One, and note its significance for probabilistic models of learning.↩︎\nWe’ll usually assume that the language of \\(\\vdash\\) is a familiar kind of propositional calculus, with a countable infinity of sentence letters, and satisfying the usual recursive constraints. That is, if \\(A\\) and \\(B\\) are sentences of the language, then so are \\(\\neg A\\), \\(A \\rightarrow B\\), \\(A \\wedge B\\) and \\(A \\vee B\\). It isn’t entirely trivial to extend some of our results to a language that contains quantifiers. This is because once we add quantifiers, intuitionistic and classical logic no longer have the same anti-theorems. But that complication is outside the scope of this paper. Note that for Theorem 6, we assume a restricted language with just two sentence letters. This merely simplifies the proof. A version of the construction we use there with those two letters being simply the first two sentence letters would be similar, but somewhat more complicated.↩︎\nWeatherson (2003) discusses what happens if we make P2\\(^*\\) or P3\\(^*\\) an axiom in place of either P2 and P3. It is argued there that this gives us too many functions to be useful in epistemology. The arguments in Williams (2012) provide much stronger reasons for believing this conclusion is correct.↩︎\nFor the reasons given in Hájek (2003), it is probably better in general to take conditional probability as primitive. But for our purposes taking unconditional probability to be basic won’t lead to any problems, so we’ll stay neutral on whether conditional or unconditional probability is really primitive.\n\n↩︎\n",
    "preview": "posts/2021-03-05-dogmatism-probability-and-logical-uncertainty/standrews.jpg",
    "last_modified": "2021-03-05T14:47:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-stalnaker-on-sleeping-beauty/",
    "title": "Stalnaker on Sleeping Beauty",
    "description": "A contribution to a book symposium on Stalnaker's Our Knowledge of the Internal World, focussing on the way his framework helps cast new light on the Sleeping Beauty problem.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2011-09-01",
    "categories": [
      "epistemology",
      "games and decisions",
      "on books"
    ],
    "contents": "\n\nContents\nStalnaker on Self-Location\nMonty Hall\nStalnaker on Sleeping Ugly\n\nThe Sleeping Beauty puzzle provides a nice illustration of the approach to self-locating belief defended by Robert Stalnaker in Our Knowledge of the Internal World (Stalnaker 2008), as well as a test of the utility of that method. The setup of the Sleeping Beauty puzzle is by now fairly familiar. On Sunday Sleeping Beauty is told the rules of the game, and a (known to be) fair coin is flipped. On Monday, Sleeping Beauty is woken, and then put back to sleep. If, and only if, the coin landed tails, she is woken again on Tuesday after having her memory of the Monday awakening erased.1 On Wednesday she is woken again and the game ends. There are a few questions we can ask about Beauty’s attitudes as the game progresses. We’d like to know what her credence that the coin landed heads should be\n\nPublished in Philosophical Studies 155: 445–456.\nBefore she goes to sleep Sunday;\nWhen she wakes on Monday;\nWhen she wakes on Tuesday; and\nWhen she wakes on Wednesday?\nStandard treatments of the Sleeping Beauty puzzle ignore (d), run together (b) and (c) into one (somewhat ill-formed) question, and then divide theorists into ‘halfers’ or ‘thirders’ depending on how they answer it. Following Stalnaker, I’m going to focus on (b) here, though I’ll have a little to say about (c) and (d) as well. I’ll be following orthodoxy in taking \\(\\frac{1}{2}\\) to be the clear answer to (a), and in taking the correct answers to (b) and (c) to be independent of how the coin lands, though I’ll briefly question that assumption at the end.\nAn answer to these four questions should respect two different kinds of constraints. The answer for day \\(n\\) should make sense ‘statically.’ It should be a sensible answer to the question of what Beauty should do given what information she then has. And the answer should make sense ‘dynamically.’ It should be a sensible answer to the question of how Beauty should have updated her credences from some earlier day, given rational credences on the earlier day.\nAs has been fairly clear since the discussion of the problem in Elga (2000), Sleeping Beauty is puzzling because static and dynamic considerations appear to push in different directions. The static considerations apparently favour a \\(\\frac{1}{3}\\) answer to (b). When Beauty wakes, there are three options available to her: It is Monday and the coin landed heads; It is Monday and the coin landed tails; It is Tuesday and the coin landed tails. If we can argue that each of those are equally probable given her evidence, we get the answer \\(\\frac{1}{3}\\). The dynamic considerations apparently favour a \\(\\frac{1}{2}\\) answer to (b). The right answer to (a) is \\(\\frac{1}{2}\\). Nothing happens on Monday or Tuesday that surprises Beauty. And credences should only change if we are surprised. So the right answer to (b) is \\(\\frac{1}{2}\\).\nSince we must have harmony between dynamic and static considerations, one of these arguments must be misguided. (In fact, I think both are, to some degree.) These days there is a cottage industry of ‘thirders’ developing accounts of credal dynamics that accord with the \\(\\frac{1}{3}\\) answer to (b).2 But all of these accounts are considerably more complex than the traditional, conditionalisation-based, dynamic theory that we all grew up with.\nThree of the many attractions of Robert Stalnaker’s new account of self-locating knowledge are (i) that it offers a way to answer all four of our questions about Sleeping Beauty, (ii) that it does so while remaining both statically and dynamically plausible, and (iii) that the dynamic theory involved is, in large part, traditional conditionalisation. I spend most of this note setting out Stalnaker’s account, and setting out his derivation of a \\(\\frac{1}{3}\\) answer to (b). I conclude with some reasons for preferring a slightly different solution of the Sleeping Beauty puzzle within the broad framework Stalnaker suggests.\nStalnaker on Self-Location\nThe picture of self-locating belief that we get from Lewis’s “Attitudes De Dicto and De Se” (Lewis 1979) has been widely adopted in recent years.3 On Lewis’s picture, the content of an attitude is a set of centered worlds. For current purposes we’ll take to centered worlds to be \\(\\langle\\)world, agent, time\\(\\rangle\\) triples. To believe that \\(S\\), where \\(S\\) is a set of centered worlds, is to believe that the triple \\(\\langle\\)your world, you, now\\(\\rangle \\in S\\).\nThe motivation for this picture comes from reflection on how to represent locational uncertainty. If you’re sure where in New York City you are, you can pick out a point on a map and say “I’m there.” If you’re not sure exactly where you are, but you have some information, you can pick out a region on the map and say “I’m somewhere in that region.” If you’re not sure who you are, but you know where everyone is, you can do the same kind of thing. And it’s plausible that this is a (somewhat) realistic situation. As one modern-day Lewisian, Andy Egan, puts it ‘I can believe that my pants are on fire without believing that Egan’s pants are on fire, and I can hope that someone turns a fire extinguisher on me right now without hoping that someone turns a fire extinguisher on Egan at 5:41pm.’’ (Egan 2004, 64) There is an important puzzle here that needs to be addressed, and can’t obviously be addressed in the framework Lewis accepted before 1979, where the content of a propositional attitude is a set of Lewisian concreta. If possible worlds are Lewisian concreta, then Lewisians like Egan are correct to respond to puzzles about location by saying, “sometimes (as when we want to know who or where we are) the world is not enough.” (Egan 2004, 64)\nBut this response is too self-centered. Not all locational thoughts are self-locational thoughts. I can be just as uncertain about where that is as about where this is, or as uncertain about who you are as about who I am. Imagine I’m watching Egan’s unfortunate adventures with his infernal trousers on a delayed video tape. I can believe his pants are on fire without believing Egan’s pants are on fire, and hope that someone turns a fire extinguisher on him then without hoping some turns a fire extinguisher on Egan at 5:41pm. Or, at least, that way of putting things sounds just as good as Egan’s original description of the case.\nFor a different example, imagine I wake at night and come to believe it is midnight. As Lewis would represent it, I believe \\(\\langle w, \\text{me}, \\text{now}\\rangle \\in \\{\\langle w, s, t\\rangle : t = \\text{midnight}\\}\\). When I wake, I think back to that belief, and judge that I may have been mistaken. How should we represent this? Not that I now believe \\(\\langle w, \\text{me}, \\text{now}\\rangle \\notin \\{\\langle w, s, t\\rangle : t = \\text{midnight}\\}\\). That’s obviously true - I know the sun is up. We want to represent something more contentious.\nThe best, I think, the Lewisian can do is to pick out some description \\(D\\) of my earlier belief and say what I believe is \\(\\langle w, \\text{me}, \\text{now}\\rangle \\notin \\{\\langle w, s, t\\rangle : (\\iota x: Dx) x\\) happens at midnight}. That is, I believe the belief that satisfies \\(D\\) doesn’t happen at midnight. Is that good enough? Well, we might imagine the debate continuing with the anti-Lewisian proposing cases where \\(D\\) will not be unique (because of forgotten similar beliefs) or will not be satisfied (because of a misrecollection of the circumstances of the belief), and so this approach will fail. And we might imagine the Lewisian responding by complicating \\(D\\), or by denying that in these cases we really do have beliefs about our earlier beliefs. In other words, we can imagine the familiar debates about descriptivism about names being replayed as debates about descriptivism about prior beliefs. As enjoyable as that may be, it’s interesting to consider a different approach.\nThere’s a more philosophical reason to worry about Lewis’s model. If we model uncertainty as a class of relationships to possible worlds, it looks like there’s a lot of actual uncertainty we won’t be able to model. Indeed, there are three kinds of uncertainty that we can’t model in this framework. First, we can’t model uncertainty about logic and mathematics. Second, if we accept the necessity of identity, we can’t model uncertainty about identity claims. Whatever it is to be uncertain about whether \\(a\\) is \\(b\\), it won’t be a distinctive relation to the set of worlds in which \\(a\\) is \\(b\\), since that’s all the worlds. Third, we can’t model uncertainty about claims about self-identity, like I’m that guy. Lewis’s framework is an improvement on the sets of possible worlds approach because it helps with this third class of cases. But it doesn’t help with the first or, more importantly, with the second. We might think that a solution to puzzles about self-identity should generalise to solve puzzles about identity more broadly. Lewis’s model doesn’t. One of Stalnaker’s key insights is that we should, and can, have a model that addresses both kinds of puzzles about identity.\nOn Stalnaker’s model, a belief is just a distinction between worlds. The content of a belief is a set of worlds, not a set of centered worlds. But worlds have more structure than we thought they had. The formal model is a bit more subtle than what I’ll sketch here, but I think I’ll include enough detail to cover the Sleeping Beauty case. In each world, each center, in Lewis’s sense, has a haecceity. A world is the Cartesian product of a Lewisian world, i.e. a world without haecceities, and a function from each contextually salient haecceity to a location. If we see a kiss, and wonder who she is, who he is, and when they are kissing, then we can think of the worlds as quadruples consisting of a haecceity-free world (perhaps a Lewisian concreta), a woman, a man and a time. So we can represent three kinds of locational doubts, not just self-locational doubt.4\nWhen an agent at center \\(c\\) believes something self-locating, e.g. that it is Monday, the content of their belief is that \\(c\\)’s haecceity is on a Monday. If they don’t know what day it is, there’s a sense in which they don’t know what they believe, since they don’t know whether what they are believing is that \\(c\\)’s center is on Monday, or that some other center’s haecceity is on Monday.5 But their belief, the belief they would express on Monday by saying “It is Monday,” has two nice features. First, it is neither trivial, like the belief that Monday is Monday, nor changing in value over time, since \\(c\\)’s center is always on Monday. Second, it is the kind of belief that people on days other than Monday can share, or dispute. And this belief can be shared by others who have the capacity to think de re about \\(c\\), even if they can’t uniquely describe it. It’s this last fact that lets Stalnaker handle the cases that proved problematic for Lewis and the neo-Lewisians. For instance, it lets Stalnaker model shared uncertainty about identity claims.\nWith all that in place, it’s time to return to Sleeping Beauty. Let’s consider two propositions. The first, \\(H\\), is that the coin landed heads. The second, \\(M\\), is what Beauty can express when she wakes on Monday by saying “It is Monday.” That is, it is a singular proposition about a wakening experience that Beauty can now have singular thoughts about (since she is now undergoing it), but which she didn’t previously have the capacity to determinately pick out. We’ll call this wakening \\(a\\). (Beauty might undergo multiple wakenings, but we’re going to focus on one for now, and call it \\(a\\).) Given these three propositions, we can describe four possibilities. Or, as we’ll somewhat inaccurately describe them, four worlds.6\n\\(H \\wedge M\\)\n\\(H \\wedge \\neg M\\)\n\\(\\neg H \\wedge M\\)\n\\(\\neg H \\wedge \\neg M\\)\nOn Sunday, Beauty’s credences are distributed over the algebra generated by the partition \\(\\{H, \\neg H\\}\\), i.e., \\(\\{\\{w_1, w_2\\}, \\{w_3, w_4\\}\\}\\). The algebra is that course-grained because she doesn’t have the capacity to think \\(M\\) thoughts. And that’s because she’s not acquainted with the relevant haecceities. So she can’t distinguish between worlds that differ only on whether \\(M\\) is true. On Sunday then, Beauty’s credences are given by \\(Pr(H) = Pr(\\neg H) = \\frac{1}{2}\\).\nWhen she wakes on Monday, two things happen. First, she becomes acquainted with \\(a\\). So she can now think about whether \\(a\\) is on Monday. That is, she can now think about whether \\(M\\) is true. So she can now carve the possibility space more finely. Indeed, now her credences can be distributed over all propositions built out of the four possibilities noted above. The second thing that happens is that Beauty rules out one of these possibilities. In particular, she now knows that \\(H \\wedge \\neg M\\), a proposition she couldn’t so much as think before, is actually false. That’s because if the coin landed heads, this very wakening could not have taken place on Tuesday.\nStalnaker’s position on Beauty’s credences uses these two facts. First Beauty ‘recalibrates’ her credences to the new algebra, then she updates by conditionalising on \\(\\neg H \\vee M\\). If after recalibration, her credences are equally distributed over the four cells of the partition, the conditionalising on \\(\\neg H \\vee M\\) will move \\(Pr(H)\\) to \\(\\frac{1}{3}\\). That is, the thirders win!\nBut we might wonder why we use just this calibration, the one where all four cells get equal credence. We’re going to come back to this question below. But first, I want to use Stalnaker’s framework to respond to an interesting objection to the thirder position.\nMonty Hall\nBoth C. S. Jenkins (2005) and Joseph Halpern (2004) have argued that the ‘thirder’ solution is undermined by its similarity to fallacious reasoning in the Monty Hall case. The idea is easy enough to understand if we simply recall the Monty Hall problem. The agent is in one of three states \\(s_1, s_2\\) or \\(s_3\\), and has reason to believe each is equally likely. She guesses which one she is in. An experimenter then selects a state that is neither the state she is in, nor the state she guessed, and tells her that she is not in that state. If she simply conditionalises on the content of the experimenter’s report, then her credence that she guessed correctly will go from \\(\\frac{1}{3}\\) to \\(\\frac{1}{2}\\). This is a bizarre failure of Reflection, so something must have gone wrong.7 Both Jenkins and Halpern suggest that the violation of Reflection that ‘thirders’ endorse in Sleeping Beauty is just as bizarre.\nBut the Sleeping Beauty puzzle is not analogous to the Monty Hall problem. That’s because in Sleeping Beauty we seem forced to have a violation of Reflection somewhere. Let’s think a bit again about Beauty’s credences on Wednesday, and let’s assume that we’re trying to avoid Reflection violations. Then on Monday (and Tuesday) her credence in \\(H\\) is \\(\\frac{1}{2}\\). Now when Beauty awakes on those days, there are three possibilities open to her. (Hopefully it won’t lead to ambiguity if I re-use the name \\(a\\) for the awakening Beauty is undergoing when thinking about \\(H\\).)\n\\(a\\) is Monday and \\(H\\)\n\\(a\\) is Monday and \\(\\neg H\\)\n\\(a\\) is Tuesday and \\(\\neg H\\)\nWhen she wakes on Wednesday, she’s in a position to reflect on these possibilities. And she can rule out the second of them. That’s what she learns when she wakes and learns it is Wednesday; that if \\(\\neg H\\), then that last awakening was on Tuesday. Now since that last awakening, nothing odd has happened to Beauty. She hasn’t had her memories erased. She might have had her memories erased between Monday and Tuesday, but that’s not relevant to the time period she’s considering. Moreover, she knows that she hasn’t had her memories erased. So I think she’s in a position to simply conditionalise on her new evidence. And that new evidence is simply that whatever else was going on when she was thinking about those three possibilities, she wasn’t in the second possibility.\nBut now we face a challenge. Beauty knows that Wednesday will come. So if her credence in \\(H\\) on Wednesday isn’t \\(\\frac{1}{2}\\), then we’ll have a violation of Reflection. The violation is that on Sunday her credence in \\(H\\) is \\(\\frac{1}{2}\\), but she knows it will go up on Wednesday. And that violation is just as bad as the violation of Reflection that ‘thirders’ endorse. But if she conditionalises when she wakes up on Wednesday, then the only way her updated credence in \\(H\\) can be \\(\\frac{1}{2}\\) is if her prior credence in the first and third options above were equal. And the only way that can happen is for her credence, when \\(a\\) is happening, in the proposition that \\(a\\) is Monday and \\(\\neg H\\) is 0. But that’s bizarre. Whether or not the thirders are right to think that she should give equal credence to that possibility as to the two others, she can’t give it credence 0. So Reflection will fail somewhere.\nTo see why Reflection is failing in these cases, it helps to look back at the requirements we need in order to get from conditionalisation to Reflection. In Rachael Briggs’s careful analysis of when Reflection holds, in Briggs (2009), Reflection is only guaranteed to hold when agents know what their evidence is. In other cases, even perfect conditionalisers may violate Reflection.\nThis assumption, namely that agents know what their evidence is, is a kind of luminosity assumption. And not surprisingly, it has been challenged by Timothy Williamson (Williamson 2000, 230–33). What is a little more surprising is that we only need a relatively weak failure of luminosity in order to get problems for reflection. The assumption that agents know what their evidence is can be broken into two parts.\nIf \\(p\\) is part of \\(S\\)’s evidence, then \\(S\\) knows that \\(p\\) is part of her evidence.\nIf \\(p\\) is not part of \\(S\\)’s evidence, then \\(S\\) knows that \\(p\\) is not part of her evidence.\nThe first part is, I think, implausible for reasons familiar from Williamson’s work. But the second is implausible even if one doesn’t like Williamson’s style of reasoning. If we think \\(p\\) must be true to be part of \\(S\\)’s evidence (as I think we should), and we think that rational agent’s can have false beliefs about anything, as also seems plausible by simple observation of how easy it is to be misled, then even a rational agent can fail to realise that \\(p\\) is not part of her evidence. The easiest way that can happen is if she falsely, but reasonably, believes \\(p\\), and hence does not realise that due to its falsity, it is not part of her evidence.\nWilliamson provides an interesting model, based on a discussion in Shin (1989), of a case where an agent does not know that something is not part of her evidence. There are currently three possible states the agent could be in: \\(s_1, s_2\\) or \\(s_3\\). An experiment will be run, and after the experiment the agent will get some evidence depending on which state she’s in.\nIf she’s in \\(s_1\\), her evidence will rule out \\(s_3\\).\nIf she’s in \\(s_2\\), her evidence will rule out \\(s_1\\) and \\(s_3\\).\nIf she’s in \\(s_3\\), her evidence will rule out \\(s_1\\).\nAssume the agent knows these conditionals before the experiment is run, and now let’s assume the experiment has been run. Let \\(xRy\\) mean that \\(y\\) is possible given the evidence \\(S\\) gets in \\(x\\). Then we can see that \\(R\\) is transitive. That means that if \\(p\\) is part of \\(S\\)’s evidence, then her evidence settles that \\(p\\) is part of her evidence. But \\(R\\) is not Euclidean. So it is possible that \\(p\\) is not part of her evidence, even though her evidence does not settle that \\(p\\) is not part of her evidence. In particular, if she is in \\(s_1\\), that she isn’t in \\(s_1\\) is not part of her evidence. But for all she can tell, she’s in \\(s_2\\). And if she’s in \\(s_2\\), her evidence does rule out her being in \\(s_1\\). So her evidence doesn’t settle that this is not part of her evidence.\nThe model is obviously an abstraction from any kind of real-world case. But as we argued above, it is plausible that there are cases where an agent doesn’t know what evidence she lacks. And this kind of case makes for Reflection failure. Assume that the agent’s prior credences are (and should be) that each state is equally likely. And assume the agent conditionalises on the evidence she gets. Then her credence that she’s in \\(s_2\\) will go up no matter what state she’s in. And she knows in advance this will happen. But there’s no obvious irrationality here; it’s not at all clear what kind of reflection-friendly credal dynamics would be preferably to updating by conditionalisation.8\nSo when an agent doesn’t know what evidence she lacks, Reflection can fail. One way to think about the Sleeping Beauty case is that something like this is going on, although it isn’t quite analogous to the Shin-Williamson example discussed above. In that example, the agent doesn’t know what evidence she lacks at the later time. In the Sleeping Beauty case, we can reasonably model Beauty as knowing exactly what her evidence is when she wakes up. Her evidence does nothing more or less than rule out \\(w_2\\). That’s something she didn’t know before waking up. But in a good sense she didn’t know that she didn’t know that. That’s because she was not in a position to even think about \\(w_2\\) as such. Since she wasn’t in a position to think about \\(a\\), couldn’t distinguish, even in thought, between \\(w_1\\) and \\(w_2\\). So any proposition she could think about, and investigate whether she knew or not, had to include either both \\(w_1\\) and \\(w_2\\), or include neither of them. So the only way she could know that she didn’t know \\(\\{w_1, w_3, w_4\\}\\) is if she tacitly knew she didn’t know that in virtue of knowing that she didn’t know \\(\\{w_1, w_2, w_3, w_4\\}\\). But she didn’t know that she didn’t know that for the simple reason that she did know that \\(\\{w_1, w_2, w_3, w_4\\}\\), i.e. the universal proposition, is true. So we have a case where Beauty doesn’t know what it is she doesn’t know at the earlier time. And like cases where the agent doesn’t know what she doesn’t know at the later time, this is a case where reflection fails.\nSo there are two reasons to be sceptical of reflection-based arguments against the ‘thirder’ solution to the Sleeping Beauty puzzle.\nThere is no plausible way for Beauty’s credence in \\(H\\) to be \\(\\frac{1}{2}\\) on both Monday and Wednesday, but reflection requires this.\nReflection is only plausible when agents know both what evidence they have, and what evidence they lack, throughout the story. And it is implausible that Beauty satisfies this constraint, since she gains conceptual capacities during the story.\nBut this isn’t a positive argument for the \\(\\frac{1}{3}\\) solution. I’ll conclude with a discussion of two arguments for the \\(\\frac{1}{3}\\) solution. Both arguments are suggested by Stalnaker’s framework, but only one of them is ultimately defensible.\nStalnaker on Sleeping Ugly\nWhen we left Stalnaker’s discussion of the Sleeping Beauty case, we had just noticed that there was a question about why Beauty should respond to being able to more finely discriminate between states by ‘recalibrating’ to a credal state where each of \\(w_1\\) through \\(w_4\\) receive equal credence. This question about calibration is crucial to the Sleeping Beauty puzzle because there are other post-calibration distributions of credence are are prima facie viable. Perhaps, given what Beauty knows about the setup, she should never have assigned any credence to \\(H \\wedge \\neg M\\). Rather, she should have made it so \\(Pr(\\neg H \\wedge M) = Pr(\\neg H \\wedge \\neg M) = \\frac{1}{4}\\), and \\(Pr(H \\wedge M) = \\frac{1}{2}\\). If she does that, the conditionalising on \\(\\neg(H \\wedge \\neg M)\\) won’t change a thing, and \\(Pr(H)\\) will still be \\(\\frac{1}{2}\\). That is, the halfers win!\nOne argument against this, and in favour of the equally weighted calibration, is suggested by Stalnaker’s ‘Sleeping Ugly’ example. Sleeping Ugly is woken up on Monday and again (with erased memories) on Tuesday however the coin lands. So when Ugly awakes, he has the capacity to think new singular thoughts, but he doesn’t get much evidence about them. In particular, he can’t share the knowledge Beauty would express by saying, “If the coin landed Heads, this is Monday.”9 Now we might think it is intuitive that Ugly’s credences when he wakes up and reflects on his situation should be equal over the four possibilities. Moreover, all Ugly does is recalibrate; since he doesn’t learn anything about which day it is, his post-awakening credence just is his recalibration. If all this is correct, and if Beauty should recalibrate in the same way as Ugly, then Beauty should recalibrate to the ‘equally weighted calibration.’ And now we’re back to victory for the thirders!\nBut there’s little reason to believe the crucial premise about how Ugly should recalibrate his credences. What we know is that Ugly doesn’t have any reason to give any more credence to any one of the four possibilities than to the others. It doesn’t at all follow that he has reason to give equal credence to each, any more than in general an absence of reasons to treat one of the \\(X\\)s differently to the others is a reason to treat them all the same.10\nThe argument I’m considering here is similar to reasoning Adam Elga has employed Elga (2004), and which I have criticised Weatherson (2005). A central focus of my criticism was that this kind of reasoning has a tendency to lead to countable additivity violations. In an important recent paper, Jacob Ross (2010) has shown that many thirder arguments similarly lead to countable additivity violations. He shows this by deriving what he calls the ‘Generalised Thirder Principle’ (hereafter, GTP) from the premises of these arguments. The GTP is a principle concerning a generalised version of the Sleeping Beauty problem. Here is Ross’s description of this class of problems.\n\nLet us define a Sleeping Beauty problem as a problem in which a fully rational agent, Beauty, will undergo one or more mutually indistinguishable awakenings, and in which the number of awakenings she will undergo is determined by the outcome of a random process. Let \\(S\\) be a partition of alternative hypotheses concerning the outcome of this random process. Beauty knows the objective chances of each hypothesis in \\(S\\), and she also knows how many time she will awaken conditional on each of these hypotheses, but she has no other relevant information. The problem is to determine how her credence should be divided among the hypotheses in \\(S\\) when she first awakens. (Ross ms, 2-3)\n\nThe GTP is a principle about this general class of problem. Here’s how Ross states it.\nGeneralized Thirder Principle\nIn any standard Sleeping Beauty problem, upon first awakening, Beauty’s credence in any given hypothesis in \\(S\\) must be proportional to the product of the hypothesis’ objective chance and the number of times Beauty will awaken conditional on this hypothesis. ... [We can] express this principle formally. For any hypothesis \\(i \\in S\\), let \\(Ch(i)\\) be the objective chance that hypothesis \\(i\\) is true, and let \\(N(i)\\) be the number of times Beauty awakens if \\(i\\) is true. Let \\(P\\) be the Beauty’s credence function upon first awakening. The GTP states ...\n\n\\[\\text{For all }i, j \\in S, \\frac{P(i)}{P(j)} = \\frac{N(i)Ch(i)}{N(j)Ch(j)} \\text{ whenever }Ch(j) >  0. \\text{ (Ross ms, 6-7)}\\]\nThe argument I’m considering seems to be committed to the GTP. In a generalised Sleeping Beauty problem, we can imagine a version of Sleeping Ugly who will awake every day that Beauty might awake. The reasoning that leads one to think that Ugly should give equal credence to each of the two days in the original Sleeping Beauty case seems to generalise to imply that Ugly should give equal credence to each day in this more general case. But if in the general example Beauty calibrates to match these credences of Ugly, then conditionalises on the information she receives, then she’ll end up endorsing the GTP. That’s an unhappy outcome. It would be better to have an argument for the \\(\\frac{1}{3}\\) solution that doesn’t imply the GTP.\nI’m going to argue that when Beauty wakes up her credences should satisfy the following two premises. (As always, I use \\(a\\) to name the awakening that Beauty is now undergoing, and I’m using \\(Cr\\) for her credence function on waking.)\n\\(Cr(a\\) is Monday and \\(H) = Cr(a\\) is Tuesday and \\(\\neg H)\\)\n\\(Cr(a\\) is Monday and \\(H) = Cr(a\\) is Monday and \\(\\neg H)\\)\nThese constraints imply, given what Beauty knows about the setup, that \\(Cr(H) = \\frac{1}{3}\\). The arguments for each premise are quite different.\nThe argument for P1 is one I mentioned above, so I’ll just sketch it quickly here. On Wednesday, Beauty’s credence in \\(H\\) should be back to \\(\\frac{1}{2}\\). But what she learns on Wednesday is \\(\\neg (a\\) is on Monday and \\(\\neg H)\\). So on Monday, her credence in \\(H\\) conditional on \\(\\neg (a\\) is on Monday and \\(\\neg H)\\) should be \\(\\frac{1}{2}\\). But given what Beauty knows about the setup of the problem, this immediately implies P1.\nThe argument for P2 requires a slightly more fanciful version of the example. Imagine that on Sunday night, Beauty is visited by a time traveller from Monday who comes back with a videotape of her waking on Monday, and tells her that it was taken on Monday. So Beauty now has the capacity to think about this very awakening, i.e., \\(a\\). This doesn’t seem to affect her credences in \\(H\\), it should still be \\(\\frac{1}{2}\\). Now imagine that her memory of this visit is erased overnight, so when she wakes up on Monday her situation is just like in the original Sleeping Beauty problem.\nCall \\(Cr_1\\) her credence function on Sunday after meeting the time traveller. And call \\(Cr_2\\) her credence function on Monday after she wakes up and reflects on her situation. It seems the only relevant difference between the situation on Sunday and the situation on Monday is that Beauty has lost the information that \\(a\\) is on Monday. The following principle about situations where an agent loses information seems plausible. If \\(Cr_{\\text{old}}\\) is the pre-loss credence function, and \\(Cr_{\\text{new}}\\) is the post-loss credence function, and \\(E\\) is the information lost, then\n\\(Cr_{\\text{old}}(p)\\) = \\(Cr_{\\text{new}}(p | E)\\)\nThe idea here is that information loss is a sort of reverse conditionalisation. Applying this, we get that \\(Cr_1(H) = Cr_2(H | a \\text{ is Monday})\\), so \\(Cr_2((H | a \\text{ is Monday}) = \\frac{1}{2}\\), so \\(Cr_2(a\\) is Monday and \\(H) = Cr_2(a\\) is Monday and \\(\\neg H)\\). And since the situation on Monday in the revised problem, i.e., the situation when Beauty’s credence function is \\(Cr_2\\) is just like the situation in the original Sleeping Beauty problem on Monday, it follows that P1 is true in the original problem. And from P1 and P2, it follows that the thirder solution is right.\nBut note a limitation of this solution. When Beauty wakes on Tuesday her credence function is defined over a different algebra of propositions to what it was defined over after meeting the time traveller. So there’s no time travel based argument that her credences on Tuesday should satisfy P2, or indeed that on Tuesday her credence in \\(H\\) should be \\(\\frac{1}{3}\\). (For similar reasons, this kind of reason does not support the GTP.)\nOne might try and argue that Beauty’s situation on Tuesday is indistinguishable from her situation on Monday, and so she should have the same credences on Tuesday. Both the premise and the inference here seem dubious. On Tuesday, Beauty knows different singular propositions, so the situation isn’t clearly indistinguishable. But more importantly, it is implausible that indistinguishability implies same credences. The relation should have the same credences in is a transitive and symmetric relation between states. The relation is indistinguishable from is neither transitive nor symmetric. So I suspect that the kind of arguments developed here leave it an open question what Beauty’s credences should be on Tuesday, and indeed whether there is a unique value for what her credences then should be.\n\n\nBriggs, Rachael. 2009. “Distorted Reflection.” Philosophical Review 118 (1): 59–85. https://doi.org/10.1215/00318108-2008-029.\n\n\nEgan, Andy. 2004. “Second-Order Predication and the Metaphysics of Properties.” Australasian Journal of Philosophy 82 (1): 48–66. https://doi.org/10.1080/713659803.\n\n\nEgan, Andy, John Hawthorne, and & Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nElga, Adam. 2000. “Self-Locating Belief and the Sleeping Beauty Problem.” Analysis 60 (4): 143–47. https://doi.org/10.1093/analys/60.2.143.\n\n\n———. 2004. “Defeating Dr. Evil with Self-Locating Belief.” Philosophy and Phenomenological Research 69 (2): 383–96. https://doi.org/10.1111/j.1933-1592.2004.tb00400.x.\n\n\nHalpern, Joseph. 2004. “Sleeping Beauty Reconsidered: Conditioning and Reflection in Asynchronous Systems.” In Oxford Studies in Epistemology, 1:111–42. Oxford: Oxford University Press.\n\n\nJenkins, C. S. 2005. “Sleeping Beauty: A Wake-up Call.” Philosophica Mathematica 13 (2): 194–201. https://doi.org/10.1093/philmat/nki015.\n\n\nLewis, David. 1979. “Attitudes de Dicto and de Se.” Philosophical Review 88 (4): 513–43. https://doi.org/10.2307/2184646.\n\n\nRoss, Jacob. 2010. “Sleeping Beauty, Countable Additivity, and Rational Dilemmas.” Philosophical Review 119 (4): 411–47. https://doi.org/10.1215/00318108-2010-010.\n\n\nShin, Hyun Song. 1989. “Non-Partitional Information on Dynamic State Spaces and the Possibility of Speculation.” Working Paper 90-11. Univesity of Michigan: Center for Research on Economic; Social Theory.\n\n\nStalnaker, Robert. 2008. Our Knowledge of the Internal World. Oxford: Oxford University Press.\n\n\nTitlebaum, Michael. 2008. “The Relevance of Self-Locating Beliefs.” Philosophical Review 117 (4): 555–605. https://doi.org/10.1215/00318108-2008-016.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nNote that I’m not assuming that Beauty’s memories are erased in other cases. This makes the particular version of the case I’m discussing a little different to the version popularised in Elga (2000). This shouldn’t make any difference to most analyses of the puzzle, but it helps to clarify some issues.↩︎\nSee, for instance, Titlebaum (2008) and the references therein.↩︎\nIncluding by me. See Egan, Hawthorne, and Brian Weatherson (2005)↩︎\nStalnaker thinks we have independent reason to treat these structured entities as simply worlds. The main point of the last few sentences was that we can adopt Stalnaker’s model while staying neutral on this metaphysical question.↩︎\nPerhaps it would be better to say that individuals and times have haecceities, rather than saying centers do. I have little idea what could tell between these options, or even if there is a substantive issue here.↩︎\nOf course worlds are considerably more detailed than this, but the extra detail is an unnecessary confusion for the current storyline.↩︎\nThe standard response is to say that the agent shouldn’t just conditionalise on the content of the experimenter’s utterance, but on the fact that the experimenter is making just that utterance. We’ll return to this idea below.↩︎\nThe idea that we should update by conditionalisation on our evidence, even when we don’t know what the evidence is, has an amusing consequence in the Monty Hall problem. The agent guesses that she’s in \\(s_i\\), and comes to know she’s not in \\(s_j\\), where \\(i \\neq j\\). If she only comes to know that she’s not in \\(s_j\\), and not something stronger, such as knowing that she knows she’s not in \\(s_j\\), then she really should conditionalise on this, and her credence that her guess was correct will go up. This is the ‘mistaken’ response to the puzzle that is frequently deprecated in the literature. But since the orthodox solutions to the puzzle rely on the agent reflecting on how she came to know \\(\\neg s_j\\), it seems that it is the right solution if she doesn’t know that she knows \\(\\neg s_j\\).↩︎\nStalnaker notes that this is a reason for thinking Beauty does learn something when she wakes up, and so there’s a reason her credence in \\(H\\) changes.↩︎\nCompare this argument for giving nothing to charity. There are thousands of worthwhile charities, and I have no reason to give more to one than any of the others. But I can’t afford to give large equal amounts to each, and if I gave small equal amounts to each, the administrative costs would mean my donation has no effect. So I should treat each equally, and the only sensible practical way to do this is to give none to each. Note that you really don’t have to think one charity is more worthy than the others to think this is a bad argument; sometimes we just have to make arbitrary choices.\n\n↩︎\n",
    "preview": "posts/2021-02-03-stalnaker-on-sleeping-beauty/dowitcher.jpg",
    "last_modified": "2021-02-05T20:45:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-epistemic-modals-and-epistemic-modality/",
    "title": "Epistemic Modals and Epistemic Modality",
    "description": "This chapter introduces the main themes of the volume, summarizes the chapters in it, and looks at the various arguments that have been raised for semantic relativism over the past decade. It concludes that two of these arguments seem to be resistant to the anti-relativist replies that have appeared in response to this work on relativism. One of these is an argument from agreement. It is argued that contextualist theories about various puzzling locutions have a hard time explaining why it is so easy for people who would happily utter the same words to describe themselves as agreeing, if those words were really context-sensitive. Another is an argument concerning attitude ascriptions. It seems there are quite different restrictions on what values the (allegedly) context-sensitive expressions can take inside and outside of attitude ascriptions. Since this isn't how context-sensitive terms usually behave, this phenomena tells against contextualism, and in favour of relativism.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      },
      {
        "name": "Andy Egan",
        "url": "https://www.andyegan.net"
      }
    ],
    "date": "2011-09-01",
    "categories": [
      "language",
      "relativism"
    ],
    "contents": "\n\nContents\nEpistemic Possibility and Other Types of Possibility\nThree Approaches to Epistemic Modals\nContextualism\nRelativism\nTwo Kinds of Relativism\nExpressivism\n\n\nEpistemic Possibility and Other Types of Possibility\nThere is a lot that we don’t know. That means that there are a lot of possibilities that are, epistemically speaking, open. For instance, we don’t know whether it rained in Seattle yesterday. So, for us at least, there is an epistemic possibility where it rained in Seattle yesterday, and one where it did not. It’s tempting to give a very simple analysis of epistemic possibility:\n\nPublished in Epistemic Modality, edited by Andy Egan and Brian Weatherson, Oxford.\nA possibility is an epistemic possibility if we do not know that it does not obtain.\nBut this is problematic for a few reasons. One issue, one that we’ll come back to, concerns the first two words. The analysis appears to quantify over possibilities. But what are they? As we said, that will become a large issue pretty soon, so let’s set it aside for now. A more immediate problem is that it isn’t clear what it is to have de re attitudes towards possibilities, such that we know a particular possibility does or doesn’t obtain. Let’s try rephrasing our analysis so that it avoids this complication.\nA possibility is an epistemic possibility if for every \\(p\\) such that \\(p\\) is true in that possibility, we do not know that \\(p\\) is false.\nIf we identify possibilities with metaphysical possibilities, this seems to rule out too much. Let \\(p\\) be any contingent claim whose truth value we don’t know. We do know, since it follows from the meaning of actually, that \\(p\\) iff actually \\(p\\) is true. But that biconditional isn’t true in any world where p’s truth value differs from its actual truth value. So the only epistemic possibilities are ones where p’s truth value is the same as it actually is. But \\(p\\) was arbitrary in this argument, so the only epistemic possibilities are ones where every proposition has the same truth value as it actually does. This seems to leave us with too few epistemic possibilities!\nA natural solution is to drop the equation of possibilities here with metaphysical possibilities. We’ve motivated this by using a proposition that is easy to know to be true, though it isn’t true in many metaphysical possibilities. There are many problems from the other direction; that is, there are many cases where we want to say that there is a certain kind of epistemic possibility, even though there is no matching metaphysical possibility. We’ll go through five such examples.\nFirst, there are necessary a posteriori claims that arise from the nature of natural kinds. The standard example here is Water is atomic. That couldn’t be true; necessarily, anything atomic is not water. But until relatively recently, it was an epistemic possibility.\nSecond, there are claims arising from true, and hence metaphysically necessary, identity and non-identity statements. A simple example here is Hesperus is not Phosphorus. This could not be true; by necessity, these celestial bodies are identical. But it was an epistemic possibility.\nThird, there are claims about location. It isn’t quite clear what proposition one expresses by saying It’s five o’clock, but, plausibly, the speaker is saying of a particular time that that very time is five o’clock. It’s plausible that if that’s true, it’s true as a matter of necessity. (Could this very time have occurred earlier or later? It doesn’t seem like it could have.) So a false claim about what time it is will be necessarily false. But often there will be a lot of epistemic possibilities concerning what time it is.\nTemporal location raises further matters beyond the necessary a posteriori. We want there to be epistemic possibilities in which it is four o’clock, five o’clock and so on. But it isn’t altogether clear whether claims like that can be true in metaphysical possibilities. If we identify a metaphysical possibility with a possible world, then it isn’t clear what would make it the case that it is four o’clock in a possible world. (What time is it in this possible world?) This might suggest there are different kinds of facts at a metaphysical possibility as at an epistemic possibility.\nFourth, there are issues about mathematics. Actually, there are two kinds of puzzle cases here. One concerns propositions that are logical consequences of our mathematical beliefs, but which we haven’t figured out yet. Twenty years ago, it certainly seemed to be an epistemic possibility that the equation \\(a^n + b^n = c^n\\) had positive integer solutions with \\(n > 2\\). Now we know that there are no such solutions. Moreover, if mathematics is necessarily true, then there isn’t even a metaphysical possibility in which there are such solutions. So we shouldn’t think that there was some metaphysical possibility that twenty years ago we hadn’t ruled out. Rather, we were just unsure what metaphysical possibilities there are.\nFinally, there are issues about logic. (Some views on the nature of logic and mathematics will deny that our fourth and fifth categories are different.) Getting the metaphysics of taste right is hard. One option that we think at least can’t be ruled out is that intuitionist logic is the correct logic of taste talk. That is, when it comes to taste, we don’t even know that it’s true that everything either is or is not tasty. But that doesn’t mean we’re committed to the existence of a possibility where it isn’t true that everything is tasty or not tasty; if such a state isn’t actual, it probably isn’t possible. The liar paradox is even harder than the metaphysics of taste. Anything should be on the table, even the dialethist view that the Liar is both true and false. That is, the Liar might be true and false. In saying that, we certainly don’t mean to commit to the existence of some possibility where the Liar is true and false. We’re pretty sure (but not quite certain!) that no such possibility exists.\nThe last two cases might be dealt with by being more careful about what an epistemic possibility is. There are quite simple cases in which we want to resist the identification of epistemic possibilities with what we don’t know to be the case. For discussion of several such cases, see Hacking (1967), Teller (1972) and DeRose (1991). If we could very easily come to know that \\(p\\) does not obtain, perhaps because \\(p\\) is clearly ruled out by things we do know, then intuitively it isn’t the case that \\(p\\) is epistemically possible. If we know that if \\(q\\) then not \\(p\\), and we know \\(q\\), then \\(p\\) is not possible, even if we haven’t put conditional and antecedent together to conclude that \\(p\\) is false. So we need to put some constraints on the epistemically possible beyond what we know to be false. Perhaps those constraints will go so far as to rule out anything inconsistent with what we know. In that case, it wasn’t possible all along that Fermat’s Last Theorem was false. And, assuming the non-classical approaches to taste and alethic paradoxes are incorrect, those approaches aren’t even possibly correct. We’re not endorsing this position, just noting that it is a way to rescue the idea that all epistemic possibilities are metaphysical possibilities.\nThe papers in this volume that most directly address these issues are by Frank Jackson, David Chalmers and Robert Stalnaker. Jackson argues against the view that accounting for epistemic possibilities requires us to think that there is a kind of possibility, conceptual possibility, that is broader than metaphysical possibility. He briefly reviews the reasons some people have had for taking this position, including those we’ve just discussed, and some of the reasons he rejected it in “From Metaphysics to Ethics.” But he adds some new arguments as well against this position, what he calls the ‘two space’ view of possibility. One argument says that if there is a possibility of any kind where water is not H\\(_2\\)O, then being water and being H\\(_2\\)O must be different properties by Leibniz’s Law. But then we have an implausible necessary connection between distinct properties. Another argument turns on the difficulty of identifying the water in these supposed conceptual possibilities that are not metaphysically possible.\nDavid Chalmers discusses what kind of thing epistemic possibilities, or as he calls them, ‘scenarios,’ might be. He discusses the strengths, weaknesses and intricacies of two proposals: what he calls the ‘metaphysical’ and ‘epistemic’ constructions. The metaphysical construction is fairly familiar: it takes epistemic possibilities to be centered possible worlds. The epistemic construction takes epistemic possibilities to be maximally possible sentences of a specially constructed language. The metaphysical construction requires several assumptions before it matches up with the intuitive notion of epistemic possibility, while the epistemic construction requires a primitive notion of epistemic possibility. But both constructions seem to illuminate the elusive notion of an epistemic possibility. Chalmers ends with a discussion of several applications of his constructions in semantics, in formal epistemology and in moral psychology.\nAnother place where one finds an important role for a distinctively epistemic (or at least doxastic) sort of possibility is in theorizing about indicative conditionals. In Robert Stalnaker’s contribution, he examines two types of accounts of indicative conditionals, which differ in where they locate the conditionality. One view analyzes assertions of indicative conditionals as a special sort of conditional assertion, and another analyzes them as an ordinary assertion of a special sort of conditional proposition. Stalnaker argues that the two views are not so different as we might initially have thought.\nThree Approaches to Epistemic Modals\nEven when we settle the issue of what epistemic possibilities are, we are left with many issues about how to talk about them. Speakers will often say that something is (epistemically) possible, or that it might be true. (It’s plausible that claims that \\(p\\) must be true, or that \\(p\\) is probable, are closely related to these, but we’ll stick to claims about (epistemic) possibility at least for this introduction.) It’s plausible to think that a proposition isn’t possible or impossible simpliciter, it’s rather that it is possible or impossible relative to some person, some group, some evidence or some information. Yet statements of epistemic possibility in plain English do not make any explicit reference to such a person, group, evidence set or information state. One of the key issues confronting a semanticist attempting to theorise about epistemic modals is what to do about this lack of a reference. We’ll look at three quite different approaches for dealing with this lack: contextualist, relativist and expressivist.\nContextualism\nConsider a particular utterance, call it \\(u\\), made by speaker \\(s\\), of the form a might be F, where the might here is intuitively understood as being epistemic in character. To a first approximation, the sentence is saying \\(a\\)’s being \\(F\\) is consistent with, or not ruled out by, a certain body of knowledge. But whose body of knowledge? Not God’s, presumably, for then a might be F would be true iff a is F is true, and that’s implausible. The contextualist answer is that the relevant body of knowledge is supplied by context.\nWhen discussing the ways in which context fills in content, some writers will start with the pronoun I as an example. And to some extent it’s a useful example. The sentence I am a fool doesn’t have truth-conditional content outside of a context of utterance. But any utterance of that sentence does express something truth conditional. Which truth conditional sentence it expresses is dependent on facts about the context of its use. In fact, it is dependent on just one fact, namely who utters it. So when Andy utters I am a fool he expresses the proposition that Andy is a fool. And when Brian utters I am a fool he expresses the proposition that Brian is a fool.\nSo far I is a useful example of a context-sensitive expression. But in many ways it is an unusual example of context-sensitivity, and focussing too much on it can lead to an overly simplistic view of how context-sensitive terms work. In particular, I has three properties that are unusual for a context-sensitive expression.\nIts content in a context is computable from the context by a simple algorithm - namely the content is the speaker.\nIts content does not depend on any properties of the intended audience of the utterance.\nIt behaves exactly the same way in embedded and unembedded contexts.\nSome terms have none of these properties. Consider, for example, we.\nThere isn’t any obvious algorithm for computing the content of a particular use of we. The content may depend on the intentions of the speaker. It may depend on which people have been talked about. In sentences of the form We are F, different values of \\(F\\) might constrain what values can be rationally assigned to we. And when that is so, the interpretation of we will (usually) be constrained to those groups.\nPerhaps most notably, it depends a lot on the audience. If \\(S\\) is talking to \\(H\\), and says We should grab some lunch, the content is that \\(S\\) and \\(H\\) should grab some lunch. And that’s the content because \\(H\\) is the intended audience of the utterance. Intended audiences can change quickly. If Andy says We will finish the paper this afternoon, then we will go for a walk, talking to Brian when he utters the first conjunct, and Fido when he utters the second, the content is that Andy and Brian will finish the paper this afternoon, then Andy and Fido will go for a walk.\nThat we has neither of the first two properties is uncontroversial. What is perhaps a little more controversial is that it does not have the third either. When we is in an unembedded context it (usually) behaves like a free (plural) variable. Under certain embeddings, it can behave like a bound variable. Barbara Partee and Phillipe Schlenker offer the following examples.\n(5.9)\nJohn often comes over for Sunday brunch. Whenever someone else comes over too, we (all) end up playing trios. (Partee 1989)\n\n(5.10)\nEach of my colleagues is so difficult that at some point or other we’ve had an argument. (Schlenker 2003)\n\nIn neither case does we contribute a group consisting of the speaker plus some salient individuals. Indeed, in neither case does it so much as contribute a group, since it is (or at least behaves like) a bound variable. There’s nothing in the contextualist story about we that prevents this.\nIt’s worthwhile reviewing these facts about we, because on the most plausible contextualist stories about might, it too has these three properties. The contextualist theory we have in mind says that the content of \\(u\\) is For all that group X could know using methods M, \\(a\\) is \\(F\\). The group \\(X\\) will usually consist of the speaker and some salient others, perhaps including the intended audience. The salient methods might include little more than easy deduction from what is currently known, or may include some wider kinds of investigation. (See DeRose (1991) for arguments that the relevant methods include more than deduction, and that they are contextually variable.)\nNow it isn’t part of the contextualist theory that there is an easy method for determining who is in \\(X\\), or what methods are in \\(M\\). So in that respect might is like we. But, just as the group denoted by we typically includes the intended audience of the utterance, the group \\(X\\) will typically include the intended audience of \\(u\\). And the methods \\(M\\) will typically include any method that can be easily carried out. This can be used to explain some phenomena about disagreement. So if Andy says, to Brian, a might be F, and Brian knows that \\(a\\) is not \\(F\\) (or can easily deduce this from what he knows), Brian can disagree with what Andy says. That is, he can disagree with the proposition that it is consistent with what members of the conversation know that \\(a\\) is \\(F\\). And, the contextualist says, that’s just what Andy did say. If Brian presents Andy with his grounds for disagreement, Andy might well retract what he said. Since arguments about disagreeing with utterances like \\(u\\) have been prominent in the literature, it is worth noting that the contextualist theory can explain at least some facts about disagreement.\nNor is it part of the contextualist theory that might behaves exactly the same way in embedded and unembedded contexts. Indeed, like we, might can behave like a bound variable. On the most natural reading of Every pedestrian fears that they might be being watched, there is no single group \\(X\\) such that every pedestrian fears that for all \\(X\\) (could easily) know, that pedestrian is being watched. Rather, every pedestrian fears that for all they themselves know, they are being watched. The naturalness of this reading is no embarrassment to the contextualist theory, since it is a common place that terms that usually get their values from context can also, in the right setting, behave like bound variables.\nIndeed, thinking about these parallels between context-sensitive expressions and epistemic modals seems to provide some support for contextualism. In his contribution to the volume, Jonathan Schaffer argues that various features of the way epistemic modals behave in larger sentences support the idea that an evaluator place must be realised in the syntax. For instance, consider the natural interpretation of “Anytime you are going for a walk, if it might rain, you should bring an umbrella.” We interpret that as saying that whenever you go for a walk, you should bring an umbrella if your evidence at that time is consistent with rain. Schaffer interprets that as evidence that there is hidden syntactic structure in epistemic modals, and argues that the contextualist offers the best account of how the hidden structure gets its semantic values.\nSo the contextualist has a lot of explanatory resources, and a lot of flexibility in their theory, which are both clear virtues. But there are some limits to the flexibility. There are some things that the contextualist, at least as we’re using the term ‘contextualist’ is committed to. In particular, the contextualist is committed to the content of a particular speech act (or at least of a particular assertion) is absolute, not assessor-relative. And they’re committed to the truth value of those contents being the same relative to any assessor. Let’s give those two commitments names.\n\\(C\\)\nThe semantic content of an assertion is the same relative to any assessors.\n\n\\(T\\)\nThe truth value of the semantic content of an assertion is the same relative to any assessors.\n\nThe first of these rules out the possibility that the semantic content of an assertion differs with respect to different groups. The second rules out the possibility that semantic contents have assessor relative truth values. Modern relativists have proposed theories that dispense with these dogmas, and we’ll investigate those in the next section, after going over some of the motivations for relativism.\nRelativism\nIn many fields, relativism is motivated by instances of “faultless disagreement,” and epistemic modals are not left out of this trend. Here is the kind of case that we used in Egan, Hawthorne, and Weatherson (2005) to motivate relativism.\n\nConsider the following kind of case. Holmes and Watson are using a primitive bug to listen in on Moriarty’s discussions with his underlings as he struggles to avoid Holmes’s plan to trap him. Moriarty says to his assistant\n\\(24\\)\nHolmes might have gone to Paris to search for me.\n\nHolmes and Watson are sitting in Baker Street listening to this. Watson, rather inexplicably, says “That’s right” on hearing Moriarty uttering (24). Holmes is quite perplexed. Surely Watson knows that he is sitting right here, in Baker Street, which is definitely not in Paris.\n\nHere we have Watson somewhat surprisingly agreeing with Moriarty. In some sense, it seems wrong for him to have done so. He should have disagreed. Well, imagine that he did, by saying “That’s not right.” The quick argument for relativism is that the contextualist cannot make sense of this. Whatever group’s knowledge Moriarty intended to be talking about when he spoke, it presumably didn’t include Holmes and Watson; it just included him and his intended audience, i.e. the underlings. And it’s true that for all they know, Holmes is in Paris. So the content of Moriarty’s utterance is true. But it seems that Watson can properly disagree with it (and can’t properly agree with it). That, we thought, was a problem.\nThere are three kinds of response to this argument on behalf of the contextualist that we think look promising. All of these responses are discussed in Fintel and Gillies (2008). We might look harder at the denotation of the ‘that’ in Watson’s reply, we might think again about what the relevant group is, and we might look at other cases where the contextualist story is more promising, as a way of motivating the first two responses. Let’s look at these in turn.\nAbove we said that Watson disagreed with Moriarty by saying “That’s not right.” But that’s potentially reading too much into the data. What seems correct is that Watson can say “That’s not right.” But that’s only to disagree with Moriarty if the ‘that’ denotes what Moriarty said. And that might not be true. It’s possible that it picks out, say, the embedded proposition that Holmes has gone to Paris. And it’s fine for Watson to disagree with that.\nEven if Watson is disagreeing with the semantic content of Moriarty’s utterance, it might be that he’s doing so properly, because what Moriarty said is false. That might be the case because it might be that, in virtue of hearing the utterance, Watson became part of the relevant group \\(X\\). Typically speaker intentions, particularly singular speaker intentions, are not the final word in determining the content of a context-sensitive expression. If Brian points over his shoulder, thinking a nice glass of shiraz is behind him, and says That is tasty, while in fact what he is pointing at is a vile confection of Vegemite infused chardonnay, he’s said something false. The simplest thing to say about a case like this is that Brian intended the denotation of ‘That’ to be the thing he was pointing at, whatever it is. Similarly, Moriarty might have intended the relevant group \\(X\\) to be whoever heard the utterance at that time, even if he didn’t know Watson was in that group. (Or it might be that, whatever Moriarty’s intentions, the semantic rules and conventions for ‘might’ in English determine that the relevant group includes everybody who heard the utterance at the time.)\nThis second response would seem somewhat ad hoc were it not for a class of examples von Fintel and Gillies describe concerning assessors from radically different contexts. Typically the anti-contextualist commentary on cases like these suggest that any hearer who knows that \\(a\\) is not \\(F\\) can disagree with \\(u\\). But that doesn’t seem to be in general true.\n\nOr consider the case of Detective Parker. He has been going over some old transcripts from Al Capone’s court case in the 1920s–Capone is being asked about where some money is in relation to a particular safe:\n\\(20\\)\nCapone: The loot might be in the safe.\nParker: ??Al was wrong/What Al said is false. The safe was cracked by Geraldo in the 80s and there was nothing inside. (2008, 86)\n\n\nThe knowledge of at least some hearers, such as Detective Parker, does not seem to count for assessing the correctness of Capone’s speech. A contextualist might suggest that’s because contemporaneous hearers are in the relevant group, and later reviewers are not.\nSo there are definitely some contextualism-friendly lines of response available to the argument for relativism from disagreement. But interestingly, some of these contextualist responses do not work as well as a response to a similar argument from agreement. Imagine that Andy, after doing some reading on the publicly available evidence, correctly concludes that it doesn’t rule out Prince Albert Victor. He doesn’t think this is very likely, but thinks it is possible. Andy hears someone on TV talking about the Ripper who says “Prince Albert Victor might have been Jack the Ripper,” and Andy says “That’s true.” Intuitively Andy is right to agree with the TV presenter, but this is a little hard to explain on the contextualist theory.\nNote that here we can’t say that Andy is agreeing because he is agreeing with the embedded proposition, namely that Prince Albert Victor was the Ripper. That’s because he doesn’t agree with that; he thinks it is an open but unlikely possibility.\nNor does it particularly matter that Andy, as one of the people watching the TV show, is part of the relevant group \\(X\\). All that would show is that if Andy knew Prince Albert Victor wasn’t the Ripper, the presenter’s assertion is false. But unless Andy is the group \\(X\\), the fact that Andy’s knowledge, or even what is available to Andy, does not rule out the Prince does not mean Andy should agree with the statement. For all Andy knows, someone else watching, perhaps even someone else the presenter intends to include in her audience, has evidence exculpating the Prince. If that’s right, then he does not know that the proposition the contextualist says the speaker asserted is true. But yet he seems justified in agreeing with the presenter. This seems like a real problem for contextualism.\nA quite different objection to contextualism comes from metasemantic considerations. The most casual reflection on the intuitive content of utterances like \\(u\\) suggests there is staggeringly little rhyme or reason to which group \\(X\\) or method \\(M\\) might be relevant. The argument here isn’t that the contextualist’s semantic proposal is mistaken in some way. Rather, the argument is that the accompanying metasemantic theory, i.e. the theory of how semantic values get fixed, is intolerably complicated. Slightly more formally, we can argue as follows.\nIf contextualism is true, the metasemantic theory of how a particular use of ‘might’ gets its semantic value is hideously complicated.\nMetasemantic theories about how context-sensitive terms get their values on particular occasions are never hideously complicated.\nSo, contextualism is false.\nThe problem with this argument, as Michael Glanzberg (2007) has argued, is that premise 2 seems to be false. There are examples of uncontroversially context-sensitive terms, like ‘that,’ for which the accompanying metasemantic theory is, by any standard, hideously complicated. So the prospects of getting to relativism from metasemantic complexity are not, we think, promising.\nBut there is a different metasemantic motivation for relativism that we think is a little more promising. Compare the difference between (1) and (2).\nThose guys are in trouble, but they don’t know that they are.\n??Those guys are in trouble, but they might not be.\nSomething has gone wrong in (2). This suggests that (2) can’t be used to express (1). That is, there’s no good interpretation of (2) where those guys are the group \\(X\\). This is a little surprising, since we’ve made the guys pretty salient. Cases like this have motivated what we called the Speaker Inclusion Constraint (hereafter SIC) in “Epistemic Modals in Context.” That is, in unembedded uses of ‘might’ the group X always includes the speaker. Now the explanation of the problem with (2) is that for the speaker to assert the first clause, she must know that the guys are in trouble, but if that’s the case, and she’s in group X, then the second clause is false.\nNow a generalisation like this doesn’t look like it should be grounded in the meaning (in some sense of ‘meaning’) of ‘might.’ For comparison, it seems to be part of the meaning of ‘we’ that it is a first-person plural pronoun. It isn’t just a metasemantic generalisation that the speaker is always one of the group denoted by ‘we.’ By analogy, it is part of the meaning of ‘might’ that the speaker is always part of the group \\(X\\).\nFurther, when the meaning of a context-sensitive expression constrains its value, those constraints still hold when the term is used as a bound variable. For instance, it is part of the meaning of ‘she’ that it denotes a female individual. If Smith is male, then the semantic content of She is happy can’t be that Smith is happy. Similarly, when ‘she’ is behaving like a bound variable, the only values it can take are female individuals. So we can’t use Every student fears she will fail the test to quantify over some students some of whom are male. And there’s no interpretation of Every class hopes we will win where it means that every class hopes that that class will win. Even when under a quantifier and an attitude ascribing verb, ‘we’ must still pick out a group that includes the speaker. The natural generalisation is that constraints on context supplied by meaning do not get overridden by other parts of the sentence.\nThe problem for contextualists about ‘might’ is that it doesn’t behave as you’d expect given these generalisations. In particular, the SIC doesn’t hold when ‘might’ is in certain embeddings. So there is a reading of Every student fears they might have failed where it means that every student fears that, for all they know, they failed. The knowledge of the speaker isn’t relevant here. Indeed, even if the speaker knows that many students did not fail, this sentence can be properly uttered. This suggests the following argument.\nIf contextualism is true, then the explanation of the SIC is that it is part of the meaning of ‘might’ that the relevant group X includes the speaker.\nIf it is part of the meaning of ‘might’ that the relevant group X includes the speaker, then this must be true for all uses of ‘might,’ included embedded uses.\nWhen ‘might’ is used inside the scope of an attitude ascription, the relevant group need not include the speaker.\nSo, contextualism is not true.\nPremise 1 would be false if the metasemantics was allowed to be systematic enough to explain why the SIC holds even though it is not part of the meaning. Premise 2 would be false if we allowed ‘might’ to have a systematically different meaning inside and outside the scope of attitude ascriptions. And premise 3 would be false if any attitude ascriptions that are made are, contrary to intuition, tacitly about the speaker’s knowledge. Since none of these seems particularly plausible, there does seem to be a problem for contextualism here.\nIn their contribution to this volume, Kai von Fintel and Thony Gillies reject one of the presuppositions of the argument we’ve just presented. Classical contextualism, what they call ‘the canon,’ says that context picks out a particular group, and an utterance of ‘It might be that \\(p\\)’ is true iff that group’s information is consistent with p. That’s what we’ve taken as the stalking horse in this section, and von Fintel and Gillies are certainly right that it is the canonical version of contextualism. Von Fintel and Gillies agree that the broad outline of this contextualist story is correct. But they deny that context picks out a determinate group, or a determinate body of information. Rather, uttering an epistemic modal will ‘put into play’ a number of propositions of the form ‘For all group \\(G\\) knows, \\(p\\).’ This ambiguity, or perhaps better indeterminacy, is crucial they argue to the pragmatic role that epistemic modals play. And once we are sensitive to it, they claim, we see that contextualism has more explanatory resources than we’d previously assumed, and so the motivation for relativism fades away.\nIn summary, there are four motivations for relativism that have been floated in the literature. These are:\nIntuitions about disagreement;\nIntuitions about agreement;\nArguments from metasemantic complexity; and\nArguments from semantic change in attitude ascriptions,\nAs noted, the third argument doesn’t seem very compelling, and it is a fairly open question whether the first works. But the second and fourth do look like good enough arguments to motivate alternatives.\nTwo Kinds of Relativism\nWe said above that contextualism is characterised by two theses, repeated here for convenience.\n\\(C\\)\nThe semantic content of an assertion is the same relative to any assessors.\n\n\\(T\\)\nThe truth value of the semantic content of an assertion is the same relative to any assessors.\n\nSo there are two ways to not be a relativist, deny (C) and deny (T). One might deny both, but we’ll leave that option out of our survey.\nWhat we call content relativism denies (C). The picture is that contextualists were right to posit a variable \\(X\\) in the structure of an epistemic modal claim. But the contextualists were wrong to think that \\(X\\) gets its value from the context of utterance. Rather, the value of \\(X\\) is fixed in part by the context of assessment. In the simplest (plausible) theory, \\(X\\) is the speaker and the assessor. So if Smith asserts that Jones might be happy, the content of that assertion, relative to Andy, is that for all Smith and Andy know, Jones is happy, while relative to Brian its content is that for all Smith and Brian know, Jones is happy.\nThe primary motivation for content relativism is that it keeps quite a bit of the contextualist picture, while allowing enough flexibility to explain the phenomena that troubled contextualism. So for the content relativist, contents are exactly the same kinds of propositions as the contextualist thinks they are. So we don’t need to tell a new kind of story about what it is for a content to be true, to be accepted, etc. Further, because we keep the variable \\(X\\), we can explain the ‘bound variable’ readings of epistemic modals discussed in the first section.\nA worry about content relativism is that the ‘metasemantic’ argument against contextualism might equally well tell against it. The worry there was that the constraints on \\(X\\) seemed to depend, in an unhappy way, on where in the sentence it appeared. The content relativist has a move available here. She can say that as a rule, whenever there’s a variable like \\(X\\) attached to a term, and that term is in an attitude ascription, then the variable is bound to the subject of the ascription. This might be an interesting generalisation. For instance, if she is a content relativist about both epistemic modals and predicates of personal taste, she has a single explanation for why both types of terms behave differently inside and outside attitude ascriptions.\nThere are two interesting ‘near cousins’ of content relativism. One is a kind of content pluralism. We might hold (a) that an assertion’s content is not relative to an assessor, but (b) some assertions have many contents. So if \\(s\\) says a might be F, and this is assessed by many hearers, \\(s\\) asserts For all s and h know, \\(a\\) is \\(F\\), for each \\(h\\) who hears and assesses the speech. Now when a hearer \\(h_1\\) does this, she’ll probably focus on one particular content of \\(s\\)’s assertion, namely that For all s and h\\(_1\\) know, \\(a\\) is \\(F\\). But the content pluralist accepts (while the content relativist denies) that even relative to \\(h_1\\), \\(s\\)’s assertion also had the content For all s and h\\(_2\\) know, \\(a\\) is \\(F\\), where \\(h_2\\) is a distinct assessor.\nAnother near cousin is the view, defended in this volume by Kent Bach, that the semantic content of an epistemic modal is typically not a complete proposition. In the case just described, it might be that the semantic content of what \\(s\\) says is For all ____ knows, \\(a\\) is \\(F\\), and that’s not a proposition. Now a given hearer, \\(h\\), might take \\(s\\) to have communicated to them that For all s and h know, \\(a\\) is \\(F\\), but that’s not because that’s the semantic content of what \\(s\\) says. It’s not the absolute content (a la contextualism), the content relative to \\(h\\) (a la content relativism) or one of the contents (a la content pluralism).\nIt’s a very big question how we should discriminate between these theories. Some readers may even worry that there is no substantive differences between the theories, they are in some sense saying the same thing in different words. One big task for future research is to clearly state the competing theories in the vicinity of here, and find arguments that discriminate between them.\nA quite different kind of relativism denies (T). This view says that the content itself of an assertion can be true for some assessors, and false for others. Such a view is not unknown in recent philosophy. In the 1970s and 1980s (and to a lesser extent in subsequent years) there was a debate between temporalists and eternalists about propositions. The temporalists thought that a tensed proposition, i.e. the content of a tensed assertion, could be true at one time and false at another. The eternalists denied this, either taking truth to be invariant across times, or in some cases denying that it even made sense to talk about truth being relative to something, e.g. a time.\nContemporary forms of truth relativism generalise the temporalist picture. The temporalists thought that propositions are true or false relative to a world-time pair. Modern relativists think that propositions are true or false relative to a world-assessor pair, or what loosely following Quine (1969) we might call a centered world. (Quine used this to pick out any world-place-time triple, but since most times and places don’t have assessors at them, world-assessor pairs, or even world-assessor-time triples, are more restricted.) For example, as a first pass at a truth-relativism about predicates of personal taste, one might propose that the proposition expressed by a typical utterance of ‘beer is tasty’ will be true at any centered world where the person at the center of the world likes the taste of beer.\nThe truth relativist has an easy explanation of the data that motivated the rejection of contextualism. Recall two puzzles for the contextualist about terms like ‘tasty’: that it is so easy to agree with claims about what’s tasty, and that reports of the form \\(X\\) thinks that beer is tasty are always about \\(X\\)’s attitude towards beer, not about \\(X\\)’s beliefs about how the speaker finds beer.\nOn the first puzzle, note that if to agree with an assertion is to agree with its propositional content, and that content is true at the center of your world iff you find beer tasty, then to agree with an assertion that beer is tasty, you don’t have to launch an inquiry into the sincerity of the speaker, you just have to check whether you like beer. If you’re in a world full in insincere speakers, and abundant beer, that’s relatively easy.\nOn the second puzzle, if propositional attitude ascriptions report the subject’s attitude towards a proposition, and if a proposition is a set of centered worlds, then the subject’s attitude towards ‘Beer is tasty’ should be given by their attitude towards whether that proposition is true in their centered world. That is, it should be given by their attitude towards beer. And that’s just what we find.\nThe extension of all this to epistemic modals is more or less straightforward. The simplest truth relativist theory says that an utterance of the form a might be F is true iff, for all the assessor at the center of the world knows, \\(a\\) is \\(F\\). As Richard Dietz (2008) has pointed out, this won’t do as it stands. If the speaker knows \\(a\\) is not \\(F\\), then their utterance seems like it should be false relative to everyone. (Conversely, a speaker who knows \\(a\\) is \\(F\\) speaks truly, relative to any assessor, when they say a must be F.) If we’re convinced of this, the solution is a mild complication of the theory. The utterance is both somewhat context-sensitive, and somewhat relative. So \\(S\\)’s utterance of a might be F is true at a centered world iff for all \\(S\\) plus the person at the center of the world know, \\(a\\) is \\(F\\). We might want to add more complications (is it knowledge that matters or available information, for example?) but that’s one candidate truth relativist theory.\nThere are three worries we might have about truth relativism. One is a very big picture worry that the very notion of truth being relative is misguided. This is a theme of Herman Cappelen and John Hawthorne’s Relativism and Monadic Truth . Another is that it overgenerates ‘explanations.’ We can’t explain cases like the Capone/Parker example. And a third is that, by making propositions so different from what we thought they were, we’ll have to redo a lot of philosophy of language that presupposed propositions have the same truth value for everyone. In particular, we’ll have to rethink what an assertion is. (That challenge is addressed – in different ways – in recent work by John MacFarlane and by Andy Egan.)\nThe strongest defence of relativism in this volume comes from John MacFarlane. His work on tense (MacFarlane 2003), and on knowledge attributions (MacFarlane 2005b), and on the broader philosophical status of relativism and other rivals to classical contextualism (MacFarlane 2005a, 2009), have been immensely influential in the contemporary debates. Here he develops a relativistic semantics for epistemic modals, along the lines of the proposals he has offered elsewhere for tense and knowledge attributions. He argues that many phenomena, several of which we’ve discussed in this introduction, raise trouble for contextualism and promote relativism. These phenomena include third-party assessments, retraction and disagreement. He argues that only the relativist can explain the troublemaking phenomena.\nExpressivism\nSo far we’ve looked at two of our three major approaches to epistemic modals. The contextualist says that which proposition is asserted by an epistemic modal depends crucially on the context of utterance. The relativist says that the contextualist is ignoring the importance of the context of assessment. The content relativist says that they are ignoring the way in which the context of assessment partially determines what is said. The truth relativist says that they are ignoring the way in which propositions uttered have different truth values at different contexts of assessment.\nThe expressivist thinks that there is a common assumption behind all of these theories, and it is a mistaken assumption. The assumption is that when we’re in the business of putting forward epistemic modals, we’re in the business of asserting things that have truth values. The expressivist rejects that assumption. They say that when we say a might be F, we’re not asserting that we are uncertain about whether \\(a\\) is \\(F\\), we’re expressing that uncertainty directly. The contextualists and relativists think that in making these utterances, we’re expressing a second-order belief, i.e. a belief about our own knowledge, or lack thereof. The expressivists think we’re expressing a much simpler mental state: uncertainty.\nOne way to motivate expressivism is to start with the anti-contextualist arguments, and then argue that relativism is not an acceptable way out. So we might, for instance, start with the argument from agreement. The expressivist notes that there are many ways to agree with a statement. If Smith says ‘Let’s have Chinese for dinner,’ and Jones agrees, there need not be any proposition that Smith asserted that Jones is agreeing to. We’re happy to call all sorts of meetings of minds agreements. So the agreement phenomena that the contextualist can’t explain, the expressivist can explain. When Smith says ‘Brown might be a spy,’ and Jones agrees, there isn’t necessarily any proposition they both accept. Rather, their agreement consists in having a common mental state, namely uncertainty about whether Brown is a spy.\nThe expressivist may then run out any number of arguments against relativism. For instance, they might argue (against content relativism) that it is a requirement of a speech act being an assertion that it have a determinate content. And they might argue, perhaps motivated by theoretical considerations about the role of assertions in conversation, that contents which vary in truth value among hearers couldn’t be contents of assertions. If true, that would rule out truth relativism. We’re moved, perhaps by elimination as much as anything, to expressivism.\nThere are more direct arguments for expressivism as well. Isaac Levi (1996, 55) motivated a view on which epistemic modals don’t have truth values by thinking about learning. Imagine someone previously thought that Brown might be a spy, perhaps on quite good grounds, then they learn that he is not a spy. If that’s all they learned, then it seems odd to say that there’s something that they previously knew, that now they don’t know. It seems learning shouldn’t destroy knowledge. That’s what happens in standard models for belief revision (which were one of Levi’s primary concerns) and it is independently plausible. But if epistemic modals express propositions, and those are true or false, then there is a proposition that the person did know and now don’t know, namely that Brown might be a spy.\nThere are clearly a few possible responses to this argument. For one thing, we could make the epistemic modal claims explicitly tensed. Both before and after the learning experience, the subject knew that Brown might, at \\(t_1\\), have been a spy, but didn’t know that Brown might, at \\(t_2\\), have been a spy. (Indeed, they learned that that was false.) Or, and this is more in keeping with the spirit of this introduction, we might spell out the epistemic modal claim. Before and after the learning experience, the subject knew that it was consistent with everything the subject knew prior to the learning experience that Brown was a spy. So there’s no information lost.\nThe problem with this move is that it seems to make epistemic modals overly complex. Intuitively, it is possible for a child to grasp a modal, and for the most natural interpretation of that modal to be epistemic, without the child having the capacity to form second order thoughts. (This point is one that Seth Yalcin uses in his argument for a kind of expressivism in this volume.) This question seems like it would be good to test empirically, though we don’t know of any existing evidence that settles the question. Introspectively, it does seem that one can think that the cat might be in the garden without thinking about one’s own epistemic or doxastic states as such. Those kinds of introspections might tell in favour of an approach which identifies epistemic modality with a distinct kind of relation to content, rather than a distinct kind of content.\nFollowing important work by Allan Gibbard (1990), there is a natural way to formalise an expressivist theory of epistemic modality. Identify a ‘context’ with a set of propositions. Sentences, whether epistemic modals or simple sentences, are satisfied or unsatisfied relative to world-context pairs, where a world and a context make a pair iff every proposition in the context is true at that world. Then an epistemic modal, say Brown might be a spy, is satisfied by such a pair iff Brown is a spy is consistent with everything in the context. A simple sentence, like White is a spy is satisfied by such a pair iff White is a spy is true at the world. The pairing becomes useful when considering, say, conjunctions. A conjunction is satisfied iff both conjuncts are satisfied. So White is a spy and Brown might be is satisfied by a world-context pair iff White is a spy at the world, and Brown’s being a spy is consistent with the context.\nSo far this looks a lot like relativism. A world-context pair is just like a centered world, with the context being what’s known by the person at the center of the world. If we apply the formalism to real-life cases, perhaps taking the contexts to be genuine contexts in the sense of Stalnaker (1978), the two formalisms might look very close indeed.\nBut there is, or at least we hope there is, a substantive philosophical difference between them. The expressivist has a restricted sense of what it is to make an assertion, and of what it is for an expression to be an expression of a truth. The expressivist most insistently does not identify satisfaction with truth. The only sentences that are true or false are sentences that are satisfied by a world context pair \\(\\langle w,c_1 \\rangle\\) iff they are satisfied by every other pair starting with the same world. The expression of such a sentence, and perhaps only of such a sentence, constitutes an assertion. Otherwise it constitutes some other speech act.\nAnd this is no mere difference in how to use the words ‘truth,’ ‘assertion’ and so on. Nor is it even just a difference about truth and assertion and so on. It hopefully makes a difference to what predictions we make about the way epistemic modals embed, especially how they embed in propositional attitude ascriptions. We used that fact to argue against expressivism in “Epistemic Modals in Context,” since we thought there were in some cases more examples of successful embedding of epistemic modals, especially in conditionals, than the expressivist would predict. On the other hand Seth Yalcin uses facts about embedding to argue, in his paper in this volume, in favour of expressivism. He argues that on a non-expressivist view, we should be able to suppose that \\(p\\) is true but might not be true, and that can’t be supposed.\nThis argument is part of the argument by elimination that Yalcin against what he calls ‘descriptivism’ about epistemic modals in this contribution to the volume. He uses ‘descriptivism’ to pick out a broad category of theories about epistemic modals that includes both contextualism and relativism. He argues against all descriptivist views, and in favour of what he calls ‘expressivism.’ He says that when someone utters an epistemic modal, they do not describe their own knowledge (or the knowledge of someone else), rather they express their own mental state. Some of Yalcin’s arguments for expressivism are related to arguments against contextualism; in particular he thinks like we do that there isn’t a viable form of contextualism. But he also thinks that there are problems for relativism, such as the difficulty in supposing Moore paradoxical propositions. He also notes that it is a puzzle for descriptivists to make sense of belief ascriptions involving epistemic modals. On a descriptivist model, a sentence like ‘\\(X\\) believes that it might be that \\(p\\)’ reports the existence of a second-order belief state. But Yalcin notes there are reasons to doubt that is right. He develops in detail an expressivist model that avoids what he takes to be shortcomings of descriptivist approaches.\nThe two papers we haven’t discussed so far, those by Eric Swanson and Stephen Yablo, are both related to this expressivist family of theories, though their positive proposals head off in distinctive directions.\nEric Swanson’s contribution locates epistemic modals within a broader category, which he calls “the language of subjective uncertainty.” He also emphasizes the diversity of epistemic modal locutions, and draws attention to the risks involved in focusing too closely on just a few examples. In the literature so far, ‘might’ and ‘must’ have tended to get the lion’s share of the attention, while other sorts of epistemic modality – including the more explicitly quantitative sorts (‘four to one against that,’ ‘there’s a 55% chance that,’ etc.) – have gone mostly unnoticed. Swanson argues that attending to other instances of the language of subjective uncertainty serves to undermine many of the standard proposals about epistemic ‘might’ and ‘must,’ and motivates a probabilistic semantics.\nSomewhat relatedly, Stephen Yablo develops a theory about epistemic modals where their primary function is not to state facts about the world, but to update the conversational score. Theories of this kind are quite familiar from the dynamic semantics tradition, but Yablo notes that the existing dynamic theories of epistemic modals are quite implausible. One of the challenges a dynamic approach to epistemic modals faces is to say how we should update a context (or a belief state) with It might be that \\(p\\) when the context previously was incompatible with \\(p\\). Yablo adopts some suggestions from David Lewis’s “A Problem about Permission” (Lewis 1979) to try and solve this puzzle.\n\n\n\nDeRose, Keith. 1991. “Epistemic Possibilities.” Philosophical Review 100 (4): 581–605. https://doi.org/10.2307/2185175.\n\n\nDietz, Richard. 2008. “Epistemic Modals and Correct Disagreement.” In Relative Truth, edited by Manuel Garcia-Carpintero and Max Kölbel, 239–64. Oxford: Oxford University Press.\n\n\nEgan, Andy, John Hawthorne, and Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nFintel, Kai Fintelvon, and Anthony S. Gillies. 2008. “CIA Leaks.” Philosophical Review 117 (1): 77–98. https://doi.org/10.1215/00318108-2007-025.\n\n\nGibbard, Allan. 1990. Wise Choices, Apt Feelings: A Theory of Normative Judgment. Cambridge, MA: Harvard University Press.\n\n\nGlanzberg, Michael. 2007. “Context, Content and Relativism.” Philosophical Studies 136 (1): 1–29. https://doi.org/10.1007/s11098-007-9145-5.\n\n\nHacking, Ian. 1967. “Possibility.” Philosophical Review 76 (2): 343–68. https://doi.org/10.2307/2183640.\n\n\nLevi, Isaac. 1996. For the Sake of the Argument:ramsey Test Conditionals, Inductive Inference and Nonmonotonic Reasoning. Cambridge: Cambridge University Press.\n\n\nLewis, David. 1979. “A Problem about Permission.” In Essays in Honour of Jaakko Hintikka on the Occasion of His Fiftieth Birthday on January 12, 1979, edited by Esa Saarinen, Risto Hilpinen, Illka Niiniluoto, and Merrill Provence, 163–75. Dordrecht: Reidel.\n\n\nMacFarlane, John. 2003. “Future Contingents and Relative Truth.” The Philosophical Quarterly 53 (212): 321–36. https://doi.org/10.1111/1467-9213.00315.\n\n\n———. 2005a. “Making Sense of Relative Truth.” Proceedings of the Aristotelian Society 105 (1): 321–39. https://doi.org/10.1111/j.0066-7373.2004.00116.x.\n\n\n———. 2005b. “The Assessment Sensitivity of Knowledge Attributions.” Oxford Studies in Epistemology 1: 197–233.\n\n\n———. 2009. “Nonindexical Contextualism.” Synthese 166 (2): 231–50. https://doi.org/10.1007/s11229-007-9286-2.\n\n\nPartee, Barbara. 1989. “Binding Implicit Variables in Quantified Contexts.” In Papers from the Twenty-Fifth Regional Meeting of the Chicago Linguistic Society, edited by Caroline Wiltshire, Randolph Graczyk, and Bradley Music. Chicago: Chicago Linguistic Society.\n\n\nQuine, W. V. O. 1969. “Propositional Objects.” In Ontological Relativity and Other Essays, 139–60. New York: Columbia University Press.\n\n\nSchlenker, Philippe. 2003. “Indexicality, Logophoricity, and Plural Pronouns.” In Afroasiatic Grammar II: Selected Papers from the Fifth Conference on Afroasiatic Languages, Paris, 2000, edited by Jacqueline Lecarme, 409–28. Amsterdam: John Benjamins.\n\n\nStalnaker, Robert. 1978. “Assertion.” Syntax and Semantics 9: 315–32.\n\n\nTeller, Paul. 1972. “Epistemic Possibility.” Philosophia 2 (4): 303–20. https://doi.org/10.1007/bf02381591.\n\n\n\n\n",
    "preview": "posts/2021-03-15-epistemic-modals-and-epistemic-modality/emoup.jpg",
    "last_modified": "2021-03-15T11:25:59-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-defending-interest-relative-invariantism/",
    "title": "Defending Interest Relative Invariantism",
    "description": "Since interest-relative invariantism (hereafter, IRI) was introduced into contemporary epistemology in the early 2000s, it has been criticised on a number of fronts. This paper responds to six different criticisms of IRI launched by five different authors. And it does so by noting that the best version of IRI is immune to the criticisms they have launched. The 'best version' in question notes three things about IRI. First, what matters for knowledge is not strictly the *stakes* the agent faces in any decision-problem, but really the *odds* at which she has to bet. Second, IRI is a relatively weak theory; it just says interests sometimes matter. Defenders of IRI have often derived it from much stronger principles about reasoning, and critics have attacked those principles, but much weaker principles would do. Third, and most importantly, interests matter because generate certain kinds of *defeaters*. It isn't part of this version of IRI that an agent can know something in virtue of their interests. Rather, the theory says that whether a certain kind of consideration is a defeater to an agent's putative knowledge that _p_ depends on their interests. This matters for the intuitive plausibility of IRI. Critics have argued, rightly, that interests don't behave in ways distinctive of grounds of knowledge. But interests do behave like other kinds of defeaters, and this undermines the criticisms of IRI.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2011-01-01",
    "categories": [
      "epistemology",
      "interest-relativity"
    ],
    "contents": "\n\nContents\nExperimental Objections\nKnowledge By Indifference and By Wealth\nTemporal Embeddings\nProblematic Conjunctions\nHolism and Defeaters\nNon-Consequentialist Cases\n\nIn recent years a number of authors have defended the interest-relativity of knowledge and justification. Views of this form are floated by John Hawthorne (2004), and endorsed by Jeremy Fantl and Matthew McGrath (2002, 2009), Jason Stanley (2005) and Brian Weatherson (2005). The various authors differ quite a lot in how much interest-relativity they allow, but what is common is the defence of interest-relativity.\nPublished in Logos and Episteme 2 (2011): 591-609.\nImage from Wikimedia Commons.\nThese views have, quite naturally, drawn a range of criticisms. The primary purpose of this paper is to respond to these criticisms and, as it says on the tin, defend interest-relative invariantism, or IRI for short. But I don’t plan to defend every possible version of IRI, only a particular one. Most of the critics of IRI have assumed that it must have some or all of the following features.\nIt is harder to know things in high-stakes situations than in low-stakes situations.\nThere is an interest-sensitive constituent of knowledge.\nIRI stands and falls with some principles connecting knowledge and action, such as the principles found in Hawthorne and Stanley (2008).\nMy preferred version of IRI has none of these three features.1\nFirst, it says that knowledge changes when the odds an agent faces change, not when the stakes change. More precisely, interests affect belief because whether someone believes \\(p\\) depends inter alia on whether their credence in \\(p\\) is high enough that any bet on \\(p\\) they actually face is a good bet. And interests affect knowledge largely because they affect belief. Raising the stakes of any bet on \\(p\\) does not directly change whether an agent believes \\(p\\), but changing the odds of the bets on \\(p\\) they face does change it. In practice raising the stakes changes the odds due to the declining marginal utility of material goods. So in practice high-stakes situations are typically long-odds situations. But knowledge is hard in those situations because they are long-odds situations, not because they are high-stakes situations.\nSo my version of IRI says that knowledge differs between these two cases.\nHigh Cost Map:\nZeno is walking to the Mysterious Bookshop in lower Manhattan. He’s pretty confident that it’s on the corner of Warren Street and West Broadway. But he’s been confused about this in the past, forgetting whether the east-west street is Warren or Murray, and whether the north-south street is Greenwich, West Broadway or Church. In fact he’s right about the location this time, but he isn’t justified in having a credence in his being correct greater than about 0.95. While he’s walking there, he has two options. He could walk to where he thinks the shop is, and if it’s not there walk around for a few minutes to the nearby corners to find where it is. Or he could call up directory assistance, pay $1, and be told where the shop is. Since he’s confident he knows where the shop is, and there’s little cost to spending a few minutes walking around if he’s wrong, he doesn’t do this, and walks directly to the shop.\n\nLow Cost Map:\nJust like the previous case, except that Zeno has a new phone with more options. In particular, his new phone has a searchable map, so with a few clicks on the phone he can find where the store is. Using the phone has some very small costs. For example, it distracts him a little, which marginally raises the likelihood of bumping into another pedestrian. But the cost is very small compared to the cost of getting the location wrong. So even though he is very confident about where the shop is, he double checks while walking there.\n\nI think the Map Cases are like the various cases that have been used to motivate interest-relativity2 in all important respects. I think Zeno knows where the shop is in High Cost Map, and doesn’t know in Low Cost Map. And he doesn’t know in Low Cost Map because the location of the shop has suddenly become the subject matter of a bet at very long odds. You should think of Zeno’s not checking the location of the shop on his phone-map as a bet on the location of the shop. If he wins the bet, he wins a few seconds of undistracted strolling. If he loses, he has to walk around a few blocks looking for a store. The disutility of the loss seems easily twenty times greater than the utility of the gain, and by hypothesis the probability of winning the bet is no greater than 0.95. So he shouldn’t take the bet. Yet if he knew where the store was, he would be justified in taking the bet. So he doesn’t know where the store is. Now this is not a case where higher stakes defeat knowledge. If anything, the stakes are lower in Low Cost Map. But the relevant odds are longer, and that’s what matters to knowledge.\nSecond, on this version of IRI, interests matter because there are interest-sensitive defeaters, not because interests form any kind of new condition on knowledge, alongside truth, justification, belief and so on. In particular, interests matter because there are interest-relative coherence constraints on knowledge. Some coherence constraints, I claim, are not interest-relative. If an agent believes \\(\\neg p\\), that belief defeats her purported knowledge that \\(p\\), even if the belief that \\(p\\) is true, justified, safe, sensitive and so on. It is tempting to try to posit a further coherence condition.\nPractical Coherence\nAn agent does not know that \\(p\\) if she prefers \\(\\varphi\\) to \\(\\psi\\) unconditionally, but prefers \\(\\psi\\) to \\(\\varphi\\) conditional on \\(p\\).\n\nBut that is too strong. For reasons similar to those gone over at the start of Hawthorne (2004), it would mean we know nearly nothing. A more plausible condition is:\nRelevant Practical Coherence\nAn agent does not know that \\(p\\) if she prefers \\(\\varphi\\) to \\(\\psi\\) unconditionally, but prefers \\(\\psi\\) to \\(\\varphi\\) conditional on \\(p\\), for any \\(\\varphi, \\psi\\) that are relevant given her interests.\n\nWhen this condition is violated, the agent’s claim to knowledge is defeated. As we’ll see below, defeaters behave rather differently to constituents of knowledge. Some things which could not plausibly be grounds for knowledge could be defeaters to defeaters for knowledge.\nRelevant Practical Coherence suffices, at least among agents who are trying to maximise expected value, to generate an interest-relativity to knowledge. The general structure of the case should be familiar from the existing literature. Let \\(p\\) be a proposition that is true, believed by the agent, and strongly but not quite conclusively supported by their evidence. Let \\(B\\) be a bet that has a small positive return if \\(p\\), and a huge negative return if \\(\\neg p\\). Assume the agent is now offered the bet, and let \\(\\varphi\\) be declining the bet, and \\(\\psi\\) be accepting the bet. Conditional on \\(p\\), the bet wins, so the agent prefers the small positive payout, so prefers \\(\\psi\\) to \\(\\varphi\\) conditional on \\(p\\). But the bet has a massively negative expected return, so unconditionally the agent does not want it. That is, unconditionally she prefers \\(\\varphi\\) to \\(\\psi\\). Once the bet is offered, the actions \\(\\varphi\\) and \\(\\psi\\) become relevant given her interests, so by Relevant Practical Coherence she no longer knows \\(p\\). So for such an agent, knowledge is interest-relative.\nCases where knowledge is defeated because if the agent did know \\(p\\), that would lead to problems elsewhere in their cognitive system, have a few quirky features. In particular, whether the agent knows \\(p\\) can depend on very distant features. Consider the following kind of case.\n\nConfused Student\nCon is systematically disposed to affirm the consequent. That is, if he notices that he believes both \\(p\\) and \\(q \\rightarrow p\\), he’s disposed to either infer \\(q\\), or if that’s impermissible given his evidence, to ditch his belief in the conjunction of \\(p\\) and \\(q \\rightarrow p\\). Con has completely compelling evidence for both \\(q \\rightarrow p\\) and \\(\\neg q\\). He has good but less compelling evidence for \\(p\\). And this evidence tracks the truth of \\(p\\) in just the right way for knowledge. On the basis of this evidence, Con believes \\(p\\). Con has not noticed that he believes both \\(p\\) and \\(q \\rightarrow p\\). If he did, he’s unhesitatingly drop his belief that \\(p\\), since he’d realise the alternatives (given his dispositions) involved dropping belief in a compelling proposition. Two questions:\nDoes Con know that \\(p\\)?\nIf Con were to think about the logic of conditionals, and reason himself out of the disposition to affirm the consequent, would he know that \\(p\\)?\n\nI think the answer to the first question is No, and the answer to the second question is Yes. As it stands, Con’s disposition to affirm the consequent is a doxastic defeater of his putative knowledge that \\(p\\). Put another way, \\(p\\) doesn’t cohere well enough with the rest of Con’s views for his belief that \\(p\\) to count as knowledge. To be sure, \\(p\\) coheres well enough with those beliefs by objective standards, but it doesn’t cohere at all by Con’s lights. Until he changes those lights, it doesn’t cohere well enough to be knowledge. Moreover (as a referee pointed out), Con’s belief is not safe. Since he could easily have ‘reasoned’ himself out of his belief that \\(p\\), the belief isn’t safe in the way that knowledge is safe.\nI think that beliefs which violate Relevant Practical Coherence fail to be knowledge for the same reason that Con’s belief that \\(p\\) fails to be knowledge. In what follows, I’ll make frequent use of this analogy; many of the objections to IRI turn out to be equally strong objections to the view that there are ever defeaters of the type Con suffers from.\nThis suggests our third point. This version of IRI does not take IRI to be a consequence of more general principles about knowledge and action. It simply says that there exist at least one pair of cases where the only relevant difference between agents in the two cases concerns their interests, but one knows that \\(p\\) and the other does not.3 I happen to think that most of the general principles that philosophers have used to try to derive IRI are false. But since IRI is much weaker than those principles, that is no reason to conclude IRI is false.4\nThe existence of interest-relativity is then quite a weak claim. There are plenty of stronger claims in the area we could make. I prefer, for instance, a version of IRI where being offered bets like \\(B\\) defeats knowledge that \\(p\\) even if the agent does not have the preferences I ascribed above. (That could be because she isn’t trying to maximise expected value, or because she’s messed up the expected value calculations.) But knowledge could be interest-relative even if I’m wrong about those cases.\nSo I’ve set out a version of IRI that lacks three features often attributed to IRI. I haven’t argued for that theory here - I do that at much greater length in (Author Paper 1). But I hope I’ve done enough to convince you that the theory is both a version of IRI, and not obviously false. In what follows, I’ll argue that the theory is immune to the various challenges to IRI that have been put forward in the literature. This immunity is, I think, a strong reason to prefer this version of IRI.\nExperimental Objections\nI don’t place as much weight as some philosophers do on the correlation between the verdicts of an epistemological theory and the gut reactions that non-experts have to tricky cases. And I don’t think the best cases for IRI relies on such a correlation holding. The best case for IRI is that it integrates nicely with an independently supported theory of belief, and that it lets us keep a number of plausible principles without drifting into skepticism.5 But still, it is nice to not have one’s theory saying exorbitantly counterintuitive things. Various experimental results, such as the results in May et al. (2010) and Feltz and Zarpentine (2010), might be thought to suggest that IRI does have consequences which are counterintuitive, or which at least run counter to the intuitions of some experimental subjects. I’m going to concentrate on the latter set of results here, though I think that what I say will generalise to related experimental work. In fact, I think the experiments don’t really tell against IRI, because IRI, at least in my preferred version, doesn’t make any unambiguous predictions about the cases at the centre of the experiments. The reason for this is related to my insistence that we concentrate on the odds an agent faces, not the stakes she faces.\nFeltz and Zarpentine gave subjects related vignettes, such as the following pair. (Each subject only received one of the pair.)\nHigh Stakes Bridge\nJohn is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a yawning thousand foot drop. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, ‘I know that my truck will make it across the bridge.’\n\nLow Stakes Bridge\nJohn is driving a truck along a dirt road in a caravan of trucks. He comes across what looks like a rickety wooden bridge over a three foot ditch. He radios ahead to find out whether other trucks have made it safely over. He is told that all 15 trucks in the caravan made it over without a problem. John reasons that if they made it over, he will make it over as well. So, he thinks to himself, ‘I know that my truck will make it across the bridge.’ (Feltz and Zarpentine 2010, 696)\n\nSubjects were asked to evaluate John’s thought. And the result was that 27% of the participants said that John does not know that the truck will make it across in Low Stakes Bridge, while 36% said he did not know this in High Stakes Bridge. Feltz and Zarpentine say that these results should be bad for interest-relativity views. But it is hard to see just why this is so.\nNote that the change in the judgments between the cases goes in the direction that IRI seems to predict. The change isn’t trivial, even if due to the smallish sample size it isn’t statistically significant in this sample. But should a view like IRI have predicted a larger change? To figure this out, we need to ask three questions.\nWhat are the costs of the bridge collapsing in the two cases?\nWhat are the costs of not taking the bet, i.e., not driving across the bridge?\nWhat is the rational credence to have in the bridge’s sturdiness given the evidence John has?\nConditional on the bridge not collapsing, the drivers presumably prefer taking the bridge to not taking it. And the actions of taking the bridge or going around the long way are relevant. So by Relevant Practical Coherence, the drivers know the bridge will not collapse in Low Stakes Bridge but not High Stakes Bridge if the following equation is true. (I assume all the other conditions for knowledge are met, and that there are no other salient instances of Relevant Practical Coherence to consider.)\n\\[\\frac{C_H}{G + C_H} > x > \\frac{C_L}{G + C_L}\\]\nwhere \\(G\\) is the gain the driver gets from taking a non-collapsing bridge rather than driving around (or whatever the alternative is), \\(C_H\\) is the cost of being on a collapsing bridge in High Stakes Bridge, \\(C_L\\) is the cost of being on a collapsing bridge in Low Stakes Bridge, and \\(x\\) is the probability that the bridge will collapse. I assume \\(x\\) is constant between the two cases. If that equation holds, then taking the bridge, i.e., acting as if the bridge won’t collapse, maximises expected utility in Low Stakes Bridge but not High Stakes Bridge. So in High Stakes Bridge, adding the proposition that the bridge won’t collapse to the agent’s cognitive system produces incoherence, since the agent won’t (at least rationally) act as if the bridge won’t collapse. So if the equation holds, the agent’s interests in avoiding \\(C_H\\) creates a doxastic defeater in High Stakes Bridge.\nBut does the equation hold? Or, more relevantly, did the subjects of the experiment believe that the equation hold? None of the four variables has their values clearly entailed by the story, so we have to guess a little as to what the subjects’ views would be.\nFeltz and Zarpentine say that the costs in “High Stakes Bridge are very costly—certain death—whereas the costs in Low Stakes Bridge are likely some minor injuries and embarrassment.” (Feltz and Zarpentine 2010, 702) I suspect both of those claims are wrong, or at least not universally believed. A lot more people survive bridge collapses than you may expect, even collapses from a great height.6 And once the road below a truck collapses, all sorts of things can go wrong, even if the next bit of ground is only 3 feet away. (For instance, if the bridge collapses unevenly, the truck could roll, and the driver would probably suffer more than minor injuries.)\nWe aren’t given any information as to the costs of not crossing the bridge. But given that 15 other trucks, with less evidence than John, have decided to cross the bridge, it seems plausible to think they are substantial. If there was an easy way to avoid the bridge, presumably the first truck would have taken it. If \\(G\\) is large enough, and \\(C_H\\) small enough, then the only way for this equation to hold will be for \\(x\\) to be low enough that we’d have independent reason to say that the driver doesn’t know the bridge will hold.\nBut what is the value of \\(x\\)? John has a lot of information that the bridge will support his truck. If I’ve tested something for sturdiness two or three times, and it has worked, I won’t even think about testing it again. Consider what evidence you need before you’ll happily stand on a particular chair to reach something in the kitchen, or put a heavy television on a stand. Supporting a weight is the kind of thing that either fails the first time, or works fairly reliably. Obviously there could be some strain-induced effects that cause a subsequent failure7, but John really has a lot of evidence that the bridge will support him.\nGiven those three answers, it seems to me that it is a reasonable bet to cross the bridge. At the very least, it’s no more of an unreasonable bet than the bet I make every day crossing a busy highway by foot. So I’m not surprised that 64% of the subjects agreed that John knew the bridge would hold him. At the very least, that result is perfectly consistent with IRI, if we make plausible assumptions about how the subjects would answer the three numbered questions above.\nAnd as I’ve stressed, these experiments are only a problem for IRI if the subjects are reliable. I can think of two reasons why they might not be. First, subjects tend to massively discount the costs and likelihoods of traffic related injuries. In most of the country, the risk of death or serious injury through motor vehicle accident is much higher than the risk of death or serious injury through some kind of crime or other attack, yet most people do much less to prevent vehicles harming them than they do to prevent criminals or other attackers harming them.8 Second, only 73% of these subjects in this very experiment said that John knows the bridge will support him in Low Stakes Bridge. This is rather striking. Unless the subjects endorse an implausible kind of scepticism, something has gone wrong with the experimental design. But if the subjects are implausibly sceptical, then we shouldn’t require our epistemological theory to track their gut reactions. (And if something has gone wrong with the experimental design, then obviously can’t be used as the basis for any objection.) So given the fact that the experiment points broadly in the direction of IRI, and that with some plausible assumptions it is perfectly consistent with that theory, and that the subjects seem unreasonably sceptical to the point of unreliability about epistemology, I don’t think this kind of experimental work threatens IRI.\nKnowledge By Indifference and By Wealth\nGillian Russell and John Doris (2009) argue that Jason Stanley’s account of knowledge leads to some implausible attributions of knowledge, and if successful their objections would generalise to other forms of IRI. I’m going to argue that Russell and Doris’s objections turn on principles that are prima facie rather plausible, but which ultimately we can reject for independent reasons.9\nTheir objection relies on variants of the kind of case Stanley uses heavily in his (2005) to motivate a pragmatic constraint on knowledge. Stanley considers the kinds of cases we used to derive IRI from Relevant Practical Coherence. So imagine an agent who faces a choice between accepting the status quo, call that \\(\\varphi\\), and taking some giant risk, call that \\(\\psi\\). The giant risk in this case will involve a huge monetary loss if \\(\\neg p\\), and a small non-monetary gain if \\(p\\). Stanley says, and I agree, that in such a case the agent doesn’t know \\(p\\), even if their belief in \\(p\\) is true, well supported by evidence, and so on. Moreover, he says, had \\(\\psi\\) not been a relevant option, the agent could have known \\(p\\). I agree, and I think Relevant Practical Coherence explains these intuitions well.\nRussell and Doris imagine two kinds of variants on Stanley’s case. In one variant the agent doesn’t care about the material loss associated with \\(\\psi \\wedge \\neg p\\). As I would put it, although their material wealth would decline precipitously in that case, their utility would not, because their utility is not tightly correlated with material wellbeing. Given that, the agent may well prefer \\(\\psi\\) to \\(\\varphi\\) unconditionally, and so would still know \\(p\\). Russell and Doris don’t claim this is a problem in itself, but they do think the conjunction of this with the previous paragraph is a problem. As they put it, “you should have reservations ... about what makes the knowledge claim true: not giving a damn, however enviable in other respects, should not be knowledge-making.” (Russell and Doris 2009, 432).\nTheir other variant involves an agent with so much money that the material loss is trifling to them. Since the difference in utility between having, say, eight billion dollars and seven billion dollars is not that high, perhaps they will again prefer \\(\\psi\\) to \\(\\varphi\\) unconditionally, so still know \\(p\\). But it is, allegedly, counterintuitive to have the knowledge that \\(p\\) turn on the agent’s wealth. As Russell and Doris say, “matters are now even dodgier for practical interest accounts, because money turns out to be knowledge making.” (Russell and Doris 2009, 433) And this isn’t just because wealth can purchase knowledge. As they say, “money may buy the instruments of knowledge ... but here the connection between money and knowledge seems rather too direct.” (Russell and Doris 2009, 433)\nThe first thing to note about this case is that indifference and wealth aren’t really producing knowledge. What they are doing is more like defeating a defeater. Remember that the agent in question had enough evidence, and enough confidence, that they would know \\(p\\) were it not for the practical circumstances. As I said in the introduction, practical considerations enter debates about knowledge in part because they are distinctive kinds of defeaters. It seems that’s what is going on here. And we have, somewhat surprisingly, independent evidence to think that indifference and wealth do matter to defeaters.\nConsider two variants on Gilbert Harman’s ‘dead dictator’ example (Harman 1973, 75). In the original example, an agent reads that the dictator has died through an actually reliable source. But there are many other news sources around, such that if the agent read them, she would lose her belief. Even if the agent doesn’t read those sources, their presence can constitute defeaters to her putative knowledge that the dictator died.\nIn our first variant on Harman’s example, the agent simply does not care about politics. It’s true that there are many other news sources around that are ready to mislead her about the dictator’s demise. But she has no interest in looking them up, nor is she at all likely to look them up. She mostly cares about literature, and will spend her day reading old novels. In this case, the misleading news sources are too distant, in a sense, to be defeaters. So she still knows the dictator has died. Her indifference towards politics doesn’t generate knowledge - the original reliable report is the knowledge generator - but her indifference means that a would-be defeater doesn’t gain traction.\nIt might be objected here that the agent doesn’t know the dictator has died because there are misleading reports around saying the dictator is alive, and she is in no position to rebut them. But this is too high a standard for knowledge. There are millions of people in Australia who know that humans are contributing to global warming on purely testimonial grounds. Many, perhaps even most, of these people would not be able to answer a carefully put together argument that humans are not contributing to global warming, such as an argument that picked various outlying statistics to mislead the reader. And such arguments certainly exist; the conservative parts of the media do as much as they can to play them up. But the mere existence of such arguments doesn’t defeat the average person’s testimonial knowledge about anthropogenic global warming. Similarly, the mere existence of misleading reports does not defeat our agent’s knowledge of the dictator’s death, as long as there is no nearby world where she is exposed to the reports. (Thanks here to an anonymous referee.)\nIn the second variant, the agent cares deeply about politics, and has masses of wealth at hand to ensure that she knows a lot about it. Were she to read the misleading reports that the dictator has survived, then she would simply use some of the very expensive sources she has to get more reliable reports. Again this suffices for the misleading reports not to be defeaters. Even before the rich agent exercises her wealth, the fact that her wealth gives her access to reports that will correct for misleading reports means that the misleading reports are not actually defeaters. So with her wealth she knows things she wouldn’t otherwise know, even before her money goes to work. Again, her money doesn’t generate knowledge – the original reliable report is the knowledge generator – but her wealth means that a would-be defeater doesn’t gain traction.\nThe same thing is true in Russell and Doris’s examples. The agent has quite a bit of evidence that \\(p\\). That’s why she knows \\(p\\). There’s a potential practical defeater for \\(p\\). But due to either indifference or wealth, the defeater is immunised. Surprisingly perhaps, indifference and/or wealth can be the difference between knowledge and ignorance. But that’s not because they can be in any interesting sense ‘knowledge makers,’ any more than I can make a bowl of soup by preventing someone from tossing it out. Rather, they can be things that block defeaters, both when the defeaters are the kind Stanley talks about, and when they are more familiar kinds of defeaters.\nTemporal Embeddings\nMichael Blome-Tillmann (2009) has argued that tense-shifted knowledge ascriptions can be used to show that his version of Lewisian contextualism is preferable to IRI. Like Russell and Doris, his argument uses a variant of Stanley’s Bank Cases.10 Let \\(O\\) be that the bank is open Saturday morning. If Hannah has a large debt, she is in a high-stakes situation with respect to \\(O\\). In Blome-Tillmann’s version of the example, Hannah had in fact incurred a large debt, but on Friday morning the creditor waived this debt. Hannah had no way of anticipating this on Thursday. She has some evidence for \\(O\\), but not enough for knowledge if she’s in a high-stakes situation. Blome-Tillmann says that this means after Hannah discovers the debt waiver, she could say\nI didn’t know \\(O\\) on Thursday, but on Friday I did.\nBut I’m not sure why this case should be problematic for any version of IRI, and very unsure why it should even look like a reductio of IRI. As Blome-Tillmann notes, it isn’t really a situation where Hannah’s stakes change. She was never actually in a high stakes situation. At most her perception of her stakes change; she thought she was in a high-stakes situation, then realised that she wasn’t. Blome-Tillmann argues that even this change in perceived stakes can be enough to make (1) true if IRI is true. Now actually I agree that this change in perception could be enough to make (1) true, but when we work through the reason that’s so, we’ll see that it isn’t because of anything distinctive, let alone controversial, about IRI.\nIf Hannah is rational, then given her interests she won’t be ignoring \\(\\neg O\\) possibilities on Thursday. She’ll be taking them into account in her plans. Someone who is anticipating \\(\\neg O\\) possibilities, and making plans for them, doesn’t know \\(O\\). That’s not a distinctive claim of IRI. Any theory should say that if a person is worrying about \\(\\neg O\\) possibilities, and planning around them, they don’t know \\(O\\). And that’s simply because knowledge requires a level of confidence that such a person simply does not show. If Hannah is rational, that will describe her on Thursday, but not on Friday. So (1) is true not because Hannah’s practical situation changes between Thursday and Friday, but because her psychological state changes, and psychological states are relevant to knowledge.\nWhat if Hannah is, on Thursday, irrationally ignoring \\(\\neg O\\) possibilities, and not planning for them even though her rational self wishes she were planning for them? In that case, it seems she still believes \\(O\\). After all, she makes the same decisions as she would as if \\(O\\) were sure to be true. But it’s worth remembering that if Hannah does irrationally ignore \\(\\neg O\\) possibilities, she is being irrational with respect to \\(O\\). And it’s very plausible that this irrationality defeats knowledge. That is, you can’t be irrational with respect to a proposition and know it. Irrationality excludes knowledge. In any case, I doubt this is the natural way to read Blome-Tillmann’s example. We naturally read Hannah as being rational, and if she is rational she won’t have the right kind of confidence to count as knowing \\(O\\) on Thursday.\nThere’s a methodological point here worth stressing. Doing epistemology with imperfect agents often results in facing tough choices, where any way to describe a case feels a little counterintuitive. If we simply hew to intuitions, we risk being led astray by just focussing on the first way a puzzle case is described to us. But once we think through Hannah’s case, we see perfectly good reasons, independent of IRI, to endorse IRI’s prediction about the case.\nProblematic Conjunctions\nBlome-Tillmann offers another argument against IRI, that makes heavy use of the notion of having enough evidence to know something. Here is how he puts the argument. (Again I’ve changed the numbering and some terminology for consistency with this paper.)\n\nSuppose that John and Paul have exactly the same evidence, while John is in a low-stakes situation towards \\(p\\) and Paul in a high-stakes situation towards \\(p\\). Bearing in mind that IRI is the view that whether one knows \\(p\\) depends on one’s practical situation, IRI entails that one can truly assert:\nJohn and Paul have exactly the same evidence for \\(p\\), but only John has enough evidence to know \\(p\\), Paul doesn’t.\n(Blome-Tillmann 2009, 328–29)\n\nAnd this is meant to be a problem, because (2) is intuitively false.\nBut IRI doesn’t entail any such thing. We can see this by looking at a simpler example that illustrates the way ‘enough’ works.\nGeorge and Ringo both have $6000 in their bank accounts. They both are thinking about buying a new computer, which would cost $2000. Both of them also have rent due tomorrow, and they won’t get any more money before then. George lives in New York, so his rent is $5000. Ringo lives in Syracuse, so his rent is $1000. Clearly, (REC) and (RAC) are true.\nREC\nRingo has enough money to buy the computer.\n\nRAC\nRingo can afford the computer.\n\nAnd (GEC) is true as well, though there’s at least a reading of (GAC) where it is false.\nGEC\nGeorge has enough money to buy the computer.\n\nGAC\nGeorge can afford the computer.\n\nFocus for now on (GEC). It is a bad idea for George to buy the computer; he won’t be able to pay his rent. But he has enough money to do so; the computer costs $2000, and he has $6000 in the bank. So (GEC) is true. Admittedly there are things close to (GEC) that aren’t true. He hasn’t got enough money to buy the computer and pay his rent. You might say that he hasn’t got enough money to buy the computer given his other financial obligations. But none of this undermines (GEC).\nNow just like George has enough money to buy the computer, Paul has enough evidence to know that \\(p\\). Paul can’t know that \\(p\\), just like George can’t buy the computer, because of his practical situation. But that doesn’t mean he doesn’t have enough evidence to know it. He clearly does have enough evidence, since he has the same evidence John has, and John knows that \\(p\\). So, contra Blome-Tillmann, IRI doesn’t entail this problematic conjunction.\nIn a footnote attached to this, Blome-Tillmann offers a reformulation of the argument.\n\nI take it that having enough evidence to ‘know \\(p\\)’ in \\(C\\) just means having evidence such that one is in a position to ‘know \\(p\\)’ in \\(C\\), rather than having evidence such that one ‘knows \\(p\\).’ Thus, another way to formulate (2) would be as follows: ‘John and Paul have exactly the same evidence for \\(p\\), but only John is in a position to know \\(p\\), Paul isn’t.’ (Blome-Tillmann 2009, 329n23)\n\nNow having enough evidence to know \\(p\\) isn’t the same as being in a position to know it, any more than having enough money to buy the computer puts George in a position to buy it. So I think this is more of a new objection than a reformulation of the previous point. But might it be a stronger objection? Might it be that IRI entails (PosK), which is false?\nPosK\nJohn and Paul have exactly the same evidence for \\(p\\), but only John is in a position to know \\(p\\), Paul isn’t.\n\nActually, it isn’t a problem that IRI says that (PosK) is true. In fact, almost any epistemological theory will imply that conjunctions like that are true. In particular, any epistemological theory that allows for the existence of defeaters which do not supervene on the possession of evidence will imply that conjunctions like (PosK) are true. For example, anyone who thinks that whether you can know that a barn-like structure is really a barn depends on whether there are non-barns in the neighbourhood that look like the structure you’re looking at will think that conjunctions like (PosK) are true. Again, it matters a lot that IRI is suggesting that traditional epistemologists did not notice that there are distinctively pragmatic defeaters. Once we see that, we’ll see that conjunctions like (PosK) are not surprising at all.\nConsider again Con, and his friend Mod who is disposed to reason by modus ponens and not by affirming the consequent. We could say that Con and Mod have the same evidence for \\(p\\), but only Mod is in a position to know \\(p\\). There are only two ways to deny that conjunction. One is to interpret ‘position to know’ so broadly that Con is in a position to know \\(p\\) because he could change his inferential dispositions. But then we might as well say that Paul is in a position to know \\(p\\) because he could get into a different ‘stakes’ situation. Alternatively, we could say that Con’s inferential dispositions count as a kind of evidence against \\(p\\). But that stretches the notion of evidence beyond a breaking point. Note that we didn’t say Con had any reason to affirm the consequent, just that he does. Someone might adopt, or change, a poor inferential habit because they get new evidence. But they need not do so, and we shouldn’t count their inferential habits as evidence they have.\nIf that case is not convincing, we can make the same point with a simple Gettier-style case.\n\nGetting the Job\nIn world 1, at a particular workplace, someone is about to be promoted. Agnetha knows that Benny is the management’s favourite choice for the promotion. And she also knows that Benny is Swedish. So she comes to believe that the promotion will go to someone Swedish. Unsurprisingly, management does choose Benny, so Agnetha’s belief is true.\nWorld 2 is similar, except there it is Anni-Frid who knows that Benny is the management’s favourite choice for the promotion, that Benny is Swedish. So she comes to believe that the promotion will go to someone Swedish. But in this world Benny quits the workplace just before the promotion is announced, and the management unexpectedly passes over a lot of Danish workers to promote another Swede, namely Björn. So Anni-Frid’s belief that the promotion will go to someone Swedish is true, but not in a way that she could have expected.\n\nIn that story, I think it is clear that Agnetha and Anni-Frid have exactly the same evidence that the job will go to someone Swedish, but only Agnetha is in a position to know this, Anni-Frid is not. The fact that an intermediate step is false in Anni-Frid’s reasoning, but not Agnetha’s, means that Anni-Frid’s putative knowledge is defeated, but Agnetha’s is not. And when that happens, we can have differences in knowledge without differences in evidence. So it isn’t an argument against IRI that it allows differences in knowledge without differences in evidence.\nHolism and Defeaters\nThe big lesson of the last few sections is that interests create defeaters. Sometimes an agent can’t know \\(p\\) because adding \\(p\\) to her stock of beliefs would introduce either incoherence or irrationality. The reason is normally that the agent faces some decision where it is, say, bad to do \\(\\varphi\\), but good to do \\(\\varphi\\) given \\(p\\). In that situation, if she adds \\(p\\), she’ll either incoherently think that it’s bad to do \\(\\varphi\\) although it’s good to do it given what is (by her lights) true. Moreover, the IRI theorist says, being incoherent in this way blocks knowledge, so the agent doesn’t know \\(p\\).\nBut there are other, more roundabout, ways in which interests can mean that believing \\(p\\) would entail incoherence. One of these is illustrated by an example alleged by Ram Neta to be hard for interest-relative theorists to accommodate.\n\nKate needs to get to Main Street by noon: her life depends upon it. She is desperately searching for Main Street when she comes to an intersection and looks up at the perpendicular street signs at that intersection. One street sign says “State Street” and the perpendicular street sign says “Main Street.” Now, it is a matter of complete indifference to Kate whether she is on State Street–nothing whatsoever depends upon it. (Neta 2007, 182)\n\nLet’s assume for now that Kate is rational; dropping this assumption introduces mostly irrelevant complications. That is, we will assume Kate is an expected utility maximiser. Kate will not believe she’s on Main Street. She would only have that belief if she took it to be settled that she’s on Main, and hence not worthy of spending further effort investigating. But presumably she won’t do that. The rational thing for her to do is to get confirming (or, if relevant, confounding) evidence for the appearance that she’s on Main. If it were settled that she was on Main, the rational thing to do would be to try to relax, and be grateful that she had found Main Street. Since she has different attitudes about what to do simpliciter and conditional on being on Main Street, she doesn’t believe she’s on Main Street.\nSo far so good, but what about her attitude towards the proposition that she’s on State Street? She has enough evidence for that proposition that her credence in it should be rather high. And no practical issues turn on whether she is on State. So she believes she is on State, right?\nNot so fast! Believing that she’s on State has more connections to her cognitive system than just producing actions. Note in particular that street signs are hardly basic epistemic sources. They are the kind of evidence we should be ‘conservative’ about in the sense of Pryor (2004). We should only use them if we antecedently believe they are correct. So for Kate to believe she’s on State, she’d have to believe the street signs she can see are correct. If not, she’d incoherently be relying on a source she doesn’t trust, even though it is not a basic source.11 But if she believes the street signs are correct, she’d believe she was on Main, and that would lead to practical incoherence. So there’s no way to coherently add the belief that she’s on State Street to her stock of beliefs. So she doesn’t know, and can’t know, that she’s either on State or on Main. This is, in a roundabout way, due to the high stakes Kate faces.\nNeta thinks that the best way for the interest-relative theorist to handle this case is to say that the high stakes associated with the proposition that Kate is on Main Street imply that certain methods of belief formation do not produce knowledge. And he argues, plausibly, that such a restriction will lead to implausibly sceptical results. But that’s not the only way for the interest-relative theorist to go. What they could, and I think should, say is that Kate can’t know she’s on State Street because the only grounds for that belief are intimately connected to a proposition that, in virtue of her interests, she needs very large amounts of evidence to believe.\nNon-Consequentialist Cases\nNone of the replies yet have leaned heavily on the last of the three points from the introduction, the fact that IRI is an existential claim. This reply will make heavy use of that fact.\nIf an agent is merely trying to get the best outcome for themselves, then it makes sense to represent them as a utility maximiser. But when agents have to make decisions that might involve them causing harm to others if certain propositions turn out to be true, then I think it is not so clear that orthodox decision theory is the appropriate way to model the agents. That’s relevant to cases like this one, which Jessica Brown has argued are problematic for the epistemological theories John Hawthorne and Jason Stanley have recently been defending.12\n\nA student is spending the day shadowing a surgeon. In the morning he observes her in clinic examining patient A who has a diseased left kidney. The decision is taken to remove it that afternoon. Later, the student observes the surgeon in theatre where patient A is lying anaesthetised on the operating table. The operation hasn’t started as the surgeon is consulting the patient’s notes. The student is puzzled and asks one of the nurses what’s going on:\nStudent: I don’t understand. Why is she looking at the patient’s records? She was in clinic with the patient this morning. Doesn’t she even know which kidney it is?\nNurse: Of course, she knows which kidney it is. But, imagine what it would be like if she removed the wrong kidney. She shouldn’t operate before checking the patient’s records. (Brown 2008, 1144–45)\n\nIt is tempting, but I think mistaken, to represent the payoff table associated with the surgeon’s choice as follows. Let Left mean the left kidney is diseased, and Right mean the right kidney is diseased.\n\n\nLeft\nRight\nRemove left kidney\n\\(1\\)\n\\(-1\\)\nRemove right kidney\n\\(-1\\)\n\\(1\\)\nCheck notes\n\\(1-\\varepsilon\\)\n\\(1-\\varepsilon\\)\n\nHere \\(\\varepsilon\\) is the trivial but non-zero cost of checking the chart. Given this table, we might reason that since the surgeon knows that she’s in the left column, and removing the left kidney is the best option in that column, she should remove the left kidney rather than checking the notes.\nBut that reasoning assumes that the surgeon does not have any obligations over and above her duty to maximise expected utility. And that’s very implausible, since consequentialism is a fairly implausible theory of medical ethics.13\nIt’s not clear exactly what obligation the surgeon has. Perhaps it is an obligation to not just know which kidney to remove, but to know this on the basis of evidence she has obtained while in the operating theatre. Or perhaps it is an obligation to make her belief about which kidney to remove as sensitive as possible to various possible scenarios. Before she checked the chart, this counterfactual was false: Had she misremembered which kidney was to be removed, she would have a true belief about which kidney was to be removed. Checking the chart makes that counterfactual true, and so makes her belief that the left kidney is to be removed a little more sensitive to counterfactual possibilities.\nHowever we spell out the obligation, it is plausible given what the nurse says that the surgeon has some such obligation. And it is plausible that the ‘cost’ of violating this obligation, call it \\(\\delta\\), is greater than the cost of checking the notes. So here is the decision table the surgeon faces.\n\n\nLeft\nRight\nRemove left kidney\n\\(1-\\delta\\)\n\\(-1-\\delta\\)\nRemove right kidney\n\\(-1-\\delta\\)\n\\(1-\\delta\\)\nCheck notes\n\\(1-\\varepsilon\\)\n\\(1-\\varepsilon\\)\n\nAnd it isn’t surprising, or a problem for an interest-relative theory of knowledge, that the surgeon should check the notes, even if she believes and knows that the left kidney is the diseased one. This is not to say that the surgeon does know that the left kidney is diseased, just that the version of IRI being defended here is neutral on that question.\nThere is a very general point here. It suffices to derive IRI that we defend principles like the following:\nWhenever maximising expected value is called for, one should maximise expected value conditional on everything one knows.\nMaximising expected value is called for often enough that there exist the kinds of pairs of cases IRI claims exist. That’s because in some cases, changing the options facing an agent will make it the case that which live option is best differs from which live option is best given \\(p\\), even though the agent antecedently knew \\(p\\).\nBut that doesn’t imply that maximising expected value is always called for. Especially in a medical case, it is hard to square an injunction like “Do No Harm!” with a view that one should maximise expected value, since maximising expected value requires treating harms and benefits symmetrically. What would be a problem for the version of IRI defended here was a case with the following four characteristics.\nMaximising expected value is called for in the case.\nConditional on \\(p\\), the action with the highest expected value is \\(\\varphi\\).\nIt would be wrong to do \\(\\varphi\\).\nThe agent knows \\(p\\).\nIt is tempting for the proponent of IRI to resist any attempted counterexample by claiming it is not really a case of knowledge. That might be the right thing to say in Brown’s case. But IRI defenders should remember that it is often a good move to deny that the first condition holds. Consequentialism is not an obviously correct theory of decision making in morally fraught situations; purported counterexamples that rely on it can therefore be resisted.\n\n\n\nBlome-Tillmann, Michael. 2009. “Contextualism, Subject-Sensitive Invariantism, and the Interaction of ‘Knowledge’-Ascriptions with Modal and Temporal Operators.” Philosophy and Phenomenological Research 79 (2): 315–31. https://doi.org/10.1111/j.1933-1592.2009.00280.x.\n\n\nBrown, Jessica. 2008. “Knowledge and Practical Reason.” Philosophy Compass 3 (6): 1135–52. https://doi.org/10.1111/j.1747-9991.2008.00176.x.\n\n\nFantl, Jeremy, and Matthew McGrath. 2002. “Evidence, Pragmatics, and Justification.” Philosophical Review 111: 67–94. https://doi.org/10.2307/3182570.\n\n\n———. 2009. Knowledge in an Uncertain World. Oxford: Oxford University Press.\n\n\nFeltz, Adam, and Chris Zarpentine. 2010. “Do You Know More When It Matters Less?” Philosophical Psychology 23 (5): 683–706. https://doi.org/10.1080/09515089.2010.514572.\n\n\nHam, Sandra A., Sarah Martin, and Harold W. Kohl III. 2008. “Changes in the Percentage of Students Who Walk or Bike to School-United States, 1969 and 2001.” Journal of Physical Activity and Health 5 (2): 205–15. https://doi.org/10.1123/jpah.5.2.205.\n\n\nHarman, Gilbert. 1973. Thought. Princeton: Princeton University Press.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHawthorne, John, and Jason Stanley. 2008. “Knowledge and Action.” Journal of Philosophy 105 (10): 571–90. https://doi.org/10.5840/jphil20081051022.\n\n\nMay, Joshua, Walter Sinnott-Armstrong, Jay G. Hull, and Aaron Zimmerman. 2010. “Practical Interests, Relevant Alternatives, and Knowledge Attributions: An Empirical Study.” Review of Philosophy and Psychology 1 (2): 265–73. https://doi.org/10.1007/s13164-009-0014-3.\n\n\nNeta, Ram. 2007. “Anti-Intellectualism and the Knowledge-Action Principle.” Philosophy and Phenomenological Research 75 (1): 180–87. https://doi.org/10.1111/j.1933-1592.2007.00069.x.\n\n\nPryor, James. 2000. “The Skeptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\n———. 2004. “What’s Wrong with Moore’s Argument?” Philosophical Issues 14 (1): 349–78. https://doi.org/10.1111/j.1533-6077.2004.00034.x.\n\n\nRussell, Gillian, and John M. Doris. 2009. “Knowledge by Indifference.” Australasian Journal of Philosophy 86 (3): 429–37. https://doi.org/10.1080/00048400802001996.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\nIt is a tricky exegetical question how many of the three features here must be read into defences of IRI in the literature. My reading is that they do not have to be read in, so it is not overly original of me to defend a version of IRI that does away with all three. But I know many people disagree with that. If they’re right, this paper is more original than I think it is, and so I’m rather happy to be wrong. But I’m going to mostly set these exegetical issues aside, and compare different theories without taking a stand on who originally promulgated them.↩︎\nSuch as the Bank Cases in Stanley (2005), or the Train Cases in Fantl and McGrath (2002).↩︎\nAnd this is true even though \\(p\\) is not a proposition about their interests, or something that is supported by propositions about their interests, and so on.↩︎\nI will consider, and tentatively support, one principle stronger than IRI in the final section. But the key point is that these general principles are not needed to defend IRI.↩︎\nThis points are expanded upon greatly in (Author Paper 1).↩︎\nIn the West Gate bridge collapse in Melbourne in 1971, a large number of the victims were underneath the bridge; the people on top of the bridge had a non-trivial chance of survival. That bridge was 200 feet above the water, not 1000, but I’m not sure the extra height would matter greatly. Again from a slightly lower height, over 90% of people on the bridge survived the I-35W collapse in Minneapolis in 2007.↩︎\nAs I believe was the case in the I-35W collapse.↩︎\nSee the massive drop in the numbers of students walking or biking to school, reported in Ham, Martin, and Kohl III (2008), for a sense of how big an issue this is.↩︎\nI think the objections I make here are similar in spirit to those Stanley made in a comments thread on Certain Doubts, though the details are new. The thread is at http://el-prod.baylor.edu/certain_doubts/?p=616.↩︎\nIn the interests of space, I won’t repeat those cases yet again here.↩︎\nThe caveats here about basic sources are to cancel any suggestion that Kate has to antecedently believe that any source is reliable before she uses it. As Pryor (2000) notes, that view is problematic. The view that we only get knowledge from a street sign if we antecedently have reason to trust it is not so implausible.↩︎\nThe target here is not directly the interest-relativity of their theories, but more general principles about the role of knowledge in action and assertion. But it’s important to see how IRI handles the cases that Brown discusses, since these cases are among the strongest challenges that have been raised to IRI.↩︎\nI’m not saying that consequentialism is wrong as a theory of medical ethics. But if it is right, so many intuitions about medical ethics are going to be mistaken that such intuitions have no evidential force. And Brown’s argument relies on intuitions about this case having evidential value. So I think for her argument to work, we have to suppose non-consequentialism about medical ethics.↩︎\n",
    "preview": "posts/2021-01-03-defending-interest-relative-invariantism/broadway.jpg",
    "last_modified": "2021-02-04T15:20:57-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-no-royal-road-to-relativism/",
    "title": "No Royal Road to Relativism",
    "description": "A reponse to _Relativism and Monadic Truth_. I argue that while the Cappelen and Hawthorne have good responses to the deductive arguments for relativism, there are various good inductive arguments for relativism that their view can't adequately respond to.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2011-01-01",
    "categories": [
      "language",
      "relativism",
      "on books"
    ],
    "contents": "\nRelativism and Monadic Truth is a sustained attack on ‘analytical relativism,’ as it has developed in recent years. The attack focusses on two kinds of arguments. One is the argument from the behaviour of operators, as developed by David Lewis (1980) and David Kaplan (1989). The other kind of argument takes off from phenomena concerning speech reports and disagreements. Such arguments play central roles in arguments by, among others, Andy Egan (2007), Max Kölbel (2008), Peter Lasersohn (2005), John MacFarlane (2003, 2007), Mark Richard (2004) and Tamina Stephenson (2007). These arguments also play a role in a paper that I co-authored with Andy Egan and John Hawthorne (Egan, Hawthorne, and Weatherson 2005).\nPublished in Analysis 71: 133-143.\nPicture by twodeadpoets via Creative Commons.\nAs the reader of Relativism and Monadic Truth can tell, John Hawthorne no longer much likes the arguments of that paper, nor its conclusions. And I think he’s right to be sceptical of some of the arguments we advanced. The objections that he and Herman Cappelen raise to arguments for relativism from speech reports and from disagreement are, I think, telling. But I don’t think those are the best arguments for relativism. (For what it’s worth, I don’t think they’re even the best arguments in the paper we co-authored.) The primary purpose of this note will be say a little about what some of these better arguments are. The core idea will be that although there is some data that is consistent with non-relativist theories, the best explanation of this data is that a kind of relativism is true. In short, we should be looking for inductive, not deductive, arguments for relativism. I’m going to fill in some details of this argument, and say a little about how it seemed to slip out of the main storyline of Relativism and Monadic Truth.\nIn Chapter 2 of Relativism and Monadic Truth, Cappelen and Hawthorne attempt to develop diagnostics for when an utterance type \\(S\\) has invariant content. They note that some relativist arguments presuppose a diagnostic based on speech reports. The idea behind the presupposed diagnostic is that if we can invariably report an utterance of \\(S\\) by \\(A\\) by saying A said that S, then \\(S\\) is semantically invariant. And they note that this diagnostic isn’t particularly reliable.\nWhat they aim to replace it with is a diagnostic based on agreement reports. Cappelen and Hawthorne are more careful on the details than I’ll be, but the rough idea is easy enough to understand. The diagnostic says that if whenever \\(A\\) and \\(B\\) utter \\(S\\), we can report that by saying A and B agree that S, and the basis for our saying this is that they made those utterances, then \\(S\\)’s content is invariant. The idea behind the test is that if there isn’t a single proposition that \\(A\\) and \\(B\\) endorse, then it would be odd to say that they agree.\nI don’t think the diagnostic is particularly plausible. The next couple of paragraphs won’t come as much of a surprise to the authors of Relativism and Monadic Truth, since the ideas come from a talk Herman Cappelen did at the Arché Summer School in July 2009. But they are central enough to the story I’m telling that they are worth including here. The core problem for this agreement based diagnostic is that sometimes we can report parties as agreeing even though they don’t agree on the truth value of any proposition. So while (1) has a disambiguation where it is true only if something like (2) is true, it also has a disambiguation where it is true as long as something like (3) is true.\nGiven that sentences like (1) could mean something like (3), there is little reason to think that agreement diagnostics will provide us clear evidence of sameness of content. Indeed, Cappelen and Hawthorne should hope that this diagnostic doesn’t always work, because the diagnostic seems to entail relativism about epistemic modals. Imagine that a detective and a psychic are both investigating a murder. They both conclude that their evidence entails that Tatort did it, and that their evidence is consistent with Tatort being dead. They are, however, ignorant of each other’s evidence, and indeed of the fact that the other is working on the investigation. Still, if each utters (4), it seems we are in a position to endorse (5).\nThis will be very hard to explain on a contextualist theory of epistemic modals, if we accept the agreement diagnostic. That’s because there’s no proposition (other than the proposition that Tatort is guilty) that they agree about. I think this is some evidence in favour of relativism, but if the contextualist wanted to argue that we should understand (4) the same way we understand (1) (on its ‘distributive’ disambiguation), it would be hard to conclusively show they were wrong.\nIn any case, it is hard to see why we should expect there to be a diagnostic of the kind Cappelen and Hawthorne are aiming for. Such diagnostics are the exception, not the rule, in social sciences. There’s no simple diagnostic for whether a particular state is democratic or not. (Is modern-day Afghanistan a democracy? What about modern-day Alabama?) Nor is there a simple diagnostic for whether a particular rule is a law. (Are internal revenue regulations laws?) But political science and jurisprudence don’t collapse in the absence of such diagnostics. Nor should philosophical semantics collapse in the absence of a simple test for context-sensitivity.\nIndeed, the situation is political science and in jurisprudence is in one sense worse than it is in semantics. We can state, admittedly in theory-laden terms, what it is for the content of a sentence type to be context-invariant or context-sensitive. It is much harder to state, even in theory-laden terms, what it is for a state to be democratic, or for a rule to be a law. The problem with thinking about the questions I asked in the previous paragraph isn’t that there’s some hidden piece of evidence we haven’t yet uncovered. It’s that the concepts do not have clear application conditions, and the hard cases fall between the clear instances and non-instances of the relevant property. In semantics we have, to a first approximation, a mere epistemic challenge.\nEven if the hunt for a diagnostic for context-sensitivity is bound to be futile, as I think it is, that doesn’t mean it is harmless. I think the structure of Cappelen and Hawthorne’s inquiry, which starts by looking for a test and then goes on to apply it, pushes us towards the wrong kind of argument. The effect of this structure is that we end up looking for deductive arguments for or against relativism, and the absence of deductive arguments for relativism is taken to be a big problem for the relativist. But we should have been looking for inductive arguments. The best case for relativism, I think, will be a kind of inference to the best explanation. For instance, a relativist might try to clean up this argument.\nOur best theory of mental content is that the contents of beliefs and desires do not satisfy Simplicity.1\nThe role of language is to express thoughts, so if the contents of belief and desire do not satisfy Simplicity, the contents of sentences and utterances probably don’t either.\nSimplicity is false as a theory of linguistic content.\nThis argument clearly isn’t valid. That’s by design; it’s meant to be an abductive argument against Simplicity about linguistic content. And of course both premises are controversial. There’s one argument for premise 1 in Lewis (1979), and another in Perry (1979). Both arguments are controversial. Indeed Cappelen and Hawthore spend some time (pages 50 to 54) responding to the Lewisian arguments, though they spend less time on Perry’s arguments.\nI’m not going to try to advance the debate here over whether premise 1 is true or not. I suspect the solution will turn on much bigger issues than can be covered in a note of this length. And that’s because I think the judgment about whether premise 1 is true will turn on quite global features of our best theory of mental content. For instance, Daniel Nolan (2006) argues that there are certain desires that we cannot understand on the modal Lewis offers. That doesn’t entail that Lewis is wrong about the nature of belief, but it does make Lewis’s theory of belief look less attractive. From the other direction, many authors working on the Sleeping Beauty problem, dating back to the problem’s introduction to the philosophical community in Elga (2000), have felt that the problem was best approached in Lewis’s Simplicity-unfriendly framework. That doesn’t entail Simplicity is wrong, but it is I think evidence against it. On the other hand, Robert Stalnaker (2008) has recently argued that this is not the best framework for thinking about the Sleeping Beauty problem, and I’ve argued (Weatherson 2011) that Stalnaker’s approach lets us see things about the Sleeping Beauty puzzle that are hidden on the standard, Lewisian, approach. So if we’re going to evaluate this kind of argument for relativism, the issues are going to get far removed from familiar disputes about distributions of words and phrases. That’s not too surprising. In general, the hard thing about abductive reasoning in philosophy is that we have to start looking at all sorts of different kinds of evidence. But that’s no reason to think that the most telling arguments won’t, at the end of the day, be abductive arguments.\nA quite different kind of argument comes from thinking about property ascription and ignorance. It’s a somewhat frequent occurrence that modern science discovers that some of our thoughts seem to depend for their truth on more variables than we realised. So it isn’t true that two accelerating objects simply have the same mass or different masses; rather, their relative mass might be different relative to different inertial frames. Or two colour patches might not be simply the same colour or simply different colours. If the colours are metamers (relative to human vision) then they will be in a good sense the same colour relative to human vision, and different colours relative to more discriminating detectors. Such cases raise challenges for the project of interpreting a language.\nAssume that the community uses terms like ‘mass.’ Indeed, assume they are sophisticated enough to distinguish mass from weight, for they know that weight is relative to a gravitational field, and gravitational fields vary in strength. But they are not sophisticated enough to know that masses are relative to inertial frames. The members of this community frequently go around saying things like “Those two objects have the same mass,” referring to \\(a\\) and \\(b\\). Call that sentence \\(M\\). We assume that the members are in a particular inertial frame, call it \\(F\\). Let’s assume (just for a few paragraphs) that the propositions that satisfy Simplicity are structured, and assume that we can represent the relation has the same mass as by a somewhat unstructured relation \\(SameMass\\). (In other words, ignore whatever internal structure \\(SameMass\\) has, since it won’t be relevant to this example.) Then it seems to me that there are three live options around.\nBy \\(M\\), the speakers express the pseudo-proposition \\(SameMass(a, b)\\), and this pseudo-proposition is not capable of being true or false, since \\(SameMass\\) is a three-place relation (between two objects and an inertial frame) and only two places are specified.\nBy \\(M\\), the speakers express the proposition \\(SameMass(a, b, F)\\), and this proposition is (capable of being) true.\nBy \\(M\\), the speakers express the proposition \\(SameMass(a, b)\\), and this is (capable of being) true relative to \\(F\\), although it might be false relative to some other inertial frame \\(F^{\\prime}\\).\nIf option 3 is correct, then it seems Simplicity fails.2 So if there are compelling arguments against options 1 and 2, and those are all the options, then Simplicity is in trouble. And it seems the relativist might make progress by pushing back against both of those options.\nThe simplest argument against option 1 is that it violates even a very weak form of the Principle of Charity. Obviously there are very many different kinds of charity principles. For instance, there are three different versions endorsed in Davidson (1970), Lewis (1974) and (Williamson 2007 Ch. 8). But any kind of Charity will imply that options 2 or 3 are preferable to option 1, since option 1 will imply that the subjects don’t even have beliefs about the relative masses of objects, whereas the other options will imply that their beliefs may well be true, and rational, and even in some cases amount to knowledge. An alternative argument against option 1 is that the members of that community would have been right to take it as a Moorean fact that some things have the same mass.3 So option 1 doesn’t look overly plausible.\nOne argument against premise 2 is that it is impossible for the members of the community, given their powers of individuation, to make singular reference to such a thing as an inertial frame. If they don’t know what an inertial frame is, then we might be sceptical of claims that they can refer to it. (Note that the thought here isn’t merely that some individuals don’t know what inertial frames are; the imagined case is that even experts don’t know about the kind of things that we would need to put into the propositions to give them simple truth values.) Another argument is that competent speakers of the language should be able to identify the number of argument places in the properties they use.\nNeither of the arguments just offered is completely compelling, though I think both are at least promising avenues for research. But both arguments do look notably weaker if we drop the assumption that the relevant propositions are structured. In an unstructured propositions framework, we perhaps don’t need to worry about the members making singular reference to things like inertial frames. We just need to have the speakers pick out (in a perhaps imperfect way) the worlds in which their beliefs are true. And in an unstructured propositions framework it isn’t clear that being unable to identify the number of arguments places in the properties they use is any more of a sign of linguistic incompetence than not knowing the individuals to which they refer. But it is a commonplace of semantic externalism that speakers can refer without knowing who it is they are referring to.\nThe arguments in the previous two paragraphs have been sketchy, to say the least. But if they can be developed into compelling arguments, then it might turn out that the case against option 2 succeeds iff propositions are structured. In that case the argument for Simplicity will turn on a very large question about the nature of propositions, namely whether they are structured or not. Again, the take home lesson is that debates in this area are not susceptible to easy resolution.\nI’ll end with a more narrowly linguistic abductive argument for relativism and against Simplicity. I think you can find the core ingredients of this argument in Egan, Hawthorne, and Weatherson (2005), though it isn’t as well individuated as it might have been. The argument takes off from what looks like a somewhat misleading claim in Cappelen and Hawthorne’s book. The context is a discussion of autocentric and exocentric uses of predicates.4 The distinction between autocentric and exocentric uses is important for thinking about the way various predicates are used, though it isn’t easy to give a theory-neutral characterisation of it. Assuming contextualism, Cappelen and Hawthorne note that it is easy to explain the distinction: “a use of a taste predicate is autocentric iff its truth conditions are given by a completion that indexes the predicate to the subject” and exocentric iff “its truth conditions are given by a completion that indexes it to a person or group other than the speaker, which may, however, include the speaker.” (104) The core idea here is clear enough, I hope, though as they say it requires a slightly different gloss if we assume relativism.5\nThe problem is what they go on to say about epistemic modals. In a footnote they say,\n\n[I]t is worth noting that there is a similar contrast between autocentric and exocentric uses of epistemic modals. If I see Sally hiding on a bus then I might in a suitable context say ‘She is hiding because I might be on the bus’ even though I know perfectly well that I am not on the bus. (‘Must’ is harder to use exocentrically, though we shall not undertake to explain this here.) (104n7)\n\nThe parenthetical remark seems mistaken, or at least misleading, and for an important reason. It is true that it is very hard to use ‘must’ exocentrically in a sentence of the form a must be F. But that doesn’t mean that it is hard to use ‘must’ exocentically. In fact it’s very easy. Almost any sentence of the form S believes that a must be F will have an exocentric use of ‘must.’ That’s because almost any use of an epistemic modal in the scope of a propositional attitude report will be ‘bound’ to the subject of that report. (I put ‘bound’ in scare quotes because although contextualists will think of this as literally a case of binding, non-contextualists may think something else is going on.) This suggests an argument for relativism about epistemic modals, one that seems to me to be quite a bit stronger than the arguments for relativism discussed in Relativism and Monadic Truth.\nUnembedded uses of epistemic modals are generally autocentric (except in the context of explanations, like ‘because I might be on the bus’).\nEpistemic modals embedded in propositional attitude reports are generally exocentric.\nThere is a good, simple relativist explanation of these two facts.\nThere is no good, simple explanation of these facts consistent with Simplicity.\nSo, relativism is true, and Simplicity is false.\nNote that I’m not for a minute suggesting that there is no Simplicity-friendly explanation of the facts to be had; just that it won’t be a very good explanation. Nor am I suggesting that the phenomena obtain universally, rather than just in most cases. But they obtain often enough to need explanation, and the best explanation will be relativist. And that, I think, is a reason to like relativism.\nThe simplest relativist explanation of premises 1 and 2 uses the idea, derived from Lewis (1979), that contents are \\(\\lambda\\)-abstracts. So the content of a must be F is roughly \\(\\lambda x.(x\\)’s evidence entails that \\(a\\) is \\(F).\\) A content \\(\\lambda x. \\phi(x)\\) is true relative to a person iff they are \\(\\phi\\), and believed by a person iff they consider themselves to be \\(\\phi\\), under a distinctively first-personal mode of presentation. Then a typical utterance of a must be F will be autocentric because if the asserter thinks it is true, they must take themselves to satisfy \\(\\lambda x.(x\\)’s evidence entails that \\(a\\) is \\(F).\\) So assuming they are speaking truly, the hearer can infer that the speaker’s evidence does indeed entail that \\(a\\) is \\(F\\). But a typical utterance of S believes that a must be F will be true just in case \\(S\\) takes themselves to satisfy \\(\\lambda x.(x\\)’s evidence entails that \\(a\\) is \\(F)\\), and hence will be true as long as \\(S\\)’s evidence, or at least what \\(S\\) takes to be their evidence, entails that \\(a\\) is \\(F\\). There’s no reference there to the speaker’s evidence, so the use of ‘must’ is exocentric. There are, to be sure, many details of this explanation that could use filling in, but what is clear is that there is a natural path from the view that contents are \\(\\lambda\\)-abstracts to the data to be explained. And that explanation is inconsistent with Simplicity.\nIs there a good, simple Simplicity-friendly explanation of the data around? I suspect there is not. There are two obvious places to look for a Simplicity-friendly explanation. We could look for an explanation that turns on the meaning of ‘must,’ or we could look for an explanation in terms of salience. On closer inspection, neither avenue is particularly promising.\nIt does seem likely that there is an available explanation of premise 1 in terms of the meaning of ‘must.’ The contextualist about pronouns has an easy explanation of why ‘we’ almost always picks out a group that includes the speaker. The explanation is just that it is part of the meaning of ‘we’ that it is a first-personal plural pronoun, so it is part of the meaning of ‘we’ that the group it picks out includes the speaker. We could argue that something similar goes on for ‘must.’ So a must be F means, roughly, that \\(x\\)’s evidence entails that \\(a\\) is \\(F\\), and it is part of the meaning of ‘must’ that \\(x\\) either is the speaker, or is a group that includes the speaker. The problem with this explanation is that it won’t extend to premise 2. And that’s because meanings (in the relevant sense) don’t change when we move into embedded contexts. For example if Jones says “Smith thinks that we will all get worse grades than she will get,” Jones isn’t accusing Smith of having the inconsistent belief that she will get lower grades than what she gets. Rather, the reference of ‘we’ is still a group that includes the speaker, not the subject of the propositional attitude report. On this model, you’d expect the truth condition of S believes that a must be F to be that \\(S\\) believes that \\(x\\)’s evidence entails that \\(a\\) is \\(F\\), where \\(x\\) is the speaker, or a group containing the speaker. But that’s typically not at all what it means. So this kind of explanation fails.\nThe problem for salience based explanations of premises 1 and 2 is that salience is too fragile an explanatory base to explain the data. Let’s say that in general we think a must be F means, roughly, that \\(x\\)’s evidence entails that \\(a\\) is \\(F\\), and \\(x\\) is generally the most salient knower in the context. Then we’d expect that it would be not too hard to read (6) in such a way that its truth condition is (6a) rather than (6b), and (7) in such a way that its truth condition is (7a) rather than (7b).\nAfter all, (6) and (7) make Jones’s evidence really salient. That evidence settles who the killer is! But, it seems, that isn’t salient enough to make (6a) or (7a) the preferred interpretation. That seems to be bad news for a salience-based explanation of the way we interpret epistemic modals.\nLike all abductive arguments, this argument is far from conclusive. One way for a proponent of Simplicity to respond to it would be to come up with a neater explanation of premises 1 and 2 in our abductive argument, without giving up Simplicity. Another way would be to argue that although there is no nice Simplicity-friendly explanation of the data, the costs of relativism are so high that we should shun the relativist explanation on independent grounds. I don’t pretend to have ready responses to either of these moves. All I want to stress is that these abductive arguments are generally stronger arguments for relativism than the arguments that are, correctly, dismissed in Relativism and Monadic Truth. Those arguments try to take a quick path to relativism, claiming that some data about reports, or disagreement, or syntax, entails relativism. I doubt any such argument works, in part because of the objections that Cappelen and Hawthorne raise. There is, as my title says, no royal road to relativism. But I doubt there’s a quick road away from relativism either. If the relativist can explain with ease patterns that perplex the contextualist, we have good reason to believe that relativism is in fact true.\n\n\nDavidson, Donald. 1970. “Mental Events.” In Experience and Theory, edited by Lawrence Foster and J. W. Swanson, 79–101. London: Duckworth.\n\n\nEgan, Andy. 2007. “Epistemic Modals, Relativism and Assertion.” Philosophical Studies 133 (1): 1–22. https://doi.org/10.1007/s11098-006-9003-x.\n\n\nEgan, Andy, John Hawthorne, and Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nElga, Adam. 2000. “Self-Locating Belief and the Sleeping Beauty Problem.” Analysis 60 (2): 143–47. https://doi.org/10.1093/analys/60.2.143.\n\n\nKaplan, David. 1989. “Demonstratives.” In Themes from Kaplan, edited by Joseph Almog, John Perry, and Howard Wettstein, 481–563. Oxford: Oxford University Press.\n\n\nKölbel, Max. 2008. “The Evidence for Relativism.” Synthese 166 (2): 375–95. https://doi.org/10.1007/s11229-007-9281-7.\n\n\nLasersohn, Peter. 2005. “Context Dependence, Disagreement and Predicates of Personal Taste.” Linguistics and Philosophy 28 (6): 643–86. https://doi.org/10.1007/s10988-005-0596-x.\n\n\nLewis, David. 1974. “Radical Interpretation.” Synthese 27 (3-4): 331–44. https://doi.org/10.1007/bf00484599.\n\n\n———. 1979. “Attitudes de Dicto and de Se.” Philosophical Review 88 (4): 513–43. https://doi.org/10.2307/2184646.\n\n\n———. 1980. “Index, Context, and Content.” In Philosophy and Grammar, edited by Stig Kanger and Sven Öhman, 79–100. Dordrecht: Reidel.\n\n\n———. 1989. “Dispositional Theories of Value.” Aristotelian Society Supplementary Volume 63 (1): 113–37. https://doi.org/10.1093/aristoteliansupp/63.1.89.\n\n\n———. 1994. “Humean Supervenience Debugged.” Mind 103 (412): 473–90. https://doi.org/10.1093/mind/103.412.473.\n\n\nMacFarlane, John. 2003. “Future Contingents and Relative Truth.” The Philosophical Quarterly 53 (212): 321–36. https://doi.org/10.1111/1467-9213.00315.\n\n\n———. 2007. “Relativism and Disagreement.” Philosophical Studies 132 (1): 17–31. https://doi.org/10.1007/s11098-006-9049-9.\n\n\nNolan, Daniel. 2006. “Selfless Desires.” Philosophy and Phenomenological Research 73 (3): 665–79. https://doi.org/10.1111/j.1933-1592.2006.tb00553.x.\n\n\nPerry, John. 1979. “The Problem of the Essential Indexical.” Noûs 13 (1): 3–21. https://doi.org/10.2307/2214792.\n\n\nRichard, Mark. 2004. “Contextualism and Relativism.” Philosophical Studies 119 (1-2): 215–42. https://doi.org/10.1023/b:phil.0000029358.77417.df.\n\n\nStalnaker, Robert. 2008. Our Knowledge of the Internal World. Oxford: Oxford University Press.\n\n\nStephenson, Tamina. 2007. “Judge Dependence, Epistemic Modals, and Predicates of Personal Taste.” Linguistics and Philosophy 30 (4): 484–525. https://doi.org/10.1007/s10988-008-9023-4.\n\n\nWeatherson, Brian. 2011. “Stalnaker on Sleeping Beauty.” Philosophical Studies 155 (3): 445–56. https://doi.org/10.1007/s11098-010-9613-1.\n\n\nWilliamson, Timothy. 2007. The Philosophy of Philosophy. Blackwell.\n\n\nSimplicity is Cappelen and Hawthorne’s name for the conjunction of theses they want to defend against the relativist. For our purposes, Simplicity about mental content is the view that the contents of beliefs and desires are propositions, and these propositions are simply true or simply false, not merely true or false relative to some or other parameter. Simplicity about linguistic content is the view that these same propositions, the ones that are simply true or false, are the contents of declarative utterances.↩︎\nI say ‘seems’ since I’m not sure exactly what it takes for there to be a notion of truth simpliciter. The argument on page 96 against the conjunction of Simplicity, Eternalism and Temporalism suggests that Cappelen and Hawthorne believe the following principle: If \\(p\\) is true in \\(C_1\\), and false in \\(C_2\\), and \\(C_1\\) and \\(C_2\\) both exist, then \\(p\\) is not either simply true or simply false. It’s not obvious to me why \\(p\\) couldn’t, in \\(C_1\\), be simply true, but I take it Cappelen and Hawthorne are using ‘simply’ in such a way as to exclude that. So option 3 is inconsistent with Simplicity.↩︎\nCompare the discussion of Moorean facts in (Lewis 1994, 489).↩︎\nThe terminology is from Lasersohn (2005).↩︎\nAs they also go on to note, things get very complicated in cases where the truth conditions turn on the nature of an idealised version of the speaker. An example of such a theory is the theory of value in Lewis (1989). One of the key points that Cappelen and Hawthorne make, and I think it is a very good point against a lot of claims for relativist theories concerning predicates of personal taste, is that this kind of case is very common when it comes to evaluative language.\n\n↩︎\n",
    "preview": "posts/2021-03-12-no-royal-road-to-relativism/royalroad.jpg",
    "last_modified": "2021-03-12T14:21:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-assertion-knowledge-and-action/",
    "title": "Assertion, Knowledge and Action",
    "description": "We argue against the knowledge rule of assertion, and in favour of integrating the account of assertion more tightly with our best theories of evidence and action. We think that the knowledge rule has an incredible consequence when it comes to practical deliberation, that it can be right for a person to do something that she can’t properly assert she can do. We develop some vignettes that show how this is possible, and how odd this consequence is. We then argue that these vignettes point towards alternate rules that tie assertion to sufficient evidence-responsiveness or to proper action. These rules have many of the virtues that are commonly claimed for the knowledge rule, but lack the knowledge rule’s problematic consequences when it comes to assertions about what to do.",
    "author": [
      {
        "name": "Ishani Maitra",
        "url": "https://lsa.umich.edu/philosophy/people/faculty/imaitra.html"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2010-03-23",
    "categories": [
      "epistemology",
      "language",
      "interest-relativity"
    ],
    "contents": "\n\nContents\nSpeaking about What to Do\nGoing to War\nBuying Flood Insurance\nArgument One: “That was your first mistake”\nArgument Two: “Actions speak louder than words”\nArgument Three: “What else could I do?”\n\nBases for Action and Assertion\nMarginal Wars\nMoore’s Paradox\nAction and Assertion\n\nIt is widely believed that the mere truth of p is insufficient for p to be properly assertable, even if p is relevant to current conversation. If a speaker simply guessed that p is true, then she shouldn’t say p, for example. There is some dissent from this view (e.g., Weiner (2005)), but it is something close to orthodoxy in the current literature on assertion that something further is needed. The most common ‘something else’ is knowledge: a speaker shouldn’t say p unless they know p. This view is nowadays commonly associated with Timothy Williamson (1996, 2000), but it has historical antecedents tracing back at least to Max Black’s (1952) paper “Saying and Disbelieving.”1 Call Williamson’s position The Knowledge Rule.\nPublished in Philosophical Studies 149: 99-118.\nWe’d like to thank Matthew Benton, Jessica Brown, Andy Egan, and Susanna Schellenberg, as well as an audience at the Bellingham Summer Philosophy Conference, for helpful discussion of earlier drafts of this paper.\nThe Knowledge Rule\nAssert that p only if you know that p.\n\nThis paper aims to raise trouble for The Knowledge Rule, and several related positions, by focussing on a particular kind of assertion. We’ll be looking at assertions about what is to be done. The boldest statement of our position is that if an agent should do X, then that agent is in a position to say that they should do X. (We’ll qualify this a little below, but it’s helpful to start with the bold position.) We argue, following Williamson’s ‘anti-luminosity’ arguments, that its being true that X is the thing to do for an agent doesn’t entail that that agent knows it’s the thing to do.2 If both these claims are true, then there will be cases where it is fine to assert that X is what to do, even though the agent doesn’t know this. So, The Knowledge Rule is mistaken. Slightly more formally, we’ll be interested in arguments of this structure.\n\nPicture by UK Parliament via Creative Commons.\n\nMaster Argument (First Attempt)\nIf act X is what to do for agent S, then S can properly assert that X is what to do (assuming that this assertion is relevant to the current conversation).\nIt is possible that X is what to do for S, even though S is not in a position to know this.\nSo, it is possible that S can properly assert that X is what to do even though she does not know, and is not even in a position to know, that X is what to do.\n\nIn section 1, we’ll motivate premise 1 with a couple of vignettes. In section 2, we’ll qualify that premise and make it more plausible. In section 3, we’ll motivate premise 2. In section 4, we’ll look at one of the positive arguments for The Knowledge Rule, the argument from Moore’s paradox, and conclude that it is of no help. In section 5, we’ll look at what could be put in place of The Knowledge Rule, and suggest two alternatives.\nThe Evidence Responsiveness Rule\nAssert that p only if your attitude towards p is properly responsive to the evidence you have that bears on p.\n\nThe Action Rule\nAssert that p only if acting as if p is true is the thing for you to do.\n\nWe’re not going to argue for these rules in detail; that would take a much longer paper. Nor are we going to decide between them. What we are going to suggest is that these rules have the virtues that are commonly claimed for The Knowledge Rule, but lack The Knowledge Rule’s problematic consequences when it comes to assertions about what to do.\nSpeaking about What to Do\nWe start by motivating premise 1 of the Master Argument with a couple of examples. Both cases are direct counterexamples to The Knowledge Rule, but we’re interested in the first instance in what the cases have in common. After presenting the vignettes, we offer three distinct arguments to show that, in such cases, it is proper for the speakers to assert what they do assert, even though they don’t know it to be true.\nGoing to War\nImagine that a country, Indalia, finds itself in a situation in which the thing for it to do, given the evidence available to its leaders, is to go to war against an enemy. (Those pacifists who think it is never right to go to war won’t like this example, but we think war can at least sometimes be justified.) But it is a close call. Had the evidence been a bit weaker, had the enemy been a little less murderous, or the risk of excessive civilian casualties a little higher, it would have been preferable to wait for more evidence, or use non-military measures to persuade the enemy to change its ways. So, while going to war is the thing to do, the leaders of Indalia can’t know this. We’ll come back to this in section 2, but the crucial point here is that knowledge has a safety constraint, and any putative knowledge here would violate this constraint.\nOur leaders are thus in a delicate position here. The Prime Minister of Indalia decides to launch the war, and gives a speech in the House of Commons setting out her reasons. All the things she says in the speech are true, and up to her conclusion they are all things that she knows. She concludes with (1).\nNow (1) is also true, and the Prime Minister believes it, but it is not something she knows. So, the Prime Minister violates The Knowledge Rule when she asserts (1). But it seems to us that she doesn’t violate any norms in making this assertion. We’ll have a lot more to say about why this is so in a few paragraphs. But first, here’s a less dramatic case that is also a counterexample to The Knowledge Rule, one that involves prudential judgments rather than moral judgments.\nBuying Flood Insurance\nRaj and Nik are starting a small business. The business is near a river that hasn’t flooded in recent memory, but around which there isn’t much flood protection. They could buy flood insurance which would be useful in a flood, naturally, but would be costly in the much more likely event that there is not a flood. Raj has done the calculations of the likelihood of a flood, the amount this would damage the business, the utility loss of not having this damage insured, and the utility loss of paying flood insurance premiums. He has concluded that buying flood insurance is the thing to do. As it happens, this was a good conclusion to draw: it does, in fact, maximise his (and Nik’s) expected utility over time. (It doesn’t maximise their actual utility, as there actually won’t be a flood over the next twelve months. So, the insurance premium is an expense they could have avoided. But that doesn’t seem particularly relevant for prudential evaluation. Prudential buyers of insurance should maximise expected utility, not actual utility. Or so we must say unless we want to be committed to the view that everyone who buys an insurance policy and doesn’t make a claim on it is imprudent.)\nBut again, it’s a close call. If there had been a little less evidence that a flood was a realistic possibility, or the opportunity cost of using those dollars on insurance premiums had been a little higher, or the utility function over different outcomes a little different, it would have been better to forego flood insurance. That suggests that safety considerations make it the case that Raj doesn’t know that buying flood insurance is the thing to do, though in fact it is.\nLet’s now assume Raj has done everything he should do to investigate the costs and benefits of flood insurance. We can imagine a conversation between him and Nik going as follows.\n\nNik: Should we get flood insurance?\nRaj: I don’t know. Hold on; I’m on the phone.\nNik: Who are you calling?\nRaj: The insurance agent. I’m buying flood insurance.\n\nThere is clearly a pragmatic tension in Raj’s actions here. But given The Knowledge Rule, there’s little else he can do. It would be a serious norm violation to say nothing in response to Nik’s question. And given that he can’t say “Yes” without violating The Knowledge Rule, he has to say “I don’t know.” Moreover, since by hypothesis buying flood insurance is the thing to do in his situation, he can’t not buy the insurance without doing the wrong thing. So, given The Knowledge Rule, he’s doing the best he can. But it’s crazy to think that this is the best he can do.\nWe think that these cases are problems for The Knowledge Rule. In particular, we think that in each case, there is a non-defective assertion of something that is not known. It seems to us intuitively clear that those assertions are non-defective, but for those who don’t share this intuition, we have three independent arguments. The arguments focus on Going to War, but they generalize easily enough to Buying Flood Insurance.\nArgument One: “That was your first mistake”\nImagine that the Prime Minister has a philosophical advisor. And the advisor’s job is to inform the Prime Minister whenever she violates a norm, and stay silent otherwise. If The Knowledge Rule is correct, then the advisor should stay silent as the Prime Minister orders the invasion, silent as the Prime Minister sets out the reasons for the invasion, then speak up at the very last line of the speech. That strikes us as absurd. It’s particularly absurd when you consider that the last line of the speech is supported by what came earlier in the speech, and the Prime Minister believes it, and asserts it, because it is well supported by what came earlier in the speech. Since we think this couldn’t be the right behaviour for the advisor, we conclude that there’s no norm violation in the Prime Minister asserting (1).\nWe’ve heard two replies to this kind of argument. According to one sort of reply, The Knowledge Rule is not meant to be an ‘all-things-considered’ norm. The defender of The Knowledge Rule can say that the Prime Minister’s assertion is defective because it violates that rule, but allow that it is nevertheless all-things-considered proper, because some other norm outweighs The Knowledge Rule on this occasion. We agree that The Knowledge Rule is not intended to be an all-things-considered norm. But even keeping clearly in mind the distinction between being defective in some respect and being defective all-things-considered, it is still deeply unintuitive to say that the Prime Minister’s assertion is defective in a respect. That is, we don’t think the philosophical advisor should speak up just at the very end of the Prime Minister’s speech even if she’s meant to observe all the norm violations (rather than just the all-things-considered norm violations).\nPerhaps the defender of The Knowledge Rule needn’t just appeal to an intuition here. Another reply we’ve heard starts from the premise that the Prime Minister’s assertion would be better, in a certain respect, if she knew that it was true. Therefore, there is a respect in which that assertion is defective, just as The Knowledge Rule requires. To this second reply, our response is that the premise is true, but the reasoning is invalid. Saying why requires reflecting a bit on the nature of norms.\nThere are lots of ways for assertions to be better. It is better, ceteris paribus, for assertions to be funny rather than unfunny. It is better for assertions to be sensitive rather than insensitive. (We mean this both in the Nozickian sense, i.e., an assertion is sensitive iff it wouldn’t have been made if it weren’t true, and in the Hallmark greeting card sense.) It is better for speakers to be certain of the truth of their assertions than for them to be uncertain. But these facts don’t imply that humour, sensitivity, or certainty are norms of assertion, for it doesn’t follow that assertions that lack humour (or sensitivity or certainty) are always defective. Similarly, the fact that it is better to know what you say than not doesn’t imply that asserting what you don’t know is always defective. In slogan form: Not every absence of virtue is a vice. We think knowledge is a virtue of assertions. (In fact, we think that pretty much every norm of assertion that has been proposed in the literature picks out a virtue of assertion.) What we deny is that the absence of knowledge is (always) a vice. Since not every absence of virtue is a vice, one can’t argue that the Prime Minister’s assertion is defective by arguing it could have been better. And that’s why the argument being considered is invalid.\nArgument Two: “Actions speak louder than words”\nIt’s a bit of folk wisdom that actions speak louder than words. It isn’t crystal clear just what this wisdom amounts to, but we think one aspect of it is that an agent incurs more normative commitments by doing X than by talking about X. But if The Knowledge Rule is right, then this piece of wisdom is in this aspect back-to-front. According to that rule, an agent incurs a greater normative commitment by saying that X is what to do than they do by just doing X. If they do X, and X is indeed what to do, then they’ve satisfied all of their normative commitments. If, by contrast, they say that X is what to do, then not only must X be what to do, but they must know this fact as well. This strikes us as completely back-to-front. We conclude that there is nothing improper about asserting that X is what to do (as the Prime Minister does), when X is in fact what to do.\nArgument Three: “What else could I do?”\nHere’s a quite different argument that Going to War is a counterexample to The Knowledge Rule.\nIf ending the speech the way she did was a norm violation, there is a better way for the Prime Minister to end her speech.\nThere is no better way for the Prime Minister to end the speech without saying something that she does not know to be true.\nSo, ending the speech the way she did was not a norm violation.\nSo, The Knowledge Rule is subject to counterexample.\nPremise 1 is a kind of ‘ought-implies-can’ principle, and as such, it isn’t completely obvious that it is true. But when we’ve presented this argument to various groups, the focus has always been on premise two. The common complaint has been that the Prime Minister could have ended the speech in one of the following ways, thereby complying with The Knowledge Rule.\nI’ve decided that going to war is the thing to do in the circumstances.\nI believe that going to war is the thing to do in the circumstances.\nIt seems to me that going to war is the thing to do in the circumstances.\nOur first reply to this suggestion is that we’d fire a speechwriter who recommended that a Prime Minister end such a speech in such a weaselly way, so this hardly counts as a criticism of premise 2. Our more serious reply is that even if the Prime Minister ended the speech this way, she’d still violate The Knowledge Rule. To see why this is so, we need to pay a little closer attention to what The Knowledge Rule says.\nNote that The Knowledge Rule is not a rule about what kind of declarative utterance you can properly make. An actor playing Hamlet does not violate The Knowledge Rule if he fails to check, before entering the stage, whether something is indeed rotten in the state of Denmark. The rule is a rule about what one asserts. And just as you can assert less than you declaratively utter (e.g., on stage), you can also assert more than you declaratively utter.3 For instance, someone who utters The F is G in a context in which it is common ground that a is the F typically asserts both that the F is G, and that a is G. Similarly, someone who utters I think that S typically asserts both asserts that they have a certain thought, and asserts the content of that thought. We can see this is so by noting that we can properly challenge an utterance of I think that S by providing reasons that S is false, even if these are not reasons that show that the speaker does not (or at least did not) have such a thought. In the context of her speech of the House of Commons, even if the Prime Minister were to end with one of the options above, she would still assert the same thing she would assert by uttering (1) in the circumstances, and she’d still be right to make such an assertion.\nBases for Action and Assertion\nOne might worry that premise 1 in our master argument is mistaken, in the following way. We said that if X is the thing to do for S, then S can say that X is what to do. But one might worry about cases where S makes a lucky guess about what is to be done. Above we imagined that Raj had taken all of the factors relevant to buying flood insurance into account. But imagine a different case, one involving Raj*, Raj’s twin in a similar possible world. Raj* decides to buy flood insurance because he consults his Magic 8-Ball. Then, even if buying flood insurance would still maximize his expected utility, it doesn’t seem right for Raj* to say that buying flood insurance is what to do.\nHere is a defence of premise 1 that seems initially attractive, though not, we think, ultimately successful. The Magic 8-ball case isn’t a clear counterexample to premise 1, it might be argued, because it isn’t clear that buying flood insurance for these reasons is the thing for Raj* to do. On one hand, we do have the concept of doing the right thing for the wrong reasons, and maybe that is the right way to describe what Raj* does if he follows the ball’s advice. But it isn’t clearly a correct way to describe Raj*. It’s not true, after all, that he’s maximising actual utility. (Remember that there will be no claims on the policy he buys.) And it isn’t clear how to think about expected utility maximisation when the entrepreneur in question relies on the old Magic 8-Ball for decision making. And we certainly want to say that there’s something wrong about this very decision when made using the Magic 8-Ball. So, perhaps we could say that buying flood insurance isn’t what to do for Raj* in this variant example, because he has bad reasons.\nBut this seems like a tendentious defence of the first premise. Worse still, it is an unnecessary defence. What we really want to focus on are cases where people do the right thing for the right reasons. Borrowing a leaf from modern epistemology, we’ll talk about actions having a basis. As well as there being a thing to do in the circumstances (or, more plausibly, a range of things to do), there is also a correct basis for doing that thing (or, more plausibly, a range of correct bases). What we care about is when S does X on basis B, and doing X on basis B is the thing to do in S’s situation. Using this notion of a basis for action, we can restate the main argument.\n\nMaster Argument (Corrected)\nIf doing X on basis B is what to do for agent S, then S can properly, on basis B, assert that X is what to do (assuming this is relevant to the conversation).\nIt is possible that doing X on basis B is what to do for S, even though S is not in a position to know, and certainly not in a position to know on basis B, that X is what to do.\nSo, it is possible that S properly can assert that X is what to do, even though she does not know, and is not even in a position to know, that X is what to do.\n\nWe endorse this version of the master argument. Since its conclusion is the denial of The Knowledge Rule, we conclude that The Knowledge Rule is mistaken. But we perhaps haven’t said enough about premise 2 to seal the argument. The next section addresses that issue.\nMarginal Wars\nThe argument for premise 2 is just a simple application of Williamson’s anti-luminosity reasoning. (The canonical statement of this reasoning is in (Williamson 2000 Ch. 4)).) Williamson essentially argues as follows, for many different values of p. There are many ways for p to be true, and many ways for it to be false. Some of the ways in which p can be true are extremely similar to ways in which it can be false. If one of those ways is the actual way in which p is true, then to know that p we have to know that situations very similar to the actual situation do not obtain. But in general we can’t know that. So, some of the ways in which p can be true are not compatible with our knowing that p is true. In Williamson’s nice phrase, p isn’t luminous, where a luminous proposition is one that can be known (by a salient agent) whenever it is true. The argument of this paragraph is called ‘an anti-luminosity argument,’ and we think that many instances of it are sound.\nThere is a crucial epistemic premise in the middle of that argument: that we can’t know something if it is false in similar situations. There are two ways that we could try to motivate this premise. First, we could try to motivate it with the help of conceptual considerations about the nature of knowledge. That’s the approach that Williamson takes. But his approach is controversial. It is criticised by Sainsbury (1995) and Weatherson (2004) on the grounds that his safety principle goes awry in some special cases. Sainsbury focuses on mathematical knowledge, Weatherson on introspective knowledge. But the cases in which we’re most interested in this paper – Indalia going to war, Raj and Nik buying flood insurance – don’t seem to fall into either of these problem categories. Nevertheless, rather than pursue this line, we’ll consider a different approach to motivating this premise.\nThe second motivation for the epistemic premise comes from details of the particular cases. In the two cases on which we’re focusing, the agents simply lack fine discriminatory capacities. They can’t tell some possibilities apart from nearby possibilities. That is, they can’t know whether they’re in one world or in some nearby world. That’s not because it’s conceptually impossible to know something that fine, but simply an unfortunate fact about their setup. If they can’t know that they’re not in a particular nearby world in which \\(\\neg\\)p, they can’t know p. Using variants of Going to War, we’ll describe a few ways this could come about.\nThe simplest way for this to come about is if war-making is the thing to do given what we know, but some of the crucial evidence consists of facts that we know, but don’t know that we know. Imagine that a crucial piece of Indalia’s case for war comes from information from an Indalian spy working behind enemy lines. As it turns out, the spy is reliable, so the leaders of Indalia can acquire knowledge from her testimony. But she could easily enough have been unreliable. She could, for instance, have been bought off by the enemy’s agents. As it happens, the amount of money that would have taken was outside the budget the enemy has available for counterintelligence. But had the spy been a little less loyal, or the enemy a little less frugal with the counterintelligence budget, she could easily have been supplying misinformation to Indalia. So, while the spy is a safe knowledge source, the Indalian leaders don’t know that she is safe. They don’t, for instance, know the size of the enemy’s counterintelligence budget, or how much it would take to buy off their spy, so for all they know, she is very much at risk of being bought off.\nIn this case, if the spy tells the Indalian leaders that p, they come to know that p, and they can discriminate p worlds from \\(\\neg\\)p worlds. But they don’t know that they know that p, so for all they know, they don’t know p. And for some p that they learn from the spy, if they don’t know p, then going to war isn’t the thing for them to do in the circumstances. So, given that they don’t know the spy is reliable, they don’t know that going to war is the thing for them to do. But the spy really is reliable, so they do know p, so going to war is indeed the thing for them to do.\nOr consider a slightly less fanciful case, involving statistical sampling. Part of the Prime Minister’s case for starting the war was that the enemy was killing his own citizens. Presumably she meant that he was killing them in large numbers. (Every country with capital punishment kills its own citizens, but arguably that isn’t a sufficient reason to invade.) In practice, our knowledge of the scope of this kind of governmental killing comes from statistical sampling. And this sampling has a margin of error. Now imagine that the Indalian leaders know that a sample has been taken, and that it shows that the enemy has killed n of his citizens, with a margin of error of m. So, assuming there really are n killings, they know that the enemy has killed between n - m and n + m of his citizens. Since knowing that he’s killed n - m people is sufficient to make going to war the thing to do, the war can be properly started.\nBut now let’s think about what the Indalian leaders know that they know in this case. The world where the enemy has killed n - m people is consistent with their knowledge. And their margin of error on estimates of how many the enemy has killed is m. So, if that world is actual, they don’t know the enemy has killed more than n - 2m of his citizens. And that knowledge might not be enough to make going to war the thing to do, especially if m is large. (Think about the case where m = n/2, for instance.) So, there’s a world consistent with their knowledge (the n - m killings world), in which they don’t know enough about what the enemy is doing to make going to war the thing to do. In general, if there’s a world consistent with your knowledge where p is false, you don’t know p. Letting p be Going to war is what to do, it follows then that they don’t know that going to war is what to do, even though it actually is the thing to do.\nAnother way we could have a borderline war is a little more controversial. Imagine a case where the leaders of Indalia know all the salient descriptive facts about the war. They know, at least well enough for present purposes, what the costs and benefits of the war might be. But it is a close call whether the war is the thing to do given those costs and benefits. Perhaps different plausible moral theories lead to different conclusions. Or perhaps the leaders know what the true moral theory is, but that theory offers ambiguous advice. We can imagine a continuum of cases where the true theory says war is clearly what to do at one end, clearly not what to do at another, and a lot of murky space between. Unless we are willing to give up on classical logic, we must think that somewhere there is a boundary between the cases where it is and isn’t what to do, and it seems in cases near the boundary even a true belief about what to do will be unsafe. That is, even a true belief will be based on capacities that can’t reliably discriminate situations where going to war is what to do from cases where it isn’t.\nWe’ve found, when discussing this case with others, that some people find this outcome quite intolerable. They think that there must be some epistemic constraints on war-making. And we agree. They go on to think that these constraints will be incompatible with the kind of cases we have in mind that make premise 2 true. And here we disagree. It’s worth going through the details here, because they tell us quite a bit about the nature of epistemic constraints on action.\nConsider all principles of the form\n(KW)\nGoing to war is N1 only if the war-maker knows that going to war is N2.\n\nwhere N1 and N2 are normative statuses, such as being the thing to do, being right, being good, being just, being utility increasing, and so on. All such principles look like epistemic constraints on war-making, broadly construed. One principle of this form would be that going to war is right only if the war-maker knows that going to war is just. That would be an epistemic constraint on war-making, and a plausible one. Another principle of this form would be that going to war is the thing to do only if the war-maker knows that going to war increases actual utility. That would be a very strong epistemic constraint on war-making, one that would rule out pretty much every actual war, and one that is consistent with the anti-luminosity argument with which we started this section. So, the anti-luminosity argument is consistent with there being quite strong epistemic constraints on war-making.\nWhat the anti-luminosity argument is not consistent with is there being any true principle of the form (KW) where N1 equals N2. In particular, it isn’t consistent with the principle that going to war is the thing to do only if the war maker knows that it is the thing to do. But that principle seems quite implausible, because of cases where going to war is, but only barely, the thing to do. More generally, the following luminosity of action principle seems wrong for just about every value of X.\n(LA)\nX is the thing for S to do only if S knows that X is the thing for her to do.\n\nNot only is (LA) implausible, things look bad for The Knowledge Rule if it has to rely on (LA) being true. None of the defenders of The Knowledge Rule has given us an argument that (LA) is true. One of them has given us all we need to show that (LA) is false! It doesn’t look like the kind of principle that The Knowledge Rule should have to depend upon. So, defending The Knowledge Rule here looks hopeless.\nNote that given premise 1 of the Master Argument, as corrected, every instance of (LA) has to be true for The Knowledge Rule to be universally true. Let’s say that you thought (LA) was true when X is starting a war, but not when X is buying flood insurance. Then we can use the case of Raj and Nik to show that The Knowledge Rule fails, since Raj can say that buying flood insurance is what to do in a case where it is what to do, but he doesn’t know this.\nOne final observation about the anti-luminosity argument. Given the way Williamson presents the anti-luminosity argument, it can appear that in all but a few cases, if p, the salient agent can know that p. After all, the only examples Williamson gives are cases that are only picked out by something like the Least Number Theorem. So, one might think that while luminosity principles are false, they are approximately true. More precisely, one might think that in all but a few weird cases near the borderline, if p, then a salient agent is in a position to know p. If so, then the failures of luminosity aren’t of much practical interest, and hence the failures of The Knowledge Rule we’ve pointed out aren’t of much practical interest.\nWe think this is all mistaken. Luminosity failures arise because agents have less than infinite discriminatory capacities. The worse the discriminatory capacities, the greater the scope for luminosity failures. When agents have very poor discriminatory capacities, there will be very many luminosity failures. This is especially marked in decision-making concerning war. The fog of war is thick. There is very much that we don’t know, and what we do know is based on evidence that is murky and ephemeral. There is very little empirical information that we know that we know. If there are certain actions (such as starting a war) that are proper only if we know a lot of empirical information, the general case will be that we cannot know that these actions are correct, even when they are. This suggests that luminosity failures, where an action is correct but not known to be correct, or a fact is known but not known to be known, are not philosophical curiosities. In epistemically challenging environments, like a war zone, they are everyday facts of life.\nMoore’s Paradox\nThere is a standard argument for The Knowledge Rule that goes as follows. First, if the Knowledge Rule did not hold, then certain Moore paradoxical assertions would be acceptable. In particular, it would be acceptable to assert q, but I don’t know that q.4 But second, Moore paradoxical assertions are never acceptable. Hence, The Knowledge Rule holds. We reject both premises of this argument.\nTo reject the first premise, it suffices to show that some rule other than The Knowledge Rule can explain the unacceptability of Moore paradoxical assertions. Consider, for example, The Undefeated Reason rule.\nThe Undefeated Reason Rule\nAssert that p only if you have an undefeated reason to believe that p.\n\nThe Undefeated Reason Rule says that q but I don’t know that q can be asserted only if the speaker has an undefeated reason to believe it. That means the speaker has an undefeated reason to believe each conjunct. That means that the speaker has an undefeated reason to believe that they don’t know q. But in every case where it is unacceptable to both assert q and assert that you don’t know q, the speaker’s undefeated reason to believe they don’t know q will be a defeater for her belief that q. If you have that much evidence that you don’t know q, that will in general defeat whatever reason you have to believe q.\nWe don’t claim that The Undefeated Reason Rule is correct. (In fact, we prefer the rules we’ll discuss in section 5.) We do claim that it provides an alternative explanation of the unacceptability of instances of q but I don’t know that q. So, we claim that it undermines the first premise of Williamson’s argument from that unacceptability to The Knowledge Rule.\nWe also think that Williamson’s explanation of Moore paradoxicality over-generates. There is generally something odd about saying q but I don’t know that q. We suspect that the best explanation for why this is odd will be part of a broader explanation that also explains, for instance, why saying I promise to do X, but I’m not actually doing to do X is also defective. Williamson’s explanation isn’t of this general form. He argues that saying q but I don’t know that q is defective because it is defective in every context to both assert q and assert that you don’t know that q. But we don’t think that it is always defective to make both of these assertions.5 In particular, if a speaker is asked whether q is true, and whether they know that q, it can be acceptable to reply affirmatively to the first question, but negatively to the second one. If so, then the second premise of Williamson’s argument from Moore paradoxicality is also false.\nImagine that the Indalian Prime Minister is a philosopher in her spare time. After the big speech to Parliament she goes to her Peninsula Reading Group. It turns out Michael Walzer and Tim Williamson are there, and have questions about the speech.\n\nTW: Do you agree that knowledge requires safety?\nPM: Yes, yes I do.\nTW: And do you agree that your belief that going to war is the thing to do is not safe?\nPM: Right again.\nTW: So, you don’t know that going to war is the thing to do?\nPM: You’re right, I don’t.\nMW: But is it the thing to do?\nPM: Yes.\n\nThe Prime Minister’s answers in this dialogue seem non-defective to us. But if Williamson’s explanation of why Moore paradoxical utterances are defective is correct, her answers should seem defective. So, Williamson’s explanation over-generates. Whether or not it is true that all assertions of sentences of the form q but I don’t know that q are defective, it isn’t true that there is a defect in any performance that includes both an assertion of q and an assertion of the speaker’s ignorance as to whether q. The Prime Minister’s performance in her reading group is one such performance. So, the explanation of Moore paradoxicality cannot be that any such performance would violate a norm governing assertion.\nTo sum up, then, we’ve argued that The Knowledge Rule (a) fails to be the only explanation of Moore paradoxicality, and (b) misclassifies certain performances that are a little more complex than simple conjunctive assertions as defective. So, there’s no good argument from Moore paradoxicality to The Knowledge Rule.\nAction and Assertion\nIf we’re right, there’s a striking asymmetry between certain kinds of assertions. In the war example, early in her speech, the Prime Minister says (2).\nThat’s not the kind of thing she could properly say if it could easily have been false given her evidence. And like many assertions, this is not an assertion whose appropriateness is guaranteed by its truth. Asserting (2) accuses someone of murder, and you can’t properly make such accusations without compelling reasons, even if they happen to be true. On the other hand, we say, the truth of (1) does (at least when it is accepted on the right basis) suffice to make it properly assertable.\nThere’s a similar asymmetry in the flood insurance example. In that example, (3) is true, but neither Raj nor Nik knows it.\nAgain, in these circumstances, this isn’t the kind of thing Raj can properly say. Even though (3) is true, it would be foolhardy for Raj to make such a claim without very good reasons. By contrast, again, we say that Raj can properly assert that the thing to do, in their circumstances, is to buy flood insurance, even though he does not know this.\nThere are two directions one could go at this point. If we’re right, any proposed theory of the norms governing assertion must explain the asymmetry. Theories that cannot explain it, like The Knowledge Rule, or the Certainty Rule proposed by Jason Stanley (2008), or the Rational Credibility Rule proposed by Igor Douven (2006), are thereby refuted.\nThe Certainty Rule\nAssert only what is certain.\n\nThe Rational Credibility Rule\nAssert only what is rationally credible.\n\nThe Certainty Rule fails since the Prime Minister is not certain of (1). And the Prime Minister can’t be certain of (1), since certainty requires safety just as much as knowledge does.\nIt’s a little harder to show our example refutes The Rational Credibility Rule. Unlike knowledge, a safety constraint is not built into the concept of rational credibility. (Since rational credibility does not entail truth, in Douven’s theory, it can hardly entail truth in nearby worlds.) But we think that safety constraints may still apply to rational credibility in some particular cases. If you aren’t very good at judging building heights of tall buildings to a finer grain than 10 meters, then merely looking at a building that is 84 meters tall does not make it rationally credible for you that the building is more than 80 meters tall. In general, if your evidence does not give you much reason to think you are not in some particular world where p is false, and you didn’t have prior reason to rule that world out, then p isn’t rationally credible. So, when evidence doesn’t discriminate between nearby possibilities, and p is false in nearby possibilities, p isn’t rationally credible.\nAnd that, we think, is what happens in our two examples. Just as someone looking at an 84 meter building can’t rationally credit that it is more than 80 meters tall, unless they are abnormally good at judging heights, agents for whom X is just barely the thing to do can’t rationally credit that X is the thing to do. By The Rational Credibility Rule, they can’t say X is the thing to do. But they can say that; that’s what our examples show. So, The Rational Credibility Rule must be wrong.\nBut we can imagine someone pushing in the other direction, perhaps with the help of this abductive argument.\nA speaker can only assert things like (2) or (3) if they know them to be true.\nThe best explanation of premise 1 of this argument is The Knowledge Rule.\nSo, The Knowledge Rule is correct.\nThis isn’t a crazy argument. Indeed, it seems to us that it is implicit in some of the better arguments for The Knowledge Rule. But we think it fails. And it fails because there are alternative explanations of the first premise, explanations that don’t make mistaken predictions about the Prime Minister’s speech. For instance, we might have some kind of Evidence Responsiveness Rule.\nThe Evidence Responsiveness Rule\nAssert that p only if your attitude towards p is properly responsive to the evidence you have that bears on p.\n\nGiven how much can be covered by ‘properly,’ this is more of a schema than a rule. Indeed, it is a schema that has The Knowledge Rule as one of its precisifications. In Knowledge and Its Limits, Williamson first argues that assertion is “governed by a non-derivative evidential rule” (249), and then goes on to argue that the proper form of that rule is The Knowledge Rule. We agree with the first argument, and disagree with the second one.6\nNote that even a fairly weak version of The Evidence Responsiveness Rule would explain what is going on with cases like (1) and (2). Starting a war is a serious business. You can’t properly do it unless your views about the war are evidence responsive in the right way. You can’t, that is, correctly guess that starting the war is the thing to do. You can correctly guess that starting the war will be utility maximizing. And you can correctly guess that starting the war would be what to choose if you reflected properly on the evidence you have, and the moral significance of the choices in front of you. But you simply can’t guess that starting the war is what to do, and be right. If you’re merely guessing that starting a war is thing to do, then you’re wrong to start that war. So, if (1) is true, and the Prime Minister believes it, her belief simply must be evidence responsive. Then, by The Evidence Responsiveness Rule, she can assert it.\nFor most assertions, however, this isn’t the case. Even if it’s true that it will rain tomorrow, the Prime Minister’s could believe that without her belief being evidence responsive. In general, p does not entail that S even believes that p, let alone that this belief of S’s is evidence responsive. But in cases like (1), this entailment does hold, and that’s what explains the apparent asymmetry that we started this section with.\nThe Evidence Responsiveness Rule also handles so called ‘lottery propositions’ nicely. If you know that the objective chance of p being true is c, where c is less than 1, it will seem odd in a lot of contexts to simply assert p. In his arguments for The Knowledge Rule, Williamson makes a lot of this fact. In particular, he claims that the best explanation for this is that we can’t know that p on purely probabilistic grounds. This has proven to be one of the most influential arguments for The Knowledge Rule in the literature. But some kind of Evidence Responsiveness Rule seems to handle lottery cases even more smoothly. In particular, an Evidence Responsiveness Rule that allows for what constitutes ‘proper’ responsiveness to be sensitive to the interests of the conversational participants will explain some odd features concerning lottery propositions and assertability.\nIn the kind of cases that motivate Williamson, we can’t say p where it is objectively chancy whether p, and the chance of p is less than 1. But there’s one good sense in which such an assertion would not be properly responsive to the evidence. After all, in such a case there’s a nearby world, with all the same laws, and with all the same past fatcs, and in which the agent has all the same evidence, in which p is false. And the agent knows all this. That doesn’t look like the agent is being properly responsive to her evidence.\nOn the other hand, we might suspect that Williamson’s arguments concerning lottery propositions overstate the data. Consider this old story from David Lewis (1996).7\n\nPity poor Bill! He squanders all his spare cash on the pokies, the races, and the lottery. He will be a wage slave all his days … he will never be rich. (Lewis 1996, 443 in reprint)\n\nThese seem like fine assertions. One explanation of the appropriateness of those assertions combines The Knowledge Rule with contextualism about assertion.8 But contextualism has many weaknesses, as shown in Hawthorne (2004) and Stanley (2005). A less philosophically loaded explanation of Lewis’s example is that proper responsiveness comes in degrees, and for purposes of talking about Bill, knowing that it’s overwhelmingly likely that he’s doomed to wage slavery is evidence enough to assert that he’ll never be rich. The details of this explanation obviously need to be filled in, but putting some of the sensitivity to conversational standards, or practical interests, into the norms of assertion seems to be a simpler explanation of the data than a contextualist explanation. (It would be a priori quite surprising if the norms of proper assertion were not context-sensitive, or interests-sensitive. The norms of appropriateness for most actions are sensitive to context and interests.) So The Evidence Responsiveness Rule seems more promising here than The Knowledge Rule.\nA harder kind of case for The Knowledge Rule concerns what we might call ‘academic assertions.’ This kind of case is discussed in Douven (2006) and in Maitra (2010). In academic papers, we typically make assertions that we do not know. We don’t know that most of the things we’ve said here are true. (Before the last sentence we’re not sure we knew that any of the things we said were true.) But that’s because knowledge is a bad standard for academic discourse. Debate and discussion would atrophy if we had to wait until we had knowledge before we could present a view. So, it seems that assertion can properly outrun knowledge in academic debate.\nAgain, a context-sensitive version of The Evidence Responsiveness Rule explains the data well. Although you don’t need to know things to assert them in philosophy papers, you have to have evidence for them. We couldn’t have just spent this paper insisting louder and louder that The Knowledge Rule is false. We needed to provide evidence, and hopefully we’ve provided a lot of it. In some contexts, such as testifying in court, you probably need more evidence than what we’ve offered to ground assertions. But in dynamic contexts of inquiry, where atrophy is to be feared more than temporary mistakes, the standards are lower. Good evidence, even if not evidence beyond any reasonable doubt, or even if not enough for knowledge, suffices for assertion. That’s the standard we typically hold academic papers to. Like with lotteries, we think the prospects of explaining these apparently variable standards in terms of a norm of assertion that is context-sensitive are greater than the prospects for explaining them in terms of contextually sensitive knowledge ascriptions.\nHere’s a different and somewhat more speculative proposal idea for a rule that also explains the asymmetry we started this section with. We call it the Action Rule.\nThe Action Rule\nAssert that p only if acting as if p is true is the thing for you to do.\n\nWe take the notion of acting as if something is true from Stalnaker (1973). Intuitively, to act as if p is true is to build p into one’s plans, or to take p for granted when acting. This, note, is not the same as using p as a basis for action. When Raj buys flood insurance, he acts as if buying flood insurance is the thing to do. But the fact that buying flood insurance is the thing to do isn’t the basis for his action. (Since he does not know this, one might suspect it wouldn’t be a good basis.) Instead his basis is what he knows about the river, and his business, and its vulnerability to flooding. When an agent is trying to maximise the expected value of some variable (e.g., utility, profit, etc.), then to act as if p is true is simply to maximise the conditional expected value of that variable, in particular, to maximise the expected value of that variable conditional on p. Even when one is not maximising any expected value, we can still use the same idea. To act as if p is to take certain conditional obligations or permissions you have – in particular, those obligations or permissions that are conditional on p – to be actual obligations or permissions.\nTo see how The Action Rule generates the intended asymmetry, we’ll need a bit of formalism. Here are the terms that we will use.\nX denotes an action, agent, circumstance triple \\(\\langle\\)XAction, XAgent, XCircumstance\\(\\rangle\\). We take such triples to have a truth value. X is true iff XAgent performs XAction in XCircumstance.\nThingToDo(X) means that X is the thing to do for XAgent in XCircumstance.\nAct(S,p) means that agent S acts as if p is true.\nAssert(S,p) means that agent S can properly assert that p.\nSo, The Action Rule is this.\n\nAssert(S,p) \\({\\rightarrow}\\) ThingToDo(Act(S,p))\n\nIn our derivations, the following equivalence will be crucial.\n\nAct(XAgent,ThingToDo(X)) \\({\\leftrightarrow}\\) X\n\nThat is, acting as if X is what to do (in your circumstances) is simply to do X (in those circumstances). And in doing X, you’re acting as if X is what to do (in your circumstances). We take this equivalence to be quite resilient; in particular, it holds under operators like ‘ThingToDo.’ So, adding that operator to the previous equivalence, we get another equivalence.\n\nThingToDo(Act(XAgent,ThingToDo(X))) \\({\\leftrightarrow}\\) ThingToDo(X)\n\nIf we substitute ThingToDo(X) for p in The Action Rule, we get this.\n\nAssert(XAgent,ThingToDo(X)) \\({\\rightarrow}\\) ThingToDo(Act(XAgent,ThingToDo(X)))\n\nBut by the equivalence we derived earlier, that’s equivalent to the following.\n\nAssert(XAgent,ThingToDo(X)) \\({\\rightarrow}\\) ThingToDo(X)\n\nSo, we get the nice result that The Action Rule is trivially satisfied for any true claim about what is to be done. That is, for the special case where p is X is the thing for you to do, The Action Rule just reduces to something like the Truth Rule. And so we get a nice explanation of why the Prime Minister and Raj can properly make their assertions about what to do in their respective circumstances.9\nTo explain the other side of the asymmetry with which we began this section, note that these biconditionals do not hold where p is an arbitrary proposition, and S an arbitrary agent.\n\nThingToDo(Act(S,p)) \\({\\leftrightarrow}\\) p\nAct(S,ThingToDo(Act(S,p))) \\({\\leftrightarrow}\\) p\n\nTo see this, let p be the proposition expressed by (4). To act as if this is true is to, inter alia, not buy flood insurance. If there won’t be a flood, buying flood insurance is throwing away money, and when you’re running a business, throwing away money isn’t the thing to do. In symbols, Act(Raj and Nik,p) is equivalent to Raj and Nik don’t buy flood insurance. But not buying flood insurance is not the thing to do. The prudent plan is to buy flood insurance. So, ThingToDo(Act(Raj and Nik,p)) is false, even though p is true. So, the first biconditional fails. Since Raj and Nik do go on to buy flood insurance, i.e., since they don’t act as if ThingToDo(Act(Raj and Nik,p)), the left-hand-side of the second biconditional is also false. But again, the right-hand-side is true. So, that biconditional is false as well. And without those biconditionals, The Action Rule doesn’t collapse into Assert(S,p) \\({\\rightarrow}\\) p.\nWe have thus far argued that The Action Rule can provide an explanation for the asymmetry we noted at the beginning of this section.10 This is not, however, meant to be anything like a complete defence of that rule. That would require a lot more than we’ve provided here. But we do think that the Action Rule can explain a lot of the phenomena that are meant to motivate The Knowledge Rule, as well as some phenomena The Knowledge Rule struggles with.But we do think The Action Rule has some virtues. We’ll close with a discussion of how it explains the two kinds of cases that we argued that The Evidence Responsiveness Rule handles well.\nTo see this, consider first ‘lottery propositions.’ If you know that the objective chance of p being true is c, where c is less than 1, it will seem odd in a lot of contexts to simply assert p. In his arguments for The Knowledge Rule, Williamson makes a lot of this fact. In particular, he claims that the best explanation for this is that we can’t know that p on purely probabilistic grounds. This has proven to be one of the most influential arguments for The Knowledge Rule in the literature.\nWe suggest that The Action Rule can offers an alternative a nice explanation for why it’s often defective to assert lottery propositions. Note first that inIn a lot of cases, it isn’t rational for us to act on p when we have only purely probabilistic evidence for it, especially when acting on p amounts to betting on p at sufficiently unfavourable odds. This point is something of a staple of the ‘interest-relative-invariantism’ literature on knowledge.11 To take a mundane case, imagine that you’re cleaning up your desk, and you come across some lottery tickets. Most are for lotteries that have passed, that you know you lost. One ticket, however, is for a future lottery, which you know you have very little chance of winning. In such a case, to act as if the ticket for the future lottery would lose would be to throw it out along with the other tickets. But that would be irrational, and not at all how we’d act in such a case. That is to say, in such a case, we don’t (and shouldn’t, rationally speaking) act as if the ticket for the future lottery will lose, even though we take that outcome to be highly probable.\nIf acting as if a lottery proposition is true isn’t the thing to do, then The Action Rule will say that asserting such a proposition defective. Therefore, we think that The Action Rule can capture why in many cases you can’t in general assert lottery propositions.\nA harder kind of case for The Knowledge Rule concerns what we might call ‘academic assertions.’ This kind of case is discussed in Douven (2006) and in Maitra (2010). In academic papers, we typically make assertions that we do not know. We don’t know that most of the things we’ve said here are true. (Before the last sentence we’re not sure we knew that any of the things we said were true.) But that’s because knowledge is a bad standard for academic discourse. Debate and discussion would atrophy if we had to wait until we had knowledge before we could present a view. So, it seems that assertion can properly outrun knowledge in academic debate.\nAcademic assertions raised a problem for The Knowledge Rule because proper assertion in the context of inquiry can outrun knowledge. But note that action in such a context can also properly outrun knowledge. It would slow down learning dramatically if people didn’t engage in various projects that really only make sense if some hypothesis is true. So, academics will study in archives, conduct experiments, write papers, etc. etc., and do so on the basis of reasons they no more know than we know the truth of the speculative claims of this paper. And this is all to the good; the alternative is a vastly inferior alternative to academia as we know it. So, in some fields, action requires much less than knowledge. Happily, in those fields, assertion also requires much less than knowledge. Indeed, the shortfalls in the two cases seem to parallel nicely. And this parallel is neatly captured by The Action Rule.\nAs we said, none of this is a knockdown case for The Action Rule. Our primary purpose is to argue against The Knowledge Rule. As long as the Action Rule is plausible, we have defeated the abductive argument for The Knowledge Rule that was discussed at the start of this section, and we think we’ve done enough to show it is plausible. We also hope we’ve made a successful case for moving the study of assertability away from rules like The Knowledge Rule, and instead have it be more tightly integrated with our best theories about evidence and action.\n\n\nBach, Kent. 2007. “Knowledge in and Out of Context.” In Knowledge and Skepticism, edited by Joseph Keim Campbell, Michael O’Rourke, and Harry S. Silverstein, 105–36. Cambridge, MA: MIT Press.\n\n\nBlack, Max. 1952. “Saying and Disbelieving.” Analysis 13 (2): 25–33. https://doi.org/10.1093/analys/13.2.25.\n\n\nCappelen, Herman, and Ernest Lepore. 2005. Insensitive Semantics: A Defence of Semantic Minimalism and Speech Act Pluralism. Oxford: Blackwell.\n\n\nDeRose, Keith. 2002. “Assertion, Knowledge and Context.” Philosophical Review 111 (2): 167–203. https://doi.org/10.2307/3182618.\n\n\nDouven, Igor. 2006. “Assertion, Knowledge and Rational Credibility.” Philosophical Review 115 (4): 449–85. https://doi.org/10.1215/00318108-2006-010.\n\n\nFantl, Jeremy, and Matthew McGrath. 2002. “Evidence, Pragmatics, and Justification.” Philosophical Review 111: 67–94. https://doi.org/10.2307/3182570.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHindriks, Frank. 2007. “The Status of the Knowledge Account of Assertion.” Linguistics and Philosophy 30 (3): 393–406. https://doi.org/10.1007/s10988-007-9019-5.\n\n\nLewis, David. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nMaitra, Ishani. 2010. “Assertion, Norms and Games.” In Assertion: New Philosophical Essays, edited by Jessica Brown and Herman Cappelen, 277–96. Oxford: Oxford University Press.\n\n\nSainsbury, Mark. 1995. “Vagueness, Ignorance and Margin for Error.” British Journal for the Philosophy of Science 46: 589–601. https://doi.org/10.1093/bjps/46.4.589.\n\n\nStalnaker, Robert. 1973. “Presuppositions.” Journal of Philosophical Logic 2 (4): 447–57. https://doi.org/10.1007/bf00262951.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\n———. 2008. “Knowledge and Certainty.” Philosophical Issues 18 (1): 35–57. https://doi.org/10.1111/j.1533-6077.2008.00136.x.\n\n\nWeatherson, Brian. 2004. “Luminous Margins.” Australasian Journal of Philosophy 82 (3): 373–83. https://doi.org/10.1080/713659874.\n\n\n———. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\nWeiner, Matthew. 2005. “Must We Know What We Say.” Philosophical Review 114 (2): 227–51. https://doi.org/10.1215/00318108-114-2-227.\n\n\nWilliamson, Timothy. 1996. “Knowing and Asserting.” Philosophical Review 105 (4): 489–523. https://doi.org/10.2307/2998423.\n\n\n———. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nTimothy Williamson (Williamson 2000 Ch. 11) has the clearest statement of the view we’re considering here. It is also defended by Keith DeRose (2002). Both DeRose and John Hawthorne (2004) deploy it extensively as a constraint on theories of knowledge. Jason Stanley (2008) argues for an even stronger constraint: that we should only assert p if we are certain that p. Igor Douven (2006) argues that truth is neither sufficient nor necessary, so the norm should be assert only what is rationally credible. Kent Bach (2007) and Frank Hindriks (2007) both suggest that the only real norm governing assertion is belief, but that since knowledge is a norm of belief, we shouldn’t generally assert what we do not know. In this paper we’re not concerned with the question of whether the rule Assert only what you know holds solely in virtue of the normative nature of assertion itself, as Williamson thinks, or partly in virtue of norms applying to related states like belief, as Bach and Hindriks suggest, but rather whether the rule is even a good rule.↩︎\nWe’ll use the expressions ‘thing to do’ and ‘what to do’ interchangeably throughout the paper. By X is what to do, we mean X ought to be done, all things considered. We take no position on whether X’s being what to do entails its being the morally right thing to do. That may be the case, but nothing we say in this paper depends on its being so.↩︎\nThe points we’re about to make are fairly familiar by now, but for more detail, see Cappelen and Lepore (2005), which played an important role in reminding the philosophy of language community of their significance.↩︎\n(Williamson 2000), for instance, shows the strength of this argument.↩︎\nThis is why we hedged a little two paragraphs ago about what precisely The Undefeated Reason Rule explains. We suspect that many in the literature have misidentified the explicandum.↩︎\nActually, our agreement with Williamson here is a bit more extensive than the text suggests. Williamson holds that part of what makes a speech act an assertion as opposed to some other kind of act is that it is governed by The Knowledge Rule. Although many philosophers agree with Williamson that The Knowledge Rule is true, this fascinating claim about the metaphysics of speech acts has been largely ignored. Translating Williamson’s work into the terminology of this paper, we’re inclined to agree that a speech act is an assertion partly in virtue of being responsive to evidence in the right way. But filling in the details on this part of the story would take us too far from the main storyline of this paper.↩︎\nWe’ve slightly modified the case. Lewis says we can say that we know Bill will never be rich. That seems to us to be a much more controversial than what we’ve included here.↩︎\nThe combination is slightly trickier to state than would be ideal. The explanation we have in mind is that S can properly assert p only if S can truly say I know that p, where ‘know’ in this utterance is context sensitive.↩︎\nThe derivation here is deliberately simplified in one way. We haven’t included anything about the bases for action or assertion. We don’t think being sensitive to bases in the formalism would make a material change, but it would obscure the structure of the argument.↩︎\nThis explanation makes some interestingly different predictions from the explanation in terms of The Evidence Responsiveness Rule. Suppose that for relatively trivial decisions, like where to go for a walk on a nice summer day, one can correctly guess that X is the thing to do. Then the Evidence Responsiveness Rule would suggest that the truth of claims about where to go for a walk is not sufficient grounds for their assertability, while the Action Rule would still imply that truth is sufficient grounds for assertability.\n  We’re not sure that this supposition – that for relatively trivial decisions, one can correctly guess that X is the thing to do – is coherent, nor what to say about assertability judgments in (imagined) cases where the supposition holds. So, we’re not sure we can really use this to discriminate between the two proposed explanations. Nevertheless, it is interesting to note how the explanations come apart. Thanks here to Susanna Schellenberg.↩︎\nSee, for instance, Fantl and McGrath (2002), Hawthorne (2004), Stanley (2005), and Weatherson (2005).\n\n↩︎\n",
    "preview": "posts/2021-01-05-assertion-knowledge-and-action/commons.jpg",
    "last_modified": "2021-02-04T22:06:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-attitudes-and-relativism/",
    "title": "Attitudes and Relativism",
    "description": "Data about attitude reports provide some of the most interesting arguments for, and against, various theses of semantic relativism. This paper is a short survey of three such arguments. First, I'll argue (against recent work by von Fintel and Gillies) that relativists can explain the behaviour of relativistic terms in factive attitude reports. Second, I'll argue (against Glanzberg) that looking at attitude reports suggests that relativists have a *more* plausible story to tell than contextualists about the division of labour between semantics and meta-semantics. Finally, I'll offer a new argument for invariantism (i.e. against both relativism and contextualism) about moral terms. The argument will turn on the observation that the behaviour of normative terms in factive and non-factive attitude reports is quite unlike the behaviour of any other plausibly context-sensitive term.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2008-12-29",
    "categories": [
      "language",
      "relativism"
    ],
    "contents": "\nData about attitude reports provide some of the most interesting arguments for, and against, various theses of semantic relativism. This paper is a short survey of three such arguments. First, I’ll argue (against recent work by von Fintel and Gillies) that relativists can explain the behaviour of relativistic terms in factive attitude reports. Second, I’ll argue (against Glanzberg) that looking at attitude reports suggests that relativists have a more plausible story to tell than contextualists about the division of labour between semantics and meta-semantics. Finally, I’ll offer a new argument for invariantism (i.e. against both relativism and contextualism) about moral terms. The argument will turn on the observation that the behaviour of normative terms in factive and non-factive attitude reports is quite unlike the behaviour of any other plausibly context-sensitive term. Before that, I’ll start with some taxonomy, just so as it’s clear what the intended conclusions below are supposed to be.\nPublished in Philosophical Perspectives 22: 527-544.\nPicture by Andreina Schoeberlein via Creative Commons.\nHow Not to be a Strawman\nHere are three mappings that we, as theorists about language, might be interested in.\nPhysical Movements \\(\\rightarrow\\) Speech Acts. I’m currently moving my fingers across a keyboard. In doing so, I’m making various assertions. It’s a hard question to say just how I manage to make an assertion by moving my fingers in this way. Relatedly, it’s a hard question to say just which assertions, requests, questions, commands etc people make by making physical movements. This mapping is the answer to that question.\nSpeech Acts \\(\\rightarrow\\) Contents. For some speech acts, their content is clear. If I assert that grass is green, the content of my assertion is that grass is green. For other speech acts, including perhaps other assertions, it is not immediately obvious what their content is. This mapping answers that question.\nContents \\(\\rightarrow\\) Truth Values. Given a content, it isn’t always clear whether it is true or false (or perhaps something else). This mapping provides the truth values of every content of every speech act.\nThe details of each of these mappings is interesting, to say the least. Indeed, a full description of those mappings would arguably include an answer to every question ever asked. We’re not likely to know that any time soon. But we can ask, and perhaps answer, interesting questions about the topology of the mappings. Here are two distinct questions we can ask about each of the three mappings.\nIs it one-one or one-many? The mapping from a natural number to its natural predecessor is one-one. That’s to say, every number is mapped to at most one other number. The mapping from a person to the children they have is one-many. That’s not to say that everyone has many children, or indeed that everyone has any children. It’s merely to say that some things in the domain are mapped to many things in the range.\nIs the mapping absolute or relative, and if relative, relative to what? Consider a mapping that takes a person A as input, and relative to any person B, outputs the first child that A has with B. This mapping is relative; there’s no such thing as the output of the function for any particular input A. All there is, is the output relative to B1, relative to B2, and so on. But note that in a good sense the mapping is one-one. Relative to any B, A is mapped to at most one person.\nA very simple model of context-sensitivity in language says that all three of these mappings are one-one and absolute. It will be helpful to have a character who accepts that, so imagine we have a person, called Strawman, who does. There are six basic ways to reject Strawman’s views. For each of the three mappings, we can say that it is one-many or we can say that it is relative.\nThis way of thinking about Strawman’s opponents gives us a nice taxonomy. The first mapping is about speech acts, the second about contents and the third about truth. Someone who disagrees with Strawman disagrees about one of these three mappings. They might disagree by being a pluralist, i.e. thinking that the mapping is one-many. Or they might disagree by being a relativist, i.e. thinking the mapping is relative to some thing external. The respective defenders of Strawman on these questions are the monists and the absolutists.\nSo the speech act pluralist is the person who thinks the first mapping is one-many. The content relativist, is the person who thinks the second is relative to some other variable (perhaps an assessor). And the truth monist is the person who thinks that contents have (at most) one truth value. All these terms are a little bit stipulative, but I think it the terminology here somewhat matches up with regular use. And it’s the terminology I’ll use throughout this paper.\nOne other nice advantage of this taxonomy is that it helps clarify just what is at issue between various opponents of Strawman. So Andy Egan (2009) has some data about uses of “you” in group settings that suggest such utterances pose a problem for Strawman. But it’s one thing to have evidence that Strawman is wrong, another altogether to know which of his views is, on a particular occasion, wrong. I think separating out Strawman’s various commitments helps clarify what is needed to isolate Strawman’s mistake on an occasion.\nIt is, I think, more or less common ground that the first of Strawman’s commitments, speech act monism, is false. The King can, by uttering “It’s cold in here,” both assert that it’s cold in here, and command his lackey to close the window. Those look like two distinct speech acts that he’s made with the one physical movement. Herman Cappellen and Ernest Lepore (Cappelen and Lepore 2005) have many more examples to show that Strawman is wrong here. Once we go beyond that though, it’s less clear that Strawman is mistaken. Perhaps by thinking about cases where, by the one physical movement, we intend to communicate p to one audience member, and q to another, we can try to motivate speech-act relativism. That’s an issue I’ll leave for another day. In contrast to what he says about speech acts, what Strawman says about content and truth is, if not universally accepted, at least popular. So I’ll call orthodox contextualism the view that Strawman is right about the content mapping and the truth mapping; each mapping is both one-one and absolute.\nIt is worthwhile noting two very separate models for content that lead to two quite distinct ways in which we can reject Strawman’s last two absolutist views. John MacFarlane’s paper on Non-Indexical Contextualism MacFarlane (2009) was extremely useful in setting up the relevant distinctions here, but the particular models for content I’m describing here are both set out in greatest detail in recent work by Andy Egan.\nThe first is the centred worlds model for content. This is the idea that for some utterance types, any token of that type expresses the same content. But that content is a set of centred worlds, that is true at some centres and false at other centres in the same world. So we might think that the content of “Beer is tasty” is, roughly, the set of possibilia who have pro-attitudes to the taste of beer. More precisely, it is the set of world-centre pairs such that the agent at (or perhaps closest to) the centre has pro-attitudes towards the taste of beer. On this view, content monism will be maintained – what an utterance of “Beer is tasty” says is invariant across assessors. (I’m assuming here that assessor-relativity is the only relativity we’re interested in.) But truth absolutism will fail, since whether that content is true for a1 and a2 will depend on what their attitudes are towards beer. This kind of centred worlds model for content is what Egan has developed in (Egan 2007).\nThe second model lets assessors get into the content-fixing mechanism, but says the content that is fixed is a familiar proposition whose truth is not assessor relative. This is easiest to explain with an example involving second-person pronouns. For some utterances of “You are a fool,” the content of that utterance, relative to x, is that x is a fool. Now whether x is indeed a fool is a simple factual question, and whether it is true isn’t assessor relative. But if some people are fools and others are not, whether the utterance is true or false depends on who is assessing it. So content relativism is true, while truth absolutism is preserved. This is a view Egan has defended for some tokens of second person pronouns (Egan 2009) .\nIn “Conditionals and Indexical Relativism” (Weatherson 2009), I called the combination of content relativism and truth absolutism “indexical relativism,” and defended such a view about indicative conditionals. I called something similar to the combination of truth relativism and content absolutism “non-indexical contextualism.” More precisely, I followed MacFarlane in using that phrase for the combination of truth relativism and content absolutism and the view that whether a speaker’s utterance is true (relative to an assessor) is a matter of whether the proposition they express is true relative to their context. I like the name “indexical relativism,” but it has also been used for theories that aren’t even heterodox in the above sense, so perhaps persisting with it would just invite confusion. (And the name implies a particular view about how the relativity works; namely that there is something like an indexical element in what’s asserted that gets its value from the context of assessment.) In other contexts I’ve used “relativism” as the label for all and only heterodox views, but this label is potentially quite confusing. Indeed, it’s a possible worry about the arguments in my “Conditionals and Indexical Relativism” that they really just support heterodoxy; a separate argument would be needed (and might not be easy to supply) against pluralist alternatives to content relativism about indicative conditionals.\nFactive Verbs and Relativism\nIn “CIA Leaks” (Fintel and Gillies 2008), Kai von Fintel and Thony Gillies raise a problem for heterodox theories about ‘might.’ (Actually they raise several, but I’m only going to deal with one of them here.) Their primary target is what I called truth relativist theories, but the argument they raise is interesting to consider from all heterodox perspectives. The problem concerns embedding of ‘might’-clauses under factive attitude verbs. They argue as follows:\nS realises that p presupposes that p.\nThis presupposition is carried over when the sentence is used as the antecedent of a conditional. So, for instance, If S realises that p, then q presupposes that p.\nBut, on standard heterodox proposals, we can properly say If S realises that it might be that p, then q, even though it isn’t true that it might be that p.\nSo heterodox proposals are false.\nHere is the example they use to make the case.\n\nBond planted a bug and some misleading evidence pointing to his being in Zuurich and slipped out. Now he and Leiter are listening in from London. As they listen, Leiter is getting a bit worried: Blofeld hasn’t yet found the misleading evidence that points to Bond’s being in Zurich. Leiter turns to Bond and says:\nIf Blofeld realizes you might be in Zurich, you can breathe easy—he’ll send his henchman to Zurich to find you. (93)\nNow the problem is that for the heterodox theorist, “You might be in Zurich,” as uttered by Leiter to Bond, expresses (relative to Bond), a proposition that is true iff for all Bond knows, Bond might be in Zurich. Just how it does this will differ for different heterodox theorists, but so far they all agree. But that isn’t the case; since Bond knows he is in London. So (34) should sound defective, since it contains a presupposition failure. But it isn’t defective, so heterodoxy is mistaken.\n\nBefore we look at how heterodox theorists might respond to this case, it’s worth looking thinking about how Strawman might respond to it. The simplest idea is to say that in It might be that p, there is a hidden variable X. The value of X is set by context. And the sentence expresses the proposition that for all X knows, p is true. (Perhaps we might use some epistemic relation other than ‘knows,’ but that’s not relevant here.)\nNow, and this is crucial, the variable X might be either free or bound. If there is nothing around to bind it, as in a simple utterance of It might be that p, then it will be free. And typically if it is free, X denotes a group consisting of the speaker and perhaps those in the same conversation. But when the might-clause is embedded under a propositional attitude ascription (factive or not), the variable X will be bound to the subject of the attitude ascription. So in Y believes that it might be that p, the value of X will simply be Y. So in Blofeld realises you might be in Zurich, the value of X is Blofeld. And hence the embedded might claim is true, since that claim is simply that for all Blofeld knows, Bond is in Zurich. Which, in the story, is true.\nThe reason for going through all of this is that the theorist who accepts truth absolutism, but rejects content absolutism, can say exactly the same thing. There is a variable X in the structure of what’s asserted. Strawman thinks that you only get a determinate assertion when you fill in the value of X. We can disagree with that; we can say that an assertion can literally contain a variable, one that potentially gets its value from assessors. That way the content of a particular assertion can be different for different assessors. Once we’ve made this move, we can rejoin Strawman’s story. This variable is either free or bound. If it is bound, we tell exactly the same story as Strawman. But, we insist, if it is free, the value of X is sometimes set by contextual features of the hearer as well as of the assessor. In the standard case, X is a group consisting of the speaker, the hearer, and perhaps some people who get in the group in virtue of their proximity to the speaker or hearer.\nSo we end up saying the same thing about the acceptability of (34) as Strawman. The content of You might be in Zurich, as embedded in (34), is quite different to the content those words would have if uttered as a standalone sentence, because the value that a key variable takes is different. For us, the value that variable takes differs for different assessors, but that’s completely irrelevant to the explanation of the acceptability of (34).\nFor the truth relativist (who is also a content absolutist) things are a little more interesting. Such a theorist will typically reject the presence of a variable like X in the structure of what is said. So they cannot appeal to the kind of explanation that we’ve offered (twice over) for how (34) may be acceptable. The solution is to simply reject the generalisation about factive verbs. Let’s start with some seemingly distant examples, in particular examples about fiction. It seems that (5) doesn’t have any false presuppositions.\nWatson realised that private detectives were (in late 19th Century London) better at solving murder mysteries than police.\nHad Watson realised earlier that private detectives were better at solving murder mysteries than police, he would have liked Holmes more than he did.\nHad Watson realised earlier that private detectives were better at solving murder mysteries than police, the early chapters of the book would have been more interesting.\nNote that this isn’t anything particular to do with subjunctive conditionals. Imagine that we are settling down to watch a new adaptation of the Holmes stories that we are told won’t be particularly faithful to the books in detail. I might properly say (8).\nIf Watson realises that private detectives were better at solving murder mysteries than police, the early scenes will be more interesting.\nBut now it is easy to see the way out of the argument for the proponent of the centred world view. The crucial thing about Watson isn’t, such a theorist will say, that he’s in another possible world. The crucial thing is that some propositions that are false relative to us (e.g. the proposition that private detectives were better at solving murder mysteries than police) are true relative to him. The true generalisation seems to be that S Vs that p, where V is factive, presupposes that p is true relative to S. And that’s true in the cases that von Fintel and Gillies describe. So it’s not true that the centred world theorist should predict that these utterances have false presuppositions. And that’s all to the good, because of course they don’t.\nGlanzberg on Metasemantics\nPapers arguing for relativism about some term t frequently, perhaps typically, start with a survey of reasons why (orthodox) contextualism about t cannot be correct. And such a survey will frequently include a sojourn through some quite specific contextualist theories, with some fairly obvious counterexamples. Egan, Hawthorne, and Weatherson (2005) sticks to the script as far as this goes. We note that a might be F can’t, for instance, mean that For all S knows, a is F, where S is the speaker. And we note that some other simple theories along the same lines can’t be true.\nIt’s interesting to think through what kind of force such tours through the possible contextualist theories could have. We might think that there’s a tacit argument that if some contextualist theory were true, it would be one of these simple ones, and none of the simple ones is true, so no contextualist theory is true. I’m not going to take a stand on exegetical debates here, so I’m not going to consider who may or may not have been making such an argument. I don’t think that it was the intended argument in Egan, Hawthorne, and Weatherson (2005), but that’s beside the point, because it is an argument that’s now being debated. The argument under consideration isn’t, or at least isn’t directly, that the contextualist’s semantic proposal is mistaken. Rather, the argument is that the accompanying metasemantic theory, i.e. the theory of how semantic values get fixed, is intolerably complicated. Slightly more formally, we can argue as follows.\nIf contextualism is true, the metasemantic theory of how a particular use of “might” gets its semantic value is hideously complicated.\nMetasemantic theories about how context-sensitive terms get their values on particular occasions are never hideously complicated.\nSo, contextualism is false.\nThe problem with this argument, as Glanzberg (2007) has argued, is that premise 2 seems to be false. If we just look at some so-called ‘automatic’ indexicals, like “I” or “here” or “now,” it just may be plausible. (But even in those cases things are tricky when we look at recordings, as in Weatherson (2002).) Once we widen our gaze though, we see that there are examples of uncontroversially context-sensitive terms, like “that,” for which the accompanying metasemantic theory is, by any standard, hideously complicated. So the prospects of getting to relativism from metasemantic complexity are not, I think, promising.\nThat isn’t the only kind of metasemantic argument for relativism though. A better argument for relativism turns on the fact that metasemantics is generally complicated. The contextualist, I think, has to make metasemantics too systematic at a key point. (Again, I’m not doing exegesis, but I do think something like this argument was intended in Egan et al (2005). I’m largely here highlighting something that I think has been thus far overlooked in the commentaries on that piece.) Consider the following pair of sentences.\nThose guys are in trouble, but they don’t know that they are.\n??Those guys are in trouble, but they might not be.\nSomething has gone wrong in (10). I conclude that (10) can’t be used to express (9). That is, there’s no good interpretation of (10) where those guys can be the denotation of X in the theory I attributed to Strawman in the previous last section. That’s to say, the value of X can’t just be the most salient individual(s) in the context, since the guys are being made rather salient. And nor can it be anaphoric on previously mentioned people, unless they are the subjects of a propositional attitude ascription. We’ll investigate this exception in what follows.\nA natural move at this stage is to adopt what in Egan, Hawthorne, and Weatherson (2005) we called the Speaker Inclusion Constraint (hereafter SIC). That is, in unembedded uses of “might” the group X always includes the speaker. Now the explanation of the problem with (10) is that for the speaker to assert the first clause, she must know that the guys are in trouble, but if that’s the case, and she’s in group X, then the second clause is false.\nIf the SIC really holds, it looks like it should hold in virtue of the meaning (in some sense of “meaning”) of “might.” As a rule, tidy generalisations like this should be part of semantics, not metasemantics. Compare two possible theories about “we.” Both theories say that “we” is a plural pronoun. One theory goes on to say that it is part of the meaning of “we” that it picks out a group that includes the speaker. That is, it puts this version of the SIC into the semantics. Another theory says that the SIC for “we” is just a nice metasemantic generalisation. I take it that the second position is very unattractive; it’s part of the meaning of “we” that it picks out a group including the speaker. And I think the relevant point generalises. At least it generalises as far as another SIC, namely the one that holds for “might.”\nSemantic constraints on indexical terms hold, as a rule, for both embedded and unembedded uses of those indexicals. You can’t use “she,” even as a bound variable, to denote (relative to any variable assignment) a male human. There’s something badly wrong with Every student thinks she will win, if some students are female and others male. As Michael Glanzberg pointed out to me, complex demonstratives headed by “this” have to pick out an object that is in some way proximal, and this applies to bound complex demonstratives as well. So we have to use “that” rather than “this” in sentences like Every connoisseur remembers that/*this first great wine they drank. For a more familiar example, you can’t interpret O’Leary thinks that I am a fool, as uttered by Daniels, to mean that O’Leary self-ascribes foolishness. In short, it seems that there are three related conclusions we can draw here. First, there are semantic constraints on the possible values of context-sensitive expressions. Second, any interesting generalisation about the possible value of a context-sensitive expression is traceable to such a semantic constraint. Third, these constraints remain in force when the expression is used in an attitude report, or as a bound variable.\nThe problem for contextualists about “might” is that it doesn’t behave at all this way. The SIC holds for unembedded uses. That implies that it is part of the meaning of “might.” So it should hold for embedded uses. But it does not. Indeed, for many embedded uses of “might,” a reading compatible with the SIC is simply unavailable. For instance, we can’t interpret Every student thinks that they might have failed as meaning that every student thinks that, for all I know, they failed. My knowledge just doesn’t matter; we’re talking about those students’ fears. This all suggests the following argument.\nIf contextualism is true, then the explanation of the SIC is that it is part of the meaning of “might” that the relevant group X includes the speaker.\nIf it is part of the meaning of “might” that the relevant group X includes the speaker, then this must be true for all uses of “might,” included embedded uses.\nWhen “might” is used inside the scope of an attitude ascription, the relevant group need not include the speaker.\nSo, contextualism is not true.\nGlanzberg argued, correctly, that it’s no problem for the contextualist if, according to their theory, metasemantics was complicated and messy. It’s not a problem because, well, metasemantics is complicated and messy. But this cuts both ways. And it is a problem for the contextualist that they have to put the SIC into metasemantics. It’s just not messy enough to go there.\nThere are two objections to this argument (both of which were pressed when this paper was presented at Arché) that are worth considering together.\n\nObjection One: There are other generalisations that do go into metasemantics\nIt’s very odd, to say the least, to use third-person pronouns to denote oneself. But this doesn’t seem to go into the meaning of the pronoun. Relatedly, it is possible to use bound third-person pronouns that take (relative to some variable assignments) oneself as value. For instance, an Australian boy can say “Whenever an Australian boy goes to the cricket, he cheers for Australia.” So probably premise 1 of the above argument, requiring that generalisations be semantic, is false. If not, premise 2, requiring that semantic constraints on unbound pronouns also constrain bound pronouns is false.\n\n\nObjection Two: The SIC is false\nEgan, Hawthorne, and Weatherson (2005) note that the SIC seems to fail in ‘game-playing’ and ‘advice’ contexts. So in a game of hide and seek, where Billy is looking for something Suzy has hidden, if he asks “Is it in the basement?” Suzy can truly say “It might be,” even if she knows it isn’t true. And a parent can tell their child “Wash that strawberry before you eat it; it might be contaminated” even if the parent knows that the strawberry has been washed.\n\nThe simplest response to the first objection is that the purported generalisation, a third-person pronoun does not denote the speaker, isn’t really a universal generalisation at all. It’s possible to refer to oneself by name; certain people in the news do it on a regular basis. For example, a famous footballer Smith might say “Smith deserves a pay raise.” In such a context, it isn’t at all odd (or at least any odder than it already is) to use third-person pronouns, e.g. by continuing the above utterance with “and if he doesn’t get one, he’s not going to play.”\nThe second objection is a little trickier, but I think it’s possible to understand these utterances as a kind of projection. The speaker is speaking from the perspective of the hearer. This isn’t an unattested phenomenon. Something like it must be going on when speakers use “we” to denote the hearer. Imagine, for instance, a policeman coming across a person staggering out of a pub and saying “We’ve had a bit much to drink it seems.” The policeman certainly isn’t confessing to dereliction of duty. Nor is this case sufficient to throw out the idea that “we” is a first person plural pronoun. Rather, the policeman is speaking from the drunk’s perspective. I suspect the same thing is going on in both of the examples above.\nSo I think both objections can be answered. But neither answer is completely convincing. And the two responses undermine each other. If we accept projection is a widespread phenomenon, then perhaps self-denotation with a third person pronoun is a kind of projection. We should then restate the argument, without assuming there’s a response to this pair of objections.\nTo do so, let’s step back from the details of the SIC. What we started with was a simple fact, that (10) can’t be used to express (9). That’s not threatened by counterexamples to the SIC, and it still needs explanation. The SIC is a proposed semantic explanation, and perhaps, if it has counterexamples, it fails. I suspect something like it is correct, but I don’t plan to rely on that suspicion here. That’s because we can be independently confident that the explanation here will be semantic, not pragmatic. We can be confident of this because there just doesn’t look to be anything like a pragmatic explanation available.\nCompare the discussion of third-person pronouns. Even if we can’t use third-person pronouns to pick out ourselves (when not speaking projectively), there is an obvious pragmatic explanation for this. We have first-person pronouns available, and if we mean to denote ourselves, using a first-person pronoun will do so in the clearest possible way. Since it is good to be clear, when we pass up the chance to use a first-person pronoun, the obvious conclusion for a hearer to draw is that we don’t mean to denote ourselves. The details of this explanation could use some filling out, but it at least has the form of an explanation. It simply doesn’t seem that any such explanation will be available for why (10) can’t be used to express (9). It’s not that (9) isn’t a thought that we might be interested in expressing. And it’s not that if we wanted to express it, we would have had an obviously preferable form of words to (10). It’s true that we have the words in (9) itself, but (a) they are longer, and (b) on the contextualist view whenever we use an epistemic modal there is some such paraphrase available, a paraphrase that typically does not defeat the acceptability of epistemic modals.\nIf there isn’t a pragmatic explanation of why (10) can’t be used to express (9), then there must be a semantic explanation. But the only semantic explanation that looks plausible from a contextualist perspective, is a semantic restriction on X. And we know, from considering the data about embedded epistemic modals, that there is no such restriction. So we have a problem for contextualism. Slightly more formally, we can offer the following argument against orthodox contextualism about epistemic modals.\nWhatever acceptability data can’t be explained pragmatically must be explained semantically.\nThere is no pragmatic explanation for why (10) can’t be used to express (9).\nIf 1 and 2 then the meaning of “might” must explain why (10) can’t be used to express (9).\nIf the meaning of “might” must explain why (10) can’t be used to express (9), and contextualism is true, there must be a restriction, provided by the meaning of “might” on the relevant group X that excludes groups like those guys.\nIf there is a restriction, provided by the meaning of “might” on the relevant group X that excludes groups like those guys, then when “might” is embedded under an attitude verb, the group X still can’t be those guys.\nIn “Those guys believe that they might be in trouble,” the relevant group X just is those guys.\nSo, contextualism is false.\nThis argument is just an argument against contextualism about “might.” It doesn’t obviously generalise very far. It’s crucial to the argument that (10) can’t be used to express (9), even when the relevant guys are made pretty salient. A similar argument against contextualism about, say, taste claims, would have to start with the premise that a clause like “but it’s tasty,” at the end of a sentence about a, couldn’t be used to express the thought that it is tasty to a. And such a premise wouldn’t be true. As Tamina Stephenson (2007) points out, make a particular dog salient and “It’s tasty” can express the thought that it is tasty to the dog. So I’m rather sceptical that the considerations here generalise to an argument against contextualism about predicates of personal taste.\nAgainst Moral Relativism\nAt first glance there seems to be very little pattern to the way that contextually sensitive terms behave in attitude ascriptions. We can find terms, like we, whose denotation inside a belief ascription is not particularly sensitive to the context of the ascribee. So in (11), we is naturally interpreted as denoting the speaker and those around her.\nOtto believes that we are fools.\nVinny the Vulture believes that rotting carcasses are tasty.\nSuzy believes that this kind of dog food is tasty.\nOn the other hand, when we use epistemic modals in belief reports, the relevant ‘knower’ is always the ascribee. Consider, for example, (14).\nJack believes that Smith might be happy.\nSo we have a progression of cases, where in (11) the contextually sensitive term ‘we’ has to get its denotation from the context of utterance, in (14) the contextually sensitive term ‘might’ gets its denotation from the context of the ascribee, and (12) and (13) show that ‘tasty’ can behave in either of these ways. I’ve been putting this all in terms that will make most sense if we are accepting truth absolutism, but the same points can be made without that assumption if we so desire.\nAs I said, at first it might look like there is no pattern here at all. But if we look at other attitudes, we see that there is an interesting pattern. The way that ‘we,’ ‘tasty’ and ‘might’ behave in belief reports is just the same as they behave in knowledge reports. We can see this in the following examples.\nOtto knows that we are fools.\nVinny the Vulture knows that rotting carcasses are tasty.\nSuzy knows that this dog food is tasty.\nJack knows that Smith might be happy.\nt behaved quite differently in belief and knowledge reports. If that were the case it would be possible in principle to find a passage of the form of (15) that’s true.\nS believes that … t …. Indeed S knows it. But S doesn’t know that … t … .\nThe problem for contextualism about “wrong” is that it is forced to violate this principle. Assume that X is wrong means that X is wrong relative to the standards of some salient group G. We’ll leave aside for now the question of whether G is determined by the speaker’s context or the assessor’s context, as well as the question of whether the sentence expresses a proposition involving G, or a proposition that is true or false relative to groups. We’ll also leave aside the question of just what it means for something to be wrong relative to the standards. (Does it mean that G actually disapproves of it, or would disapprove of it under reflection, or that it doesn’t have properties that G wants to promote, or something else?) We’ll simply assume that there have been people whose standards are different to ours in ways that make a difference for the wrongness of actions. If that isn’t the case, we hardly have a relativism worthy of the name. It’s obviously controversial just what could be an example of this, but I’ll take as my example Jefferson Davis’s belief that helping fugitive slaves was wrong. It seems true to say Davis had this belief, so (16) is true.\nDavis believed that helping fugitive slaves was wrong.\nDavis knew that helping fugitive slaves was wrong.\nNow neither the truth of (16), nor the falsity of (17) is, on its own, sufficient to undermine contextualism about “wrong.” The truth of (16) is consistent with the claim that “wrong” behaves like “might.” So in attitude ascriptions, what matters is the ascribees context. And the falsity of (17) is consistent with the claim that “wrong” behaves like “we,” and (17) is false because what helping fugitive slaves was wrong expresses in our context is false.\nRather, the problem is that an adequate account of “wrong” has to account both for the truth of (16) and the falsity of (17). And that doesn’t seem to be possible. At least it isn’t possible without supposing that “wrong” behaves differently in knowledge reports and belief reports. And we’ve seen some reasons above to believe that that’s not how context-sensitive terms behave, whether the term is one like “we,” for which an orthodox theory seems best, or like “might,” for which a heterodox theory seems best.\nI’ll end with some objections that I’ve encountered since I’ve started discussing this argument, with my replies to each of them.\n\nObjection: This is question-begging against the moral relativist.\n\nThis is the most common reaction I’ve heard, but I do find it hard to make sense of. It is hard to see just which premise is question-begging. Nothing in moral relativism as such prevents us accepting the truth of Davis believed that helping fugitive slaves was wrong, and nothing in moral relativism prevents us from rejecting Davis knew that helping fugitive slaves was wrong. There is, I say, a problem with doing both of these things, as we should want to do. But if an argument is going to be rejected as question-begging because the other side can’t simultaneously accept all of its premises, well we won’t have many arguments left to work with.\nA little more seriously, the relativist theories that I’m opposing here are semantic theories. If we can’t reject them because they commit us to endorsing sentences that we (the opponents of the view) can see to be false, then it is hard to know what could count as an argument in semantics. It’s no defence of a view to say that its proponents cannot see it is false, if the rest of us can see it.\n\nObjection: We would see the knowledge claim (17) is true, if only we didn’t have anti-relativist prejudices.\n\nThis might well be right; it’s certainly hard to know when one is prejudice free. Perhaps all that’s going on is that we don’t want to be committed in any way to saying that it’s wrong to help fugitive slaves, and we’re worried that accepting (17) would, in some way, so commit us.\nBut note how much I’ve done to stack the deck in favour of pro-relativist intuitions, and to dissipate this worry. The argument is coming at the end of a whole paper defending relativism. Earlier in this very paper I defended some relativist views from arguments using factive attitude verbs by noting that it is tricky to state just what factivity comes to. In particular, I noted that we can sometimes say A knows that S in circumstances where we would not, indeed could not truthfully, utter S. And I repeated this observation at the start of this section. I think I’ve done as much as possible to (a) overcome anti-relativist prejudice, and (b) frame the argument in such a way as to make it as easy as possible to accept (17). But even in this frame, I still say we can see that it is false.\n\nObjection: This is only an objection to one kind of context-sensitivity in moral terms, the kind we associate with traditional moral relativism. But it doesn’t show that moral terms are in no way context-sensitive. We’d expect that they are, since some moral terms are gradable adjectives, and gradable adjectives are context-sensitive.\n\nThere’s a really interesting philosophical position around here. Start with the idea that we should be invariantists, perhaps realists in some quite strong sense, about moral comparatives. Perhaps this could be tied to the fairly intuitive idea that comparatives are what’s crucial to morality. Then say that terms like “right” and “wrong” pick out, in a context-sensitive way, points on the moral scale. So some kind of contextualism, presumably orthodox, is right for those terms. This position is immune to the objection given above, because (16) turns out to be true, and (17) false, if we interpret “wrong” to mean above the actually contextually-salient level of wrongness, on the objectively correct wrongness scale.\nBut I think a similar pair of examples show that this won’t work. Assume that we’re talking about various people’s charitable giving in a context where we don’t hold people to super-high standards. So the charitable actions of, say, Bill Gates count as laudable in our circumstances. (I assume that on the merits Gates’s donations are for the good; determining whether this is true is well outside the scope of this paper.) Now consider a philosopher, call him Peter, who doesn’t believe in the moral supererogatory, so he thinks anything less than the best you can do is morally wrong. It seems to me that, as uttered in our context, (18) expresses a truth and (19) a falsehood.\nPeter believes that Bill Gates’s level of charitable donation is wrong.\nPeter knows that Bill Gates’s level of charitable donation is wrong.\nAnd I don’t think there’s a good contextualist explanation for this pair of judgments. If “wrong” was just a simple context-sensitive term in the way suggested, then (18) should be false, because Peter doesn’t believe that Bill Gates’s level of charitable donations do rise to the level of wrongness that is, as it happens, is salient in our context. But intuitively, (18) is true.\nThe same kind of objection can be raised to a more prominent kind of theory that takes a certain kind of normative standard to be set by context, namely classical contextualism about “knows.” Assume it is common ground that George has excellent, but not irrefutable, evidence that he has hands. Assume also that we’re in a low standards context for knowledge. And assume that René thinks knowledge requires objective certainty. Then it seems that we can truly say (20), but not (21).\nRené believes that George doesn’t know he has hands.\nRené knows that George doesn’t know he has hands.\nAgain, the pattern here is very hard to explain on any kind of contextualist theory, be it orthodox or heterodox\n\nObjection: Normative terms might be sui generis. Perhaps they are the only counterexamples to the pattern in (15).\n\nAnything could be the exception to any rule we like. But it’s bad practice to assume that we have an exception on our hands. If we heterodox theorists simply responded to von Fintel and Gillies’ argument from factive verbs by saying that we had an exception to a general pattern here, we would, quite rightly, not be taken seriously. Contextualists and relativists about normative terms should be held to the same standard.\n\nObjection: Relativism does have counterintuitive consequences, but all theories have some counterintuitive consequences. Arguably everyone is going to have to accept some kind of error theory, and this is a relatively harmless kind of error to attribute to the folk.\n\nIf we were convinced, perhaps by one or other of the contemporary developments of Mackie’s argument from queerness (Mackie 1977), that no non-relativistic moral theory is possible (apart from a Mackie-style moral error theory), that would be an interesting argument for moral relativism. Certainly I’d be more willing to accept that (16) and (17) don’t have the same kind of context-sensitivity than I would be to accept that, say, it’s not wrong to torture babies.\nIt is well beyond the scope of this paper to adjudicate such debates in any detail, but I am a little sceptical that we will ever face such a choice. Generalising wildly, most of the time our choice is between (a) accepting a moral error theory, (b) accepting some odd semantic consequences, as outlined here, or (c) rejecting some somewhat plausible claim about the nature of moral judgment, such as motivational internalism. (Without internalism there’s nothing to make moral properties “queer” in Mackie’s sense, for example.) Arguments that present a trilemma such as this deserve to be judged on their merits, but my feeling is that we should normally take option (c). That’s not to say this objection is obviously flawed, or that the argument I’ve offered is a knock-down refutation of relativism. It clearly is not. But I think it is a challenge that moral relativists and contextualists should face up to.\n\n\n\nCappelen, Herman, and Ernest Lepore. 2005. Insensitive Semantics: A Defence of Semantic Minimalism and Speech Act Pluralism. Oxford: Blackwell.\n\n\nEgan, Andy. 2007. “Epistemic Modals, Relativism and Assertion.” Philosophical Studies 133 (1): 1–22. https://doi.org/10.1007/s11098-006-9003-x.\n\n\n———. 2009. “Billboards, Bombs and Shotgun Weddings.” Synthese 166 (2): 251–79. https://doi.org/10.1007/s11229-007-9284-4.\n\n\nEgan, Andy, John Hawthorne, and Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nFintel, Kai Fintelvon, and Anthony S. Gillies. 2008. “CIA Leaks.” Philosophical Review 117 (1): 77–98. https://doi.org/10.1215/00318108-2007-025.\n\n\nGlanzberg, Michael. 2007. “Context, Content and Relativism.” Philosophical Studies 136 (1): 1–29. https://doi.org/10.1007/s11098-007-9145-5.\n\n\nMacFarlane, John. 2009. “Nonindexical Contextualism.” Synthese 166 (2): 231–50. https://doi.org/10.1007/s11229-007-9286-2.\n\n\nMackie, John. 1977. Ethics: Inventing Right and Wrong. London: Penguin.\n\n\nStephenson, Tamina. 2007. “Judge Dependence, Epistemic Modals, and Predicates of Personal Taste.” Linguistics and Philosophy 30 (4): 487–525. https://doi.org/10.1007/s10988-008-9023-4.\n\n\nWeatherson, Brian. 2002. “Misleading Indexicals.” Analysis 62 (4): 308–10. https://doi.org/10.1093/analys/62.4.308.\n\n\n———. 2009. “Conditionals and Indexical Relativism.” Synthese 166 (2): 333–57. https://doi.org/10.1007/s11229-007-9283-5.\n\n\n\n\n",
    "preview": "posts/2021-03-12-attitudes-and-relativism/zurich.jpg",
    "last_modified": "2021-03-12T14:55:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-conditionals-and-indexical-relativism/",
    "title": "Conditionals and Indexical Relativism",
    "description": "I set out and defend a view on indicative conditionals that I call “indexical relativism”. The core of the view is that which proposition is (semantically) expressed by an utterance of a conditional is a function of (among other things) the speaker’s context and the assessor’s context. This implies a kind of relativism, namely that a single utterance may be correctly assessed as true by one assessor and false by another.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2008-12-29",
    "categories": [
      "language",
      "conditionals",
      "relativism"
    ],
    "contents": "\nThis paper is about a class of conditionals that Anthony Gillies (ms) has dubbed ‘open indicatives,’ that is, indicative conditionals “whose antecedents are consistent with our picture of the world” (1). I believe that what I say here can eventually be extended to all indicative conditionals, but indicatives that aren’t open raise special problems, so I’ll set them aside for today. In Weatherson (2001) I argued for an epistemic treatment of open indicatives, and implemented this in a contextualist semantics. In this paper I want to give another argument for the epistemic approach, but retract the contextualism. Instead I’ll put forward a relativist semantics for open indicatives. The kind of relativism I’ll defend is what I’ll dub ‘indexical relativism.’\nPublished in Synthese 166: 333-357.\nPicture by angus mcdiarmid via Creative Commons.\nI’ve changed my mind since Weatherson (2001) largely because of developments since I wrote that paper. There have been six primary influences on this paper, listed here in order that they become relevant to the paper.\nThe arguments that Jason Stanley (along with co-authors) puts forward in his (2007) for the view that all effects of context on semantic content are syntactically triggered, and in particular involve context setting the value for a tacit or overt variable.\nJohn MacFarlane’s defences, starting with MacFarlane (2003) of semantic relativism.\nJohn MacFarlane’s recent work, including MacFarlane (2009) at distinguishing the view that propositional truth can vary between different contexts in the same world, and the view that the truth of an utterance can be assessor-sensitive.\nTamina Stephenson’s (2007) arguments in favour of a variable PROJ whose value is set by assessors.\nPhilippe Schlenker’s (2003) idea, modelled on some examples from Barbara Partee (1989) that plural variables can be ‘partially bound.’\nAnthony Gillies’s (2009) suggestions for how to explain the acceptability of the ‘import-export’ schema in an epistemic theory of indicative conditionals.\nI think it is noteworthy, in light of the claims one sometimes hears about philosophy not making progress, that most of the building blocks of the theory defended here weren’t even clearly conceptualised at the time I wrote the earlier paper.\nThis paper is in seven sections. The paper starts with an argument in favour of an epistemic treatment of open indicatives, namely that only the epistemic theory can explain our judgments about inferences involving open indicatives. The argument isn’t completely original, indeed much of what I’ll say here can be found in Stalnaker (1975), but I don’t think the scope of this argument has been sufficiently appreciated. There are a large family of epistemic theories, and in section two I’ll set out some of the choice points that an epistemic theorist faces. I’ll also introduce a fairly simple epistemic theory, not the one I favour actually, that I’ll focus on in what follows. My preferred theory has several more bells and whistles, but I don’t think those are relevant to the issues about relativism and indexicalism that I’ll focus on here, and including them would just complicate the discussion needlessly.\nIn section three I look at four ways a theory could say that the truth of an utterance type is sensitive to context.1 The four ways are generate by the ways the theory answers two questions. First, is the truth of the utterance type sensitive to facts about the context of utterance, as contextualists say, or to facts about the context of evaluation, as relativists say? Second, does the utterance type express different propositions in different contexts, as indexicalists say, or does it express a proposition that takes different truth values in different contexts, as non-indexicalists say? In using the term ‘indexicalist,’ I’m implicitly assuming the theory, most associated with Jason Stanley (2007) , that the way an utterance type can express different propositions in different contexts is that it has a variable in its semantic structure, and different contexts assign different values to this variable.\nThree of the four options generated by the two questions, indexical contextualism, non-indexical relativism, and non-indexical contextualism, have received some coverage in the literature. The fourth option, indexical relativism, has not been as widely discussed. In section four I say a little about its motivations, including its connection to recent work by Tamina Stephenson. The variable in the semantics for open indicatives is a plural variable; roughly it takes as values all those propositions that are known by the salient people in the context. In section five I note some odd properties about bound plural pronouns that will become relevant to the story that follows. The short version is that some plural pronouns can have their values set partially by antecedent linguistic material, and partially by context. So if a pronoun v refers to the Xs, it might be that a is one of the Xs because v is bound to a term that denotes a, and b is one of the Xs because b is contextually salient in the right kind of way to be one of the things that v denotes.\nIn section six I put forward the arguments against contextualism, and in particular against indexical contextualism. I start with some arguments from ‘faultless disagreement,’ and go through four reasons why these might not be too strong. I then discuss some arguments from facts about when two people have said the same thing, or have said different things. These arguments tell against simple forms of indexical contextualism, but not against more sophisticated versions. But I conclude with a somewhat simpler argument, an argument from what I’ll call easy agreement, that does seem to undermine indexical contextualism.\nFinally in section seven I’ll argue against non-indexical theories of open indicatives. The primary argument will be that the indexicalist has a good explanation of what’s going on in McGee’s ‘counterexamples to modus ponens,’ an explanation borrowed from some recent work by Anthony Gillies, but the non-indexicalist does not. The indexicalist’s explanation is that these arguments contain fallacies of equivocation; on the non-indexicalist position, it is hard to see how they equivocate. The upshot of the final two sections is that indexical relativism is correct.\nInferences Involving Conditionals\nI’m going to start by offering an argument for an epistemic treatment of conditionals. The argument isn’t particularly new, I’m basically just offering an extension of the argument in Stalnaker (1975), but I don’t think the force of it has been fully appreciated. The argument starts with the observation that any instance of any of the following inference schema seems acceptable when the conclusion is an open indicative.\nNot A or B; so, If A then B\nAll Fs are Gs; so, If Fa then Ga\nf(x) = f(y); so If f(a) = c then f(b) = c\nHere are some instances of each of these inferences.\nEither Jack won’t come to the party or Jill will; so if Jack comes to the party, so will Jill.\nAll of Kim’s students failed; so if Alex was one of Kim’s students, then Alex failed.\nPeter’s mother is Paul’s mother; so if Peter’s mother is Mary, Paul’s mother is Mary.\nQuite a lot has been written about (1)/(4) and I don’t propose to add to it. It is arguable that part of the explanation for its attractiveness comes from pragmatic properties of disjunctions, and if that’s right it would complicate the story I want to tell here. Instead I’ll focus on the other two inferences.\nJonathan Ichikawa pointed out to me that (3) is not particularly compelling in cases where it is common ground in the conversation that f(a) is not c. For instance, it is at least odd to say “Peter’s mother is Jane, and she’s also Paul’s mother. So Peter’s mother is Paul’s mother; so if Peter’s mother is Mary, Paul’s mother is Mary.” I think intuitions differ on these cases. I don’t find the inferences as bad as many informants do, and generally speaking intuitions about knowledge-contravening indicatives are rather fuzzy. So I’m just going to focus on the case where the conclusions are open.\nErnest Adams (1998) has objected to the claim that instances of (2) always seem like acceptable inferences using the following example.\nEveryone who was at the party is a student. So if the Chancellor was at the party, the Chancellor is a student. (Adams 1998: 289)\nI actually think this sounds like a perfectly fine inference. If I say that everyone at the party was a student, and someone takes me to thereby to be committed to the claim that if the Chancellor was at the party, she too is a student, then I won’t complain. But perhaps my intuitions here are odd. Here is an intuition that I feel more comfortable with. If the conclusion of (7) is an open indicative, that is if it isn’t ruled out that the Chancellor is a student, then the inference in (7) sounds perfectly fine to me.\nAdams has to object to (2) because it provides counterexamples to a thesis he defends at some length. This thesis is that an inference from a single premise to a conditional If p then q is a good inference iff necessarily the probability of q given p is not lower than the probability of the premise. Schema (3) is also a problem here as well. In each case it isn’t too hard to find instances where the probability of the premise is arbitrarily high, but the probability of the conclusion’s consequent given its antecedent is arbitrarily low. For instance, let the salient probabilities be as below: \\[\\begin{aligned}\nPr(f(a) = f(b) = d) &= 1-x \\\\\nPr(f(a) = c \\wedge f(b) = e) &= x\\end{aligned}\\]\nIf we let x be arbitrarily small, then the probability of the premise f(a) = f(b) will be arbitrarily high. But the conditional probability of f(b) = c given f(a) = c will be arbitrarily low. So the probability preservation property Adams has highlighted isn’t what is always preserved in good inferences.\nEach of the inferences in (1) to (3) is, in some sense, a good inference. It is easy to prove from that fact, and the assumption that good inferences are valid implications, that the conditional If p then q is true if p is false or q is true. If we assume Modus Ponens is valid (as I will throughout) then we can strengthen this conditional to a biconditional. It is obviously easy to prove this using the goodness of (1). Here is a proof that uses just (3), and some weak assumptions about truth. Line 1 is the only assumption, and every line seems to follow from the one before it.\nThe truth value of p is the truth value of p & q\nSo if the truth value of p is true, then the truth value of p & q is true\nSo if p is true, then p & q is true\nSo if p, then p & q\nSo if p, then q\nI’ve assumed here that we can treat The truth value of p is true, p is true, and p as equivalent, but this seems uncontroversial. And on pretty much any conditional logic there is, the move from (4) to (5) will be valid. Assuming bivalence, (1) is equivalent to the disjunction p is false or q is true. So we can infer from that disjunction to If p, then q without using the schema (1).\nThis might all suggest that open indicatives should be interpreted as material implications. But there is some data that tells against that. This suggestion from Richard Bradley (2000) seems correct.\n\n[O]ne cannot be certain that B is not the case if one thinks that it is possible that if A then B, unless one rules out the possibility of A as well. You cannot, for instance, hold that we might go to the beach, but that we certainly won’t go swimming and at the same time consider it possible that if we go to the beach we will go swimming! To do so would reveal a misunderstanding of the indicative conditional (or just plain inconsistency). (Bradley 2000, 220)\n\nMore generally, someone who regards A as an epistemic possibility, but knows that B is false, should regard If A, B also as something they know to be false. Bradley puts this in probabilistic terms as follows.\nPreservation Condition:\nIf Pr(A) > 0 but Pr(B) = 0, then Pr(A \\(\\rightarrow\\) B) = 0\n\nThis isn’t obviously the best formulation of his principle. In the example, what matters is not that A has non-zero probability, but that it is something that might be true. (These are different. The probability that the average temperature in Ithaca on January 1 next year will be exactly 32 degree Fahrenheit is 0, but that might be the exact temperature.) The structure of the inference looks to be what is given in (8), where Kp means the relevant agents knows that p.\n\\(\\neg\\)K \\(\\neg\\)A\nK\\(\\neg\\)B\nSo, K\\(\\neg\\)(If A, B)\nBut this is not valid if the conditional is a material implication. So now it is impossible to accept all of the intuitively plausible principles about inference involving conditionals are truth-preserving. There must be some other explanation of the reasonableness of all these inferences other than their being valid implications.\nThe best explanation I know of this ‘reasonableness’ is the one endorsed by Daniel Nolan (2003) as an explanation of inferences like (1). Nolan says that given an epistemic theory of the indicative, we can say that each of the inferences has the following property. Any speaker who knows the premise is in a position to truly assert the conclusion. Call an inference like this, where knowledge of the premise implies truth of the conclusion, epistemically acceptable. If we are confusing valid implications with epistemically acceptable inferences, this could explain why all of (1) through (3) seem reasonable. More impressively, this hypothesis of Nolan’s explains why (8) seems reasonable, given an epistemic theory of indicatives. If we know that A is true in some epistemic possibilities, but B is false in all of them, then all the epistemically salient alternatives where A is true will be ones where B is false. So (8) will turn out to be a good inference, by Nolan’s criteria. So (8), like (1) through (3), is epistemically acceptable. So given Nolan’s epistemic account of reasonable inference, and an epistemic theory of indicative conditionals, we can explain the reasonableness of all five problematic inferences. In the absence of any other good explanation of this reasonableness, this seems to me to be a good reason to accept both Nolan’s account and an epistemic theory of indicative conditionals.\nThe Simple Epistemic Theory of Conditionals\nFor concreteness, I’ll work in this paper with a very simple theory of conditionals. I assume that in general a conditional If p, q has the logical form C(p, q, X), where C is the conditional relation, and X is a plural variable that denotes some propositions taken as fixed in the context. The simple epistemic theory makes two additions to this basic assumption.\nFirst, there is some epistemic relation R such a proposition s is among the X iff some salient individual i stands in relation R to s. We’ll use R(i) to represent those propositions. It will become important later that X is genuinely a plural variable, so R(i) is not a set of propositions, or a fusion of propositions (whatever that would be). Rather, I just mean to be plurally referring to the propositions that stand in relation R to i. (Note that I’m not saying anything here about how i is determined; my preferred theory is that it is the evaluator of any conditional utterance, but nothing in the simple epistemic theory turns on this.)\nA very conservative version of the theory says that R is the knowledge relation. One can liberalise the theory in two respects. First, we can say that R is the ‘position to know’ relation. Second, we can say that sRi iff someone salient to i knows that S. A maximally liberal version of the theory says that sRi iff someone salient to i is in a position to know that S. I’m not going to argue for this here, but I think this maximally liberal option is the way to proceed, so that’s what I’m going to adopt for the sake of exposition. Nothing turns on this adoption in what follows. Indeed the arguments against contextualism and for relativism are stronger the more constrained R is, so this is tilting the playing field away from my preferred outcome.\nSecond, the simple theory I have in mind says that C is basically just the a priori entailment relation. So C(p, q, X) is true iff p plus X a priori entail q. If you want to say that entailment is a relation between a set and a proposition, the claim is that the union of {p} and {s: s is among the X} a priori entail that q.\nThere are several ways in which one might want to complicate the simple epistemic theory. My preferred theory involves some of these complications. Here are some complications that have been proposed in various ways.\nFirst, we might change C to stipulate that whenever p and q are true, C(p, q, X) is true. This is equivalent to endorsing strong centring in the sense of Lewis (1973). Assuming every proposition in X is true, as I’ve done above, means that we’ve already guaranteed that C(p, q, X) is false when p is true and q is false.\nSecond, we might deny bivalence in the following way. Say that C(p, q, X) is true iff p and X a priori entail that q, false if it is not true and also p and X entail \\(\\neg\\)q, and indeterminate in truth value otherwise. Going down this path allows one to endorse conditional excluded middle, as supported by Stalnaker (1981). Denying bivalence does not compel acceptance of conditional excluded middle, but it becomes an interesting possibility once you go down this path.\nThird, we might say that R is a disjunctive relation, so some propositions are among the R(i) because they stand in an epistemic relation to i, and others are in because they are in some sense ‘fixed facts’ of i’s world. Nolan (2003) uses a quite different formalism, but if we wanted to translate his theory into this formalism, that’s what we’d do.\nFourth, we could make C a more complicated relation. In particular, we could make it in a sense non-monotonic, e.g. by saying that C(p, q, X) holds iff the epistemic probability of q given p and X is sufficiently high. If C is non-monotonic in this sense, then we can have a conditional logic that looks like some of the conditional logics in Lewis (1973).\nFor what it’s worth, I favour the first and (a version of) the second of these complications, but not the third and fourth. Defending those preferences would take us too far afield however. What I mostly want to show in this paper is that whatever form of epistemic theory we adopt, we should adopt what I’ll call an indexical relativist version of that theory. So I’ll just presuppose the simple epistemic theory throughout, because the general form of the argument should be easily adoptable whichever complications one adds on. The task of the next section is to introduce indexical relativism.\nFour Kinds of Sensitivity\nLet’s say that one is tempted towards a kind of moral relativism. So when old Horace, way back when, “Premarital sex is morally worse than driving drunk” he said something true in some sense, and when modern Kayla now says “Driving drunk is morally worse than premarital sex,” she also says something true in a sense. How might we formalise these intuitions? (Not, I might add, intuitions that I share.) There are a few simple options, breaking down along two distinct axes. To save space, I’ll write P for pre-marital sex, D for driving drunk, and < for the relation is morally worse than, in what follows.2\nThe first axis concerns the nature of propositions about the moral. One option is to say that moral codes are part of the propositions that are the content of Horace’s and Kayla’s utterances. For example, we might say that when old Horace makes his utterance, its content is the proposition P < D in MO, where MO is Horace’s old moral code. Conversely, when Kayla makes her utterance, its content is the proposition D < P in MN. where MN is Kayla’s new moral code. This option, as Sayre-McCord (1991) notes, treats ‘moral’ as being like ‘legal.’ When we say “Insulting the Thai monarch is illegal,” the content of our utterance is the proposition Insulting the Thai monarch is illegal in L, where L is some salient legal code. That’s why typical utterances of that sentence in Bangkok are true, but typical utterances of it in St Andrews are false. Call this option indexicalism, since it thinks there is an indexical element in the semantic structure of what Horace and Kayla say. Because this will become crucial later, what I’m taking to be essential to indexicalism is simply the view that there is a moral code in the proposition expressed, not that it is the moral code of the speaker.\nA quite different option is to say that the content of Horace’s utterance is simply the proposition P < D. The relativism comes in because it turns out that propositions are true or false relative to, inter alia, moral codes. The proposition P < D is true in MO, and false in MN. The analogy here is to the way the very same proposition can be true in one world false in another. This option, which I’ll call non-indexicalism, says that moral codes function much like worlds; they are things relative to which propositions are true or false. The non-indexicalist takes Kaplan (1989) to be on the right track in saying that propositions are true or false relative to world-time pairs, but thinks that the indices relative to which propositions are true or false are even more fine-grained than that.\nThe second axis concerns which context is relevant to the truth of the utterance. One option is to say that it is the context of utterance. A second option is to say that it is the context of evaluation. Following Macfarlane (2007; MacFarlane 2009), I’ll call the first option contextualism, and the second option relativism. The point that’s worth focussing on here is that what choice we make here cuts across the choice we make on the first axis. So there are four options available. To set these out, we need to introduce a third character (call him Deval) who assesses Horace’s and Kayla’s utterances. For concreteness, call Deval’s moral code MA, and say that it agrees with MN on the point at issue. Then here are the four options we have.\nIndexical Contextualism. The propositions that are the content of Horace’s and Kayla’s utteranes include moral codes, and which code that is is determined by features of their utterance. So the content of Horace’s utterance is the proposition P < D in MO, and Kayla’s the proposition D < P in MN. Deval should assess each of them as having uttered truths.\nNon-indexical Relativism. The propositions that are the content of their utterances do not include moral codes, and their utterances are only true or false relative to a moral code provided by an assessor. So the content of Horace’s utterance is simply P < D, and Kayla’s D < P. Since in MAD < P is true, Deval should assess Horace’s utterance as false, and Kayla’s as true.\nNon-indexical contextualism. The propositions that are the content of their utterances do not include moral codes, but the truth-value a moral utterance is the truth-value of its content in the context it is expressed in. So the content of Horace’s utterance is simply P < D, and Kayla’s D < P. Since Horace makes his utterance in a context where P < D is part of the prevailing moral code, his utterance is true. So Deval and Kayla should assess it as true, even though they think the proposition Horace expressed is false. This isn’t contradictory; a person in another possible world can make a true utterance by expressing a proposition that is (actually) false, and for the non-indexical contextualist, moral codes are in a way like worlds. Kayla’s utterance is true as well, since it is made in a different context.\nIndexical relativism. The propositions that are the content of Horace’s and Kayla’s utteranes include moral codes, and which code that is is determined by features of the context of assessment. So when Deval hears of these two utterances, he should interpret the content of Horace’s utterance to be P < D in MA, and of Kayla’s to be D < P in MA. Since the latter proposition is true, he should interpret Kayla’s utterance as true, and Horace’s as false.\nIt might make it easier to picture these positions in a small table, as follows.\nm0.29 | m0.29 | m0.29   & The speaker’s moral code matters to utterance truth (contextualism) & The assessor’s moral code matters to utterance truth (relativism) Propositions include moral codes (indexicalism) & Indexical Contextualism & Indexical Relativism Propositions are true or false relative to moral codes (non-indexicalism) & Non-indexical Contextualism & Non-indexical Relativism\nThe modern discussion of non-indexical relativism, though not under that name, traces to MacFarlane (2003). The modern discussion of non-indexical contextualism, under that name, traces to Macfarlane (2007; MacFarlane 2009). Much of this paper, and all of this section, is about setting out the distinctions that MacFarlane makes in the latter papers between indexicalism and contextualism. But once we do that, we see that there is a position, indexical relativism, that hasn’t had much attention. I plan to change that.\nBefore we get on to the content of indexical relativism, a small note on nomenclature is in order. I’ve picked the names I have so (a) we’ll have a compositional naming scheme and (b) we get ‘non-indexical contextualism’ to denote what it denotes in MacFarlane’s terminology. This does mean using the term ‘indexical relativism’ in a slightly different way to how it has been used in the part. Einheuser (2008) and López de Sa (2007a) each use ‘indexical relativism’ to mean just what I’ve meant by ‘indexical contextualism.’ Kölbel (2004) also uses the term ‘indexical relativism,’ though López de Sa (2007b) argues that he too just means contextualism, and I’m inclined to agree. So though the name had been previously used, it had not been used to express a distinctive view.\nOn the other hand, there had been some discussions of the position I’m calling ‘indexical relativism.’ In Egan, Hawthorne, and Weatherson (2005) we call such a position ‘content relativism,’ though Cappelen (2008) uses that term for a slightly different position. In MacFarlane (2005) he discusses ‘assessment indexicality,’ a property sentences have if they express different propositions in different contexts. So there doesn’t seem to be a settled terminology for this corner of the table, and I propose to take ‘indexical relativism’ for it.\nIndexical Relativism\nIn “Judge Dependence, Epistemic Modals, and Predicates of Personal Taste,” Tamina Stephenson proposes a variant on Peter Lasersohn’s (2005) relativist account of predicates of personal taste. Stephenson proposes that predicates of personal taste always encode relations between an object and an assessor. So when we say “Warm beer is tasty” we express some proposition of the form Warm beer is tasty to X. So far, this is not particularly new. What is interesting is Stephenson’s suggestion that some of the time (but not always) there is a ‘silent nominal’ PROJ, whose value is the ‘judge.’ So the utterance will be true as judged by y iff warm beer is tasty to y. There are several advantages to positing a tacit parameter. One that Stephenson stresses is that in some cases, e.g. when we are talking about the tastiness of various brands of cat food, we can let the value of this parameter be the cat rather than any human. But, by letting it by default take the value PROJ, Stephenson shows that we can accommodate most of the intuitions that motivate Lasersohn’s relativism.\nNow Stephenson is not an indexical relativist, as I’ve defined that position. For according to the indexical relativist, propositions are only true or false relative to worlds. And Stephenson has propositions be world-time-judge triples. But I think we can adopt her idea to set out a kind of indexical relativism. I’ll first say how this could go in the moral case, then apply it to conditionals.\nThe moral indexical relativist says that the context-neutral content of an utterance like Kayla’s is not a complete proposition. Rather, it is a propositional frame that we might express as D < P in M(PROJ), where M(x) is x’s moral code, and PROJ is (as always) the judge. Relative to any judge, the content of her utterance is that D < P in that judge’s moral code. So relative to Horace, the content of her utterance is the false proposition D < P in MO, and relative to Deval it is the true proposition D < P in MA. That’s why it is fine for Horace to say “I disagree,” or “That’s false,” or “She speaks falsely,” and fine for Deval to say “I agree,” or “That’s true,” or “She speaks truly.” Now I reject any kind of moral relativism, so this isn’t my theory for moral language, but it’s a theory that could in principle work.\nWhat I will defend is indexical relativism for indicative conditionals. In general, the content of an indicative conditional If p, q is C(p, q, X), where the propositions in X are the ‘background’ propositions relative to which the conditional is assessed, and C is the conditional relation3. An epistemic theory of indicatives says that the value of X is (by default) R(x), where r is some epistemic relation (on a broad construal of ‘epistemic’) and x is a salient individual. The indexical relativist position is that the content of an utterance of a conditional is (by default) a propositional frame that we might express as C(p, q, R(PROJ)). Relative to an assessor a, the content is C(p, q, R(a)).\nThe argument for this position will come in sections 6 and 7. In section 6 I’ll argue against indexical contextualism. The argument will be that if indexical contextualism were true, it should be harder to agree with an utterer of a conditional than it actually is. Then in section 7, I’ll argue for indexicalism. The argument will be that we need to posit the third argument place in the conditional relation to explain what goes wrong in some arguments that are alleged to be both instances of modus ponens and invalid. I’ll argue (not particularly originally) that these arguments involve a shift of a tacit parameter, namely X. This suggests that X exists. Between those two arguments, we can conclude that indexical relativism is true. Before that, I want to look at some arguments against indexical relativism.\nIn Egan, Hawthorne, and Weatherson (2005) we mention two arguments against this position. One is that indexical relativism is incompatible with a Stalnakerian account of the role of assertion. Assertions, we said, are proposals to add something, namely their content, to the context set. But if the content of an assertion is different in different contexts, then it is impossible to add it to the context set. And that, we thought, was a problem. I now think there’s a relatively simple way around this.4 If you want to add a proposition to the context set, then there has to be a context. And relative to any context, a conditional does have a content. So given any context, the content of the conditional (relative to that context) can be added to the context set. And that’s all the Stalnakerian account requires.\nPerhaps a stronger version of this objection is that even if you can figure out, given the rules, what move a speaker is making according to this theory, this isn’t a move that sensible speakers should want to make. So imagine that A says that if p, then q, and says this because they have just discovered something no one else knows, namely \\(\\neg\\)(p & \\(\\neg\\)q). Now B hears this, not because A tells her, but because of a fortuitous echo. B takes A to be expressing the proposition C(p, q, R(B)), and proposing that it be added to the context set. But that’s a terrible proposal, we might object, because A has no reason to know that C(p, q, R(B)), since she knows nothing about B’s knowledge. Since there is nothing wrong with A’s utterance, and the theory interprets her as making an indefensible proposal to add something to the context set, the theory is wrong.\nThis objection is potentially a powerful one, and any version of indexical relativism must say something about such an objection. What I say is that the objection misconstrues R(B). If B is considering an utterance by A, even if B does not know that A is the author of that utterance, then any proposition that A knows is among the R(B). I think this holds quite generally. If A knowably asserts that p, and B considers it and says “That might not be true,” what B says is false, even if B does not know whether p is true. The reason is that B is taking A’s knowledge to be, for the time being, relevant to the context of her utterance. So in short, the knowledge on which A relies for her utterance is carried in to every context in which that very utterance is assessed. That’s why it is acceptable for A to make such a sweeping proposal, namely that for every x who evaluates her utterance, C(p, q, R(x)) should be added to the common ground of x’s context. This response relies heavily on specific features of the interaction between conditionals and context, and I don’t think it generalises very far. It may be that this style of objection does defeat some prima facie plausible versions of indexical relativism, though it does not touch indexical relativism about open indicatives.\nA related objection, to a quite different proposal, is made in King and Stanley (2005). They oppose the theory that the semantic content of an utterance is something like a character. They say the content has to be a proposition, and the reason for this is that “Our understanding of a sentence in a context is due to a compositional procedure that calculates the content of the whole sentence from the referential contents of its parts.” This seems like a good reason for not taking the semantic content of a sentence in general to be its character. And we might worry that it could be extended to an argument against the view that the content of a conditional is a function from contexts of assessment to propositions. But on closer inspection it seems like no such generalisation is possible. After all, someone interpreting a conditional can assign a value to the variable X that takes different values in different contexts of assessment. They can just ‘replace’ PROJ with themselves when interpreting the conditional. The important point here is that the view that the utterance does not have a context-neutral semantic content is consistent with it having a content relative to any interpreter, and hence to interpreters discovering its content (relative to them).\nThe other objection we made to indexical relativism in the earlier paper was that it left as unexplained some phenomena about the behaviour of epistemic modals in propositional attitude reports that the non-indexical theory could explain. I still think this is an advantage of the non-indexical theory, but I don’t think it is decisive. (In general, it isn’t a deal breaker that one theory has to take as a brute fact something that a rival theory explain, although it does count in favour of the rival.) I’ll say more about this objection when we discuss propositional attitude reports in more detail in section 6. But first, I need to introduce some facts about the behaviour of plural pronouns that my indexical relativist theory will exploit.\nPartial Binding\nPhilosophical orthodoxy has it that all pronouns fall into one of two broad categories. On the one hand, there are deictic pronouns, whose job it is to refer (presumably directly) to a contextually salient object. On the other hand, there are pronouns whose job it is to denote (one way or another), an object denoted earlier in the discourse. Examples of the latter kind include the tokens of ‘she’ in (5.1) to (5.3).\nIf Suzy enters the competition, she will win.\nEvery student will get the grade that she deserves.\nIf a dictator has a daughter, she is pampered by the state.\nI’m not going to go into the (very interesting) debates about how many different kinds of pronouns are represented by the three tokens of ‘she’ above, nor about which of these pronouns are directly referential, which are quantifiers, and which are neither of these. All I want to note is that the pronouns represented here fall into a different category than simple deictic pronouns, that refer to a contextually salient individual.\nThomas (McKay 2006 Ch. 9) has argued that the behaviour of plural pronouns mirrors the behaviour of singular pronouns. He shows that for every different kind of singular pronoun we can find, or even purport to find, we can find plural pronouns behaving the same way. The following three sentences, which are about a film school where girls make films in large groups, have pronouns that behave just like the three tokens of ‘she’ above.\nIf some girls enter the competition, they will win.\nSome students will produce a better film than we expect them to.\nIf a student dislikes some girls, their work suffers.\nWhat is quite noteworthy about plural pronouns, however, is that they need not fall into one of the two major categories I mentioned at the start of the section.5 It is possible to have a plural pronoun whose denotation is determined partially by context, and partially by the denotation of earlier parts of the discourse. Consider, for example, (5.7), as uttered by Jason.\nIf Ted comes over, we’ll go and get some beers.\nIt seems the ‘we’ there denotes Ted and Jason. It denotes Jason because it’s a first-person plural pronoun, and Jason is the speaker, and the speaker is always among the denotata of a first-person plural pronoun. Arguably, the ‘we’ is anaphoric on ‘Ted,’ but this does not mean it denotes only Ted. Rather, it means that Ted is among the denotata of ‘we,’ the others being determined by context. One might object that really ‘we’ in (5.7) is deictic, and Ted is among its denotata because he has been made salient. I think that’s probably a mistake, but I don’t want to press the point. Rather, I’ll note some other cases where such an explanation is unacceptable. The following example, due to Jeff King (p.c.) shows that ‘we’ can behave like a donkey pronoun.\nIf any friend comes over, we’ll go and get some beers.\nIntuitively that’s true, as uttered by Jason, just in case for some salient class of friends, if any member of that class comes over, Jason and that friend will go and get beers. Now it is controversial just how to account for donkey pronouns in general, and I’m not going to take a side on that. But however they work, donkey pronouns seem to fall on the second side of the divide I mentioned at the top of the section. And first person singular pronouns are paradigmatic instances of pronouns that get their reference from context. What’s notable is that first-person plural pronouns can display both kinds of features.\nThis is not a particularly new point. Example (5.9) was introduced by Barbara Partee in 1989, and there is a longer discussion of the phenomena by Phillipe Schlenker in his (2003). The latter paper is the source for (5.10) and (5.11).\nJohn often comes over for Sunday brunch. Whenever someone else comes over too, we (all) end up playing trios. Partee (1989)\nEach of my colleagues is so difficult that at some point or other we’ve had an argument. Schlenker (2003)\n[Talking about John] Each of his colleagues is so difficult that at some point or other they’ve had an argument. Schlenker (2003)\nSchlenker describes what is going on here as ‘partial binding,’ and I’ll follow his lead here. The ‘we’ in (5.10) is bound to the earlier quantifier phrase ‘Each of my colleagues,’ but, as above, this does not mean that it merely denotes (relative to a variable assignment) one of the speaker’s colleagues. Rather, it denotes a plurality that includes the colleague, and includes the speaker. And the speaker is supplied as one of the denotata by context.\nThe reason for mentioning all this here is that my theory of how conditionals behave involves, among other things, partial binding. The general semantic structure of a conditional is C(p, q, X), where X is a plural variable that denotes some propositions. I think X can, indeed often is, partially bound. In simple cases a proposition is among the X just in case it stands in relation R to a salient individual i. In more complex cases, X is partially bound to an earlier phrase, and in virtue of that some proposition s is among the X. But the propositions that stand in relation R to i are also among the X, because X is only partially bound to the earlier proposition. If there were no other instances of partial binding in natural language, this would be a fairly ad hoc position to take. But there shouldn’t be any theoretical problem with assuming that tacit variables can behave the way that overt pronouns behave.\nAgainst Indexical Contexualism\nOne usual way to argue for relativist theories is to appeal to instances of faultless disagreement. It is natural to think that such arguments could work in the case of open indicatives. Since Gibbard (1981) there has been a lot of discussion over cases where A knows \\(\\neg\\)(p & \\(\\neg\\)q), and B knows \\(\\neg\\)(p & q). It seems A can truly, even knowledgeably, say If p, q, and B can truly, even knowledgeably, say If p, \\(\\neg\\)q. And, in the right context, it might seem that this is a case where A and B disagree. One might try and argue that the only way to explain this faultless disagreement between A and B is through some variety of relativistic semantics. I think that will be a hard argument to make out for four reasons.\nFirst, some people hold that the notion of a faultless disagreement is incoherent. I suspect that’s wrong, and the concept is coherent, but making this argument stick would require showing that faultless disagreement is indeed coherent. I want the argument for indexical relativism about open indicatives to not rely on the coherence of faultless disagreement.\nSecond, two people can disagree without there being any proposition that one says is true and the other is false. (This should be familiar from debates about non-cognitivism in ethics.) If A says “I like ice cream” and B says “I don’t like ice cream,” then there is a natural sense in which they are disagreeing, for instance. But arguments from disagreement for relativism generally require that when two people disagree, there is a proposition that one accepts and the other rejects, and that may not be true.\nThird, there is some special reason to think that this is what happens in the conditionals case. In this case A and B are having a conditional disagreement. Perhaps we intuit that A and B are disagreeing merely because of this conditional disagreement. For comparison, if A and B had made a conditional bet, we would describe them as having made a bet in ordinary discourse, even if the bet is not realised because the condition is not satisfied.\nFinally, as Grice (1989) showed, there can be cases where we naturally describe A and B as disagreeing in virtue of two utterances, even though (a) those utterances are simple assertions, and (b) the assertions are consistent. Grice’s case is where A says that p or q is true, and B says that p or r is true, with stress on r, where q and r are obviously incompatible. Perhaps the natural thing to say here too is that A and B have a conditional disagreement; conditional on \\(\\neg\\)p, A thinks that q and B thinks that r. So this argument seems to need a lot of work.\nAn apparently stronger argument comes from indirect speech reports. It seems that in any case where a speaker, say Clarke, says “If the doctor didn’t do it, the lawyer did,” then in any other context, we can report that by saying “Clarke said that if the doctor didn’t do it, the lawyer did.” This might look to pose a problem for indexical contextualism.\nAssuming that the content of If p, q is C(p, q, R(i)), where i is some person made salient by the context of utterance. Let p be The doctor didn’t do it, and q be The lawyer did it, c be Clarke, and h be the person who reports what Clarke said. Then it seems that what Clarke said is C(p, q, R(c)). But it seems that the content of what comes after the that in the report is C(p, q, R(h)). But since R(c) might not be the same as R(h), this should look like a bad report.\nI think this is something of a problem for the indexical contextualist, but it isn’t beyond repair. It could be that the variable i in the speech report is bound to the name at the start of the report, so the value of i in Clarke said C(p, q, R(i)) is simply Clarke herself. This is a slightly odd kind of binding, but it isn’t impossible, so this doesn’t quite rule out a contextualist theory.\nAs MacFarlane (2009) argues, the felicity of homophonic reports does not raise a problem for either kind of non-indexical theory. I’ll argue that it also doesn’t pose a problem for indexical relativism.\nThe indexical relativist thinks that, on its most natural interpretation, the content of Clarke’s utterance is C(p, q, R(PROJ)). Similarly, it might be thought that the natural interpretation of what comes after the that in the report is C(p, q, R(PROJ)). So it isn’t surprising that the report is acceptable.\nI can imagine an objector making the following speech. “Assume that Clarke’s utterance was sincere. Then it seems natural to say that Clarke believes that if the doctor didn’t do it, the lawyer did. But it is odd to think that Clarke believes C(p, q, R(PROJ)). What she believes is C(p, q, R(c)). The only way to get belief reports to work on the indexical contextualist theory is to insist that the i is bound to the subject of the report. But once you say that it is ad hoc to deny that the i is also bound to the subject in a speech report. And not just ad hoc, it implies that (relative to our context) Clarke doesn’t believe what she says, for she says C(p, q, R(PROJ)) but believes C(p, q, R(c)). On the other hand if the i is bound to the speaker in a speech report, so what Clarke is said to have said is C(p, q, R(c)), then (a) you have no advantage over the indexical contextualist, and (b) you can’t explain why the report is felicitous, since you say she says something else, namely C(p, q, R(PROJ)).”\nI have three responses to this critic. The first is that I’m not convinced that having a different treatment of belief reports and speech reports, letting i be PROJ in speech reports and the believer in belief reports, is too terrible. The argument that it would be ad hoc to treat speech reports and belief reports separately seems weak. It is worse if we end up, because of the structure of the theory, accusing Clarke of insincerity. One way of avoiding that response is to accept the binding proposal. But it isn’t the only way. If we make two assumptions about C and R, we can sidestep the danger.\nThe first assumption is that C is monotonic in the sense that C(p, q, X + Y) is entailed by C(p, q, X). The second is that xRp is true in case someone salient to x bears R to p, and the utterer of the judged sentence is salient (in this sense) to the judge. (Note this is exactly the assumption that I made earlier to defend my proposal about what effect uttering a conditional has on the Stalnakerian context.) Now not all indexical relativists will want to make these assumptions, but I’m happy to do so. Now if Clarke believes If p, q, she believes C(p, q, R(c)). So she either believes she knows \\(\\neg\\)(p & \\(\\neg\\)q), or believes she knows some things that (perhaps unbeknownst to her) entail it. That means that, whoever the judge of her utterance is, she believes that R(PROJ) either includes or entails \\(\\neg\\)(p & \\(\\neg\\)q). So she believes C(p, q, R(PROJ)), as required.\nSo I don’t think the indexical relativist has to concede to the critic that speech reports involve binding in this way. But I might be wrong about this, so my second and third responses concede this point, and argue that it doesn’t harm the indexical relativist position.\nThe second response is that even with this concession, the indexical relativist has a small advantage over the indexical contextualist. Drawing on Stephenson’s work, we could argue that (a) PROJ is often the value of a tacit variable, and (b) whenever it is the default value of a variable, then that variable is bound to the subject of a propositional attitude report. If that is the case, then the indexical relativist could unify a number of different cases that would have to be treated separately by the indexical contextualist. Still, it is true that the non-indexicalist has an even larger advantage here, since they can explain why this (apparent) binding holds, but I don’t think this advantage is decisive. This is the one argument for non-indexicalism that I mentioned in section 4 might still have some force, though not I think enough to override the argument for indexicalism in the next section.\nThe third response is that the indexical relativist has a simple explanation of why the reports are natural, even on the assumption that the i is bound to the speaker. First consider a similar case. Imagine Clarke had simply said “The lawyer did it,” i.e. q. It would be natural to report her as having said that the lawyer actually did it. Now one can imagine being surprised at this. Clarke said something contingently true, but we reported her using a proposition that the lawyer actually did it, that is necessarily true. How is this possible? Well, it is because in saying q, she immediately and obviously commits herself to Actually q, and if a speaker immediately and obviously commits themselves to a proposition in virtue of an utterance, then it is natural to report them as having said that proposition. Speakers are generally committed to the truth of the utterances from their own perspective, so Clarke is committed to C(p, q, R(c)). (Arguably that is all she is committed to, as opposed to C(p, q, R(PROJ)).) So we can report her as having said C(p, q, R(PROJ)). And if i is bound to the speaker, that is what we do report her as having said by saying “Clarke said that if the doctor didn’t do it, the lawyer did.”6\nSo both the argument from disagreement and the argument from speech report against indexical contextualism have run up against some blocks. There is another argument, however, that is effective against it. This is an argument from easy agreement. Assume again that Clarke said “If the doctor didn’t do it, the lawyer did.” Assume that an arbitrary person, call him Rebus, knows that Clarke made this utterance, and knows that either the doctor or the lawyer did it, that is knows that \\(\\neg\\)(p & \\(\\neg\\)q). On that basis alone, it will be natural for Rebus to make any of the following utterances. “I agree”; “That’s right”; “That’s true”; “What she said is true”; “She spoke truly.” Any of these are hard to explain on the indexical contextualist view, according to which agreement should be harder to get than this.\nOn the indexical contextualist view, Clarke said C(p, q, R(c)). Now on most accounts of what r is, Rebus need not know that \\(\\neg\\)(p & \\(\\neg\\)q) is one of the propositions in R(c). He need not know that Clarke know this, or that Clarke could have known this, or really anything else. As long as he knows that Clarke made this utterance, it seems acceptable for him to agree with it using any of the above formulations.\nThere are two ways that the indexical contextualist might try to explain this agreement. First, they could try to argue that it is acceptable for Rebus to agree with Clarke’s utterance despite not agreeing with the propositional content of it. Second, they could try to argue that it is the case, in any case fitting the above description, that he agrees with the propositional content of what Clarke said.\nThe first approach does not seem particularly attractive. Not only does it seem theoretically implausible, it is hard to find independent reason to believe that this is how agreement works. Generally if a speaker utters some term with a contextually sensitive term in it, then another speaker will not agree with the utterance unless they agree with the proposition we get by filling in the appropriate value for the contextually sensitive term. Or, perhaps more precisely, they will not accept all five of above forms of agreement.\nThis is how agreement works when the utterance contains an explicit indexical like ‘I.’ Note that if Clarke had said “I like Hibs,” Rebus could say “I agree” if he too likes Hibs. But he couldn’t have said, for instance, “What she said is true” unless Clarke liked Hibs. This is why the variety of forms of agreement matters. Perhaps more contentiously, I think this is also what happens when the original utterance involves quantifiers with tacit domain restriction, or comparative adjectives with tacit comparison classes, or modals, or any other kind of context sensitive language. So the indexical contextualist should look at a second option.\nThe second option is to say that the proposition Rebus knows, \\(\\neg\\)(p & \\(\\neg\\)q), will be one of the R(c). There are two ways to do this. First, we could say that R(c) includes all propositions that are known by anyone, so as long as Rebus knows \\(\\neg\\)(p & \\(\\neg\\)q), it is one of the R(c). But this just about reduces back to the material implication theory of indicatives, since any conditional will be true as long as anyone knows the corresponding material implication. And that is implausible. Second, we could say that R(c) includes any proposition known by anyone who hears c’s utterance. That would again ensure that \\(\\neg\\)(p & \\(\\neg\\)q) is one of the R(c). But again, it is fairly implausible. For one thing, it doesn’t seem that the truth of the conditionals I’m writing in this paper depend on how wide a readership the paper has. For another, under some assumptions this again collapses into the material implication theory. Assume that there is an omniscient deity. Then for any conditional If p, q, the deity’s knowledge is among R(c), and if \\(\\neg\\)(p & \\(\\neg\\)q) is true, then it is one of the R(c). But then If p, q will be true, which was not what we wanted. Now we don’t know that there is an omniscient deity, but it seems reasonable to require that our semantic theories be at least consistent with the existence of such a deity.\nSo I think the indexical contextualist has no explanation of this agreement phenomena. But the indexical relativist has no such problem. When Clarke’s utterance is being judged by Rebus, it expresses the proposition C(p, q, R(Rebus)), so \\(\\neg\\)(p & \\(\\neg\\)q) is clearly one of the propositions in the third clause. That’s why agreement with another’s utterance of a conditional is so easy.\nSimilarly there is no problem for a non-indexical relativist. It is a little trickier to know whether there is a problem here for the non-indexical contextualist. It is easy to see why many of the locutions Rebus could use are acceptable. Rebus does, after all, accept the proposition that Clarke expresses, namely C(p, q). The only complication concerns “She spoke truly.” There is a sense in which that’s not really true. After all, the non-indexical contextualist thinks that Clarke’s utterance was false, since they think that an utterance is true iff the proposition it expresses is true in its context. If we think, as probably isn’t compulsory, that “She spoke truly” means what the theorist means by saying the utterance is true, then there is a problem for the non-indexical contextualist.\nSetting aside those complications, what is clear is that the phenomena of agreement raises a problem for the indexical contextualist, and not for the relativist. We can put the problem another way. If indexical contextualism is true, it should be possible for Rebus to say “Clarke’s utterance”If the doctor didn’t do it, the lawyer did\" was not true, but if the doctor didn’t do it, the lawyer did.\" Again, this seems like it should be possible on theoretical grounds, and it is possible for most contextually sensitive sentences. But this doesn’t seem to be a coherent speech on Rebus’s part. This is of course just another manifestation of the phenomena that agreement with conditionals is easy. If Rebus accepts that if the doctor didn’t do it, the lawyer did, then he accepts that Clarke’s utterance was true. The indexical contextualist can’t explain this, so indexical contextualism is false.\nAgainst Non-Indexicalism\nThe argument for an indexicalist account of indicatives is that they allow an elegant account of what is going on in apparent counterexamples to modus ponens, such as the cases due to Vann McGee (1985). What these cases turn on is that right-embedded conditionals, like If p, then if q, r seem equivalent, in some sense, to conditional with conjunctive antecedents, in this case If p and q, r. Given this equivalence, and the triviality of If p and q, p and q, we get the result that (7.1) is trivial.\nIf p, then if q, p and q\nAnd if (7.1) is genuinely trivial, then a number of awkward consequences follow. Perhaps the worst of these consequences is that we seem to get counterexamples to modus ponens. Let p be some truth that isn’t knowable. Since (7.1) is trivial, it is true. And by hypothesis p is true. But on pretty much any epistemic theory of conditionals, If q, p and q will not be true. So we have counterexamples to modus ponens. (This is more like McGee’s ‘lungfish’ example than the more widely cited example about the 1980 election, but the structure I think is basically the same, and the solution I offer will generalise to all these cases.)\nWhat I’m going to say about these cases borrows heavily from some remarks by Anthony Gillies (2009). Gillies makes two observations that point towards a solution to the puzzle McGee’s cases raise.\nFirst, we cannot in general assert both of the premises, namely (7.1) and p, in contexts where the conclusion, namely If q, p and q is not assertable. This might need to be qualified in cases where people don’t know what they can assert, but it is largely right. As Gillies demonstrates by close attention to the cases, some kind of context shift between the premises and conclusion is needed in order to assert the conclusion after the premises have been asserted.\nSecond, there are many reasons to believe that part of what why (7.1) seems trivial is that we evaluate its consequent relative to a context in which p is taken to be part of the evidence. Gillies formalises this by having the antecedent play two separate roles, first as a constituent of the conditional uttered, and second as a context-modifier relative to which the consequent is interpreted. The formal theory I’m building here is quite different to Gillies’ because of very different starting assumptions, but I will adopt Gillies’ idea to the framework I’m using.\nDespite Gillies’ first observation, there are still three reasons to take seriously the challenge McGee’s cases raise. Two of these involve using modus ponens under the scope of a supposition, and the third involves agents who don’t know what they know. The first problem concerns the following implication.\n\\(1\\)\nIf p, if q, p and q\nPremise\n\\(2\\)\nIf \\(\\neg\\)p, if q, \\(\\neg\\)p and q\nPremise\n\\(3\\)\np or \\(\\neg\\)p\nLogical truth\n\\(4\\)\np\nAssumption for argument by cases\n\\(5\\)\nIf q, p and q\nModus Ponens, 1, 4\n\\(6\\)\n(If q, p and q) or (If q, \\(\\neg\\)p and q)\nOr introduction, 5\n\\(7\\)\n\\(\\neg\\)p\nAssumption for argument by cases\n\\(8\\)\nIf q, \\(\\neg\\)p and q\nModus Ponens, 2, 7\n\\(9\\)\n(If q, p and q) or (If q, \\(\\neg\\)p and q)\nOr introduction, 8\n\\(10\\)\n(If q, p and q) or (If q, \\(\\neg\\)p and q)\nArgument by cases, 3, 4-6, 7-9\nBut on the simple epistemic theory we’ve been using here, (10) will not be true in cases where the truth value of p is unknown, even though it seems to follow from two trivialities and a logical truth. (I’m assuming here either that classical logic is correct, or that p is decidable.) Now it might be noted here that on some theories, particularly those that follow Stalnaker (1981) in accepting conditional excluded middle, (10) will be true. But even on those Stalnakerian theories, there will be cases where neither disjunct of (10) will be determinately true. And we can rerun a version of this argument, taking as premises that (1), (2) and (3) are determinately true, to derive as a conclusion that one or other disjunct is determinately true.\nThe third reason is similar to the second. We can use modus ponens in the scope of a reductio proof. Or, more colloquially, we can use modus tollens. But the following argument does not look to be particularly compelling.\n\\(1\\)\nIf p, if q, p and q\nPremise\n\\(2\\)\nIt is not the case that if q, p and q\nPremise\n\\(3\\)\nNot p\nModus Tollens, 1, 2\nIt may be objected that modus tollens is more controversial than modus ponens. But since we can derive it using just modus ponens and reductio ad absurdum, this objection looks weak. So this would be a bad result.\nIt might be thought best to say here that modus ponens doesn’t preserve truth, but it does preserve knowledge. If a subject is in knows each premise, they can know the conclusion. But that doesn’t seem right either, though the cases are slightly obscure. Assume a perfectly rational S knows that p, but does not know that she knows that p, and in fact for all she knows she knows, q and \\(\\neg\\)p is true. Again assuming (7.1) is trivial, she knows it, and she knows that p, but on an epistemic interpretation of the conditional, she won’t know If q, p and q, since she doesn’t know she knows that p.\nSo there is a serious problem here. Once we accept that (7.1) is trivial, a lot of unfortunate consequences follow for the epistemic theory of conditionals. Any explanation of why (7.1) seems trivial will, I think, have to start with Gillies’ insight that when we interpret (7.1), we evaluate its consequent relative to a context where p is taken as given. How might we do this? Three options spring to mind.\nThe first option is Gillies’ theory is that it is part of the meaning of the conditional that its consequent be interpreted relative to a context where its antecedent is part of the background information. That has the nice result that (7.1) is indeed trivial. It seems, however, to lead to all the problems mentioned above. Gillies’ response to these is to develop a new theory of validity, which has the effect that while modus ponens is itself valid, it can’t be used inside the scope of suppositions, as I frequently did above. This is a very interesting theory, and it may well work out, but I’m going to try to develop a more conservative approach.\nThe second option is to say that just uttering a conditional, If A, B, adds A to the background information. This seems like a bad option. For one thing, there is no independent reason to believe that this is true. For another, it can’t explain what is wrong with the following kind of argument.\nBurns knows that if p, then if q, p and q\nBurns knows that it is not the case that if q, p and q\nBurns is logically perfect, and knows the logical consequences of everything he knows\nSo, Burns knows not p\nIn a case where Burns doesn’t know whether p is true, and Burns is indeed logically perfect, then intuitively (1), (2) and (3) are true, but (4) are false. And since no conditionals were asserted, it is hard to see how the context was shifted.\nThe third, and best, option is to say that the variable in the semantics of an embedded conditional is partially bound to the antecedent. Normally when we say If q, p and q, the content of that is C(q, p and q, X), and normally X is R(PROJ). The view under consideration says that when that conditional is itself the consequent of a conditional, the variable X is partially bound to the antecedent of the conditional. So the value of X is p plus whatever is supplied by context.\nThe contextualist says that that value is R(i), where i is usually the speaker. So the semantic content of (7.1) is C(p, C(q, p and q, p + R(i)), R(i)). And that will be trivial since the middle term is trivial. The relativist says that the contribution of context to X is R(PROJ). So the semantic content of (7.1) is C(p, C(q, p and q, p + R(PROJ)), R(PROJ)). And again, that is trivial.\nThis gives us a natural explanation of what is going on in the McGee cases. There is simply an equivocation between premise and conclusion in all of the cases. What follows from C(p, C(q, p and q, p + R(x)), R(x)) and p is C(q, p and q, p + R(x)). But that’s not what we normally express by If q, p and q. At least, it isn’t what we express once we’ve made it clear that p is not part of the background information. (Here is where Gillies’ observation that the McGee cases seem to require a context shift between stating the premises and stating the conclusion becomes relevant.) So although modus ponens is valid, the McGee cases are simply not instances of modus ponens, since there is an equivocation in the value of a tacit variable.\nIt might be argued that this is too much of a concession to McGee. Some people have the judgment that (7.1) is not always trivial, in particular that conditionals If A, then if B, A are not always trivially true. Personally I don’t get these readings, but I note that the theory allows for their possibility. After all, binding need not be compulsory. We can interpret the ‘she’ in If Suzy enters the race, she will win deictically, if that’s what makes the best sense in the context. Perhaps in cases where people are hearing the false readings of If A, then if B, A, all that is going on is that the tacit indexical in the embedded conditional is unbound. Similarly, if one’s reaction to seeing the McGee arguments is to interpret the embedded conditionals as false, I suspect what is going on is that one is hearing the variables here as unbound. As I said, I don’t get these readings, but I can explain where these readings come from.\nThe story I’m telling about the McGee cases is hardly new. Indeed, the view that the McGee cases are not strictly speaking instances of Modus Ponens is old enough to have been disparaged by William Lycan in his attacks on Modus Ponens.\n\nBut this very strict sense of ‘instance’ is neither specific nor intended in logic textbooks ... What students and professional philosophers have always been told is that barring equivocation or overt indexicals, arguments of the sentential form If A, B; A; therefore, B are valid arguments, period ... One can continue to insist that Modus Ponens is valid for the strict sense of ‘instance,’ but at the price of keeping us from telling easily and uncontroversially when a set of ordinary English sentences is an ‘instance’ of an argument form. (Lycan 1993, 424, notation slightly altered)\n\nBut why should we give any privilege to overt indexicals? Tacit variables can be just as important in determining which form an argument takes. For example, the following argument is, on the most natural interpretation of each sentence, invalid.\nNo foreigner speaks a foreign language.\nSégolinè is a foreigner.\nFrench is a foreign language.\nSégolinè does not speak French.\nThat is invalid on its most natural reading because the tacit variable attached to ‘foreign’ in premises 1 and 3 takes a different value. No one would reasonably say that we should rewrite the logic books so the argument form No F Rs a G; Fa; Gb; so \\(\\neg\\)Rab is not valid on this account. Lycan is right about the downside of this point. There is no way to tell easily and uncontroversially what the form of an argument in natural language is. But we should never have believed such careful matters of interpretation would be easy. (They say life wasn’t meant to be.)\nHaving said that, on the indexical relativist proposal offered here, it isn’t that hard to tell what the value of X in a typical indicative is. It is usually R(PROJ), and there might be a very short list of circumstances where it takes any other value. Any indexical account faces a potential cost that it makes interpretation more difficult than it might otherwise be, since the hearer has to determine the value for the indexical. The fact that X is usually R(PROJ) minimises that cost. What is new to my proposal is that X might be partially bound in the McGee cases. But that only helps the interpretative task, since it reduces the task to a familiar problem interpreters face when the speaker uses a partially bound plural pronoun.\nBut the primary point of this proposal is not to offer a new solution to the McGee cases. Rather it is to note one of the requirements of this kind of (relatively familiar) solution. An equivocation solution requires that there be something in the semantic content of the conditional that takes different values in the consequent of premise 1 and in the conclusion. And non-indexical theories, by definition, can’t say that there is any such thing. For the whole point of such theories is to deny that the content of a conditional is always different in contexts with different information sets. So they cannot say the McGee arguments (or the other arguments I surveyed above that use Modus Ponens in embedded contexts) involve equivocation. But then it is hard to say what is wrong with those arguments. So these theories seem, implausibly, to be committed to denials of Modus Ponens. That’s a sufficient reason, I think, to be an indexicalist.\nLet’s take stock. In section 6 I argued that the indexical contextualist has no explanation of why it is so easy to agree with another’s utterance of a conditional. In this section I argued that only the indexicalist can offer a satisfactory explanation of what is going on in the McGee argument. The upshot of these two arguments is that we should be indexical relativists. For only the indexical relativist can (a) explain the agreement data and (b) explain what goes wrong in the McGee arguments.\nAs a small coda, let me mention one other benefit of the partial binding account. When I presented an earlier version of this paper at the LOGOS workshop on Relativising Utterance Truth, the following objection was pressed to the argument in section 1 for an epistemic treatment of indicatives. It is true that when we know that f(a) = f(b), then we are prepared to assert If f(a) = x, then f(b) = x. But it is also true that when we merely suppose that f(a) = f(b), then we are prepared to infer inside the scope of the supposition that If f(a) = x, then f(b) = x. The epistemic account cannot satisfactorily explain this. At the time I didn’t know how to adequately explain these intuitions, but now it seems the partial binding story can do the work. It seems that inside the scope of a supposition that p, the value of X is p + Y, where Y is the value X would otherwise have had. That is, the variable in the conditional is partially bound to the supposition that governs the discourse. That explains why all the inferences mentioned in section one are acceptable, even when the premise is merely a supposition.\nCoda: Methodological Ruminations\nThe version of relativism defended here is conservative in a number of respects. Three stand out.\nFirst, it is conservative about what propositions are. The propositions that are the content of open indicatives (relative to contexts of assessment) are true or false relative to worlds, not to judges, or epistemic states, or anything of the sort.\nSecond, it is (somewhat) conservative about how the sentences get to have those propositions as content. The standing meaning of the sentence contains a variable place that gets filled by context. To be sure, it is a plural variable that can be partially bound, but there is independent evidence that plural variables can be partially bound. And of course, and this is a radical step, its value can be different for different assessors of the one utterance. But from the indexical relativist perspective, the contextualist theory that values for variables are set by contexts of utterance is an overly hasty generalisation from the behaviour of a few simple indexicals. (It isn’t clear even clear that the contextualist theory can account for simple pronouns, like ‘you’ or ‘now’ as they appear in sentences like the one you are now reading, so this generalisation might have been very poorly motivated in the first place.)\nThird, it is conservative about the motivation for relativism. I haven’t relied on intuitions about faultless disagreement, which is an inherently controversial topic. Rather, I’ve argued that we can motivate relativism well enough by just looking at the grounds on which people agree with earlier utterances. I think there is a general methodological point here; most of the time when theorists try to motivate relativism using cases of disagreement, they could derive most of their conclusions from careful studies of cases of agreement. This method won’t always work; I don’t think you can replicate the disagreement-based arguments for moral relativism with arguments from agreement for example. But I think that is a weakness with moral relativism, rather than a weakness with the methodology of focussing on agreement rather than disagreement with arguing for relativism.\nNow one shouldn’t fetishise epistemic conservativeness. But a relativism that requires less of a revision of our worldview should be more plausible to a wider range of people than a more radical relativist view. And that’s what I’ve provided with the indexical relativist theory defended here.\n\n\nAdams, Ernest. 1998. A Primer on Probability Logic. Palo Alto: CSLI.\n\n\nBradley, Richard. 2000. “A Preservation Condition for Conditionals.” Analysis 60 (3): 219–22. https://doi.org/10.1093/analys/60.3.219.\n\n\nCappelen, Herman. 2008. “Content Relativism.” In Relativising Utterance Truth, edited by Manuel Garcia-Carpintero and Max Kölbel, 265–86. Oxford: Oxford University Press.\n\n\nCappelen, Herman, and Ernest Lepore. 2005. Insensitive Semantics: A Defence of Semantic Minimalism and Speech Act Pluralism. Oxford: Blackwell.\n\n\nEgan, Andy, John Hawthorne, and Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nEinheuser, Iris. 2008. “Three Forms of Truth-Relativism.” In Relativising Utterance Truth, edited by Manuel Garcia-Carpintero and Max Kölbel, 187–203. Oxford: Oxford University Press.\n\n\nGillies, Anthony S. 2009. “On Truth-Conditions for If (but Not Quite Only If ).” Philosophical Review 118 (3): 325–49. https://doi.org/10.1215/00318108-2009-002.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nKaplan, David. 1989. “Demonstratives.” In Themes from Kaplan, edited by Joseph Almog, John Perry, and Howard Wettstein, 481–563. Oxford: Oxford University Press.\n\n\nKing, Jeff, and Jason Stanley. 2005. “Semantics, Pragmatics and the Role of Semantic Content.” In Semantics Vs Pragmatics, edited by Zoltan Szabó, 111–64. Oxford: Oxford University Press.\n\n\nKölbel, Max. 2004. “Indexical Relativism Vs Genuine Relativism.” International Journal of Philosophical Studies 12 (3): 297–313. https://doi.org/Indexical Relativism vs Genuine Relativism.\n\n\nLasersohn, Peter. 2005. “Context Dependence, Disagreement and Predicates of Personal Taste.” Linguistics and Philosophy 28 (6): 643–86. https://doi.org/10.1007/s10988-005-0596-x.\n\n\nLewis, David. 1973. Counterfactuals. Oxford: Blackwell Publishers.\n\n\nLópez de Sa, Dan. 2007a. “(Indexical) Relativism about Values: A Presuppositional Defense.”\n\n\n———. 2007b. “The Many Relativisms and the Question of Disagreement.” International Journal of Philosophical Studies 15 (2): 339–48. https://doi.org/10.1080/09672550701383871.\n\n\n———. 2008. “Presuppositions of Commonality.” In Relativising Utterance Truth, edited by Manuel Garcia-Carpintero and Max Kölbel, 297–310. Oxford University Press.\n\n\nLycan, William. 1993. “MPP, RIP.” Philosophical Perspectives 7: 411–28. https://doi.org/10.2307/2214132.\n\n\nMacfarlane, John. 2007. “Semantic Minimalism and Nonindexical Contextualism.” In Context-Sensitivity and Semantic Minimalism: New Essays on Semantics and Pragmatics, edited by Gerhard Preyer and Georg Peter, 240–50. Oxford University Press.\n\n\nMacFarlane, John. 2003. “Future Contingents and Relative Truth.” The Philosophical Quarterly 53 (212): 321–36. https://doi.org/10.1111/1467-9213.00315.\n\n\n———. 2005. “Making Sense of Relative Truth.” Proceedings of the Aristotelian Society 105 (1): 321–39. https://doi.org/10.1111/j.0066-7373.2004.00116.x.\n\n\n———. 2009. “Nonindexical Contextualism.” Synthese 166 (2): 231–50. https://doi.org/10.1007/s11229-007-9286-2.\n\n\nMcGee, Vann. 1985. “A Counterexample to Modus Ponens.” Journal of Philosophy 82 (9): 462–71. https://doi.org/10.2307/2026276.\n\n\nMcKay, Thomas. 2006. Plural Predication. Oxford: Oxford University Press.\n\n\nNolan, Daniel. 2003. “Defending a Possible-Worlds Account of Indicative Conditionals.” Philosophical Studies 116 (3): 215–69. https://doi.org/10.1023/B:PHIL.0000007243.60727.d4.\n\n\nPartee, Barbara. 1989. “Binding Implicit Variables in Quantified Contexts.” In Papers from the Twenty-Fifth Regional Meeting of the Chicago Linguistic Society, edited by Caroline Wiltshire, Randolph Graczyk, and Bradley Music. Chicago: Chicago Linguistic Society.\n\n\nSayre-McCord, Geoffrey. 1991. “Being a Realist about Relativism (in Ethics).” Philosophical Studies 61 (1-2): 155–76. https://doi.org/10.1007/bf00385839.\n\n\nSchlenker, Philippe. 2003. “Indexicality, Logophoricity, and Plural Pronouns.” In Afroasiatic Grammar II: Selected Papers from the Fifth Conference on Afroasiatic Languages, Paris, 2000, edited by Jacqueline Lecarme, 409–28. Amsterdam: John Benjamins.\n\n\nStalnaker, Robert. 1975. “Indicative Conditionals.” Philosophia 5 (3): 269–86. https://doi.org/10.1007/bf02379021.\n\n\n———. 1981. “A Defence of Conditional Excluded Middle.” In Ifs, edited by William Harper, Robert C. Stalnaker, and Glenn Pearce, 87–104. Dordrecht: Reidel.\n\n\nStanley, Jason. 2007. Language in Context: Selected Essays. Oxford University Press.\n\n\nStephenson, Tamina. 2007. “Judge Dependence, Epistemic Modals, and Predicates of Personal Taste.” Linguistics and Philosophy 30 (4): 487–525. https://doi.org/10.1007/s10988-008-9023-4.\n\n\nWeatherson, Brian. 2001. “Indicative and Subjunctive Conditionals.” The Philosophical Quarterly 51 (203): 200–216. https://doi.org/10.1111/j.0031-8094.2001.00224.x.\n\n\nIt is actually a little tricky to say just what the relevant types are. I mean to use the term so that two people make an utterance of the same type if their utterances use the same words, with the same syntax, and the same elided material, and the same meaning. So two utterances of “I am Australian” could be of the same type, even if offered by different people. In some cases, e.g. “Steel is strong enough,” it might be controversial whether two utterances that intuitively have different contents are either (a) of the same type, or (b) use terms with different meanings, or (c) have different elided material. I’ll try to stay neutral on this point.↩︎\nI don’t mean to suggest that these are the only options. I’m leaving off options on which there are contextual effects on semantic content that are not syntactically triggered, for example. My reason for doing that is that there are, I think, good reasons for thinking that the context-sensitivity of indicative conditionals is syntactically triggered, so I don’t need to investigate non-syntactic triggers here.↩︎\nI’m assuming throughout that it is sufficient for the truth of C(p, q, X) that it is a priori that p plus X entail q. I also think that’s necessary, but I won’t lean on this assumption.↩︎\nThe move I’m about to make bears at least a family resemblance to some moves López de Sa (2008) makes in defending what he calls ‘indexical relativism,’ though he means something different by that phrase.↩︎\nI thought this was a way in which plural pronouns were unlike singular pronouns, but Zoltán Szabó suggested persuasively that singular pronouns could also be partially bound in the sense described below. The interesting cases concern pronouns that seem to refer to objects made from multiple parts, with circumstances of utterance determining some parts of the referent, and the other parts being the denotata of the terms to which the pronoun is partially bound. I’m not going to take a stand on whether such pronouns exist, but if Szabó’s suggestion is correct, then the need to take X to be a plural variable is lessened.↩︎\nThis response is similar to some of the arguments for speech act pluralism in Cappelen and Lepore (2005).\n\n↩︎\n",
    "preview": "posts/2021-03-12-conditionals-and-indexical-relativism/leith.jpg",
    "last_modified": "2021-03-12T15:10:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-deontology-and-descartess-demon/",
    "title": "Deontology and Descartes's Demon",
    "description": "In this paper, I defend a broadly Cartesian position about doxastic freedom. At least some of our beliefs are freely formed, so we are responsible for them. Moreover, this has consequences for epistemology. But the some here is crucial. Some of our beliefs are not freely formed, and we are not responsible for those. And that has epistemological consequences too. Out of these considerations a concept of doxastic responsibility arises that is useful to the externalist in responding to several challenges. I will say at some length how it supports a familiar style of externalism response to the New Evil Demon problem, and I will note some difficulties in reconciling internalism with the idea that justification is a kind of blamelessness. The internalist, I will argue, has to say that justification is a kind of praiseworthiness, and this idea that praise is more relevant to epistemic concepts than blame will be a recurring theme of the paper.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2008-09-01",
    "categories": [
      "epistemology",
      "epistemic norms",
      "epistemic voluntarism"
    ],
    "contents": "\n\nContents\nDigesting Evidence\nResponding to the Involuntarists\nHow to Control Your Temper\nVoluntariness about Belief\nRyan and Steup\nInvoluntarism about Perceptual Beliefs\nEpistemological Consequences\nThe New Evil Demon Problem\nPraise and Blame\n\nDigesting Evidence\nIn his Principles of Philosophy, Descartes says,\n\nFinally, it is so manifest that we possess a free will, capable of giving or withholding its assent, that this truth must be reckoned among the first and most common notions which are born with us. (Descartes 1644/2003, para. xxxix)\n\nIn this paper, I am going to defend a broadly Cartesian position about doxastic freedom. At least some of our beliefs are freely formed, so we are responsible for them. Moreover, this has consequences for epistemology. But the some here is crucial. Some of our beliefs are not freely formed, and we are not responsible for those. And that has epistemological consequences too. Out of these considerations a concept of doxastic responsibility arises that is useful to the externalist in responding to several challenges. I will say at some length how it supports a familiar style of externalism response to the New Evil Demon problem, and I will note some difficulties in reconciling internalism with the idea that justification is a kind of blamelessness. The internalist, I will argue, has to say that justification is a kind of praiseworthiness, and this idea that praise is more relevant to epistemic concepts than blame will be a recurring theme of the paper.\nWhile the kind of position I am adopting has been gaining supporters in recent years, it is still largely unpopular. The arguments of William Alston (1988) have convinced many that it is a mistake to talk of doxastic freedom, or doxastic responsibility. The short version of this argument is that our beliefs are involuntary, and freedom and responsibility require voluntariness. The longer, and more careful, argument involves drawing some distinctions between ways in which we might come to be in a state. It helps to start with an example where the normative facts are relatively uncontroversial, namely digestion.\nImagine that Emma eats a meat pie, and due to a malfunction in her stomach the pie is not properly digested, leading to some medical complications. Is Emma responsible for her ill-health? Well, that depends on the back-story. If Emma knew that she could not properly digest meat pies, but ate one anyway, she is responsible for the illness via her responsibility for eating the pie. Even if Emma did not know this, she might be responsible for the state of her stomach. If her stomach could not digest the pie because it had been damaged by Emma’s dietary habits, and say Emma knew that her diet could damage her stomach, then Emma is responsible for the state of her stomach and hence for the misdigestion of the pie and hence for her ill-health. But if neither of these conditions obtain, if it just happens that her stomach misdigests the pie, then Emma is not responsible for her ill-health. Even though the cause of her ill-health is something that her stomach does, he is not responsible for that since her stomach is not under her voluntary control. Put another way, her responsibility for maintaining her own health means that she is responsible for the type of digester she is, but he is not responsible for this token digestion.\nSimplifying a little, Alston thinks that the case of belief is similar. Say that Emma has a false belief that p. Is she responsible for this piece of doxastic ill-health? Again, that depends on the back story. If Emma believes that p because she was careless in gathering evidence, and the evidence would have pointed to ~p, then she is responsible for being a bad gatherer of evidence. If Emma has been negligent in maintaining her doxastic health, or worse if she has been doing things she knows endangers doxastic health, then she is responsible for being the type of believer she is. But she is never responsible merely for the token belief that is formed. Her mind simply digests the evidence she has, and Emma’s responsibility only extends to her duty to gather evidence for it, and her duty to keep her mind in good working order. She is not responsible for particular acts of evidential digestion.\nBut these particular acts of evidential digestion are the primary subject matters of epistemology. When we say Emma’s belief is justified or unjustified, we frequently mean that it is a good or bad response to the evidence in the circumstances. (I am obviously here glossing over enormous disputes about what makes for a good response, what is evidence, and what relevance the circumstances have. But most theories of justification can be fit into this broad schema, provided we are liberal enough in interpreting the terms ‘good,’ ‘evidence’ and ‘circumstances.’) If Emma is not responsible for her response to the evidence, then either we have to divorce justification from responsibility, or we have to say that the concept of justification being used in these discussions is defective.\nWe can summarise these considerations as a short argument. The following formulation is from Sharon (Ryan 2003, 49).\nIf we have any epistemic obligations, then doxastic attitudes must sometimes be under our voluntary control.\nDoxastic attitudes are never under our voluntarily control.\nWe do not have any epistemic obligations.\nRyan goes on to reject both premises. (And she does so while interpreting “voluntary control” to mean “direct voluntary control”; the response is not meant to sidestep Alston’s argument.) Matthias Steup (2000, 2008) also rejects both premises of this argument. I am more sympathetic to premise 1, but I (tentatively) agree with them, against what sometimes seems to be orthodoxy, that premise 2 fails. That is, I endorse a kind of doxastic voluntarism. (Just what kind will become clearer as we go along.) There are four questions that anyone who endorses voluntarism, and wants to argue that this matters epistemologically, should I think answer. These are:\nWhat is wrong with current arguments against voluntarism?\nWhat does the voluntariness of (some) beliefs consist in?\nWhich kinds of beliefs are voluntary?\nWhat difference does the distinction between these classes make for epistemology?\nMy answer to (A) will be similar to Ryan’s, and to Steup’s, but with I think enough differences in emphasis to be worth working through. My answer to (B), however, will be a little more different. I am going to draw on some work on self-control to argue that some beliefs are voluntary because they are the result of exercises of, or failures to exercise, self-control. My answer to (C) is that what I will call inferential beliefs are voluntary, while perceptual beliefs are not. Ryan and Steup sometimes seem to suggest that even perceptual beliefs are voluntary, and I do not think this is true. The consequence for this, I will argue in answering (D), is that inferential beliefs should be judged by how well they respond to the evidence, while perceptual beliefs should be judged by how well they reflect reality. When an agent has misleading evidence, their inferential beliefs might be fully justified, but their perceptual beliefs, being misleading, are not.\nI will detail my answers to those four questions in sections 2, 4, 6 and 7. In between I will discuss recent work on self-control (section 3) and the contrast between my answer to (B) and other voluntarist answers (section 5). In section 8 I will say how my partially voluntarist position gives the externalist a way to avoid the New Evil Demon problem. And in section 9 I will make a direct argument for the idea that justification is a kind of praiseworthiness, not a kind of blamelessness.\nBefore we start, I want to note two ways, other than Ryan’s, of formulating an argument against doxastic responsibility. These are going to seem quite similar to Ryan’s formulation, but I think they hide important differences. The first version uses the idea that some doings (or states) are volitional. That is, we do them (or are in them) because we formed a volition to do so, and this volition causes the doing (or state) in the right kind of way.\nIf we have any epistemic obligations, then either the formation or maintenance of doxastic attitudes must sometimes be volitional.\nThe formation or maintenance of doxastic attitudes is never volitional.\nWe do not have any epistemic obligations.\nI will not argue against premise 2 of this argument, though Carl Ginet (1985, 2001) (1985, 2001) has done so. But I think there’s little to be said for premise 1. The principle behind it is that we are only responsible for volitional doings. And that principle is very dubious. We could run the kind of regress arguments against it that Gilbert Ryle (1949) offers. But it is simpler to note some everyday counterexamples. Borrowing an example from Angela M A. M. Smith (2005), if I forget a friend’s birthday, that is something I am responsible and blameworthy for, but forgetting a birthday is not volitional. (Below I will offer a Rylean argument that we are sometimes praiseworthy for doings that are not volitional.) So this argument fails. Alternatively, we could run the argument by appeal to freedom.\nIf we have any epistemic obligations, then doxastic attitudes must sometimes be free.\nDoxastic attitudes are never free.\nWe do not have any epistemic obligations.\nPremise 1 of this argument is more plausible. But, as we’ll see presently, premise 2 is not very plausible. Whether Descartes was right that premise 2 is obviously false, it does seem on reflection very hard to defend. So this argument fails. Ryan’s formulation is interesting because it is not clear just which of the premises fails. As I said, I am going to suggest that premise 2 fails, and that doxastic attitudes are voluntary. But this will turn on some fine judgments about the voluntary/involuntary boundary. If I am wrong about those judgments, then the arguments below will suggest that premise 1, not premise 2, in Ryan’s formulation fails. Either way though, the argument is unsuccessful.\nResponding to the Involuntarists\nThere are two kinds of argument against the idea that belief is voluntary. One kind, tracing back to Bernard Williams (1976), holds that the possibility of voluntary belief can be shown to be incoherent by reflection on the concept of belief. This argument is no longer widely endorsed. Nishi Shah (2002) provides an excellent discussion of the problems with Williams’ argument, and I have nothing to add to his work. I will focus on the other kind, that claims we can see that belief is involuntary by observing differences between beliefs and paradigm cases of voluntary actions. I will make three objections to these arguments. First, the argument looks much less plausible once we distinguish between having a belief and forming a belief. Second, the argument seems to rely on inferring from the fact that we do not do something (in particular, believe something that we have excellent evidence is false) to the conclusion that we can not do it. As Sharon Ryan (2003) points out, this little argument overlooks the possibility that we will not do it. Third, the argument relies on too narrow a conception of what is voluntary, and when we get a more accurate grasp on that concept, we’ll give up the argument. Here is a representative version of the argument from William Alston.\n\nCan you, at this moment, start to believe that the United States is still a colony of Great Britain, just by deciding to do so? … [S]uppose that someone offers you $500,000,000 to believe it, and you are much more interested in the money than in believing the truth. Could you do what it takes to get that reward? . . . Can you switch propositional attitudes toward that proposition just by deciding to do so? It seems clear to me that I have no such power. Volitions, decisions, or choosings don’t hook up with anything in the way of propositional attitude inauguration, just as they don’t hook up with the secretion of gastric juices or cell metabolism. (Alston 1988, 122)\n\nNow Alston does note, just one page earlier, that what is really relevant is whether our being in a state of belief is voluntary, not whether the activity of belief formation is voluntary. But he thinks nevertheless that issues about whether we can form beliefs, any old beliefs it seems, voluntarily matters to the question about the voluntariness of belief states.\nIf we think about what it is to be in a state voluntarily, this all seems beside the point. We can see this by considering what it is to be in a political state voluntarily. Consider Shane, who was born into Victoria. His coming to be in Victoria was hence not, in any way, voluntary. Shane is now a grown man, and he has heard many travellers’ tales of far away lands. But the apparent attractions of Sydney and other places have no pull on Shane; he has decided to stay in Victoria. If he has the capacity to leave Victoria, then Shane’s continued presence in Victoria is voluntary. Similarly, we are voluntarily in a belief state if we have the capacity to leave it, but choose not to exercise this capacity. Whether the belief was formed voluntarily is beside the point.\nIf Shane leaves a state, the natural place to leave is for another state, perhaps New South Wales or South Australia. It might be thought that if we leave a belief state, we have to move into another belief state. So to have this capacity to leave, we need the ability to form beliefs voluntarily. Not at all. The capacity to become uncertain, i.e. to not be in any relevant belief state, is capacity enough. (If Shane has a boat, and the capacity to flourish at sea, then perhaps he too can have the capacity to leave Victoria without the capacity to go into another state.)\nBut do we have the capacity to become uncertain? Descartes appeared to think so; arguably the point of the First Meditation is to show us how to exercise this capacity. Moreover, this capacity need not be one that we exercise in any particularly nearby possible worlds. We might exercise our freedom by always doing the right thing. As Descartes goes on to say in the Fourth Meditation.\n\nFor in order to be free, there is no need for me to be capable of going in each of two directions; on the contrary, the more I incline in one direction – either because I clearly understand that reasons of truth and goodness point that way, or because of a divinely produced disposition of my inmost thoughts – the freer is my choice. (Descartes 1641/1996, 40)\n\nThis seems like an important truth. Someone who is so sure of their own interests and values, and so strong-willed as to always aim to promote them, cannot in a certain sense act against their own self-interest and values. But this does not make their actions in defence of those interests and values unfree. If it did, we might well wonder what the value of freedom was. And note that even if there’s a sense that our character could not have done otherwise, this in no way suggests their actions are outside their control. Indeed, a person who systematically promotes the interests and values they have seems an exemplar of an agent in control. The character I am imagining here is in important respects unlike normal humans. We know we can, and do, act against our interests and values. But we can become more or less like them, and it is important to remember, as Descartes does, that in doing so we do not sacrifice freedom for values or interests.\nJohn Cottingham (2002) interprets Descartes here as suggesting that there is a gap between free action and voluntary action, contrasting his “strongly compatibilist notion of human freedom” (350) with the “doxastic involuntarism” (355) suggested by the following lines of the Third Meditation.\n\nYet when I turn to the things themselves which I think I perceive very clearly, I am so convinced by them that I spontaneously declare: let whoever can do so deceive me, he will never bring it about that I am nothing, so long as I continue to think that I am something … (Descartes 1641/1996, 25)\n\nNow there are two questions here. The first is whether Descartes intended to draw this distinction. That is, whether Descartes thought that the kind of free actions that he discusses in the Fourth Meditations, the free action where we are incapable of going in the other directions, are nevertheless involuntary. I do not have any informed opinions about this question. The second is whether this kind of consideration supports the distinction between the free and the voluntary. And it seems to me that it does not. Just as Descartes says the free person will be moved by reasons in the right way, it seems natural to say that a person who acts voluntarily will be responsive to reasons. Voluntary action does require freedom from certain kinds of coercion, but the world does not coerce us when it gives us reason to believe one thing rather than another. If we have voluntary control over our beliefs, then we should be compelled by the sight of rain to believe it is raining.\nIn her discussion of the puzzle of imaginative resistance, Tamar Szabó Gendler (2000) notes that philosophers have a tendency to read too much into intuitions about certain cases. What we can tell from various thought experiments is that in certain circumstances we will not do a certain thing. But getting from what we will not do to what we can not do is a tricky matter, and it is a bad mistake to infer from will not to can not too quickly. Matthias Steup (2000) points out that if you or I try to stick a knife into our hand, we similarly will not do it. (I assume a somewhat restricted readership here.) But this is no evidence that we cannot do it. And Sharon Ryan (2003) notes that we will not bring ourselves to run over pedestrians for no reason. For most of us, our moral sense prevents acting quite this destructively. Yet our continued avoiding of pedestrians is a series of free, even voluntary, actions. We could run over the pedestrians, but we will not. Since forming false beliefs is a form of self-harm, it is not surprising that it has a similar phenomenology, even if it is genuinely possible.\nIt might be argued that we will engage in small forms of self-harm that we can do when the financial rewards are great enough. So we should be able to form this belief about the United States for a large amount sum of money. But I suspect that the only way to exercise the capacity to believe the United States is still a colony is by first suspending my belief that it is no longer a colony. And the only way I can do that is by generally becoming more sceptical of what I have been told over the years. Once I get into such a sceptical mood, I will be sceptical of claims that I will get half a billion dollars should I have this wild political belief. So I will not form the belief in part because the ‘promisor’ lacks the capacity to sufficiently convince me that I will be richly rewarded for doing so. This looks like a lack of capacity on their part, not my part.\nThe final point to make about this argument, and those like it, is that if we are to conclude that belief formation is never voluntary, then we need to compare it to all kinds of voluntary action. And Alston really only ever compares belief formation to volitional action. If this does not exhaust the range of voluntary action, then belief formation might be properly analogous to some other voluntary action. Indeed, this turns out to be the case. To see so, we need to make a small detour through modern work on self-control.\nHow to Control Your Temper\nTo start, let’s consider three examples of a person failing to keep a commitment they have made about what the good life is. The three ways will be familiar from Gary Watson’s discussion of recklessness, weakness and compulsion Watson (1977), and the discussion of these cases by Jeanette Kennett and Michael Smith Kennett and Smith (1996b, 1996a). My characterisation of the cases will turn out to differ a little from theirs, but the cases are similar. Each of the examples concerns a character Murray, who has decided that he should not swear around his young son Red. He resolves to do this, and has been working on curbing his tendency to swear whenever anything bad happens. But three times over the course of the day he breaks his commitment.1\nThe first time comes when Murray puts his hand down on a hot plate that he did not realise was on. The searing pain undermines his self-control, and he is unable to stop himself from swearing loudly through the pain.\nThe second time comes when Murray drops and breaks a wine glass. Murray does not lose his self-control, but he does not exercise the self-control he has. He temporarily forgets his commitment and so, quite literally, curses his misfortune. On doing so he immediately remembers that Red is around, and the commitment he has made, and regrets what he did.\nThe third time comes on the tram home, when Murray gets into a disagreement with a political opponent. Murray can not find the words to express what he feels about the opponent without breaking his commitment. So he decides, without much reason, that his need to express what he feels outweighs his commitment, and starts describing his opponent using language he would, all things considered, not have used around young Red.\nThe first and third cases are close to textbook cases of compulsion and recklessness. Note in the first case that when Murray reflects back on what happened, he might be irritated that his work on reducing his tendency to swear has not been more successful. But he will not be upset that he did not exercise more self-control on that occasion. He did not have, no normal person would have, the amount of self-control he would have needed to stop swearing then. All that would help is having the disposition to say different things when his self-control is defeated. And that is not a disposition he can acquire on the spot.\nI have described the first case as one where Murray’s self-control is undermined. This is a term taken from recent work by Richard Holton and Stephen Shute (2007), who carefully distinguish between self-control being undermined by a provocation, and it being overwhelmed by a provocation. Undermining occurs when the provocation causes the agent to have less self-control than they usually have; overwhelming occurs when the provocation is too much for the agent’s control. The difference is relevant to them, because they are interested in what it is for an agent to lose control. That seems to be what happens here. After all, the things one would naturally do afterwards (jumping around, screaming, swearing if one’s so disposed) do not seem particularly controlled by any measure.\nSimilarly I have accepted Watson’s description of cases like the third as instances of recklessness, but we should not think this necessarily contrasts with weakness. It might be that in this case Murray is both weak and reckless. He is not akratic, if we stipulatively define akrasia as acting against one’s better judgment. But if we accept Richard Holton’s view that weakness of will consists in being “too ready to reconsider their intentions” (Holton 1999, 241), then in this case Murray is weak-willed.2This seems to be the right way to talk about the case to me. With these details in place, we can talk about what’s crucial to this essay, the contrast with the second case.\nIn the second case Murray fails to exercise self-control. He could have prevented himself from swearing in front of his son. Breaking a wine glass is irritating, but it neither undermines nor, necessarily, overwhelms self-control. Murray had the capacity to think about his resolution to not swear in front of Red. And if he had exercised this capacity, he would not have sworn when he did.\nIn the first case, Murray will only regret his lack of prior work at changing his dispositions in cases where his control fails. In the second case he will regret that, but he will also regret what he did on that occasion, for he could have kept his resolution, had only he thought of it. This regret seems appropriate, for in the second case he did something wrong at the time he swore, as well perhaps as having done something wrong earlier. (Namely, not having worked hard enough on his dispositions.) This difference in regret does not constitute the difference between compulsion and a case where self-control fails, but it is pretty good evidence that this is a failure of self-control.\nSo the second case is not one where Murray was compelled. He had the capacity to keep his commitment, and nothing was stopping him exercising this control, but he failed to do so. His failure was a failure of self-control. Murray’s self-control is, in this case, overwhelmed by the provocation. But it need not have been. Within some fairly broad limits, how much self-control we exercise is up to us.3 Murray’s failure of self-control is culpable because anyone with the capacity for self-control Murray has could have avoided breaking his commitment. I am not going to try to offer an analysis of what it is to have a capacity, but I suspect something like the complicated counterfactual analysis Kennett and Smith offer, and that Smith offers elsewhere (M. Smith 1997, 2003), is broadly correct.4\nKennett and Smith stress two things about this capacity that are worth noting here. First, having this kind of capacity is part of what it is to be rational. That is, being rational requires thinking of the right thing at the right time. As Ryle says, “Intelligently reflecting how to act is, among other things, considering what is pertinent and disregarding what is inappropriate.”(Ryle 1949, 31) Second, Kennett and Smith note that exercises of this capacity cannot be volitional. Following Davidson (1963), they say they cannot be actions. I find this terminology somewhat strained. Catching a fast moving ball is an action, I would say, but it does not seem to be volitional. So I will use ‘volitional action’ for this Davidsonian sense of action.\nMany recent philosophers have endorsed the idea that some of the mental states for which we hold people responsible are not voluntary, or at least are not volitional. Adams (1985; Heller 2000; Owens 2000) and Hieronymi (2008) note ways in which we appropriately blame people for being in certain states, where being in that state is not volitional. Something like this idea seems to be behind Ryle’s several regress arguments against the intellectualist legend. It just is not true that what we do divides cleanly into outcomes of conscious thought on the one hand, and mere bodily movements (a la digestion) on the other.5 Rather there is a spectrum of cases from pure ratiocination at one end to pure bodily movement at the other. And some of the things in the middle of this spectrum are proper subjects of reactive attitudes. The focus in this literature has been on blame, but some states in the middle of this spectrum are also praiseworthy.\nConsider some action that is strikingly imaginative, e.g. a writer’s apt metaphor or, say, a cricket captain’s imaginative field placements. It seems that, assuming the field settings are successful, the captain deserves praise for being so imaginative. But of course the captain did not, really could not, first intend to imagine such field settings, then carry out that intention. So something for which the captain deserves praise, his act of imagination, is not volitional. So not all praiseworthy things we do are volitional.\nThere are two responses to this argument that I can imagine, neither of them particularly plausible. First, we might think that the captain’s imagination is simply a remarkable feature of nature, as the Great Barrier Reef is. It is God, or Mother Nature, who should be praised, not the captain. Now it seems fair to react to some attributes of a person this way. A person does not deserve praise for having great eyesight, for example. But such a reaction seems grossly inappropriate, almost dehumanising, in this case. To be sure, we might also praise God or Mother Nature for yielding such an imaginative person, but we’ll do that as well as rather than instead of, praising the person. Second, we might praise the captain for his work in studying the game, and thinking about possible ways to dismiss batsmen, rather than this particular action. But if that is what we praise the captain for, we should equally praise the captain’s opponent, a hard working dullard. And that does not seem right. The hard-working dullard deserves praise for his hard work in the lead up, but the hard-working imaginative skipper deserves praise for what he does in the game too. So reactive attitudes, particularly praise, are appropriately directed at things people do even if these things are not volitional.\nThe key point of this section then is that responsibility outruns volition. Some actions are blameworthy because they are failures of self-control. Some actions are praiseworthy because they are wonderful feats of imagination. But neither failing to exercise self-control, nor exercising imagination, needs be volitional is order to be a locus of responsibility. I will argue in the next section that these considerations support the idea of responsibility for beliefs.\nVoluntariness about Belief\nHere is a situation that will seem familiar to anyone who has spent time in a student household. Mark is writing out the shopping list for the weekly grocery shop. He goes to the fridge and sees that there is a carton of orange juice in the fridge. He forms the belief that there is orange juice in the fridge, and hence that he does not need to buy orange juice. As it turns out both of these beliefs are false. One of his housemates finishes off the orange juice, but stupidly put the empty carton back in the fridge. When Mark finds this out, he is irritated at his housemate, but he is also irritated at himself. He did not have to draw the conclusion that there was orange juice in the fridge. He was, after all, living in a student house where people do all sorts of dumb things. That his housemate might have returned an empty container to the fridge was well within the range of live possibilities. Indeed had he even considered the possibility he would have thought it was a live possibility, and checked whether the container was empty before forming beliefs about what was needed for the shopping.\nExamples like this can be easily multiplied. There are all sorts of beliefs that we form in haste, where we could have stopped to consider the various realistic hypotheses consistent with the evidence, and doing so would have stopped us forming the belief. Indeed, unless one is a real master of belief formation, it should not be too hard to remember such episodes frequently from one’s everyday life. These conclusions that we leap to are voluntary beliefs; we could have avoided forming them. And not only could we have avoided these formations, but we would have if we had followed the methods for belief formation that we approve of. That seems enough, to me, to say the formation is voluntary. This is not the only way that voluntary doings, like calling a relevant possibility to mind, can matter to belief. The next example will be a little more controversial, but it points at the importance of dismissing irrelevant possibilities.\nLater that evening, Mark is watching his team, Geelong, lose another football game. Geelong are down by eight goals with fifteen minutes to go. His housemates are leaving to go see a movie, and want to know if Mark wants to come along. He says that he is watching the end of the game because Geelong might come back. One of his housemates replies, “I guess it is possible they’ll win. Like it is possible they’ll call you up next week to see if you want a game with them.” Mark replies, “Yeah, you are right. This one’s over. So, which movie?” Mark does nott just give in to his housemates, he forms the belief that Geelong will lose. Later that night, when asked what the result of the game was, he says that he did nott see the final score, but that Geelong lost by a fair bit. (In a recent paper (Weatherson 2005) I go into a lot more detail on the relation between not taking possibilities seriously, and having beliefs. The upshot is that what Mark does can count as belief formation, even if his credence that Geelong will lose does not rise.)\nNow it is tempting, or perhaps I should say that I am tempted, to view the housemate as offering Mark a reason to believe that Geelong will lose. We could view the housemate’s comments as shorthand for the argument that Geelong’s winning is as likely as Mark’s playing for Geelong, and since the latter will not happen, neither will the former. And maybe that is part of what the housemate is doing. But the larger part is that she is mocking Mark for his misplaced confidence. And the point of mocking someone, at least the point of constructive mockery like this, is to get them to change their attitudes. Mark does so, by ceasing to take seriously the possibility that Geelong will come back. In doing so, he exercises a capacity he had for a while, the capacity to cease taking this unserious possibility seriously, but needed to be prompted to use.\nIn both cases I say Mark’s belief formation is voluntary. In the first case he forms the belief because he does not exercise his doxastic self-control. He should have hesitated and not formed a belief until he checked the orange juice. And he would have done so if only he’d thought of the possibility that the container was empty. But he did not. And just as things we do because we do not bring the right thing to mind, like Murray’s swearing in the second case, are voluntary and blameworthy, Mark’s belief is voluntary and blameworthy. In the second case, he forms the belief by ceasing to take an unserious possibility seriously. In most cases of non-perceptual, non-testimonial belief formation, there is a counter-possibility that we could have taken seriously. Skill at being a believer involves not taking extreme possibilities, from Cartesian sceptical scenarios to unlikely footballing heroics, seriously. Exercises of such skill are rarely, if ever, volitional. But just like other mental activities that are not volitional can be voluntary and praiseworthy, not taking an extreme possibility seriously can be voluntary and praiseworthy.6\nI have made two claims for Mark’s beliefs in the above two cases. First, they are instances of voluntary belief formation. In each case he could have done otherwise, either by exercising or failing to exercise his capacity to take various hypotheses seriously. Second, they are appropriate subjects of praise and blame. I imagine some people will agree with the second point but not the first. They will say that only volitional actions are voluntary, even though things we do like bringing relevant considerations to mind are praiseworthy or blameworthy. Such people will agree with most of what I say in this paper. In particularly they’ll agree that the examples involving Mark undermine Alston’s argument against the applicability of deontological concepts in epistemology. So I am not going to die in a ditch over just what we call voluntary. That is, I won’t fuss too much over whether we want to say premise 2 in Ryan’s formulation of the argument is shown to be false by these examples (as I say) or premise 1 is shown to be false (as such an objector will say.) I will just note that it is hard for such people to say intuitive things about the second instance of Murray’s swearing, and this seems like a strong reason to not adopt their position.7\nRyan and Steup\nSharon Ryan has a slightly different view. She thinks that the truth of voluntarism consists in the fact that we hold certain beliefs intentionally. She does not offer an analysis of what it is to do something intentionally, except to say that consciously deciding to do something is not necessary for doing it intentionally, but doing it purposefully is (Ryan 2003, 70–71) In a similar vein, she says “When there’s a car zooming toward me and I believe that there is, I’m believing freely because I’m believing what I mean to believe.” (Ryan 2003, 74) This is said to be an intentional, and I take it a voluntary, belief.\nIt seems to me that there’s a large difference between things we voluntarily do, and things we mean to do, or do purposefully. There are several things we do voluntarily without meaning to do them. Murray’s swearing in the second example above is one instance. When we misspeak, or (as I frequently do) mistype, we do things voluntarily without meaning to do them. I do not mean by mistype cases where we simply hit the wrong key, but such cases as where I write in one more negation than I meant to, or, as I did earlier this evening, write “S is justified in believing that p” when I meant to write “S is justified in believing that she is justified in believing that p.” These are voluntary actions because I had the capacity to get it right, but did not exercise the capacity. But they are not things I meant to do. (I suspect there are also cases where we do things because we mean to do them, but they are not voluntary. These include cases where we train ourselves to produce a reflexive response. But I will not stress such cases here.)\nMatthias Steup (2008) argues that if compatibilism is true about free action, then our beliefs are free. His argument consists in running through the most plausible candidates to be compatibilist notions of freedom, and for each candidate that is plausible, showing that at least some of our beliefs satisfy the purported conditions on free actions. I agree with a lot of what Steup says, indeed this paper has been heavily influenced by what he says. But one crucial analogy fails I think. Steup is concerned to reject the premise that if \\(\\Phi\\)-ing is free, one \\(\\Phi\\)s because one has formed the intention to \\(\\Phi\\). His response centres around ‘automatic’ actions, such as the things we do when starting our drive to work: inserting the key, shifting into reverse, etc.\n\nThe question is whether they are caused by any antecedently formed intentions. I don’t think they are. … I didn’t form an intention to … shift into reverse…. I do things like that automatically, without thinking about them, and I assume you do too. But one can’t form an intention to \\(\\Phi\\) without thinking about \\(\\Phi\\)ing … Just one more example: I’d like to see the person who, just before brushing her teeth, forms the intention to unscrew the cap of the toothpaste tube. (Steup 2008, 383)\n\nI suspect that Steup simply has to look in the mirror. It is true that we do not usually form conscious intentions to shift into reverse, or unscrew the cap, but not all intentions are conscious. If we were asked later, perhaps by someone who thought we’d acted wrongly, whether we intended to do these things, the natural answer is yes. The best explanation of this is that we really did have an intention to do them, albeit an unconscious one. (I am indebted here to Ishani Maitra.)\nSteup is right that free actions do not require a prior intention, but his examples do not quite work. The examples I have used above are the Rylean regress stoppers, such as acts of imagination, and actions that we do because we did not think, like Murray’s swearing. If asked later whether he intended to say what he said, Murray would say yes in the third example, but (I think) no in the first and second. Intuitively, I think, he did not have such an intention.8\nInvoluntarism about Perceptual Beliefs\nIn some early 1990s papers, Daniel Gilbert and colleagues defended a rather startling thesis concerning the relation of comprehension and belief (Gilbert, Krull, and Malone 1990; Gilbert 1991; Gilbert, Tafarodi, and Malone 1993) Casual introspection suggests that when one reads or hears something, one first comprehends it and then, if it is backed by sufficient reasons, believes it. Gilbert (1991) argues against this seeming separation of comprehension and belief, and in favour of a view said to derive from Spinoza. When we comprehend a sentence, we add it to our stock of beliefs. If the new belief is implausible given our old beliefs, then we “unbelieve” it.9\nWe may picturesquely compare the two models of belief and comprehension to two models for security. The way security works at a nightclub is that anyone can turn up at the door, but only those cleared by the guards are allowed in. On the other hand, the way security works at a shopping mall is that anyone is allowed in, but security might remove those it regards as undesirable. Intuitively, our minds work on the nightclub model. A hypothesis can turn up and ask for admission, but it has to be approved by our cognitive security before we adopt it as a belief. Gilbert’s position is that we work on the shopping mall model. Any hypothesis put in front of us is allowed in, as a belief, and the role of security is to remove troublemakers once they have been brought inside.\nNow I do not want to insist Gilbert’s theory is correct. The experimental evidence for it is challenged in a recent paper (Hasson, Simmons, and Todorov 2005). But I do want to argue that if it is correct, then there is a kind of belief that is clearly involuntary. We do not have much control over what claims pass in front of our eyes, or to our ears. (We have some indirect control over this – we could wear eye shades and ear plugs – but no direct control, which is what’s relevant.) If all such claims are believed, these are involuntary beliefs. To be sure, nothing Gilbert says implies that we can not quickly regain voluntary control over our beliefs as we unbelieve the unwanted inputs. But in the time it takes to do this, our beliefs are out of our control.\nGilbert’s theory is rather contentious, but there are other kinds of mental representations that it seems clear we can not help forming. In The Modularity of Mind, Jerry Fodor (1983) has a long discussion of how the various input modules that he believes to exist are not under our voluntary control.10 If I am sitting on a train opposite some people who are chatting away, I can not help but hear what they say. (Unless, perhaps, I put my fingers in my ear.) This is true not just in the sense that I can not help receive the sound waves generated by their vocalisations. I also can not help interpreting and comprehending what they are saying. Much as I might like to not be bothered with the details of their lives, I can not help but hear what they say as a string of English sentences. Not just hearing, but hearing as happens automatically.\nThis automatic ‘hearing as’ is not under my voluntary control. I do not do it because I want to do it, or as part of a general plan that I endorse or have chosen to undertake. It does not reflect any deep features of my character. (Frankly I would much rather that I just heard most of these conversations as meaningless noise, like the train’s sound.) But I do it, involuntarily, nonetheless. This involuntariness is reflected in some of our practices. A friend tells me not to listen to X, because X is so often wrong about everything. Next I see the friend I say that I now believe that p, and when the friend asks why, I say it is because X said that p. The friend might admonish me. They will not admonish me for being within hearing range of X; that might have been unavoidable. And, crucially, they will not admonish me for interpreting X’s utterances. Taken literally, that might be what they were asking me not to do. But they’ll know it was unavoidable. What they were really asking me not to do was the one relevant thing that I had control over, namely believe what X said.\nAs Fodor points out at length, both seeing as and hearing as are generally outside voluntary control. Our perceptual systems, and by this I am including verbal processing systems, quickly produce representations that are outside voluntary control in any sense. If any of these representations amount to beliefs, then there are some involuntary beliefs that we have. So we might think that in the case above, although it was up to me to believe that p, it was not up to me to believe that, say, X said that p, because this belief was produced by a modular system over which I have no control.\nThis is not the position that Fodor takes. He thinks that beliefs are not produced by input modules. Rather, the non-modular part of the mind, the central processor, is solely responsible for forming and fixing beliefs. And the operation of this central processor is generally not mandatory, at least not in the sense that the operation of the modules is mandatory. Whether this is right seems to turn (in part) on a hard question to do with the analysis of belief.\nLet us quickly review Fodor’s views on the behaviour of input modules. The purpose of each module is to, within a specified domain, quickly and automatically produce representations of the world. These are, as on the nightclub model, then presented to cognition to be allowed in as beliefs or not. Here is how Fodor puts it.\n\nI am supposing that input systems offer central processes hypotheses about the world, such hypotheses being responsive to the current, local distribution of proximal stimulations. The evaluation of these hypotheses in light of the rest of what one knows is one of the things that central processes are for; indeed, it is the fixation of perceptual belief.(Fodor 1983, 136)\n\nBut these representations do not just offer hypotheses. They can also guide action prior to being ‘approved’ by the central processes. That, at least, seems to be the point of Fodor’s discussion of the evolutionary advantages of having fast modules (Fodor 1983, 70–71). The core idea is that when one is at risk of being eaten by a panther, there is much to be said for a quick, automatic, panther recognition device. But there is just as much to be said for acting immediately on one’s panther recognition capacities rather than, say, searching for possible reasons why this panther appearance might be deceptive. And browsing reason space for such evidence of deceptions is just what central processes, in Fodor’s sense, do. So it seems the natural reaction to seeing a panther should be, and is, guided more-or-less directly by the input modules not central processes.\nSo these ‘hypotheses’ are representations with belief-like direction of fit, i.e. they are responsive to the world, that guide action in the way that beliefs do. These are starting to sound a lot like beliefs. Perhaps we should take a Gilbert-style line and say that we automatically believe what we perceive, and the role of Fodorian central processes is not to accept or reject mere hypotheses, but to unbelieve undesirable inputs.11 There are a number of considerations that can be raised for and against this idea, and perhaps our concept of belief is not fine enough to settle the matter. But let’s first look at three reasons for thinking these inputs are not beliefs.\nFirst, if they are beliefs then we are often led into inconsistency. If we are looking at a scene we know to be illusory, then we might see something as an F when we know it is not an F. If the outputs of visual modules are beliefs, then we inconsistently believe both that it is and is not F. Perhaps this inconsistency is not troubling, however. After all, one of the two inconsistent beliefs is involuntary, so we are not responsible for it. So this inconsistency is not a sign of irrationality, just a sign of defective perception. And that is not something we should be surprised by; the case by definition is one where perception misfires.\nSecond, the inputs do not, qua inputs, interact with other beliefs in the right kind of way. Even if we believe that if p then q, and perceive that p, we will not even be disposed to infer that q unless and until p gets processed centrally. On this point, see Stich (1978) and (Fodor 1983, 83–86). The above considerations in favour of treating inputs as beliefs turned heavily on the idea that they have the same functional characteristics as paradigm beliefs. But as David Braddon-Mitchell and Frank Jackson (2007, 114–23) stress, functionalism can only be saved from counterexamples if we include these inferential connections between belief states in the functional charactisation of belief. So from a functionalist point of view, the encapsulation of input states counts heavily against their being beliefs.\nFinally, if Fodor is right, then the belief-like representation of the central processes form something like a natural kind. On the other hand, the class consisting of these representations plus the representations of the input modules looks much more like a disjunctive kind. Even if all members of the class play the characteristic role of beliefs, we might think it is central to our concept of belief that belief is a natural kind. So these inputs should not count as beliefs.\nOn the other hand, we should not overestimate the role of central processes, even if Fodor is right that central processes are quite different to input systems. There are two related features of the way we process inputs that point towards counting some inputs as beliefs, and hence as involuntary beliefs. The first feature is that we do not have to put any effort into believing what we see. On the contrary, as both Descartes and Hume were well aware, we believe what we see by default, and have to put effort into being sceptical. The second feature is that, dramatic efforts aside, we can only be so sceptical. Perhaps sustained reflection on the possibility of an evil demon can make us doubt all of our perceptions at once. But in all probability, at least most of the time, we can not doubt everything we see and hear.12 We can perhaps doubt any perceptual input we receive, but we can not doubt them all.\nIn the picturesque terms from above, we might think our security system is less like a nightclub and more like the way customs appears to work at many airports. (Heathrow Airport is especially like this, but I think it is not that unusual.) Everyone gets a cursory glance from the customs officials, but most people walk through the customs hall without even being held up for an instant, and there are not enough officials to stop everyone even if they wanted to. Our central processes, faced with the overwhelming stream of perceptual inputs, are less the all-powerful nightclub bouncer and more the overworked customs official, looking for the occasional smuggler who should not be getting through.\nThe fact that inputs turn into fully fledged beliefs by default is some reason to say that they are beliefs as they stand. It is noteworthy that what Gilbert et al’s experiments primarily tested was whether sentences presented to subjects under cognitive load ended up as beliefs of the subjects. Now this could be because comprehending a sentence implies, at least temporarily, believing it. But perhaps a more natural reading in the first instance is that inputted sentences turn into beliefs unless we do something about it. Gilbert et al are happy inferring that in this case, the inputs are beliefs until and unless we do that something. This seems to be evidence that the concept of belief philosophers and psychologists use include states that need to be actively rejected if they are not to acquire all the paradigm features of belief. And that includes the inputs from Fodorian modules.\nThat argument is fairly speculative, but we can make more of the fact that subjects can not stop everything coming through. This implies that there will be some long disjunctions of perceptual inputs that they will end up believing no matter how hard they try. Any given input can be rejected, but subjects only have so much capacity to block the flow of perceptual inputs. So some long disjunctions will turn up in their beliefs no matter how hard they try to keep them out. I think these are involuntary beliefs.\nSo I conclude tentatively that perceptual inputs are involuntary beliefs, at least for the time it would take the central processes to evaluate them were it disposed to do so. And I conclude less tentatively that subjects involuntarily believe long disjunctions of perceptual inputs. So some beliefs are involuntary.\nSpace considerations prevent a full investigation of this, but there is an interesting connection here to some late medieval ideas about evidence. In a discussion of how Descartes differed from his medieval influences, Matthew L. Jones writes “For Descartes, the realignment of one’s life came about by training oneself to assent only to the evident; for the scholastics, assenting to the evident required no exercise, as it was automatic.” (Jones 2006, 84)13 There is much contemporary interest in the analysis of evidence, with Timothy Williamson’s proposal that our evidence is all of our knowledge being a central focus (Williamson 2000 Ch. 9). I think there’s much to be said for using Fodor’s work on automatic input systems to revive the medieval idea that the evident is that which we believe automatically, or perhaps it is those pieces of knowledge that we came to believe automatically. As I said though, space prevents a full investigation of these interesting issues.\nEpistemological Consequences\nSo some of our beliefs, loosely speaking the perceptual beliefs, are spontaneous and involuntary, while other beliefs, the inferential beliefs, are voluntary in that we have the capacity to check them by paying greater heed to counter-possibilities. (In what follows it will not matter much whether we take the spontaneous beliefs to include all the perceptual inputs, or just the long disjunctions of perceptual inputs that are beyond our capacity to reject. I will note the few points where it matters significantly.) This has some epistemological consequences, for the appropriate standards for spontaneous, involuntary beliefs are different to the appropriate standards for considered, reflective beliefs. I include in the latter category beliefs that were formed when considered reflection was possible, but was not undertaken.\nTo think about the standards for spontaneous beliefs, start by considering the criteria we could use to say that one kind of animal has a better visual system than another. One dimension along which we could compare the two animals concerns discriminatory capacity – can one animal distinguish between two things that the other cannot distinguish? But we would also distinguish between two animals with equally fine-grained visual representations, and the way we would distinguish is in terms of the accuracy of those representations. Some broadly externalist, indeed broadly reliabilist, approach has to be right when it comes to evaluating the visual systems of different animals.\nThings are a little more complicated when it comes to evaluating individual visual beliefs of different animals, but it is still clear that we will use externalist considerations. So imagine we are looking for standards for evaluating particular visual beliefs of again fairly basic animals. One very crude externalist standard we might use is that a belief is good iff it is true. Alternatively, we might say that the belief is good iff the process that produces it satisfied some externalist standard, e.g. it is generally reliable. Or we might, in a way, combine these and say that the belief is good iff it amounts to knowledge, incorporating both the truth and reliability standards. It is not clear which of these is best. Nor is it even clear which, if any, animals without sophisticated cognitive systems can be properly said to have perceptual beliefs. (I will not pretend to be able to evaluate the conceptual and empirical considerations that have been brought to bear on this question.) But what is implausible is to say that these animals have beliefs, and the relevant epistemic standards for evaluating these beliefs are broadly internal.\nThis matters to debates about the justificatory standards for our beliefs because we too have perceptual beliefs. And the way we form perceptual beliefs is not that different from the way simple animals do. (If the representations of input processes are beliefs, then it does not differ in any significant way.) When we form beliefs in ways that resemble those simple believers, most notably when we form perceptual beliefs, we too are best evaluated using externalist standards. The quality of our visual beliefs, that is, seems to directly track the quality of our visual systems. And the quality of our visual system is sensitive to external matters. So the quality of our visual beliefs is sensitive to external matters.\nOn the other hand, when we reason, we are doing something quite different to what a simple animal can do. A belief that is the product of considered reflection should be assessed, inter alia, by assessing the standards of the reflection that produced it. To a first approximation, such a belief seems to be justified if it is well supported by reasons. Some reasoners will be in reasonable worlds, and their beliefs will be mostly true. Some reasoners will be in deceptive worlds, and many of their beliefs will be false. But this does not seem to change what we say about the quality of their reasoning. This, I take it, is the core intuition behind the New Evil Demon problem, that we’ll address much more below.\nSo we’re naturally led to a view where epistemic justification has a bifurcated structure. A belief that is the product of perception is justified iff the perception is reliable; a belief that is (or could have been) the product of reflection is justified iff it is well-supported by reasons.14 This position will remind many of Ernest Sosa’s view that there is animal knowledge, and higher knowledge, or scientia (Sosa 1991, 1997). And the position is intentionally similar to Sosa’s. But there is one crucial difference. On my view, there is just one kind of knowledge, and the two types of justification kick in depending on the kind of knower, or the kind of knowing, that is in question. If we simply form perceptual beliefs, without the possibility of reconsidering them (in a timely manner), then if all goes well, our beliefs are knowledge. Not some lesser grade of animal knowledge, but simply knowledge. To put it more bluntly, if you’re an animal, knowledge just is animal knowledge. On the other hand, someone who has the capacity (and time) to reflect on their perceptions, and fails to do so even though they had good evidence that their perceptions were unreliable, does not have knowledge. Their indolence defeats their knowledge. Put more prosaically, the more you are capable of doing, the more that is expected of you.\nThe New Evil Demon Problem\nThe primary virtue of the above account, apart from its intuitive plausibility, is that it offers a satisfactory response to the New Evil Demon argument. The response in question is not new; it follows fairly closely the recent response due to Clayton Littlejohn (2009), who in turn builds on responses due to Kent Bach (1985) and Mylan Engel (1992). But I think it is an attractive feature of the view defended in this paper that it coheres so nicely with a familiar and attractive response to the argument.\nThe New Evil Demon argument concerns victims of deception who satisfy all the internal standards we can imagine for being a good epistemic agent. So they are always careful to avoid making fallacious inferences, they respect the canons of good inductive and statistical practice, they do not engage in wishful thinking, and so on. The core intuition of the New Evil Demon argument is that although these victims do not have knowledge (because their beliefs are false), they do have justified beliefs. Since the beliefs do not satisfy any plausible externalist criteria of justification, we conclude that no externalist criteria can be correct. The argument is set out by Stewart Cohen (1984).\nA fairly common response is to note that even according to externalist epistemology there will be some favourable epistemic property that the victim’s beliefs have, and this can explain our intuition that there is something epistemically praiseworthy about the victim’s beliefs. My approach is a version of this, one that is invulnerable to recent criticisms of the move. For both this response and the criticism to it, see James Pryor (2001). I am going to call my approach the agency approach, because the core idea is that the victim of the demon is in some sense a good doxastic agent, in that all their exercises of doxastic agency are appropriate, although their perception is quite poor and this undermines their beliefs.\nAs was noted above, the quality of our visual beliefs is sensitive to external matters. This is true even for the clear-thinking victim of massive deception. Denying that the victim’s visual beliefs are as good as ours is not at all implausible; indeed intuition strongly supports the idea that they are not as good. What they are as good at as we are is exercising their epistemic agency. That is to say, they are excellent epistemic agents. But since there is more to being a good believer than being a good epistemic agent, there is also for example the matter of being a good perceiver, they are not as good at believing as we are.\nSo the short version of my response to the New Evil Demon problem is this. There are two things we assess when evaluating someone’s beliefs. We evaluate how good an epistemic agent they are. And we evaluate how good they are at getting evidence from the world. Even shorter, we evaluate both their collection and processing of evidence. Externalist standards for evidence collection are very plausible, as is made clear when we consider creatures that do little more than collect evidence. The intuitions that the New Evil Demon argument draws on come from considering how we process evidence. When we consider beliefs that are the products of agency, such as beliefs that can only be arrived at by extensive reflection, we naturally consider the quality of the agency that led to those beliefs. In that respect a victim might do as well as we do, or even better. But that is no threat to the externalist conclusion that they are not, all things considered, as good at believing as we are.\nAs I mentioned earlier, this is similar to a familiar response to the argument that James Pryor considers and rejects. He considers someone who says that what is in common to us and the clear-thinking victim is that we are both epistemically blameless. The objection he considers says that the intuitions behind the argument come from confusing this notion of being blameless with the more general notion of being justified. This is similar to my idea that the victim might be a good epistemic agent while still arriving at unjustified beliefs because they are so bad at evidence collection. But Pryor argues that this kind of deontological approach cannot capture all of the intuitions around the problem.\nPryor considers three victims of massive deception. Victim A uses all sorts of faulty reasoning practices to form beliefs, practices that A could, if they were more careful, could see were faulty. Victim B was badly ‘brought up,’ so although they use methods that are subtly fallacious, there is no way we could expect B to notice these mistakes. Victim C is our paradigm of good reasoning, though of course C still has mostly false beliefs because all of their apparent perceptions are misleading. Pryor says that both B and C are epistemically blameless; C because they are a perfect reasoner and B because they cannot be blamed for their epistemic flaws. But we intuit that C is better, in some epistemic respects, than B. So there is some internalist friendly kind of evaluation that is stronger than being blameless. Pryor suggests that it might be being justified, which he takes to be an internalist but non-deontological concept.\nThe agency approach has several resources that might be brought to bear on this case. For one thing, even sticking to deontological concepts we can make some distinctions between B and C. We can, in particular, say that C is epistemically praiseworthy in ways that B is not. Even if B cannot be blamed for their flaws, C can be praised for not exemplifying those flaws. It is consistent with the agency approach to say that C can be praised for many of their epistemic practices while saying that, sadly, most of C’s beliefs are unjustified because they are based on faulty evidence, or on merely apparent evidence.\nThe merits of this kind of approach can be brought out by considering how we judge agents who are misled about the nature of the good. Many philosophers think that it is far from obvious which character traits are virtues and which are vices. Any particular example is bound to be controversial, but I think it should be uncontroversial that there are some such examples. So I will assume that, as Simon Keller (2005) suggests, it is true but unobvious that patriotism is not a virtue but a vice.\nNow consider three agents D, E and F. D takes patriotism to extremes, developing a quite hostile strand of nationalism, which leads to unprovoked attacks on non-compatriots. E is brought up to be patriotic, and lives this way without acting with any particular hostility to foreigners. F is brought up the same way, but comes to realise that patriotism is not at all virtuous, and comes to live according to purely cosmopolitan norms. Now it is natural to say that D is blameworthy in a way that E and F are not. As long as it seems implausible to blame E for not working through the careful philosophical arguments that tell against following patriotic norms, we should not blame E for being somewhat patriotic. But it is also natural to say that F is a better agent than either D or E. That is because F exemplifies a virtue, cosmopolitanism, that D and E do not, and does not exemplify a vice, patriotism, that D and E do exemplify. F is in this way praiseworthy, while D and E are not.\nThis rather strongly suggests that when agents are misled about norms, a gap will open up between blamelessness and praiseworthiness. We can say that Pryor’s victim C is a better epistemic agent than A or B, because they are praiseworthy in a way that A and B are not. And we can say this even though we do not say that B is blameworthy and we do not say that being a good epistemic agent is all there is to being a good believer.\nAt this point the internalist might respond with a new form of the argument. A victim of deception is, they might intuit, just as praiseworthy as a regular person, if they perform the same inferential moves. I think at this point the externalist can simply deny the intuitions. In general, praiseworthiness is subject to a degree of luck. (Arguably blameworthiness is as well, but saying so sounds somewhat more counterintuitive than saying praiseworthiness is a matter of luck.) For example, imagine two people dive into ponds in which they believe there are drowning children. The first saves two children. The second was mistaken; there are no children to be rescued in the pond they dive into. Both are praiseworthy for their efforts, but they are not equally praiseworthy. The first, in particular, is praiseworthy for rescuing two children. As we saw in the examples of the writer and the good cricket captain above, praiseworthiness depends on outputs as well as inputs, and if the victim of deception produces beliefs that are defective, i.e. false, then through no fault of their own they are less praiseworthy.\nPraise and Blame\nAs Pryor notes, many philosophers have thought that a deontological conception of justification supports an internalist theory of justification. I rather think that is mistaken, and that at least one common deontological understanding of what justification is entails a very strong kind of externalism. This is probably a reason to not adopt that deontological understanding.\nAssume, for reductio, that S’s belief that p is justified iff S is blameless in believing that p. I will call this principle J=B to note the close connection it posits between justification and blamelessness. Alston (1988) seems to identify the deontological conception of justification with J=B, or at least to slide between the two when offering critiques. But one of Alston’s own examples, the ‘culturally isolated tribesman,’ suggests a principle that can be used to pull these two ideas apart. The example, along with Pryor’s three brains case, suggests that A1 is true.\nA1\nIt is possible for S to have a justified but false belief that her belief in p is justified.\n\nA1 is a special instance of the principle that justification does not entail truth. Some externalists about justification will want to reject the general principle, but all internalists (and indeed most externalists) will accept it. Now some may think that the general principle is right, but that beliefs about what we are justified in believing are special, and if they are justified they are true. But such an exception seems intolerably ad hoc. If we can have false but justified beliefs about some things, then presumably we can have false but justified beliefs about our evidence, since in principle our evidence could be practically anything. So the following situation seems possible; indeed it seems likely that something of this form happens frequently in real life. S has a false but justified belief that e is part of her evidence. S knows both that anyone with evidence e is justified in believing p in the absence of defeaters, and that there are no defeaters present. So S comes to believe, quite reasonably, that she is justified in believing that p. But S does not have this evidence, and in fact all of her evidence points towards ~p.15 So it is false that she is justified in believing p.\nThe following principle seems to be a reasonable principle concerning blameless inference.\nA2\nIf S blamelessly believes that she is justified in believing that p, and on the basis of that belief comes to believe that p, then she is blameless in believing that p.\n\nThis is just a principle of transfer of blameworthiness. The quite natural thought is that you do not become blameworthy by inferring from I am justified in believing p to p. This inference is clearly not necessarily truth-preserving, but that is not a constraint on inferences that transfer blameworthiness, since not all ampliative inferences are blameworthy. (Indeed, many are praiseworthy.) And it is hard to imagine a less blameworthy ampliative inference schema than this one.\nWe can see this more clearly with an example of A2. Suzy sees a lot of Fs and observes they are all Gs. She infers that it is justified for her to conclude that all Fs are Gs. Now it turns out this is a bad inference. In fact, G is a gruesome predicate in her world, so that is not a justified inference. But Suzy, like many people, does not have the concept of gruesomeness, and without it had no reason to suspect that this would be a bad inference. So she is blameless. If all that is correct, it is hard to imagine that she becomes blameworthy by actually inferring from what she has so far that all Fs are in fact Gs. Perhaps you might think her original inference, that it is justified to believe all Fs are Gs, was blameworthy, but blame can not kick in for the first time when she moves to the first order belief.\nI am now going to derive a contradiction from A1, A2 and J=B, and a clearly consistent set of assumptions about a possible case of belief.\nS justifiedly, but falsely, believes that she is justified in believing p. (Assumption - A1)\nOn the basis of this belief, S comes to believe that p. (Assumption)\nS blamelessly believes that she is justified in believing that p. (1, J=B)\nS blamelessly believes that p. (2, 3, A2)\nS is justified in believing that p. (4, J=B)\nIt is false that S is justified in believing that p. (1)\nOne of A1, A2 and J=B has to go. If you accept J=B, I think it has got to be A1, since A2 is extremely plausible. But A1 only fails if we accept quite a strong externalist principle of justification, namely that justification entails truth. More precisely, we’re led to the view that justification entails truth when it comes to propositions about our own justification. But as we saw above, that pretty directly implies that justification entails truth when it comes to propositions about our own evidence. And, on the plausible assumption that evidence can be practically anything, that leads to there being a very wide range of cases where justification entails truth. So J=B entails this strong form of externalism.\nThis does not mean that internalists cannot accept a deontological conception of justification. But the kind of deontological conception of justification that is left standing by this argument is quite different to J=B, and I think to existing deontological conceptions of justification. Here’s what it would look like. First, we say that a belief’s being justified is not a matter of it being blameless, but a matter of it being in a certain way praiseworthy. Second, we say that the inference from I am justified in believing that p to p is not praiseworthy if the premise is false. So if we tried to run the above argument against J=P (the premise that justified beliefs are praiseworthy) it would fail at step 4. So anyone who wants to hold that justification is (even in large part) deontological, and wants to accept that justification can come apart from truth, should hold that justification is a kind of praiseworthiness, not a kind of blamelessness.\n\n\nAdams, Robert Merrihew. 1985. “Involuntary Sins.” Philosophical Review 94 (1): 3–31. https://doi.org/10.2307/2184713.\n\n\nAlston, William. 1988. “The Deontological Conception of Epistemic Justification.” Philosophical Perspectives 2: 257–99. https://doi.org/10.2307/2214077.\n\n\nBach, Kent. 1985. “A Rationale for Reliabilism.” Monist 68 (2): 246–63. https://doi.org/10.5840/monist198568224.\n\n\nBraddon-Mitchell, David, and Frank Jackson. 2007. The Philosophy of Mind and Cognition, Second Edition. Malden, MA: Blackwell.\n\n\nCohen, Stewart. 1984. “Justification and Truth.” Philosophical Studies 46 (3): 279–95. https://doi.org/10.1007/bf00372907.\n\n\nCottingham, John. 2002. “Descartes and the Voluntariness of Belief.” Monist 85 (3): 343–60. https://doi.org/10.5840/monist200285323.\n\n\nDavidson, Donald. 1963. “Actions, Reasons and Causes.” Journal of Philosophy 60 (23): 685–700. https://doi.org/10.2307/2023177.\n\n\nDescartes, René. 1641/1996. Meditations on First Philosophy, Tr. John Cottingham. Cambridge: Cambridge University Press.\n\n\n———. 1644/2003. The Principles of Philosophy, Tr. John Veitch. Champaign, IL: Project Gutenberg.\n\n\nEngel, Mylan. 1992. “Personal and Doxastic Justification in Epistemology.” Philosophical Studies 67 (2): 133–50. https://doi.org/10.1007/bf00373694.\n\n\nFodor, Jerry A. 1983. The Modularity of Mind. Cambridge, MA: MIT Press.\n\n\nGendler, Tamar Szabó. 2000. “The Puzzle of Imaginative Resistance.” Journal of Philosophy 97 (2): 55–81. https://doi.org/10.2307/2678446.\n\n\nGilbert, Daniel T. 1991. “How Mental Systems Believe.” American Psychologist 46 (2): 107–19. https://doi.org/10.1037//0003-066x.46.2.107.\n\n\nGilbert, Daniel T., Douglas S. Krull, and Patrick S. Malone. 1990. “Unbelieving the Unbelievable: Some Problems in the Rejection of False Information.” Journal of Personality and Social Psychology 59 (4): 601–13. https://doi.org/10.1037//0022-3514.59.4.601.\n\n\nGilbert, Daniel T., Romin W. Tafarodi, and Patrick S. Malone. 1993. “You Can’t Not Believe Everything You Read.” Journal of Personality and Social Psychology 65 (2): 221–33. https://doi.org/10.1037//0022-3514.65.2.221.\n\n\nGinet, Carl. 1985. “Contra Reliabilism.” Monist 68 (2): 175–87. https://doi.org/10.5840/monist198568218.\n\n\n———. 2001. “Deciding to Believe.” In Knowledge, Truth and Duty, edited by Matthias Steup, 63–76. Oxford: Oxford University Press.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nHasson, Uri, Joseph P. Simmons, and Alexander Todorov. 2005. “Believe It or Not: On the Possibility of Suspending Belief.” Psychological Science 16 (7): 566–71. https://doi.org/10.1111/j.0956-7976.2005.01576.x.\n\n\nHeller, Mark. 2000. “Hobartian Voluntarism: Grounding a Deontological Conception of Epistemological Justification.” Pacific Philosophical Quarterly 81 (2): 130–41. https://doi.org/10.1111/1468-0114.00099.\n\n\nHieronymi, Pamela. 2008. “Responsibility for Believing.” Synthese 161 (3): 357–73. https://doi.org/10.1007/s11229-006-9089-x.\n\n\nHolton, Richard. 1999. “Intention and Weakness of Will.” The Journal of Philosophy 96 (5): 241–62. https://doi.org/10.2307/2564667.\n\n\n———. 2003. “How Is Strength of Will Possible?” In Weakness of Will and Varities of Practical Irrationality, edited by Sarah Stroud and Christine Tappolet, 39–67. Oxford: Oxford University Press.\n\n\n———. 2004. “Rational Resolve.” Philosophical Review 113 (4): 507–35. https://doi.org/10.1215/00318108-113-4-507.\n\n\nHolton, Richard, and Stephen Shute. 2007. “Self-Control in the Modern Provocation Defence.” Oxford Journal of Legal Studies 27 (1): 49–73. https://doi.org/10.1093/ojls/gql034.\n\n\nJones, Matthew L. 2006. The Good Life in the Scientific Revolution: Descartes, Pascal, Leibniz and the Cultivation of Virtue. Chicago: University of Chicago Press.\n\n\nKeller, Simon. 2005. “Patriotism as Bad Faith.” Ethics 115 (3): 563–92. https://doi.org/10.1086/428458.\n\n\nKennett, Jeanette, and Michael Smith. 1996a. “Frog and Toad Lose Control.” Analysis 56 (2): 63–73. https://doi.org/10.1111/j.0003-2638.1996.00063.x.\n\n\n———. 1996b. “Philosophy and Commonsense: The Case of Weakness of Will.” In The Place of Philosophy in the Study of Mind, edited by Michaelis Michael and John O’Leary-Hawthorne, 141–57. Norwell, MA: Kluwer. https://doi.org/10.1017/CBO9780511606977.005.\n\n\nLittlejohn, Clayton. 2009. “The Externalist’s Demon.” Canadian Journal of Philosophy 39 (3): 399–434. https://doi.org/10.1353/cjp.0.0054.\n\n\nOwens, David. 2000. Reason Without Freedom: The Problem of Epistemic Responsibility. New York: Routledge.\n\n\nPryor, James. 2001. “Highlights of Recent Epistemology.” British Journal for the Philosophy of Science 52 (1): 95–124. https://doi.org/10.1093/bjps/52.1.95.\n\n\nRyan, Sharon. 2003. “Doxastic Compatibilism and the Ethics of Belief.” Philosophical Studies 114 (1-2): 47–79. https://doi.org/10.1023/A:1024409201289.\n\n\nRyle, Gilbert. 1949. The Concept of Mind. New York: Barnes; Noble.\n\n\nShah, Nishi. 2002. “Clearing Space for Doxastic Voluntarism.” The Monist 85 (3): 436–45. https://doi.org/10.5840/monist200285326.\n\n\nSmith, Angela M. 2005. “Responsibility for Attitudes: Activity and Passivity in Mental Life.” Ethics 115 (2): 236–71. https://doi.org/10.1086/426957.\n\n\nSmith, Michael. 1997. “A Theory of Freedom and Responsibility.” In Ethics and Practical Reason, edited by Garrett Cullity and Berys Gaut, 293–317. Oxford: Oxford University Press.\n\n\n———. 2003. “Rational Capacities.” In Weakness of Will and Varities of Practical Irrationality, edited by Sarah Stroud and Christine Tappolet, 17–38. Oxford: Oxford University Press.\n\n\nSosa, Ernest. 1991. Knowledge in Perspective. New York: Cambridge University Press.\n\n\n———. 1997. “Reflective Knowledge in the Best Circles.” Journal of Philosophy 94 (8): 410–30. https://doi.org/10.2307/2564607.\n\n\nSteup, Matthias. 2000. “Doxastic Voluntarism and Epistemic Deontology.” Acta Analytica 15 (1): 25–56.\n\n\n———. 2008. “Doxastic Freedom.” Synthese 161 (3): 375–92. https://doi.org/10.1007/s11229-006-9090-4.\n\n\nStich, Stephen. 1978. “Beliefs and Subdoxastic States.” Philosophy of Science 45 (4): 499–518. https://doi.org/10.1086/288832.\n\n\nWatson, Gary. 1977. “Skepticism about Weakness of Will.” Philosophical Review 86 (3): 316–39. https://doi.org/10.2307/2183785.\n\n\nWeatherson, Brian. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\nWilliams, Bernard. 1976. “Deciding to Believe.” In Problems of the Self, 136–51. Cambridge: Cambridge University Press.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nThe cases, especially the second, were inspired by Richard Holton’s discussion of resolutions to prevent ‘automatic’ actions like smoking or sleeping in. See Holton (2003, 2004).↩︎\nWhether Murray is akratic is a slightly more complicated question than I have suggested in the text. If akrasia is acting against one’s judgment, then he is not; if akrasia is acting against one’s considered judgment, then he is. ‘Akrasia’ is a technical term, so I do not think a huge amount turns on what we say about this question.\nThere is an interesting historical precedent for Holton’s theory of weakness of will. Ryle hints at a similar position to Holton’s when he says “Strength of will is a propensity the exercise of which consist in sticking to tasks’ that is, in not being deterred or diverted. Weakness of will is having too little of this propensity.” (1949, 73) But the idea is not well developed in Ryle. We’ll return below to the differences between Ryle’s and Holton’s theories.↩︎\nHolton (2003) compares self-control to a muscle that we can exercise. We can make a similar point to the one in the text about physical muscles. If I try to lift a box of books and fail, that does not show I lack the muscular capacity to lift the box; I might not have been trying hard enough.↩︎\n(Ryle 1949, 71ff) also offers a counterfactual account of capacities that seems largely accurate.↩︎\nAs I read him, Ryle takes this fact to reveal an important weakness in Descartes’ theory of mind.↩︎\n(Ryle 1949, 29ff) stresses the importance of calling the right things to mind to rational thought and action. I am using a case here where Mark deliberately casts an option from his mind, but the more general point is that what possibilities we call to mind is a crucial part of rational action, and can be praiseworthy or blameworthy, whether or not it is volitional.↩︎\nRyle seems to have taken an intermediate position. He holds, I think, the view that voluntary acts are culpable acts where we had the capacity to do otherwise (71). So Mark’s belief about the orange juice is voluntary because he had the capacity to retain doubt, and nothing prevented him exercising it. But the belief about the football is not voluntary because we should not talk about praiseworthy acts being voluntary or involuntary. The last point is the kind of error that (Grice 1989, Ch.1) showed us how to avoid.↩︎\nIf so, Murray is not weak-willed according to Holton’s theory of will, but, since he does not keep his resolution, he is weak-willed according to Ryle’s otherwise similar theory. This seems to be an advantage of Holton’s theory over Ryle’s. Murray’s problem is not that his will was weak, it is that it was not called on. More generally, Ryle’s identification of weakness of will with irresoluteness seems to fail for people who frequently forget their resolutions. These people are surely irresolute, but (in agreement with Holton’s theory) I think they are not weak-willed.↩︎\nThe evidence for this view is set out in Gilbert, Krull, and Malone (1990; Gilbert, Tafarodi, and Malone 1993).↩︎\nAs he says, they have a mandatory operation. See pages 52-55 in particular, but the theme is central to the book.↩︎\nTo be clear, the position being considered here is not that we automatically believe p when someone says p to us, but that we automatically believe that they said that p.↩︎\nAs noted in the last footnote, when I talk here about what we hear, I mean to include propositions of the form S said that p, not necessarily the p that S says.↩︎\nJones attributes this view to Scotus and Ockham, and quotes Pedro Fonseca as saying almost explicitly this in his commentary on Aristotle’s Metaphysics.↩︎\nThere is a delicate matter here about individuating beliefs. If I look up, see, and hence believe it is raining outside, that is a perceptual belief. I could have recalled that it was raining hard a couple of minutes ago, and around here that kind of rain does not stop quickly, and formed an inferential belief that it was raining outside. I want to say that that would have been a different belief, although it has the same content. If I do not say that, it is hard to defend the position suggested here when it comes to the justificatory status of perceptual beliefs whose contents I could have otherwise inferred.↩︎\nI am assuming here that evidence of evidence need not be evidence. This seems likely to be true. In Bayesian terms, something can raise the probability of e, while lowering the probability of p, even though the probability of p given e is greater than the probability of p. Bayesian models are not fully general, but usually things that are possible in Bayesian models are possible in real life.\n\n↩︎\n",
    "preview": "posts/2021-01-11-deontology-and-descartess-demon/bike.jpg",
    "last_modified": "2021-02-05T20:43:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-the-bayesian-and-the-dogmatist/",
    "title": "The Bayesian and the Dogmatist",
    "description": "It has been argued recently that dogmatism in epistemology is incompatible with Bayesianism. That is, it has been argued that dogmatism cannot be modelled using traditional techniques for Bayesian modelling. I argue that our response to this should not be to throw out dogmatism, but to develop better modelling techniques. I sketch a model for formal learning in which an agent can discover a posteriori fundamental epistemic connections. In this model, there is no formal objection to dogmatism.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2007-08-01",
    "categories": [
      "epistemology",
      "scepticism"
    ],
    "contents": "\n\nContents\nSceptical Arguments\nDogmatism and a Bayesian Objection\nBayes and Keynes\nIn Defence of Dynamism\nThe Dogmatist and the Keynesian\nThe Problem of the Priors\nThe Problem of Old Evidence\nWhy Should We Care?\n\n\nThere is a lot of philosophically interesting work being done in the borderlands between traditional and formal epistemology. It is easy to think that this would all be one-way traffic. When we try to formalise a traditional theory, we see that its hidden assumptions are inconsistent or otherwise untenable. Or we see that the proponents of the theory had been conflating two concepts that careful formal work lets us distinguish. Either way, the formalist teaches the traditionalist a lesson about what the live epistemological options are. I want to argue, more or less by example, that the traffic here should be two-way. By thinking carefully about considerations that move traditional epistemologists, we can find grounds for questioning some presuppositions that many formal epistemologists make.\nPublished in Proceedings of the Aristotelian Society 107: 169-185.\nImage by hiwhataboutyou via Creative Commons.\nTo make this more concrete, I’m going to be looking at a Bayesian objection to a certain kind of dogmatism about justification. Several writers have urged that the incompatibility of dogmatism with a kind of Bayesianism is a reason to reject dogmatism. I rather think that it is reason to question the Bayesianism. To put the point slightly more carefully, there is a simple proof that dogmatism (of the kind I envisage) can’t be modelled using standard Bayesian modelling tools. Rather than conclude that dogmatism is therefore flawed, I conclude that we need better modelling tools. I’ll spend a fair bit of this paper on outlining a kind of model that (a) allows us to model dogmatic reasoning, (b) is motivated by the epistemological considerations that motivate dogmatism, and (c) helps with a familiar problem besetting the Bayesian.\nI’m going to work up to that problem somewhat indirectly. I’ll start with looking at the kind of sceptical argument that motivates dogmatism. I’ll then briefly rehearse the argument that shows dogmatism and Bayesianism are incompatible. Then in the bulk of the paper I’ll suggest a way of making Bayesian models more flexible so they are no longer incompatible with dogmatism. I’ll call these new models dynamic Keynesian models of uncertainty. I’ll end with a brief guide to the virtues of my new kind of model.\nSceptical Arguments\nLet H be some relatively speculative piece of knowledge that we have, say that G. E. Moore had hands, or that it will snow in Alaska sometime next year. And let E be all of our evidence about the external world. I’m not going to make many assumptions about what E contains, but for now E will stay fairly schematic. Now a fairly standard sceptical argument goes something like this. Consider a situation S in which our evidence is unchanged, but in which H is false, such as a brain-in-vat scenario, or a zombie scenario, or a scenario where the future does not resemble the past. Now a fairly standard sceptical argument goes something like this.\nTo know H we have to be in a position to know we aren’t in S\nWe aren’t in a position to know that we aren’t in S\nSo, we don’t know H\nThere are a few immediate responses one could make, but which I’m going to dismiss without argument here. These include claiming the setup is incoherent (as in, e.g., Williamson (2000)), rejecting the closure principle behind premise 1 (as in, e.g., Dretske (1971), accepting the conclusion (the sceptical response), or saying that in different sceptical arguments, one or other of these positions is correct. Instead I want to look at responses that question premise 2. In particular, I want to look at responses that offer us reasons to accept premise 2, since it seems here that the sceptic is at her strongest. (If the sceptic merely insists that premise 2 is reasonable, we can reply either that it isn’t, as I’m inclined to think, or that here is a case where intuition should be revised.)\nMany epistemologists will write papers responding to ‘the sceptic.’ I think this is a mistake, since there are so many different possible sceptics, each with different arguments for premise 2. (And, of course, some sceptics do not argue from sceptical scenarios like this one.) Here are, for instance, three arguments that sceptics might give for premise 2.\nSomeone in S can’t discriminate her situation from yours.\nIndiscriminability is symmetric.\nIf you can’t discriminate our situation from S, you can’t know you’re not in S.\nSo you can’t know you’re not in S.\nSomeone in S has the same evidence as you do.\nWhat you can know supervenes on what your evidence is.\nSo, you can’t know you are not in S.\nThere is no non-circular argument to the conclusion that you aren’t in S.\nIf you were able to know you’re not in S, you would be able to produce a non-circular argument that concluded that you aren’t in S.\nSo you can’t know that you aren’t in S.\nI won’t say much about these arguments, save that I think in each case the second premise is very implausible. I suspect that most non-philosophers who are moved by sceptical arguments are tacitly relying on one or other of these arguments, but confirming that would require a more careful psychological study than I could do. But set those aside, because there’s a fourth argument that is more troubling. This argument takes its inspiration from what we might call Hume’s exhaustive argument for inductive scepticism. Hume said that we can’t justify induction inductively, and we can’t justify it deductively, and that exhausts the justifications, so we can’t justify induction. A similar kind of argument helps out the general sceptic.\nIf you know you aren’t in S, you know this a priori, or a posteriori\nYou can’t know you aren’t in S a posteriori\nYou can’t know you aren’t in S a priori\nSo, you can’t know you aren’t in S\nThis seems to be a really interesting argument to me. To make things simpler, I’ll stipulate that by a posteriori knowledge, I just mean knowledge that isn’t a priori. That makes the first premise pretty secure, as long as we’re assuming classical logic.1 Lots of philosophers take its third premise for granted. They assume that since it is metaphysically possible that you could be in S, this can’t be something you can rule out a priori. That strikes me as a rather odd capitulation to infallibilism. But I won’t push that here. Instead I’ll look at denials of the second premise.\nDogmatism and a Bayesian Objection\nSomeone who denies the second premise says that your empirical evidence can provide the basis for knowing that you aren’t in S, even though you didn’t know this a priori. I’m going to call such a person a dogmatist, for reasons that will become clear shortly. The dogmatist is not a sceptic, so the dogmatist believes that you can know H. The dogmatist also believes a closure principle, so the dogmatist also believes you can know E \\({\\supset}\\) H. If the dogmatist thought you could know E \\({\\supset}\\) H a priori, they’d think that you could know a priori that you weren’t in S. (This follows by another application of closure.) But they think that isn’t possible, so knowing E \\({\\supset}\\) H a priori isn’t possible. Hence you know E \\({\\supset}\\) H a posteriori.\nIf we reflect on the fact that E is your total evidence, then we can draw two conclusions. The first is that the dogmatist thinks that you can come to know H on the basis of E even though you didn’t know in advance that if E is true, then H is true. You don’t, that is, need antecedent knowledge of the conditional in order to be able to learn H from E. That’s why I’m calling them a dogmatist. The second point is that the dogmatist is now running head on into a piece of Bayesian orthodoxy.\nTo see the problem, note that we can easily prove (A), for arbitrary E, H and K.2\n\\(A\\)\nPr(E \\({\\supset}\\) H E \\({\\wedge}\\) K) \\({\\leq}\\) Pr(E \\({\\supset}\\) H K), with equality iff Pr(E \\({\\supset}\\) H E \\({\\wedge}\\) K) = 1\n\nProof:\n\n1.\nPr(E \\({\\supset}\\) H K) =\n\n\nPr(E \\({\\supset}\\) H E \\({\\wedge}\\) K) Pr(E K) +\n\n\nPr(E \\({\\supset}\\) H \\({\\lnot}\\)E \\({\\wedge}\\) K) Pr(\\({\\lnot}\\)E K)\nProb theorem\n2.\nPr(E \\({\\supset}\\) H \\({\\lnot}\\)E \\({\\wedge}\\) K) = 1\nLogic\n3.\nPr(E \\({\\supset}\\) H  E \\({\\wedge}\\) K) \\({\\leq}\\) 1\nProb theorem\n4.\nPr(E \\({\\supset}\\) H K) \\({\\geq}\\)\n\n\nPr(E \\({\\supset}\\) H E \\({\\wedge}\\) K) Pr(E K) +\n\n\nPr(E \\({\\supset}\\) H E \\({\\wedge}\\) K) Pr(\\({\\lnot}\\)E K)\n1, 2, 3\n5.\nPr(E  K) + Pr(\\({\\lnot}\\)E K) = 1\nProb theorem\n6.\nPr(E \\({\\supset}\\) H K) \\({\\geq}\\) Pr(E \\({\\supset}\\) H E \\({\\wedge}\\) K)\n4, 5\n\nIt is clear enough from the proof that line 6 is an equality iff line 3 is an equality, so we have proven (A). Now some authors have inferred from this something like (B) from (A).3\n\\(B\\)\nIt is impossible to go from not being in a position to know E \\({\\supset}\\) H to being in a position to know it just by receiving evidence E.\n\nThe transition here should raise an eyebrow or two. (A) is a principle of probability statics. (B) is a principle of epistemological kinematics. To get from (A) to (B) we need a principle linking probability and epistemology, and a principle linking statics and kinematics. Fortunately, orthodox Bayesian confirmation theory offers us suggestions for both principles. We’ll write Cr(A) for the agent’s credence in A, and CrE(A) for the agent’s credence in A when updated by receiving evidence E.\nLearning:\nIf CrE(A) \\({\\leq}\\) Cr(A), then it is impossible to go from not being in a position to know A to being in a position to know it just by receiving evidence E.\n\nBayes:\nCrE(A) = Cr(A  E). That is, learning goes by conditionalisation.\n\nA quick browse at any of the literature on Bayesian confirmation theory will show that these principles are both widely accepted by Bayesians. Philosophers, even Bayesians, make false assumptions, so neither of these principles is obviously true. Nevertheless, I’m going to accept Learning at least for the sake of argument. I’m going to argue instead that the inference from (A) to (B) fails because Bayes fails. That is, I’m going to accept that if we could prove a principle I’ll call Lower is true, then dogmatism in the sense I’m defending it fails.\nLower.\nCrE(E \\({\\supset}\\) H) is less than or equal to Cr(E \\({\\supset}\\) H).\n\nNow there is a bad argument around here that the dogmatist might make. It might be argued that since the Bayesian approach (including Bayes) involves so much idealisation it could not be applicable to real agents. That’s a bad argument because the Bayesian approach might provide us with a good model for real agents, and models can be useful without being scale models. As long as the Bayesian model is the most appropriate model in the circumstances, then we can draw conclusions for the real world from facts about the model. The problem arises if there are alternative models which seem to fit just as well, but in which principles like Lower are not true. If there are alternative models that seem better suited (or at least just as well suited) to modelling the situation of initial evidence acquisition, and those models do not make Lower true, then we might think the derivation of Lower in the Bayesian model is a mere consequence of the arbitrary choice of model. In the next section I will develop just such a model. I won’t argue that it is the best model, let alone the only alternative to the Bayesian model. But I will argue that it is as good for these purposes as the Bayesian model, and it does not imply Lower.\nBayes and Keynes\nThe traditional Bayesian model of a rational agent starts with the following two principles.\nAt any moment, the agent’s credal states are represented by a probability function.\nFrom moment to moment, the agent’s credal states are updated by conditionalisation on the evidence received.\nOver recent decades many philosophers have been interested in models that relax those assumptions. One particular model that has got a lot of attention (from e.g. Isaac Levi (1974, 1980), Richard Jeffrey (1983), Bas Fraassen (1990), Alan Hájek (2000, 2003) and many others) is what I’ll call the static Keynesian model. This model has the following features.\nAt any moment, the agent’s credal states are represented by a set of probability functions, called their representor.\nThe agent holds that p is more probable than q iff the probability of p is greater than the probability of q according to all probability functions in their representor. The agent holds that p and q are equally probable iff the probability of p is equal to the probability of q according to all probability functions in their representor.\nFrom moment to moment, the agent’s credal states are updated by conditionalising each of the functions in the representor on the evidence received.\nThe second point is the big attraction. It allows that the agent need not hold that p is more probable than q, or q more probable than p, or that p and q are equally probable, for arbitrary p and q. And that’s good because it isn’t a rationality requirement that agents make pairwise probability judgments about all pairs of propositions. Largely because of this feature, I argued in an earlier paper that this model could be use to formalise the key philosophical ideas in Keynes’s Treatise on Probability. That’s the reason I call this a ‘Keynesian’ model.\nThe modifier ‘static’ might seem a little strange, because the agent’s representor does change when she receives new evidence. But the change is always of a certain kind. Her ‘hypothetical priors’ do not change. If at t1 her evidence is E1 and her representor R1, and at t2 her evidence is E2 and her representor R2, then there is a ‘prior’ representor R0 such that the following two claims are true for all probability functions Pr.\nPr \\({\\in}\\) R1\\({\\leftrightarrow}\\) [\\({\\exists}\\)Pr0\\({\\in}\\) R0: \\({\\forall}\\)p (Pr(p) = Pr0(p E1)]\nPr \\({\\in}\\) R2\\({\\leftrightarrow}\\) [\\({\\exists}\\)Pr0\\({\\in}\\) R0: \\({\\forall}\\)p (Pr(p) = Pr0(p E2)]\nThat is, there is a set of probability functions such that the agent’s representor at any time is the result of conditionalising each of those functions on her evidence. I’ll call any model with this property a static model, so the model described above is the static Keynesian model.\nNow there is a lot to like about the static Keynesian model, and I have made extensive use of it previous work. It is a particularly useful model to use when we need to distinguish between risk and uncertainty in the sense that these terms are used in Keynes’s 1937 article “The General Theory of Employment.”4 The traditional Bayesian model assumes that all propositions are risky, but in real life some propositions are uncertain as well, and in positions of radical doubt, where we have little or no empirical evidence, presumably most propositions are extremely uncertain. And using the static Keynesian model does not mean we have to abandon the great work done in Bayesian epistemology and philosophy of science. Since a Bayesian model is a (degenerate) static Keynesian model, we can say that in many circumstances (namely circumstances where uncertainty can be properly ignored) the Bayesian model will be appropriate. Indeed, these days it is something like a consensus among probabilists or Bayesians that the static Keynesian model is a useful generalisation of the Bayesian model. For example in Christensen (2005) it is noted, almost as an afterthought, that the static Keynesian model will be more realistic, and hence potentially more useful, than the traditional Bayesian model. Christensen doesn’t appear to take this as any kind of objection to Bayesianism, and I think this is just the right attitude.\nBut just as the static Keynesian is more general than the Bayesian model, there are bound to be interesting models that are more general than the static Keynesian model. One such model is what I call the dynamic Keynesian model. This model has been used by Seth Yalcin to explicate some interesting semantic theories, but to the best of my knowledge it has not been used for epistemological purposes before. That should change. The model is like the static Keynesian model in its use of representors, but it changes the way updating is modelled. When an agent with representor R receives evidence E, she should update her representor by a two step process.\nReplace R with U(R, E)\nConditionalise U(R, E), i.e. replace it with {Pr( E): Pr is in U(R, E)}\nIn this story, U is a function that takes two inputs: a representor and a piece of evidence, and returns a representor that is a subset of the original representor. Intuitively, this models the effect of learning, via getting evidence E, what evidential relationships obtain. In the static Keynesian model, it is assumed that before the agent receives evidence E, she could already say which propositions would receive probabilistic support from E. All of the relations of evidential support were encoded in her conditional probabilities. There is no place in the model for learning about fundamental evidential relationships. In the dynamic Keynesian model, this is possible. When the agent receives evidence E, she might learn that certain functions that were previously in her representor misrepresented the relationship between evidence and hypotheses, particularly between evidence E and other hypotheses. In those cases, U(R, E) will be her old representor R, minus the functions that E teaches her misrepresent these evidential relationships.\nThe dynamic Keynesian model seems well suited to the dogmatist, indeed to any epistemological theory that allows for fundamental evidential relationships to be only knowable a posteriori. As we’ll see below, this is a reason to stop here in the presentation of the model and not try and say something systematic about the behaviour of U. Instead of developing the model by saying more about U, we should assess it, which is what I’ll do next.\nIn Defence of Dynamism\nIn this section I want go over three benefits of the dynamic Keynesian model, and then say a little about how it relates to the discussion of scepticism with which we opened. I’m not going to say much about possible objections to the use of the model. That’s partially for space reasons, partially because what I have to say about the objections I know of is fairly predictable, and partially because the model is new enough that I don’t really know what the strongest objections might be. So here we’ll stick to arguments for the view.\nThe Dogmatist and the Keynesian\nThe first advantage of the dynamic Keynesian model is that because it does not verify Lower, it is consistent with dogmatism. Now if you think that dogmatism is obviously false, you won’t think this is much of an advantage. But I tend to think that dogmatism is one of the small number of not absurd solutions to a very hard epistemological problem with no obvious solution, so we should not rule it out pre-emptively. Hence I think our formal models should be consistent with it. What is tricky is proving that the dynamic Keynesian model is indeed consistent with it.\nTo see whether this is true on the dynamic Keynesian model, we need to say what it is to lower the credence of some proposition. Since representors map propositions onto intervals rather than numbers, we can’t simply talk about one ‘probability’ being a smaller number than another.5 On the static Keynesian model, the most natural move is to say that conditionalisation on E lowers the credence of p iff for all Pr in the representor, Pr(p) > Pr(p  E). This implies that if every function in the representor says that E is negatively relevant to p, then conditionalising on E makes p less probable. Importantly, it allows this even if the values that Pr(p) takes across the representor before and after conditionalisation overlap. So what should we say on the dynamic Keynesian model? The weakest approach that seems viable, and not coincidentally the most plausible approach, is to say that updating on E lowers the credence of p iff the following conditions are met:\nFor all Pr in U(R, E), Pr(p E) < Pr(p)\nFor all Pr in R but not in U(R, E), there is a Pr\\(^\\prime\\) in U(R, E) such that Pr\\(^\\prime\\)(p  E) < Pr(p)\nIt isn’t too hard to show that for some models, updating on E does not lower the credence of E \\({\\supset}\\) H, if lowering is understood this way. The following is an extreme example, but it suffices to make the logical point. Let R be the minimal representor, the set of all probability functions that assign probability 1 to a priori certainties. And let U(R, E) be the singleton of the following probability function, defined only over Boolean combinations of E and H: Pr(E \\({\\wedge}\\) H) = Pr(E \\({\\wedge}\\) \\({\\lnot}\\)H) = Pr(\\({\\lnot}\\)E \\({\\wedge}\\) H) = Pr(\\({\\lnot}\\)E \\({\\wedge}\\) \\({\\lnot}\\)H) = \\(\\frac{1}{4}\\). Then the probability of E \\({\\supset}\\) H after updating is \\(\\frac{3}{4}\\). (More precisely, according to all Pr in U(R, E), Pr(E \\({\\supset}\\) H) = \\(\\frac{3}{4}\\).) Since before updating there were Pr in R such that Pr(E \\({\\supset}\\) H) < \\(\\frac{3}{4}\\), in fact there were Pr in R such that Pr(E \\({\\supset}\\) H) = 0, updating on E did not lower the credence of E \\({\\supset}\\) H. So the dynamic Keynesian model does not, in general, have as a consequence that updating on E lowers the credence of E \\({\\supset}\\) H. This suggests that Lower in general is not true.\nIt might be objected that if evidence E supports our knowledge that E \\({\\supset}\\) H, then updating on E should raise the credence of E \\({\\supset}\\) H. And if we define credence raising the same way we just defined credence lowering, updating on E never raises the credence of E \\({\\supset}\\) H. From a Keynesian perspective, we should simply deny that evidence has to raise the credence of the propositions known on the basis of that evidence. It might be sufficient that getting this evidence removes the uncertainty associated with those propositions. Even on the static Keynesian model, it is possible for evidence to remove uncertainty related to propositions without raising the probability of that proposition. A little informally, we might note that whether an agent with representor R is sufficiently confident in p to know that p depends on the lowest value that Pr(p) takes for Pr \\({\\in}\\) R, and updating can raise the value of this ‘lower bound’ without raising the value of Pr(p) according to all functions in R, and hence without strictly speaking raising the credence of p.\nThe above illustration is obviously unrealistic, in part because U could not behave that way. It’s tempting at this stage to ask just how U does behave so we can work out if there are more realistic examples. Indeed, it’s tempting to try to attempt to provide a formal description of U. This temptation should be resisted. The whole point of the model is that we can only learn which hypotheses are supported by certain evidence by actually getting that evidence. If we could say just what U is, we would be able to know what was supported by any kind of evidence without getting that evidence. The best we can do with respect to U is to discover some of its contours with respect to evidence much like our own. And the way to make those discoveries will be to do scientific and epistemological research. It isn’t obvious that, say, looking for nice formal properties of U will help at all.\nThe Problem of the Priors\nOne really nice consequence of the dynamic Keynesian approach is that it lets us say what the representor of an agent with no empirical information should be. Say a proposition is a priori certain iff it is a priori that all rational agents assign credence 1 to that proposition. Then the representor of the agent with no empirical evidence is {Pr: \\({\\forall}\\)p: If p is a priori certain, then Pr(p) = 1}. This is the minimal representor I mentioned above. Apart from assigning probability 1 to the a priori certainties, the representor is silent. Hence it treats all propositions that are not a priori certain in exactly the same way. This kind of symmetric treatment of propositions is not possible on the traditional Bayesian conception for logical reasons. (The reasons are set out in the various discussions of the paradoxes of indifference, going back to Bertrand (1888).) Such a prior representor is consistent with the static Keynesian approach, but it yields implausible results, since conditionalising on E has no effect on the distribution of values of Pr(p) among functions in the representor for any p not made a priori certain by E. (We’ll say p is made a priori certain by E iff E \\({\\supset}\\) p is a priori certain.) So if this is our starting representor, we can’t even get probabilistic evidence for things that are not made certain by our evidence.6 So on the static Keynesian model, this attractively symmetric prior representor is not available.\nI think one of the motivations of anti-dogmatist thinking is the thought that we should be able to tell a priori what is evidence for what. If it looking like there is a cow in front of us is a reason to think there is a cow in front of us, that should be knowable a priori. I think the motivation for this kind of position shrinks a little when we realise that an a priori prior that represented all the connections between evidence and hypotheses would have to give us a lot of guidance as to what to do (epistemically speaking) in worlds quite unlike our own. Moreover, there is no reason we should have lots that information. So consider, for a minute, a soul in a world with no spatial dimensions and three temporal dimensions, where the primary source of evidence for souls is empathic connection with other souls from which they get a (fallible) guide to those souls’ mental states. When such a soul conditionalises on the evidence “A soul seems to love me” (that’s the kind of evidence they get) what should their posterior probability be that there is indeed a soul that loves them? What if the souls have a very alien mental life, so they instantiate mental concepts very unlike our own, and souls get fallible evidence of these alien concepts being instantiated through empathy? I think it’s pretty clear we don’t know the answers to these questions. (Note that to answer this question we’d have to know which of these concepts were grue-like, and which were projectable, and there is no reason to believe we are in a position to know that.) Now those souls are presumably just as ignorant about the epistemologically appropriate reaction to the kinds of evidence we get, like seeing a cow or hearing a doorbell, as we are about their evidence. The dynamic Keynesian model can allow for this, especially if we use the very weak prior representor described above. When we get the kind of evidence we actually get, the effect of U is to shrink our representors to sets of probability functions which are broadly speaking epistemically appropriate for the kind of world we are in. Before we got that evidence, we didn’t know how we should respond to it, just like the spaceless empathic souls don’t know how to respond to it, just like we don’t know how to respond to their evidence.\nIt is a commonplace observation that (a) prior probabilities are really crucial in Bayesian epistemology, but (b) we have next to no idea what they look like. I call this the problem of the priors, and note with some satisfaction that the dynamic Keynesian model avoids it. Now a cynic might note that all I’ve done is replace a hand-wavy story about priors with a hand-wavy story about updating. That’s true, but nevertheless I think this is progress. The things I’m being deliberately unclear about, such as what U should look like for E such as “Some other non-spatial tri-temporal soul seems to love me” are things that (a) my theory says are not a priori knowable, and (b) I don’t have any evidence concerning. So it isn’t surprising that I don’t have much to say about them. It isn’t clear that the traditional Bayesian can offer any story, even by their own lights, as to why they are less clear about the structure of the prior probability conditional on such an E.\nThe Problem of Old Evidence\nWhen we get evidence E, the dynamic Keynesian model says that we should do two things. First, we should throw out some probability functions in our representor. Second, we should conditionalise those that remain. But this is a normative condition, not a description of what actually happens. Sometimes, when we get evidence E, we may not realise that it is evidence that supports some theory T. That is, we won’t sufficiently cull the representor of those probability functions where the probability of T given E is not high. Housecleaning like this is hard, and sometimes we only do it when it becomes essential. In this case, that means we only do it when we start paying serious attention to T. In that case we may find that evidence E, evidence we’ve already incorporated, in the sense of having used in conditionalisation, gives us reason to be more confident than we were in T. In such a case we’ll simply cull those functions where probability of T given E is not high, and we will be more confident in T. That’s how old evidence can be relevant on the dynamic Keynesian model. Since we have a story about how old evidence can be relevant, there is no problem of old evidence for the dynamic Keynesian.\nFamously, there is a problem of old evidence for traditional Bayesians. Now I’m not going to rehearse all the arguments concerning this problem to convince you that this problem hasn’t been solved. That’s in part because it would take too long and in part because I’m not sure myself that it hasn’t been solved. But I will note that if you think the problem of old evidence is a live problem for traditional Bayesians, then you have a strong reason for taking the dynamic Keynesian model seriously.\nWhy Should We Care?\nThe sceptic’s opening move was to appeal to our intuition that propositions like E \\({\\supset}\\) H are unknowable. We then asked what reasons we could be given for accepting this claim, because the sceptic seems to want to derive quite a lot from a raw intuition. The sceptic can respond with a wide range of arguments, four of which are mentioned above. Here we focussed on the sceptic’s argument from exhaustion. E \\({\\supset}\\) H isn’t knowable a priori, because it could be false, and it isn’t knowable a posteriori, because, on standard models of learning, our evidence lowers its credibility. My response is to say that this is an artefact of the model the sceptic (along with everyone else) is using. There’s nothing wrong with using simplified models, in fact it is usually the only way to make progress, but we must be always wary that our conclusions transfer from the model to the real world. One way to argue that a conclusion is a mere artefact of the model is to come up with a model that is sensitive to more features of reality in which the conclusion does not hold. That’s what I’ve done here. The dynamic Keynesian model is sensitive to the facts that (a) there is a distinction between risk and uncertainty and (b) we can learn about fundamental evidential connections. In the dynamic Keynesian model, it isn’t true that our evidence lowers the probability of E \\({\\supset}\\) H. So the anti-sceptic who says that E \\({\\supset}\\) H is knowable a posteriori, the person I’ve called the dogmatist, has a defence against this Bayesian argument. If the response is successful, then there may well be other applications of the dynamic Keynesian model, but for now I’m content to show how the model can be used to defend the dogmatic response to scepticism.\n\n\nBertrand, Joseph Louis François. 1888. Calcul Des Probabilités. Paris: Gauthier-Villars et fils.\n\n\nChristensen, David. 2005. Putting Logic in Its Place. Oxford: Oxford University Press.\n\n\nCohen, Stewart. 2005. “Why Basic Knowledge Is Easy Knowledge.” Philosophy and Phenomenological Research 70 (2): 417–30. https://doi.org/10.1111/j.1933-1592.2005.tb00536.x.\n\n\nDretske, Fred. 1971. “Conclusive Reasons.” Australasian Journal of Philosophy 49 (1): 1–22. https://doi.org/10.1080/00048407112341001.\n\n\nFraassen, Bas Fraassenvan. 1990. “Figures in a Probability Landscape.” In Truth or Consequences, edited by J. M. Dunn and A. Gupta, 345–56. Amsterdam: Kluwer.\n\n\nHawthorne, John. 2002. “Deeply Contingent a Priori Knowledge.” Philosophy and Phenomenological Research 65 (2): 247–69. https://doi.org/10.1111/j.1933-1592.2002.tb00201.x.\n\n\nHájek, Alan. 2000. “Objecting Vaguely to Pascal’s Wager.” Philosophical Studies 98: 1–16. https://doi.org/10.1023/A:1018329005240.\n\n\n———. 2003. “What Conditional Probability Could Not Be.” Synthese 137 (3): 273–323. https://doi.org/10.1023/B:SYNT.0000004904.91112.16.\n\n\nJeffrey, Richard. 1983. “Bayesianism with a Human Face.” In Testing Scientific Theories, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.\n\n\nKeynes, John Maynard. 1937. “The General Theory of Employment.” Quarterly Journal of Economics 51 (2): 209–23. https://doi.org/10.2307/1882087.\n\n\nLevi, Isaac. 1974. “On Indeterminate Probabilities.” Journal of Philosophy 71 (13): 391–418. https://doi.org/10.2307/2025161.\n\n\n———. 1980. The Enterprise of Knowledge. Cambridge, MA.: MIT Press.\n\n\nWhite, Roger. 2006. “Problems for Dogmatism.” Philosophical Studies 131 (3): 525–57. https://doi.org/10.1007/s11098-004-7487-9.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nPerhaps not a wise assumption around here, but one that I’ll make throughout in what follows.↩︎\nAgain, the proof uses distinctively classical principles, in particular the equivalence of A with (A \\({\\wedge}\\) B) \\({\\vee}\\) (A \\({\\wedge}\\) \\({\\lnot}\\)B.) But I will take classical logic for granted throughout. David Jehle pointed out to me that the proof fails without this classical assumption.↩︎\nRoger White (2006) and Stewart Cohen (2005) endorse probabilistic arguments against people who are, in my sense, dogmatists. John Hawthorne (2002) also makes a similar argument when arguing that certain conditionals, much like E \\({\\supset}\\) H, are a priori.↩︎\nThe clearest statement of the distinction that I know is from that paper.\n\nBy ‘uncertain’ knowledge, let me explain, I do not mean merely to distinguish what is known for certain from what is only probable. The game of roulette is not subject, in this sense, to uncertainty; nor is the prospect of a Victory bond being drawn. Or, again, the expectation of life is only slightly uncertain. Even the weather is only moderately uncertain. The sense in which I am using the term is that in which the prospect of a European war is uncertain, or the price of copper and the rate of interest twenty years hence, or the obsolescence of a new invention, or the position of private wealth owners in the social system in 1970. About these matters there is no scientific basis on which to form any calculable probability whatever. We simply do not know. Nevertheless, the necessity for action and decision compels us as practical men to do our best to overlook this awkward fact and to behave exactly as we should if we had behind us a good Benthamite calculation of a series of prospective advantages and disadvantages, each multiplied by its appropriate probability, waiting to be summed. (Keynes 1937, 114–15)\n\n↩︎\nStrictly speaking, the story I’ve told so far does not guarantee that for any proposition p, the values that Pr(p) takes (for Pr in the representor) form an interval. But it is usual in more detailed presentations of the model to put constraints on the representor to guarantee that happens, and I’ll assume we’ve done that.↩︎\nThe argument in the text goes by a little quickly, because I’ve defined representors in terms on unconditional probabilities and this leads to complications to do with conditionalising on propositions of zero probability. A better thing to do, as suggested by Hájek (2003), is to take conditional probability as primitive. If we do this we’ll define representors as sets of conditional probability functions, and the a priori representor will be {Pr: If p \\({\\supset}\\) q is a priori certain, then Pr(q p) = 1}. Then the claim in the text will follow.\n\n↩︎\n",
    "preview": "posts/2021-01-11-the-bayesian-and-the-dogmatist/dogmatic.jpg",
    "last_modified": "2021-02-05T20:42:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-13-review-of-david-lewis/",
    "title": "Review of “David Lewis”",
    "description": "A review of _David Lewis_ by Daniel Nolan.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2007-01-01",
    "categories": [
      "book review",
      "on books",
      "David Lewis",
      "history of analytic"
    ],
    "contents": "\nDavid Lewis left us with a rich and highly integrated body of work. Its richness means that any serious student of philosophy should study it closely. And at first this looks like it should be something that any student should be able to tackle. Lewis is one of the great stylists of his generation, and his views on most topics are expressed with admirable clarity. But the integration makes it difficult for the beginner to get a foothold. Any place you look it seems you will have to master five other topics before you really understand what Lewis says on this topic. So a systematic introduction to Lewis’s views is needed, and Daniel Nolan’s new book provides one.\n\nPublished in Mind 116: 191-193.\nThere are two types of reader who will most benefit from Nolan’s book.\nThe book will obviously be very valuable for students, especially undergraduates. It would make an excellent textbook for upper level classes on metaphysics or philosophy of mind where Lewis’s views were an important part of the course.\nAlthough it is mostly not pitched at experts, the book should also have value for professional philosophers because of how it draws out the connections between Lewis’s views. For instance, someone working on the rule-following paradoxes who was interested in learning more about the notion of semantic eligibility that does so much work in Lewis’s solution to these paradoxes could learn here how this notion is related to notions from the theory of properties and the analysis of physical law. But this is not just a textbook, and the critical element of Nolan’s exposition should be helpful at points even to experts.\nNolan’s summary starts with Lewis’s commitment to realism, both scientific and metaphysical. He then spends three chapters setting out the building blocks we need for Lewis’s metaphysics (properties, other times and other worlds) and showing how these can get used to rich picture of reality, one replete with causation, laws, dispositions and chances. Chapter five is on Lewis’s distinctive metaphysics of mind, and discusses how it relates to functionalist theories and the identity theory. Chapters six and seven are on content, mental and linguistic respectively. Chapter eight surveys Lewis’s views on ethics and value theory, and the last chapter is on Lewis’s methodology, especially his use of Ramsey sentences as a way of defining theoretical terms.\nThere is much to like through all of this. Although all the parts of the metaphysical picture are set out throughout Lewis’s writings, this is the best systematic exposition of the picture in a single place. The treatment of causation, which cuts through a lot of complicated discussion to get to the essence of Lewis’s theory, is especially useful. The discussion of Lewis’s ethical views, focussing on his complicated relationship to consequentialism and virtue ethics, does an excellent job of drawing a relatively systematic theory out of scattered remarks from several obscure sources.\nObviously there is a lot that could not be covered in this kind of book. So there is very little on perception, nothing on philosophy of mathematics, next to nothing on formal philosophy save a small discussion of the semantics of counterfactuals, nothing on Lewis’s arguments that desire and belief are separate existences and so on. These must have been hard cuts, but given the target audience I think they were the right ones. If anything I would have been tempted to cut even more to allow a little more space to the topics covered. I doubt that typical readers will get much out of the discussion of indeterminate probabilities towards the end of chapter 6 for example, as interesting as that should be to experts.\nAlthough the book is primarily expository there is a good amount of critical discussion interspersed throughout. Lewis’s single strangest view, that dispositions must be grounded in intrinsic properties of the bearer of the disposition, comes in for extended and well-targeted criticism. And Nolan raises some interesting cases that suggest the cases not covered by Lewis’s story in “Mad Pain and Martian Pain” could be closer to home than Lewis wants. It might be worried that this much critical engagement will undermine the effectiveness of the book as a text, but I think it is all beneficial. For one thing, the criticisms often help highlight the contours of the theory. But there is a deeper reason too. A student learning Lewisian philosophy shouldn’t just be learning a bunch of Lewis’s theories. They should be learning something about how to do philosophy, which means putting forward theories and criticisms of theories. The criticisms Nolan makes, all of them the kind of criticism that Lewis would have taken seriously and even have made in other circumstances, help teach the student how progress is made within the paradigm Lewis established.\nLewis had a weakness for fantastic examples. His work is littered with stories of Martians and gods and wizards and infallible predictors. He thought, as I do, that these were perfectly appropriate in the contexts he used them. But they create difficulties because students, and professionals, don’t see the relevance of these fantasies to real world analysis. Nolan does the student, and the instructor, a real service by replacing these examples with down-to-earth ones. The sections on causation and causal decision theory in particular are clarified by these changed examples.\nThere are a few things that may have been done differently given the target audience. The bibliography only includes those works by Lewis that are cited in the text; it should have been a complete bibliography of Lewis’s work. Nolan from time to time refers to things that Lewis’s critics say without referring to those critics by name, let alone citing a reference. The book would be a more useful resource if it pointed explicitly to where the reader might see these criticisms set out in more depth. And on one or two occasions the book presupposes much more knowledge than its primary target reader will have. In the discussion of the view that all belief is de se belief, for example, Nolan suddenly presupposes familiarity with causal descriptivism about names without so much as introducing descriptivism.\nBut the virtues of the book outweigh these possible imperfections. Lewis’s philosophical work should be taught to as many of the next generation of philosophers as possible. (Not to mention the present generation.) Those of us engaged in this task would find our job easier if we had a clear and systematic presentation of Lewis’s philosophy. Now we do.\n\n\n\n",
    "preview": "posts/2021-03-13-review-of-david-lewis/nolan-lewis.jpg",
    "last_modified": "2021-03-13T10:26:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-the-asymmetric-magnets-problem/",
    "title": "The Asymmetric Magnets Problem",
    "description": "There are many controversial theses about intrinsicness and duplication. The first aim of this paper is to introduce a puzzle that shows that two of the uncontroversial sounding ones can’t both be true. The second aim is to suggest that the best way out of the puzzle requires sharpening some distinctions that are too frequently blurred, and adopting a fairly radical reconception of the ways things are.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2006-11-16",
    "categories": [
      "metaphysics",
      "intrinsic properties",
      "Humeanism"
    ],
    "contents": "\n\nContents\nTwo Theses about Duplication\nThree Distinctions\nThe Asymmetric Magnets Problem\nThe Principles and the Problem\nTwo Worries About Locality\nThe solution and its problems\nWrapping Up\n\nThere are many controversial theses about intrinsicness and duplication. The first aim of this paper is to introduce a puzzle that shows that two of the uncontroversial sounding ones can’t both be true. The second aim is to suggest that the best way out of the puzzle requires sharpening some distinctions that are too frequently blurred, and adopting a fairly radical reconception of the ways things are.\nPublished in Philosophical Perspectives 20: 479-92.\nImage by oskay via Creative Commons.\nTwo Theses about Duplication\nIn all of David Lewis’s discussions of intrinsicness and duplication, he held that the two concepts are connected by a tight circle of interdefinition. Duplicates share all of their intrinsic features, and objects that share all of their intrinsic features are duplicates. (Lewis 1983b, 1983a; Langton and Lewis 1998). Both of these claims are a little controversial. One might hold that some impure properties that aren’t shared by all duplicates, like having George Clooney as a part, are nevertheless intrinsic since gaining or losing them seems to amount to a non-Cambridge change (Weatherson 2006). And one might hold that some properties which don’t differ between duplicates by definition, such as being a duplicate of the Louvre as it actually is, are nevertheless extrinsic (Dunn 1990). So maybe Lewis’s tight circle of interdefinition is not beyond question. But the following principle seems utterly uncontroversial to me.\n\nThanks to the Philosophy Program at the RSSS, ANU, where this was first drafted, to audiences at University of Manitoba and Stanford University, and to the attendees at my seminar on David Lewis at Cornell University. I am especially grateful to Ben Caplan, John Hawthorne, Ishani Maitra, Raul Saucedo and Wolfgang Schwarz.\nIntrinsicness Principle\nIf a and b differ in their pure intrinsic features, they are not duplicates;\nIf a and b have the same pure intrinsic features, then they are duplicates\nThat conjunction is the first of our (hitherto) uncontroversial theses. The second needs a bit more work to state formally.\nIt is fairly intuitive that whether two objects are duplicates is not an emergent feature of reality. In some sense, whether two complex are duplicates just depends on the properties of their parts and the relations between their parts. But this claim does turn out to be controversial; David Lewis (1983b) has controverted it. To a first approximation, his theory says that whether two objects are duplicates depends on whether they share the same perfectly natural properties. If there are any perfectly natural properties that are emergent, i.e. which are properties that complex objects have but not in virtue of the properties of or relations between their parts, then whether two objects are duplicates will also be emergent. Now Lewis doesn’t think there are any emergent perfectly natural properties, since the existence of such properties would be incompatible with the thesis of Humean Supervenience. But Lewis doesn’t think that Humean Supervenience is a necessary truth, let alone a conceptual truth, but at best a contingent truth. So the principle that duplication is not emergent is not something that is true in virtue of the concept of duplication.\nStill, nothing in Lewis’s views suggest that the following principle is false. If all the fundamental properties are not emergent, i.e. they are properties that complex things have in virtue of the fundamental properties of and relations between their parts, then duplication is not emergent. We might try and formalise this as follows. If all the fundamental properties are not emergent, then if the parts of x and y are duplicates, then x and y are duplicates. This principle is, however, too strong. It doesn’t account for the possibility that the parts of x and y are arranged differently. For instance, in the following example, the fusion of a and b is not a duplicate of the fusion of c and d, even though a is a duplicate of c and b is a duplicate of d.\n\nThe problem is that the arrangement of the two objects is different. So what we need is a principle that says that if all the perfectly natural properties are not emergent properties, and if the parts of x and y are duplicates, and those parts are arranged the same way, then x and y are duplicates. Saying this formally is not exactly trivial. The following version uses the idea of an isometry1.\nParts Principle\n\nThis principle holds in all worlds in which no fundamental properties or quantities are emergent. If X and Y are sets of material objects, a is the fusion of the members of X and b is the fusion of the members of Y, f is a function X \\({\\rightarrow}\\) Y, and i is an isometry defined on the space that X and Y are in, and the following conditions hold:\nFor all x in X, f(x) is a duplicate of x; and\nFor all x in X, if r exactly occupied by x, then i(r)\nis the region exactly occupied by f(x), then a and b are duplicates.\n\nThe Parts Principle is not as easy to state as the Intrinsicness Principle, but I think the idea it is expressing is fairly intuitive. Nevertheless, I think the two principles cannot both be true.\nThree Distinctions\nThe problem I’ll be focussing on looks rather simple, but it brings out several points that seem to have metaphysical interest. In particular, it highlights the importance of three distinctions that are easy to blur when doing metaphysics. It will make the exposition of the puzzle easier to place these distinctions up front.\nThe first distinction is between features and properties. Most metaphysicians accept that to fully characterise the world, we need to do more than say what exists. As well as saying what there is, we need to say how the things that exist are. It is easy to assume that to do that, we need to say what properties things have. But this need not be correct, or at least it need not be correct if we are looking to characterise the world in the most fundamental way. It might be that the fundamental features of reality are quantities, i.e. features that objects have to different degrees or in different amounts. Properties, like being green are features, but quantities, like mass or velocity are also features, just features that can be instantiated to different degrees or magnitudes. So feature is a more general category than property, and so as to not beg any questions, I’ll talk about intrinsic features and fundamental features rather than intrinsic properties and fundamental properties throughout. My solution to the problem will involve assuming that at least some of the fundamental features of reality are indeed quantities not properties.\nThe second distinction is between fundamental features and perfectly natural features. Fundamental features are features that do not obtain in virtue of other features obtaining. The fundamental features are part of a minimal basis we need for characterising reality. Generally fundamental features are related to other fundamental features by exceptionless laws, though this is not part of their definition. What is definitional is that they are basic and that they provide a basis for characterising the world without redundancy. (As a Humean, I’d also say that there are no necessary connections between distinct fundamental features, but that is a controversial metaphysical thesis, not a defining characteristic.) Perfectly natural features are features that make for primitive objective resemblance between things that instantiate them. By a primitive objective resemblance, I mean an objective resemblance that does not obtain in virtue of sharing more basic (in the limit, fundamental) properties. David Lewis (1983b) assumes, without much by way of argument as far as I can see, that the fundamental features are the perfectly natural features. My solution to the problem will involve rejecting that identity.\nThe final distinction is between the thesis that all the fundamental non-spatiotemporal features of reality are intrinsic properties of points, and the thesis that these features are local features. Jeremy Butterfield (2006) has stressed the importance of this distinction for metaphysics. A feature is local to a point iff it is intrinsic to arbitrarily small regions around that point. For example, the slope of a curve at a point is local to that point, even though it isn’t intrinsic to the point. So locality and intrinsicness can come apart. (This raises interesting questions about, for example, whether it is best to state the thesis of Humean Supervenience in terms of local properties or in terms of intrinsic properties of points.) I’ll say more about the importance of this distinction in section 5.\nI’ve already made use of these distinctions in setting out the principles about intrinsicness in section one. (In particular, it is crucial that the Parts Principle is stated in terms of fundamental rather than perfectly natural features.) Using them we can get to our central puzzle, the Asymmetric Magnets Problem.\nThe Asymmetric Magnets Problem\nOur puzzle is similar to the spinning sphere, often thought to raise a problem for Humean Supervenience (Armstrong 1980). The similarity is not in respect of its target; the puzzle is meant to be a puzzle for everyone who accepts those two principles, not just the Humean. Rather, the similarity is in that the puzzle is set in a world where there are homogeneous physical objects. Such a world is in many ways quite distant from actuality. But I think such worlds are useful fictions for elucidating the conceptual connections between central concepts in metaphysics. The puzzle is also set in a world with Euclidean spatial geometry. Again this is a fiction, but a useful one for working out conceptual connections.\nIn this world, some of the fundamental features are what we’ll call vector features. (This is a much smaller deviation from actuality.) Vector features are either quantities like velocity the value of which is a vector, or properties like having velocity v, where v is some vector. In particular, the strength and direction of the magnetic field throughout the world is a fundamental feature of the world. I’ll assume that both space-time regions and physical objects can have these vector features, although I’m only going to focus on the field strength and direction at a point in a physical object. Finally, I’ll assume that all of the fundamental physical quantities in the world are local. So there are no fundamental emergent quantities in the world. It might be worried that the last two assumptions are inconsistent, and that vector quantities could not be local. I’ll come back to this worry in section 5.\nSome of the things in this world are magnets. These are homogeneous objects with a uniform non-zero magnetic field throughout. I’m going to represent the magnetic field strength and direction of such a magnet with an arrow pointing towards the north pole of the magnet. The length of the arrow is proportional to the strength of the field.\nThe simplest kind of magnet is a bar magnet, just like the kind I used to play with in primary school. (Apart from being homogeneous of course!) These are cuboids with equal heights and depths, and a long length in the direction of their magnetic field. Suzy is playing with some magnets and, tiring of using her magnets to grab the other childrens’, decides to sharpen one end of each of her magnets for use as a weapon. The teacher notices this, confiscates the weaponised magnets, and lays them out on her desk. Here is what they look like from the teacher’s point of view.\n\nI’ve added the labels.\nEach magnet has one sharp end and one flat end. Each also has one north pole and one south pole. And, of course, each has one end to the right (from the teachers’ point of view) and one end to the left. The distribution of these properties of ends is different in the three cases.\nA’s north pole is sharp and to the right.\nB’s north pole is sharp and to the left.\nC’s north pole is flat and to the left.\nQuestion: Which of the magnets are duplicates?\nAnswer: A and B, but not C.\nI hope you agree with the answer! If not, let me provide a small argument.\nA and B are intrinsic duplicates because we could ‘line up’ A and B by picking A up, spinning it around, and moving it across a bit. And that’s only possible if the two objects are duplicates. This idea, that objects that can be transformed into one another by simple geometric transformations such as rotation and translation is a very deep part of our conceptual scheme. Consider, for example, Euclid’s proof of proposition 4.\n\nLet ABC, DEF be two triangles having the two sides AB, AC equal to the two sides DE, DF respectively, namely AB to DE and AC to DF, and the angle BAC equal to the angle EDF. I say that the base BC is also equal to the base EF, the triangle ABC will be equal to the triangle DEF … For, if the triangle ABC be applied to the triangle DEF, and if the point A be placed on the point D and the straight line AB on E, then the point B will also coincide with E, because AB is equal to DE. Again, AB coinciding with DE, the straight line AC will also coincide with DF, because the angle BAC is equal to the angle EDF; hence the point C will also coincide with the point F, because AC is again equal to DF. But B also coincided with E; hence the base BC will coincide with the base EF … and will be equal to it. Thus the whole triangle ABC will coincide with the whole triangle DEF, and will be equal to it. (Euclid 1956, 247–48)\n\nAs many mathematicians have pointed out over the centuries, this is not Euclid’s finest moment as a geometer. The idea he’s pushing is clear enough. If ABC and DEF satisfy the assumptions, then you can pick up ABC and place it on DEF, so that the sides and vertices all coincide. Does this prove that the sides and angles in the original triangle are equal? Not really, or at least not without the assumption that picking up ABC and moving it around doesn’t change its side lengths or angle magnitudes. And Euclid hadn’t said anything at that stage of the Elements to justify this assumption.\nSo qua axiomatic geometer Euclid has blundered here. But there is more to life than axiomatic geometry. There is, for instance, metaphysics. And the assumption Euclid is using here is, I think, a sound metaphysical intuition. (If it weren’t, the complaints about this fundamental proof in Euclid would have been earlier, and more frequent, than they actually were.) That intuition is, I think, that intrinsic properties are not, ceteris paribus, changed merely by moving objects around. Of course other things are not always equal; the intrinsic properties of a car are not preserved if you drive it into a wall. But the kind of abstract motion that Euclid is contemplating when he moves ABC onto DEF, or that I’m contemplating when I think about moving B around so it lines up with A, does not destroy intrinsic properties. So that’s an argument that A and B are intrinsically alike.\nOn the other hand, A and B each have a property that C lacks. Their magnetic field points towards their sharp end. This is in some sense a relational property, it is defined in part in terms of two things pointing in the same direction, but it doesn’t seem like a relation between the magnet and anything else. In general, properties that things have in virtue of relations between their parts are intrinsic properties. (It is intrinsic to the earth, for example, that more of its surface is wet than dry, even though this property is defined in terms of a relation.) So this is an intrinsic property of A. And, given the plausibility of the Intrinsicness Principle, that’s a reason to think that A and C are not intrinsic.\nThe Principles and the Problem\nHere then is our problem. Try to answer the following question given the two principles: Is the direction of a vector feature an intrinsic feature of its bearer or not? If yes, then A and B are not duplicates. If no, then A and C are duplicates. (In fact all three are duplicates, though I won’t prove this.) Neither way does it turns out that A and B are duplicates, but C is not, as we need. The aim of this section is to spell out that little argument in more detail, so we can see how the principles relate to the problem.\nFirst, we’ll assume that the direction of a vector feature is an intrinsic feature of its bearer. We need a way to rigidly denote directions, so we’ll call the direction that the vector in A points d1, and the direction that the vector in B points d2. Since d1\\({\\neq}\\) d2, A and B differ in their intrinsic properties. By the second clause of the Intrinsicness Principle, it follows that A and B are not duplicates.\nSecond, we’ll assume that the direction of a vector feature is not an intrinsic feature of its bearer. Now we want to show that B and C are duplicates. To do this we’ll use the Parts Principle. All of the fundamental quantities are local, so the Parts Principle applies. Now let the members of X and Y be the point-sized parts of A and C. Let l be the distance from the tip of the pointed end of A to the tip of the pointed end of C. The isometry i is a translation with length l and direction d1, i.e. a function that maps any point to the point that is distance l away from it in direction d1. This isometry maps A onto C. By the first clause of the Intrinsicness Principle, and the assumption that direction is not intrinsic, every point in A is a duplicate of any point in C. So by the Parts Principle, A and C are required.\nThe conclusion is that if we want to say that A and B are duplicates, but A and C are not duplicates, then we can’t hold on to both the Intrinsicness Principle and the Parts Principle.\nI think we should give up the Parts Principle. In particular, we should say that the Parts Principle holds only if all the perfectly natural features of reality are local, and this might fail to hold even if all the fundamental features of reality are local. The need for the distinction between these possibilities is, I think, the main lesson of the problem. But before we get to that conclusion, I want to address an objection to the argument so far.\nTwo Worries About Locality\nI can an imagine an objection to this argument along the following lines. In the setup of the problem, I said that some of the fundamental features of reality are vector-valued quantities. I also said that all of the fundamental features of reality are local. But these assumptions are inconsistent. Vector properties are not intrinsic properties of points. (Since we’re trying to hold on to the Intrinsicness Principle, we have to accept this.) Hence they are not, in the salient sense, local features of reality.\nI think this objection is sound all the way to the last step. As noted above, we need to distinguish between local properties and intrinsic properties of points. The distinction is common in mathematics, but has not been paid sufficient attention in metaphysics. Jeremy Butterfield’s (2006) is an important exception, one that was very influential on this paper.\nSo it is fair to say that all fundamental features of reality are local if all the facts about the world supervene on facts about the distribution of fundamental features in arbitrarily small regions, plus facts about the spatiotemporal arrangement of those regions. And there is no reason to think that positing vector features as fundamental is inconsistent with the fundamental features being local in this sense. Butterfield argues, persuasively, that velocity properties in Newtonian mechanics are not intrinsic properties of points, but he stresses that this doesn’t mean they are not local in this sense. He is focussing on velocity, and what he says doesn’t immediately translate to all vector properties. (It matters to his argument, for example, that velocities are conceptually connected to the positions of objects at different times, in a way that, for example, magnetic fields are not.) But I think his conclusions are independently plausible. Indeed, the argument from isometry above is an argument for the very same conclusion. So the short version of my reply is to concede that once we’ve allowed vector properties as fundamental, we can’t say that all the fundamental features of reality are intrinsic properties of points and spatiotemporal relations between them, but this is consistent with saying that all the fundamental features of reality are local.\nOnce we’ve said that, however, a different kind of objection becomes salient. It might be thought that if the fundamental features are intrinsic properties of regions not of points, the natural version of the Parts Principle is slightly weaker than as stated. In particular, we should focus our attention to cases where the sets X and Y consist of objects with positive size. Because this weakening flows naturally from the definition of locality, it doesn’t look like an ad hoc weakening. However this weakening does not at the end of the day help to save the Parts Principle. That’s because we can find a different way to divide up A and C into parts of positive size so that the Parts Principle still applies. A sketch of how we’ll (start to) divide up A is here.\n\nThe idea is that we make one large square part, and then divide the rest of A up into infinitely many diamonds. We do this recursively. Note that we start with a triangle whose base is to the left and vertex to the right. We create from this a diamond whose four vertices are the vertex of the triangle, and the midpoints of each of the three sides of the triangle. If we imagine cutting this diamond out of the magnet, we’d be left with two small triangles, each with a base on the left and a vertex on the right. We can do the same trick to create diamonds and (in imagination) cut them out, leaving us with four triangles. Repeat this until we have an infinity of diamonds. The fusion of all these diamonds with the large square will be our original magnet. Moreover, since every part is symmetric around the axis perpendicular to d1, each part will be a duplicate of the corresponding part in C. So the Parts Principle still tells us, falsely, that A and C are duplicates. We have to look somewhere else to avoid the problem.\nThe solution and its problems\nThe Asymmetric Magnets Problem looks easy. It is easy to say intuitively why A and B are duplicates, but C is not. The reason was given at the end of section three. In both A and B, the magnetic field ‘points’ in the same direction that the physical object does, while in C this is not the case. The difficulty arises when we try and shoehorn this intuition into a formal theory. We need to say that it is intrinsic to the magnet that its magnetic field points the same way it does. And we need to say this without saying that the direction of the magnetic field is itself intrinsic. I know of one way to do this, but it involves some overheads. I’m not going to argue for this at any length here, but I think the difficulty of providing a general solution to the Asymmetric Magnets Problem is one of many reasons to think that we should learn to live with these overheads.\nMy solution starts with Lewis’s definition of duplication. I gave a rough statement of this above; we now need a more precise statement. For Lewis, two objects are duplicates iff there is a mapping m from parts of one to parts of the other that (a) is an isomorphism and (b) for all n-place perfectly natural properties P, and all parts x1, …, xn of the first object, Px1…xn iff Pm(x1)…m(xn). So the objects are duplicates if their parts have the same natural properties, and stand in the same perfectly natural relations. That’s how Lewis’s theory goes; now we have to start adding variations. The first variation is quite radical, but one we have independent reason to make.\nAs John Hawthorne (2006) and David Denby (2001) have argued, Lewis’s theory of properties has difficulties accounting for quantities. Hawthorne notes that if we just take individual mass properties, e.g. having mass 17kg, having mass 42ng etc as perfectly natural, there is no way to state physical laws involving mass, such as the law of gravitation, as simple statements where all predicates denote perfectly natural properties. But the theory of laws in Lewis (1983b) says that all physical laws are simple statements where all predicates denote perfectly natural properties. This is something of a problem. For different reasons, Denby suggests that we take determinables as being perfectly natural. The individual mass properties are perfectly natural, he suggests, but not fundamental. What is fundamental is the determinable, mass, of which they are determinate.\nI think we should make a more radical move in the interests of simplicity. What reason do we have for thinking that the fundamental ways things are are properties rather than quantities or magntitudes? Very little reason, I’d say. Modern physics seems much more concerned with quantities than properties. What properties it is concerned with, such as being positively charged, seem to be derived from more fundamental quantities, such as charge. It would perhaps be convenient for formal semantics if the world had an object-property structure to match the subject-predicate structure of simple sentences. But we have no reason to believe the world will be so accommodating. It might turn out that there are a few fundamental quantities in the world. A quantity is a feature that objects have to different degrees. We can identify each value a quantity takes with a property. (Examples are properties like having mass 17kg.) But that shouldn’t make us think that the properties are metaphysically primary. They might be derived from the quantities. Hawthorne’s and Denby’s arguments push us towards that conclusion, and I’ll show here that assuming quantities are primary helps us state a solution to the Asymmetric Magnets Problem.\nSome quantities take simple values. The values of the mass quantity, for instance, are sufficiently simple that they can be represented by real numbers. But not all quantities are like that. In some cases the values are structured entities, which are composed of a magnitude and some other some other part or parts. Vector quantities are like this. We can naturally think of vectors as structured entities composed of a direction and a magnitude. I’m going to assume, at least for the sake of solving this problem, that any perfectly natural quantity takes values that either are magnitudes (as mass does) or takes values that are structured entities composed, among other things, of a magnitude. (This is an empirical assumption, and it may well not be true. If it is not true, the analysis of duplication below will need to be made more complicated.) For ease of exposition, I’ll say that a function \\(f\\) is perfectly natural iff it maps objects onto values, such that there is some perfectly natural quantity such that for any \\(x, f(x)\\) is the value that quantity takes with respect to \\(x\\). So if the quantity is mass, \\(f(x)\\) is \\(x\\)’s mass. And I’ll say that \\(|f(x)|\\) is the magnitude of this value, in the sense described above. (The notation here is slightly non-standard, since I allow that magnitudes may be negative numbers. For example, if \\(f\\) represents charge and \\(x\\) is negatively charged, then \\(|f(x)|\\) may be a negative number.)\nNow for the definition of duplication. Two objects are duplicates iff there is a mapping \\(m\\) from parts of one to parts of the other that (a) is an isomorphism and (b) for all \\(n\\)-place natural functions \\(f\\), and all parts \\(x_1, ..., x_n\\) of the first object, \\(|f(x_1,\\dots, x_n)| = |f(m(x_1),\\dots,f(m(x_n))|\\). So the objects are duplicates if the magnitudes of each of the natural quantities of each of their parts are the same. This allows that the quantities can vary without loss of intrinsic character, provided there is no variation in magnitude.\nThe Asymmetric Magnets Problem suggests a view on which the directions of vector features are indirectly relevant to the intrinsic nature of objects. ‘Indirectly’ because changing the direction doesn’t change the intrinsic properties of objects. But ‘relevant’ because the direction can matter, as we see when comparing A and C. The definition of duplication in terms of quantities that take structured values allows us to capture this indirect relevance. We’ll do so by defining a feature whose magnitude varies depending on how the object’s shape and the direction of its vector features are coordinated.\nLet \\(f\\) be a function representing some perfectly natural quantity such that \\(f(x)\\) is a vector. That is, \\(f\\) represents some perfectly natural vector quantity. This quantity may or may not be fundamental, though it will be fundamental in the cases under consideration here. Let \\(c\\) be a function that takes an object as input and returns its geometric centre as output. (By the geometric centre of \\(x\\) I mean the centre of mass of an object with the same shape as \\(x\\) and uniform mass density throughout.) Now suppose that the following function is perfectly natural. \\[g(x, y, z) =_{df} \\text{the cosine of the angle between }f(x)\\text{ and the ray from }g(y)\\text{ to }g(z)\\]\nThe motivation for taking this to be perfectly natural (but obviously not fundamental) is that it delivers the right results about the Asymmetric Magnets Problem, and it seems to deliver those results for the right reasons. To see it delivers the right results, just apply the above definition of duplication. Two objects are duplicates iff there is an isomorphism \\(m\\) from the parts of one to the parts of the other such that for all \\(n\\)-place natural functions \\(f\\), and all parts \\(x_1, ..., x_n\\) of the first object, \\(|f(x_1,\\dots, x_n)| = |f(m(x_1),\\dots,f(m(x_n))|\\). To make the discussion easier, we’ll redraw the magnets with some salient parts labelled.\n\nAny isomorphism from A to B that satisfy this constraint has to map A1 to B1, and A2 to B2. And any isomorphism from A to C that satisfy this constraint has to map A1 to C1, and A2 to C2. Now let f be the function whose value is represented by the arrow, and let g be the function defined as above. So if A and B are duplicates, it must be the case that g(A, A1, A2) = g(B, B1, B2). It is easy to verify that since f(A) points in the same direction as the ray from the centre of A1 to the centre of A2, \\(g(\\text{A}, \\text{A}_1, \\text{A}_2) = 1\\), and similarly the ray from the centre of B1 to the centre of B2 points in the same direction as f(B), g(B, B1, B2) = 1. So there is no reason here to doubt that A and B are duplicates. On the other hand, since the ray from the centre of C1 to the centre of C2 is in the opposite direction to f(C), g(C, C1, C2) = -1. So there is no isomorphism from parts of A to parts of C that preserves the value of perfectly natural properties, so A and C are not duplicates, as required.\nI don’t doubt that there are other ways to solve this problem, so I certainly won’t try arguing that this is the only solution. But I think it works, and the reason it works is because the values of natural quantities are structured entities, in this case vectors. Because they have structure, we can use one part of the structure (i.e. the magnitude) in determining what is directly relevant to intrinsicness, and another part of the structure (in this case the direction) in determining what is indirectly relevant. So it’s an important advantage of using quantities rather than properties as the centrepiece of our metaphysics that the values of natural quantities can be structured entities, and having something like structured quantities seems crucial to solving this problem.\nAlthough g is represents a perfectly natural quantity, it does not represent a fundamental quantity. Instead, it represents a quantity whose value supervenes on the distribution of other perfectly natural quantities. So we have to allow that there is a distinction between the fundamental quantities and the perfectly natural quantities. I don’t think this is a cost of the theory; there is no way to capture the idea that directions are indirectly relevant without distinguishing between the perfectly natural and the fundamental, so the Asymmetric Magnets Problem is a reason to make such a distinction. (I’m indebted here to Ben Caplan.)\nWe can reduce the apparent cost of this distinction by noting that one reason we might have for blocking redundant natural quantities does not apply here. (By a redundant quantity, I just mean one that supervenes on the fundamental quantities.) We don’t want to say that disjunctive properties like being grue, that supervene on other natural properties, are perfectly natural. But that’s not primarily because of the supervenience, but because of the fact that grueness doesn’t make for resemblance amongst the things that instantiate it. So at least that reason for caring about redundancy doesn’t apply here. (I’m indebted here to Raul Saucedo.)\nFinally, it is crucial to my solution that A, B and C have these parts. If A, B and C are extended simples, I can’t run the argument I make here. Indeed, if they are extended simples, it looks like they are duplicates by my definition. That seems bad. I think this is a problem that we don’t need to worry about, because this isn’t a real possibility. I’ll concede for the sake of argument that there are such things as extended simples. What I don’t see any need to concede the possibility of are asymmetric extended simples. In general, the way that we deduce that an object has parts is by noting it has different properties at different places. (This point is made in Sider (2003).) I think this is just the right strategy to use, as a matter of necessity. If an object has different properties in different locations, it has different parts in those different locations. So there could not be extended simples that are asymmetric magnets. The case where my theory produces the wrong result is an impossible case.\nWrapping Up\nThis paper has had several ambitions, some loftier than others. The most basic aim has been to introduce the Asymmetric Magnets Problem, and argue that it is going to be hard work for a systematic theory of intrinsicness to account for the facts about the problem. The more profound aims involve tearing apart concepts that metaphysicians often take for granted are interchangeable. My solution to the problem involves distinguishing local features from intrinsic features of points, fundamental features from perfectly natural features, and, most importantly, features from properties. The last of these is I think the biggest point. If we come to believe that quantities, not qualities, are the fundamental ways things are, then quite a bit of metaphysical orthodoxy needs rewriting. Some of that rewriting may be simple; just a matter of crossing out ‘li’ and writing in ‘nt’ in the middle of some words. But changes in fundamental metaphysics tend not to be isolated, and the rewriting project may lead to more wide-ranging changes. (Egan (2004) makes this point well, with an important illustration.) Now I certainly haven’t given anything like a conclusive argument in this paper that we should set about that project immediately. I have, however, provided one reason to think the project will eventually be necessary, and I suspect that more reasons will be provided in the future.\n\n\n\nArmstrong, David. 1980. “Identity Through Time.” In Time and Cause: Essays Presented to Richard Taylor, edited by Peter van Inwagen, 67–78. Dordrecht: Reidel.\n\n\nButterfield, Jeremy. 2006. “Against Pointillisme about Mechanics.” British Journal for the Philosophy of Science 57 (4): 709–53. https://doi.org/10.1093/bjps/axl026.\n\n\nDenby, David. 2001. “Determinable Nominalism.” Philosophical Studies 102 (3): 297–327. https://doi.org/0.1023/A:1010314926955.\n\n\nDunn, J. Michael. 1990. “Relevant Predication 2: Intrinsic Properties and Internal Relations.” Philosophical Studies 60 (3): 177–206. https://doi.org/10.1007/bf00367469.\n\n\nEgan, Andy. 2004. “Second-Order Predication and the Metaphysics of Properties.” Australasian Journal of Philosophy 82 (1): 48–66. https://doi.org/10.1080/713659803.\n\n\nEuclid. 1956. The Thirteen Books of the Elements, Tr. Thomas l. Heath. New York: Dover.\n\n\nHawthorne, John. 2006. “Quantity in Lewisian Metaphysics.” In Metaphysical Essays, 229–37. Oxford: Oxford University Press.\n\n\nLangton, Rae, and David Lewis. 1998. “Defining ‘Intrinsic’.” Philosophy and Phenomenological Research 58 (2): 333–45. https://doi.org/10.2307/2653512.\n\n\nLewis, David. 1983a. “Extrinsic Properties.” Philosophical Studies 44 (2): 197–200. https://doi.org/10.1007/bf00354100.\n\n\n———. 1983b. “New Work for a Theory of Universals.” Australasian Journal of Philosophy 61 (4): 343–77. https://doi.org/10.1080/00048408312341131.\n\n\nSider, Theodore. 2003. “Maximality and Microphysical Supervenience.” Philosophy and Phenomenological Research 66 (1): 139–49. https://doi.org/10.1111/j.1933-1592.2003.tb00247.x.\n\n\nWeatherson, Brian. 2006. “Intrinsic Vs. Extrinsic Properties.” In The Stanford Encyclopedia of Philosophy (Fall 2006 Edition), edited by Edward N. Zalta. Metaphysics Research Lab, Stanford University.\n\n\nYaglom, I. M. 1962. Geometric Transformations i. Random House: New York.\n\n\nAn isometry is “a transformation that does not change the distance between points” (Yaglom 1962, 11). That is, it is a function from points to points that doesn’t change distances. Although the isometry is initially defined as a function on points, it can obviously be extended to a function from regions to regions. If r is a region, i.e. a set of points, then f(r) is {f(x): x \\({\\in}\\) r}.↩︎\n",
    "preview": "posts/2021-01-03-the-asymmetric-magnets-problem/magnets.jpg",
    "last_modified": "2021-02-04T15:31:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-doing-philosophy-with-words/",
    "title": "Doing Philosophy With Words",
    "description": "This paper discusses the coverage of ordinary language philosophy in Scott Soames' \"Philosophical Analysis in the Twentieth Century\". After praising the book's virtues, I raise three points where I dissent from Soames' take on the history. First, I suggest that there is more to ordinary language philosophy than the rather implausible version of it that Soames sees to have been destroyed by Grice. Second, I argue that confusions between analyticity, necessity and priority are less important to the ordinary language period than Soames takes them to be. Finally, I claim that Soames' criticisms of Ryle turn in part on attributing reductionist positions to Ryle that Ryle did not hold.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2006-10-25",
    "categories": [
      "book symposium",
      "history of analytic",
      "language",
      "on books"
    ],
    "contents": "\n\nContents\nGrice and The End of Ordinary Language\nA Whig History?\nGetting the Question Right\nConclusion\n\nScott Soames (2003) has written two wonderfully useful books that will be valuable introductions to twentieth century philosophy. The books arose out of his well-received classes on the history of twentieth century history at Princeton, and will be valuable to anyone teaching similar courses. I shall be relying on them as I teach such a course at Cornell.\n\nPublished in Philosophical Studies 135 (2007): 429-37.\nThe books consist of detailed case studies of important twentieth-century works. They are best read alongside those original texts. Anyone who works through the canon in this way will have an excellent introduction to what twentieth century philosophers were trying to do. The selections are judicious, and while some are obvious classics some are rather clever choices of papers that are representative of the type of work being done at the time. And Soames doesn’t just point to the most important works to study, but the most important sections of those works.\n\nThanks to David Chalmers, Michael Fara, John Fischer, Tamar Szabó Gendler, James Klagge, Michael Kremer, Ishani Maitra, Aidan McGlynn, Alva Noë, Jonathan Weinberg and Larry Wright.\nSoames’s discussion of these pieces is always built around an analysis of their strengths and weaknesses. He praises the praiseworthy, but the focus, at least in the sections I’m discussing (ordinary language philosophy from Wittgenstein to Grice), is on where these philosophers go wrong. This is particularly so when the mistakes are representative of a theme. There are three main mistakes Soames finds in philosophers of this period. First, they rely logical positivism long after it had been shown to be unviable. Second, they disregard the principle that semantics should be systematic. Third, they ignore the distinction between necessity and a priority. All three constitute major themes of Soames’s book, and indeed of twentieth century philosophy as Soames sees it.\nThese books concentrate, almost to a fault, on discussion of philosophers’ published works, as opposed to the context in which they are written. Apart from occasionally noting that some books were released posthumously, we aren’t told whether the philosophers who wrote them are alive, and only in one case are we told when a philosopher was born. This kind of external information does not seem important to Soames. He is the kind of historian who would prefer a fourth reading of Austin’s published works to a first reading of his wartime diaries. And he’d prefer to spend the evening working on refutations, or charitable reformulations, of Austin’s arguments to either. I’m mostly sympathetic to this approach; this is history of philosophy after all. We can leave discussions of the sociology of 1950s Oxford to those better qualified. But this choice about what to write about has consequences.\nMost of Soames’s chapters focus almost exclusively on a particular book or paper. The exceptions are like the chapter on Sense and Sensibilia, where Soames contrasts Austin’s discussion with Ayer’s response. We learn a lot about the most important works that way, but less about their intellectual environment. So the book doesn’t have much by way of broad discussion about overall trends or movements. There’s very little, for example, about who were the influencers and who the influenced. There’s nothing about how anyone not called ‘Wittgenstein’ changed their positions in response to criticism. One assumes from the chronology that Ryle’s influence on Austin was greater than Austin’s influence on Ryle, for example, but Soames is silent on whether this is true.\nSoames says at one point that, “[Ryle] was, along with Wittgenstein, J. L. Austin, and Paul Grice, one of the prime movers in postwar philosophy in England.” (68). But we aren’t really told why this is so, apart from the discussion of some prominent works of these four philosophers. (Perhaps Soames has taken the maxim Show it, don’t say it rather completely to heart.) Nor are why told why the list includes those four, and not, say, Strawson or Geach or Anscombe. Actually Anscombe’s absence reminds us that there is almost no discussion of women in philosophy in the book. That’s not Soames fault, it’s a reflection of a long-running systematic problem in philosophy that the discipline has a hard time recruiting and retaining women. Could some of that be traced back to what was going on in the ordinary language period? That kind of questions can’t be addressed by the kind of history book that Soames has written, where the focus is on the best philosophical writing, and not on the broader philosophical community.\nOne of the other consequences of the format is that, by necessity, many important figures are left out, on pain of writing a fifteen-volume book. In the period under discussion here there was historically important work by (among many others) Nelson Goodman, Wilfrid Sellars and Roderick Chisholm, some of which connects up closely to the themes and interests of the ordinary language philosophers, but none of which is as much as mentioned. (Goodman is mentioned in the epilogue as someone Soames regrets not covering.)\nNow this can’t be a complaint about the book Soames has written, because it would have been impossible to cover any more figures than he did in the style and depth that he did. And it would have been impossible to tell in detail the story of how Ryle’s impact on the philosophical world differed from Austin’s, or of the painfully slow integration of women into the top echelons of philosophy, without making the book be even more monumental than it is. All we’re left with is a half-hearted expression of regret that he didn’t write a different kind of book, one that told us more about the forest, even as we value what he says about the tallest of the trees.\nGrice and The End of Ordinary Language\nThere is one place where Soames stops to survey the field, namely his discussion of the impact of Grice’s work on the ordinary language tradition. Soames argues that with Grice’s William James lectures, the idea of ordinary language philosophy had “run their course.” The position seems to be that Grice overthrew a paradigm that had been vibrant for two decades, but was running out of steam by the time of Grice’s James lectures. How plausible is this?\nThe first step is to work out just what it was that Grice (1989) refuted. When summarising the ordinary language paradigm that he takes Grice to have overthrown, Soames is uncharacteristically harsh. In Soames’s summary one of the characteristic activities of an ordinary language philosopher is “opportunistically assembling reminders about how philosophically significant words are used in ordinary settings” (216). That may be a fair enough description of some mid-century work, but it isn’t a fair summary of the best of the work that Soames has spent the previous two hundred odd pages discussing. It all suggests that Grice didn’t so much overthrow ordinary language philosophy as much as badly done ordinary language philosophy, and this category might not include Strawson, Ryle, Austin and so on.\nMore importantly, it isn’t entirely clear just what it was Grice did that caused this paradigm shift. In Soames’s telling it seems the development of the speaker meaning/semantic meaning distinction was crucial, but Austin (1962) at least already recognised this distinction, indeed appealed to it twice in Sense and Sensibilia. Soames mentions the discussion on pages 89 to 91 of Sense and Sensibilia of phrases like “I see two pieces of paper,” and there is also the intriguing discussion on pages 128-9 of the relation between accurate and true where Austin goes close to stating Grice’s submaxim of concision.\nThe other suggestion is that Grice restored the legitimacy and centrality of systematic semantic theorising. It’s true Grice did that, but this doesn’t show we have to give up ordinary language philosophy unless it was impossible to be an ordinary language philosopher and a systematic semanticist. And it isn’t clear that this really is impossible. It hardly seems inconsistent with the kind of philosophy Austin did (especially in his theory of perception) that one endorse a systematic semantic theory. (Though Austin himself rarely put forward systematic analyses.) Notably, there are plenty of very systematic formal semanticists who take Strawson’s work on descriptions seriously, and try and integrate it into formal models. So we might wonder why Grice’s work shouldn’t have led to a kind of ordinary language philosophy where we paid more careful attention to system-building.\nMore broadly, we might wonder whether the ordinary language period really did end. The analysis of knowledge industry (strangely undiscussed in a work on analysis in the twentieth century) seemed to putter along much the same before and after the official demise of ordinary language philosophy. And there are affinities between the ordinary language philosophers and important contemporary research programs, e.g. the ‘Canberra Plan’ as described by Frank Jackson (1998). So perhaps before we asked who killed ordinary language philosophy (It was Professor Grice! In Emerson Hall!! With the semantics/pragmatics distinction!!!) we should have made sure there was a corpse. More on this point presently.\nA Whig History?\nOne of the major themes of Soames’s discussion is that there are some systematic problems in twentieth century philosophy that are righted by the heroes at the end of the story. I already mentioned the heroic role assigned to Grice. But the real star of the show is Kripke (1980), who comes in as a deus ex machina at the end showing how different necessity and a priority are, and thereby righting all manner of grievous wrongs. That Kripke is an important figure in twentieth century philosophy is hardly a matter of dispute, but Soames does stretch a little to find errors for our hero to correct.\nSome of the complaints about philosophers collapsing the necessary/a priori distinction do hit the target, but don’t leave deep wounds in their victims. For instance, Soames quotes Ryle (1954) arguing (in Dilemmas) that perception cannot be a physiological process because if it were we couldn’t know whether we saw a tree until we found out the result of complicated brain scans. Soames points out, perfectly correctly, that the seeing might be necessarily identical to the brain process even if we don’t know, and even can’t know without complicated measurements, whether they are identical. Soames is right that Ryle has made an epistemological argument here when a metaphysical argument was needed. But rewriting Ryle so he makes that metaphysical argument isn’t hard. If my seeing the tree is necessarily identical to the brain process, and the brain process is (as Ryle and Soames seem to agree it is) individuated by the brain components that implement it, then I couldn’t have seen the tree had one of the salient neurons in my brain been silently replaced with a functionally equivalent silicon chip. Since it is possible that I could have seen a tree even if a salient neuron was replaced with a functionally equivalent silicon chip, the seeing and the brain process are not necessarily identical. So while Ryle might have slipped here, and Kripke’s work does help us correct the slip, the consequences of this are basically verbal.\nA more important charge of ignoring the necessary/a priori distinction comes in Soames’s discussion of Wittgenstein’s deflationism about philosophy. Here is the salient passage.\n\nHis deflationary conception of philosophy is also consistent with, and even derivative from, his new ideas about meaning plus a set of unquestioned philosophical presuppositions he brings to the enterprise. The philosophical presuppositions include the then current and widespread assumptions that (i) that philosophical theses are not empirical, and hence must be necessary and a priori, and (ii) that the necessary, the a priori and the analytic are one and the same. Because he takes these assumptions for granted, he takes it for granted that if there are any philosophical truths, they must be analytic (29).\n\nThis seems to me to be mistaken twice over.\nFirst, it isn’t clear to me that there is any appeal to concepts of necessity in the passages in Wittgenstein Soames is summarising here, and metaphysical necessity simply doesn’t seem to have been a major interest of Wittgenstein’s. Wittgenstein does appear to reason that if a proposition is not empirical it is a priori, but that inference doesn’t go via claims about necessity, and isn’t shown to be fallacious by any of Kripke’s examples.\nSecond, it simply isn’t true that philosophers in Wittgenstein’s time took for granted that the analytic and the a priori were one and the same. To be sure, many philosophers in the early twentieth century (including many argue the younger Wittgenstein) argued against Kant’s claim that they are distinct, but this isn’t quite the same as taking for granted they are identical. And there are a few places where Wittgenstein appears to accept that some propositions are synthetic a priori. For example in Remarks on the Foundations of Mathematics he says it is synthetic a priori that there is no reddish green, (Part III, para 39) and goes on to say this about primes.\n\nThe distribution of primes would be an ideal example of what could be called synthetic a priori, for one can say that it is at any rate not discoverable by an analysis of the concept of a prime number. (Wittgenstein 1956 Part III, para 42)\n\nNow it is far from obvious what the connection is between remarks such as these and the remarks about the impossibility of philosophical theses in the Investigations. Indeed it is not obvious whether Wittgenstein really believed in the synthetic a priori at any stage of his career. But given his lack of interest in metaphysical necessity, and openness to the possibility of synthetic a priori claims, it seems unlikely that he was, tacitly or otherwise, using the argument Soames gives him to get the deflationary conclusions.1\nGetting the Question Right\nAs I mentioned above, Soames’s is the kind of history that focuses on the works of prominent philosophers, rather than their historical context. There’s much to be gained from this approach, in particular about what the greats can tell us about pressing philosophical questions. But one of the costs is that in focussing on what they say about our questions, we might overlook their questions. In most cases this is a trap Soames avoids, but in the cases of Austin and Ryle the trap may have been sprung.\nSoames sees Austin in Sense and Sensibilia as trying to offer us a new argument against radical scepticism.\n\nAustin’s ultimate goal is to undermine the coherence of skepticism. His aim is not just to show that skepticism is unjustified, or implausible, or that it is a position no one has reason to accept. Rather, his goal is to prevent skepticism from getting off the ground by denying skeptics their starting point. (173-4)\n\nBut we don’t get much of an interpretative argument that this is really Austin’s goal. Indeed, Soames concedes that Austin “doesn’t always approach these questions directly” (172). I’d say he does very little to approach them at all. To be sure, many contemporary defenders of direct realism are interested in its anti-sceptical powers, but there’s little to show Austin was so moved. Scepticism is not a topic that even arises in Sense and Sensibilia until the chapter on Warnock, after Austin has finished with the criticism of Ayer that takes up a large part of the book. And Soames doesn’t address the question of how to square the somewhat dismissive tone Austin takes towards scepticism in “Other Minds” with the view here propounded that Austin put forward a fairly radical theory of perception as a way of providing a new answer to the sceptic.\nIf Austin wasn’t trying to refute the sceptic, what was he trying to do? The simplest explanation is that he thought direct realism was true, sense-data theories were false, and that “there is noting so plain boring a the constant repetition of assertions that are not true, and sometimes no even faintly sensible; if we can reduce this a bit, it will all be to the good.” (Austin 1962, 5) I’m inclined to think that in this case the simplest explanation is the best, that Austin wrote a series of lectures on perception because he was interested in the philosophy of perception. Warnock says that “Austin was genuinely shocked by what appeared to his eye to be recklessness, hurry, unrealism, and inadequate attention to truth” (Warnock 1989, 154) and suggests this explained not only why Austin wrote the lectures but their harsh edge.\nThere is one larger point one might have wanted to make out of a discussion of direct realism, or that one might have learned from a discussion of direct realism, that seems relevant to what comes later in Soames’s book. If we really see objects, not sense-data, then objects are constituents of intentional states. That suggests that public objects might be constituents of other states, such as beliefs, and hence constituents of assertions. Soames doesn’t give us a discussion of these possible historical links between direct realism and direct reference, and that’s too bad because there could be some fertile ground to work over here. (I’m no expert on the history of the 1960s, so I’m simply guessing as to whether there is a historical link between direct realism and direct reference to go along with the strong philosophical link between the two. But it would be nice if Soames has provided an indication as to whether those guesses were likely to be productive or futile.)\nSoames gives us no inkling of where theories of direct reference came from, save from the brilliant mind of Kripke. Apart from the absence of discussion of any connection between direct realism and direct reference, there’s no discussion of the possible connections between Wittgenstein’s later theories and direct reference, as Howard Wettstein (2004) has claimed exist. And there’s no discussion of the (possibly related) fact that Kripke was developing the work that went into Naming and Necessity at the same time as he was lecturing and writing on Wittgenstein, producing the material that eventually became Wittgenstein on Rules and Private Language. Kripke is presented here as the first of the moderns2, and in many ways he is, but the ways in which he is the last (or the latest) of the ordinary language philosophers could be a very valuable part of a history of philosophy.3\nMatters are somewhat more difficult when it comes to Ryle’s The Concept of Mind. Ryle predicted that he would “be stigmatised as ‘behaviourist’” (Ryle 1949, 327) and Soames obliges, and calls him a verificationist to boot.\n\nIf beliefs and desires were private mental states [says Ryle], then we could never observe the beliefs and desires of others. But if we couldn’t observe them, then we couldn’t know that they exist, [which we can.] … This argument is no stronger than verificationism in general, which by 1949 when The Concept of Mind was published, had been abandoned by its main proponents, the logical positivists, for the simple reason that every precise formulation of it had been decisively refuted (97-8).\n\nBut Ryle’s position here isn’t verificationism at all, it’s abductophobia, or fear of inference to underlying causes. Ryle doesn’t think the claim of ghosts in the machine is meaningless, he thinks it is false. The kind of inference to underlying causes he disparages here is exactly the kind of inference to unobservables that paradigm verificationists, especially Ayer, go out of their way to allow, and in doing so buy all end of trouble.4 And abductophobia is prevalent among many contemporary anti-verificationists, particularly direct realists such as McDowell (1996), Brewer (1999) and Smith (2003) who think that if we don’t directly observe beer mugs we can never be sure that beer mugs exist. I basically agree with Soames that Ryle’s argument here (and the same style of argument recurs repeatedly in The Concept of Mind) is very weak, but it’s wrong to call it verificationist.\nThe issue of behaviourism is trickier. At one level Ryle surely is a behaviourist, because whatever behaviourism means in philosophy, it includes what Ryle says in The Concept of Mind. Ryle is the reference-fixer for at least one disambiguation of behaviourist. However we label Ryle’s views though, it’s hard to square what he says his aims are with the aims Soames attributes to him. In particular, consider Soames’s criticism of Ryle’s attempt to show that we don’t need to posit a ghost in the machine to account for talk of intelligence. (Soames is discussing a long quote from page 47 of The Concept of Mind.)\n\nThe description Ryle gives here is judicious, and more or less accurate. But it is filled with words and phrases that seem to refer to causally efficacious internal mental states—inferring, thinking, interpreting, responding to objections, being on the lookout for this, making sure not to rely on that, and so on. Unless all of these can be shown to be nothing more than behavioral dispositions, Ryle will not have succeeded in establishing that to argue intelligently is simply to manifest a variety of purely behavioral dispositions. (106)\n\nAnd Soames immediately asks\n\nSo what are the prospects of reducing all this talk simply to talk about what behavior would take place in various conditions? (106)\n\nThe answer, unsurprisingly, is that the prospects aren’t good. But why this should bother Ryle is never made clear. For Ryle only says that when we talk of mental properties we talk about people’s dispositions, not that we talk about their purely behavioural dispositions. The latter is Soames’s addition. It is rejected more or less explicitly by Ryle in his discussion of knowing how. “Knowing how, then, is a disposition, but not a single-track disposition like a reflex or a habit … its exercises can be overt or covert, deeds performed or deeds imagined, words spoken aloud or words heard in one’s head, pictures painted on canvas or pictures in the mind’s eye.” (1949, 46–47). Nor should Ryle feel compelled to say that these dispositions are behavioural, given his other theoretical commitments.\nRyle is opposed in general to talk of ‘reduction’ as the discussion of mechanism on pages 76ff shows. To be sure there he is talking about reduction of laws, but he repeatedly makes clear that he regards laws and dispositions as tightly connected (1949, 43, 123ff) and suggests that we use mental concepts to signal that psychological rather than physical laws are applicable to the scenario we’re discussing (167). Moreover, he repeatedly talks about mental events for which it is unclear there is any kind of correlated behavioural disposition, e.g. the discussion of Johnson’s stream of consciousness on page 58 and the extended discussion of imagination in chapter 8. Ryle’s claim that “Silent soliloquy is a form of pregnant non-sayings” (269) hardly looks like the claim of someone who wanted to reduce all mental talk to behavioural dispositions, unless one leans rather hard on ‘pregnant.’ But we aren’t told whether Soames leans hard on this word, for he never quite tells us why he thinks all the dispositions that Ryle considers must be behavioural dispositions, rather than (for example) dispositions to produce other dispositions.\nTo be sure, from a modern perspective it is hard to see where the space is that Ryle aims to occupy. He wants to eliminate the ghosts, so what is left for mind to be but physical stuff, and what does physical stuff do but behave? He’s not an eliminativist, so he’s ontologically committed to minds, and he hasn’t left anything for them to be but behavioural dispositions. So we might see it (not unfairly) but that’s not how Ryle sees it.5 Soames sees Ryle as an ancestor of a reductive materialist like David Lewis, and a not very successful one at that. But the Ryle of The Concept of Mind has as much in common with non-reductive materialists, especially when he says that “not all questions are physical questions” (1949, 77), insists that “men are not machines, not even ghost-ridden machines” (1949, 81) and describes Cartesians rather than mechanists as “the better soldiers” (1949, 330) in the war against ignorance. Perhaps a modern anti-dualist should aim for a reduction of the mental to the physical, but Ryle thought no such reduction was needed to give up the ghost, and the historian should record this.\nConclusion\nAs I said at the top, Soames has written two really valuable books. For anyone who wants to really understand the most important philosophical work written between 1900 and 1970, reading through the classics while constantly referring back to Soames’s books to have the complexities of the philosophy explained will be immensely rewarding. Those who do that might feel that the people who skip reading the classics and just read Soames’s books get an unreasonably large percentage of the benefits they’ve accrued. As noted once or twice above I have some quibbles with some points in Soames’s story, but that shouldn’t let us ignore what a great service Soames has provided by providing these surveys of great philosophical work.\n\n\nArmstrong, D. M. 2000. “Black Swans: The Formative Influences in Australian Philosophy.” In Rationality and Irrationality, edited by Berit Brogaard and Barry Smith, 11–17. Kirchberg: Austrian Ludwig Wittgenstein Society.\n\n\nAustin, J. L. 1962. Sense and Sensibilia. Oxford: Oxford University Press.\n\n\nBrewer, Bill. 1999. Perception and Reason. Oxford: Oxford University Press.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nJackson, Frank. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nKripke, Saul. 1980. Naming and Necessity. Cambridge: Harvard University Press.\n\n\nMcDowell, John. 1996. Mind and World. Cambridge, MA: Harvard University Press.\n\n\nRyle, Gilbert. 1949. The Concept of Mind. New York: Barnes; Noble.\n\n\n———. 1954. Dilemmas. Cambridge: Cambridge University Press.\n\n\nSmith, Michael. 2003. “Rational Capacities.” In Weakness of Will and Varities of Practical Irrationality, edited by Sarah Stroud and Christine Tappolet, 17–38. Oxford: Oxford University Press.\n\n\nSoames, Scott. 1998a. “More Revisionism about Reference.” In The New Theory of Reference, edited by Paul Humphreys and James Fetzer, 65–87. Dordrecht: Kluwer.\n\n\n———. 1998b. “Revisionism about Reference: A Reply to Smith.” In The New Theory of Reference, edited by Paul Humphreys and James Fetzer, 13–35. Dordrecht: Kluwer.\n\n\n———. 2003. Philosophical Analysis in the Twentieth Century. Princeton: Princeton University Press.\n\n\nWarnock, G. J. 1989. J. L. Austin. London: Routledge.\n\n\nWettstein, Howard. 2004. The Magic Prism. Oxford: Oxford University Press.\n\n\nWittgenstein, Ludwig. 1956. Remarks on the Foundations of Mathematics. New York: Macmillan.\n\n\nI’m grateful to many correspondants for discussions about Wittgenstein. They convinced me, inter alia, that it would be foolish of me to commit to strong views of any kind about the role of the synthetic a priori in Wittgenstein’s later thought, and that the evidence is particularly messy because Wittgenstein wasn’t as centrally concerned with these concepts as we are.↩︎\nThe first of what David Armstrong (2000) has aptly called “The Age of Conferences.”↩︎\nJust in case this gets misinterpreted, what I’m suggesting here is that Kripke (and his audiences) might have been influenced in interesting ways by philosophy of the 1950s and 1960s, not that Kripke took his ideas from those philosophers. The latter claim has been occasionally made, but on that ‘debate’ (Soames 1998b, 1998a) I’m 100% on Soames’s side.↩︎\nIt would be particularly poor form of me to use a paradigm case argument without discussing Soames’s very good dissection of Malcolm’s paradigm case argument in chapter 7 of his book. So let me note my gratitude as a Cornellian for all the interesting lines of inquiry Soames finds suggested in Malcolm’s paper – his is a paradigm of charitable interpretation, a masterful discovery of wheat where I’d only ever seen chaff.↩︎\nOf course he couldn’t have seen it that way since in 1949 he wouldn’t have had the concept of ontological commitment.\n\n↩︎\n",
    "preview": "posts/2021-01-05-doing-philosophy-with-words/soames.jpeg",
    "last_modified": "2021-02-05T15:23:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-humeans-arent-out-of-their-minds/",
    "title": "Humeans Aren't Out of Their Minds",
    "description": "Humeans about causation say that in some situations, whether C causes E depends on events far away from C and E. John Hawthorne has objected to this feature of the view. Whether one has a mind depends on what causal relations obtain between the parts of one's brain. But whether one has a mind does not depend on what happens far far away. I reply on behalf of the Humean. In the cases Hawthorne is worried about, the Humean can and should deny the problematic long-range dependence. One advantage of Humeanism is that it lets us make sense of the idea that the laws could be different in different parts of the universe. In the cases Hawthorne is worried about, that's exactly what is happening - the laws are different here to how they are over there. And what causal relations obtain here is only dependent on what happens in places the laws are the same.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2006-10-25",
    "categories": [
      "metaphysics",
      "mind",
      "Humeanism",
      "notes"
    ],
    "contents": "\nHumeanism is “the thesis that the whole truth about a world like ours supervenes on the spatiotemporal distribution of local qualities.” (Lewis 1994, 473) Since the whole truth about our world contains truths about causation, causation must be located in the mosaic of local qualities that the Humean says constitute the whole truth about the world. The most natural ways to do this involve causation being in some sense extrinsic. To take the simplest possible Humean analysis, we might say that c causes e iff throughout the mosaic events of the same type as c are usually followed by events of type e. For short, the causal relation is the constant conjunction relation. Whether this obtains is determined by the mosaic, so this is a Humean theory, but it isn’t determined just by c and e themselves, so whether c causes e is extrinsic to the pair. Now this is obviously a bad theory of causation, but the fact that causation is extrinsic is retained even by good Humean theories of causation. John Hawthorne (2004) objects to this feature of Humeanism. I’m going to argue that his arguments don’t work, but first we need to clear up three preliminaries about causation and intrinsicness.\n\nPublished in Noûs 41: 529-535.\nFirst, my wording so far has been cagey because I haven’t wanted to say that Humeans typically take causation to be an extrinsic relation. That’s because the greatest Humean of them all, David Lewis, denies that causation is a relation at all, and hence that it is an extrinsic relation (Lewis 2004b). We can go some way to avoiding this complication by talking, as Hawthorne does, about properties of regions, and asking the property of containing a duplicate of c that causes a duplicate of e is intrinsic or extrinsic.1 Humeans typically take causation to be extrinsic in this sense.\nSecond, nothing in Humeanism requires that causation is extrinsic in that sense. If one analysed causation as that intrinsic relation that actually most tightly correlates with the constant conjunction relation, then one would have guaranteed that causation was an intrinsic relation. Moreover, one would have a perfectly Humean theory of causation. (A perfectly awful theory, to be sure, but still a Humean one.) Peter Menzies (1996, 1999) develops a more sophisticated version of such a theory, and though Menzies describes his view as anti-Humean, one can locate the relation we’ve defined here in the Humean mosaic, so such an approach might be consistent with Humeanism in the intended sense.\nThird, there is good reason, independent of Humeanism, to accept that causation is extrinsic. As Ned Hall (2004) argues, it is very hard to square the intrinsicness of causation with the possibility of causation by omission. Given the choice between these two, I’m going to accept causation by omission without much hesitation. There is one powerful objection to the possibility of causation by omission, namely that if there is any causation by omission then there is a lot more than is intuitively plausible. But since Sarah McGrath (2005) has a good response to that objection, I feel happy accepting there is causation by omission. So I accept that causation is extrinsic, for reasons totally independent of Humeanism. Since Hawthorne appeals to no feature of Humeanism beyond the Humean’s acceptance of the extrinsicness of causation, we can take his argument to be an argument against the causal extrinsicalist, i.e. the theorist who accepts causation is extrinsic in the above sense. To see that the argument doesn’t go through, we need to consider what exactly the causal extrinsicalist is committed to. I’ll explore this by looking at some other examples of properties of regions.\nSome regions contain uncles and some do not. This seems to be an extrinsic property of regions. My house does not contain any uncles right now, but there are duplicates of it, in worlds where my brothers have children, where it does contain an uncle, namely my counterpart. Consider the smallest region containing the earth from the stratosphere in from the earth’s formation to its destruction. Call this region e. Any duplicate of e also contains uncles, including several uncles of mine. You can’t duplicate the earth without producing a duplicate of me who is, in the duplicate world, the nephew of the duplicates of my uncles. So it is intrinsic to e that it contain an uncle, even though this is an extrinsic property of regions. (There is much more discussion of extrinsic properties that are possessed intrinsically in Humberstone (1996).)\nThis possibility, that a region might intrinsically possess an extrinsic property, poses a problem for Hawthorne’s argument. Here is his presentation of it.\n\nAn intrinsic duplicate of any region wholly containing me will contain a being with my conscious life.\nThere are causal requirements on my conscious life.\nTherefore, Humeanism is false. (Hawthorne 2004, 351–52)\n\nThe problem is that this argument isn’t valid. What follows from (1) and (2) is that any region containing Hawthorne must possess some causal properties intrinsically. (As Hawthorne argues on page 356.) And what Humeanism entails is that causal properties are extrinsic properties of regions. But there is no incompatibility here, for it is possible that extrinsic properties are possessed intrinsically, as we saw in the discussion of uncles.\nHawthorne’s argument would go through if Humeans, and causal extrinsicalists more generally, were committed to the stronger claim that regions never possess causal properties intrinsically. But it doesn’t seem that Humeans should be committed to this claim. Consider again e and all its duplicates. Any such duplicate will contain a duplication of the event of Booth’s shooting Lincoln, and Lincoln dying.2 Will it also be the case that duplicate-Booth’s shooting in this world causes duplicate-Lincoln’s dying? If so, and this seems true, then it is intrinsic to e that it contains an event of a shooting causing a dying, even though the property of containing a shooting causing a dying is extrinsic.\nIt would be a bad mistake to offer the following epistemological argument that in all duplicates of e, duplicate-Booth’s shooting causes duplicate-Lincoln’s dying.\nIf there was a duplicate of e where duplicate-Booth’s shooting does not cause duplicate-Lincoln’s dying, then we would not know whether Booth’s shooting causes Lincoln’s dying without investigating what happens outside e.\nWe can know that Booth’s shooting caused Lincoln’s dying without investigating outside e.\nSo, there is no duplicate of e where duplicate-Booth’s shooting does not cause duplicate-Lincoln’s dying.\nThe problem with this argument is that even there are worlds containing such duplicates, we might know a priori that we do not live in such a world, just as we know a priori that we do not live in a world where certain kinds of sceptical scenarios unfold (Hawthorne 2002; Weatherson 2005).\nA better argument against the existence of such a world is that if it is possible, it should be conceivable. But it is basically impossible to conceive such a world. Even if throughout the universe shootings like Booth’s are usually followed by something other than dying, say that shooting in most parts of the universe causes diseases to be cured, the large-scale regularity within e (or its duplicate) of shootings being followed by dying suffices to ground the claim that shootings cause dyings in a good Humean theory. The crucial assumption here is that local regularities count for more than global regularities. If the local regularities deviate too far from the global regularities, then Humeans can and should say that different nomic claims (and hence causal claims) are true in this part of the world to the rest of the universe. If they say this, they can say that regions can have causal features (such as containing a shooting causing a dying) intrinsically even though causal features are extrinsic properties.\nTo illustrate the kind of Humean theory that would have such a consequence, consider the following variant on the constant conjunction theory of causation. The theory I’m imagining says that c causes e iff whenever an event of the same type as c occurs within a 50 mile radius of where c occurred, it was followed by an event of type e. Call this the 50 mile constant conjunction theory of causation.3 On the 50 mile constant conjunction theory of causation, it won’t be intrinsic to Ford’s Theatre that it contained a causal event involving Booth shooting Lincoln, but it will be intrinsic to any sphere of radius 50 miles or more centred on the theatre that it contains such a causal event. So on this theory causal properties can be intrinsic to a region, though they are still extrinsic properties of such a region.\nThat’s a very implausible Humean theory, but when we look at the details of David Lewis’s Humean picture, we can see the outlines of a more plausible theory with the same consequences. Lewis of course doesn’t offer a simple regularity theory of causation. Rather, he first argues that laws are the extremely simple, extremely informative true propositions (Lewis 1973, 73). That is, he offers a sophisticated regularity theory of laws. Then he analyses counterfactual dependence in terms of lawhood (Lewis 1973, 1979). Finally he analyses causation in terms of counterfactual dependence (Lewis 2004a). The philosophical theory meets the Humean mosaic most closely on the issue of what a law is. If we can offer a theory of laws that allows extra sensitivity to local facts, while remaining Humean, we can plug this back into Lewis’s theories concerning counterfactual dependence and hence causation without upsetting its Humean credentials.\nNow there is a good reason to think that a Humean theory of laws should be locally sensitive. (I’m indebted here to long ago conversations with James Chase.) Humeans typically believe in fairly unrestricted principles of recombination. And they believe that laws are not necessarily true. So there could be worlds with very different laws. So there is a world which ‘patches’ together part of the world with laws L1 with a world with laws L2. If the parts are large and isolated enough, it would be foolish to say that within those parts nothing is law-governed, or that within those regions there is no counterfactual dependence, or no causation. Much better to say that regularities obtaining within such a region are sufficiently simple and informative to count as laws. In our patchwork world, the laws might simply say In r1, L1and in r2, L2. Provided the terms denoting the regions are not too gruesome, these will plausibly be Humean, even Lewisian, laws.\nLet’s bring all this back to Hawthorne’s example. Hawthorne argues that certain causal facts are intrinsic to the region containing his body. The challenge for the Humean is to say how this could be so when Hawthorne could be embedded in a world where very different regularities obtain. The simple answer is to say that in such worlds, laws like In r, L, where r picks out the region Hawthorne’s body occupies, and L picks out a real-world law, will be true, simple and informative. It is informative because any duplicate of Hawthorne’s body is a very complicated entity, containing billions of billions of particles interacting in systematic ways, ways that are nicely summarised by real-world laws. Simplicity is a little harder to make out, but note that there is a reasonably sharp boundary between Hawthorne’s body and the rest of the world (Lewis 1993), so there should be a natural enough way to pick it out. In other words, even if we embed a Hawthorne duplicate in a world with very different regularities, Humeans will still have good reason to say that the laws, and hence the facts about counterfactual dependence and causation, inside that duplicate are not changed. So not only is it logically possible that Hawthorne’s premises are true and his conclusion false, we can motivate a Humean position that endorses the truth of Hawthorne’s premises and the falsity of the conclusion.\nSince Hawthorne’s argument is invalid then, we can accept the premises without giving up Humeanism. But I think it is worthwhile to note that his (1) also can be questioned. Hawthorne notes that it is rejected by those such as Dretske and Lewis who say that phenomenal character is determined in part by kind membership. (See Lycan (2001) for a longer defence of this kind of rejection of (1).) Hawthorne thinks that the intuitive plausibility of (1) constitutes a serious objection to those views. But by reflecting a little on the phenomenology of what I’ll call totality qualia, we can undermine the intuitive case for (1).\nTweedledee is facing a perfectly symmetrical scene. His visual field is symmetric, with two gentle mountains rising to his left and his right and a symmetric plain in between them. All he can hear are two birds singing in perfect harmony, one behind his left ear and one behind his right ear. The smells of the field seem to envelope him rather than coming from any particular direction. There is a cool breeze blowing directly on his face. It’s a rather pleasant scene, and the overwhelming feeling is one of symmetry.\nTweedledum is very much like Tweedledee. Indeed, Tweedledum contains a duplicate of Tweedledee as a proper part. But Tweedledum also has some sensors in his skin, and brain cells in what corresponds to a suspiciously empty part of Tweedledee’s brain, that allow him to detect, and feel, where the magnetic fields are in the vicinity. And sadly, though Tweedledum is facing a duplicate of the scene facing Tweedledee, there is a major disturbance in the magnetic field just to Tweedledum’s left. This produces a jarring sensation in Tweedledum’s left side. As a consequence, Tweedledum does not share Tweedledee’s feeling of symmetry.\nWhether a picture is symmetric is a property of its internal features, but it is also a feature that can be destroyed without changing the internal features by just adding more material to one side. It is a totality property of pictures, a property the picture has because it stops just where it does.4 Similarly, totality qualia are qualia that we have in part because we don’t have any more feelings than we actually do. Feelings of symmetry are totality qualia in this sense, as are many of the feelings of calm and peacefulness associated with Tweedledee’s state. It is not intuitive that totality qualia should be intrinsic to a region. Indeed, it seems intuitive that a duplicate of me that was extended to produce more sensory features would lack these feelings. Hence a duplicate of me would not share my conscious life in all respects, so Hawthorne’s premise (1) is also false. To be sure, these totality qualia are a somewhat speculative suggestion, but the Humean does not need them since Hawthorne’s anti-Humean argument is invalid.\n\n\nHall, Ned. 2004. “Causation and the Price of Transitivity.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 181–203. Cambridge: MIT Press.\n\n\nHawthorne, John. 2002. “Deeply Contingent a Priori Knowledge.” Philosophy and Phenomenological Research 65 (2): 247–69. https://doi.org/10.1111/j.1933-1592.2002.tb00201.x.\n\n\n———. 2004. “Humeans Are Out of Their Minds.” Noûs 38 (2): 351–58. https://doi.org/10.1111/j.1468-0068.2004.00473.x.\n\n\nHumberstone, I. L. 1996. “Intrinsic/Extrinsic.” Synthese 108 (2): 205–67. https://doi.org/10.1007/bf00413498.\n\n\nLewis, David. 1973. Counterfactuals. Oxford: Blackwell Publishers.\n\n\n———. 1979. “Counterfactual Dependence and Time’s Arrow.” Noûs 13 (4): 455–76. https://doi.org/10.2307/2215339.\n\n\n———. 1994. “Humean Supervenience Debugged.” Mind 103 (412): 473–90. https://doi.org/10.1093/mind/103.412.473.\n\n\n———. 2004a. “Causation as Influence.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 75–106. Cambridge: MIT Press.\n\n\n———. 2004b. “Void and Object.” In Causation and Counterfactuals, edited by John Collins, Ned Hall, and L. A. Paul, 277–90. Cambridge: MIT Press.\n\n\nLycan, William. 2001. “The Case for Phenomenal Externalism.” Philosophical Perspectives 15: 17–35. https://doi.org/10.1111/0029-4624.35.s15.2.\n\n\nMcGrath, Sarah. 2005. “Causation by Omission: A Dilemma.” Philosophical Studies 123 (1-2): 125–48. https://doi.org/10.1007/s11098-004-5216-z.\n\n\nMenzies, Peter. 1996. “Probabilistic Causation and the Pre-Emption Problem.” Mind 105 (417): 85–117. https://doi.org/10.1093/mind/105.417.85.\n\n\n———. 1999. “Intrinsic Versus Extrinsic Conceptions of Causation.” In Causation and Laws of Nature, edited by Howard Sankey, 313–29. Dordrecht: Kluwer.\n\n\nSider, Theodore. 2001. “Maximality and Intrinsic Properties.” Philosophy and Phenomenological Research 63 (2): 357–64. https://doi.org/10.1111/j.1933-1592.2001.tb00109.x.\n\n\n———. 2003. “Maximality and Microphysical Supervenience.” Philosophy and Phenomenological Research 66 (1): 139–49. https://doi.org/10.1111/j.1933-1592.2003.tb00247.x.\n\n\nWeatherson, Brian. 2005. “Scepticism, Rationalism and Externalism.” Oxford Studies in Epistemology 1: 311–31.\n\n\nThis move requires the not wholly uncontroversial assumption that regions are the kinds of things that can have properties. But I’ll happily make that assumption here. Note that the formulation here allows that the property denoted might be intrinsic for some c and e and extrinsic for others. I’ll say causation is extrinsic if the property denoted is extrinsic for some choice of c and e, even if it is intrinsic for others, as it might be if, for example, no region could possess the property because c is a part of e.↩︎\nThere is a potential complication here in that arguably in some such worlds, e.g. worlds where there is another planet on the opposite side of the sun to duplicate-earth where people are immediately ‘resurrected’ when the ‘die’ on duplicate-earth. In such a world you might say that duplicate-Lincoln doesn’t really die on duplicate-Earth, but merely has the duplicate-earth part of his life ended. We’ll understand ‘dying’ in such a way that this counts as dying.↩︎\nI assume here that events can be properly said to have locations. Spelling out this assumption in detail will require some serious metaphysics, particularly when it comes to omissions. Where exactly does my omission to stop the Iraq War take place? Here at my kitchen table where I am? In Iraq, where the war is? In Washington, if that’s where I’d be were I doing something to stop the war? These questions are hard, though not so hard that we should give up on the very natural idea that events have locations.↩︎\nTed Sider (2001, 2003) stresses the importance to a theory of intrinsicness of properties that are instantiated in virtue of the object not bearing relations to other objects. My example here is closely modeled on examples from his papers.\n\n↩︎\n",
    "preview": "posts/2021-01-05-humeans-arent-out-of-their-minds/mind.jpg",
    "last_modified": "2021-02-05T15:23:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-12-questioning-contextualism/",
    "title": "Questioning Contextualism",
    "description": "This chapter argues against the pragmatism that shows that there is a striking disanalogy between the behavior of “knows” in questions and the behavior of “knows” in terms. The chapter discusses that different people may have different standards for knowledge, so perhaps they may mean different things by a statement, because they communicate to meet their preferred standards for knowledge. Standards for knowledge are not the kind of thing that people can differ on without making a mistake, in the way one would say that different people can have different immediate goals (about what to have for dinner) without making a mistake. This explains that one does not just adopt questioners' standards for knowledge while answering their knowledge questions. So, it is argued that questions involving “knows” should have the three properties—speaker, clarification, and different answers. However, the counter argument is that the proposition can be true relative to some contexts and false relative to other contexts, just as temporality about propositions that a proposition can be true at some times and false at other times, and the utterance is true if the proposition is true only in the context of the utterance.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2006-07-31",
    "categories": [
      "language",
      "epistemology",
      "contextualism"
    ],
    "contents": "\n\nContents\nBasic Indexicals in Questions\nOther Type B terms in Questions\nQuestions about Knowledge\nObjections and Replies\n\nThere are currently a dizzying variety of theories on the market holding that whether an utterance of the form A knows that p is true depends on pragmatic or contextual factors. Even if we allow that pragmatics matters, there are three questions to be answered. First, whose interests matter? Here there are three options: the interests of A matter, the interests of the person making the knowledge ascription matter, or the interests of the person evaluating the ascription matter. Second, which kind of pragmatic factors matter? Broadly speaking, the debate here is about whether practical interests (the stakes involved) or intellectual interests (which propositions are being considered) are most important. Third, how do pragmatic factors matter? Here there is not even a consensus about what the options are.\nPublished in Aspects of Knowing, edited by Stephen Hetherington, Elsevier, 133-147.\nPicture via Järnvägsmuseet.\nThis paper is about the first question. I’m going to present some data from the behaviour of questions about who knows what that show it is not the interests of the person making the knowledge ascription that matter. This means the view normally known as contextualism about knowledge-ascriptions is false. Since that term is a little contested, and for some suggests merely the view that someone’s context matters, I’ll introduce three different terms for the three answers to the first question.\nConsider a token utterance by B of A knows that p. This utterance is being evaluated by C. A semantic pragmatist about knowledge ascriptions says that whether C can correctly evaluate that utterance as true or false depends on some salient party’s context or interests.1 We can produce a quick taxonomy of semantic pragmatist positions by looking at which of the three parties is the salient one.\nType A pragmatism\nA’s context or interests matter.\n\nType B pragmatism\nB’s context or interests matter.\n\nType C pragmatism\nC’s context or interests matter.\n\nType A pragmatism is defended by Hawthorne (2004) under the name ‘subject-sensitive invariantism’ and Stanley (2005) under the name ‘interest-relative invariantism.’ The theory commonly known as contextualism, defended by Cohen (1986), DeRose (1995) and Lewis (1996), is a form of Type B pragmatism. On their theory, the semantic value of ‘knows’ depends on the context in which it is uttered, i.e. B’s context. (And the salient feature of B’s context is, broadly speaking, B’s interests.) Recently MacFarlane (2009) has outlined a very different variant of Type B pragmatism that we will discuss below. Type C pragmatism is the radical view that the token utterance does not have a context-invariant truth value, so one and the same utterance can be properly evaluated as true by one evaluator and false by another. This position is outlined, and defended, by MacFarlane (2005).\nThe purpose of this paper is to argue against Type B pragmatism. I’ll show that there is a striking disanalogy between the behaviour of ‘knows’ in questions and the behaviour of terms for which a Type B-type theory is true. The best explanation for this disanalogy is that Type B theories, contextualism included, are false.2\nI won’t be addressing the second or third questions here, but I’ll assume (basically for ease of exposition) that the right answer to the second has more to do with practical than intellectual interests. So a sceptical possibility becomes relevant not because anyone is actively considering it, but because it makes a difference to someone’s actions. Everything I say here should be restatable given any answer to that question, so little beyond exposition turns on this.\nBasic Indexicals in Questions\nAs well as considering these three pragmatic theories about ‘know,’ we can consider similar theories about other words. We’ll start with words that are universally considered to be indexicals. Consider the following three theories about ‘here.’\nType A pragmatism\nA token of ‘here’ denotes the location of the subject of the sentence in which it appears.\n\nType B pragmatism\nA token of ‘here’ denotes the location of the speaker.\n\nType C pragmatism\nA token of ‘here’ denotes the location of the person evaluating the sentence.\n\nSo when C evaluates B’s utterance A is here, the Type A pragmatist says that it gets evaluated as true iff A is where A is, the Type B pragmatist says that it gets evaluated as true iff A is where B is, and the Type C pragmatist says that it gets evaluated as true iff A is where C is. Obviously Type B pragmatism is true in general about ‘here,’ as it is for all obvious indexicals.3 What we’re interested in for now, however, is not its truth but the kind of evidence that can be adduced for it. One very simple way of separating out these three theories involves questions, as in the following example.\nWatson is at a party in Liverpool, and he is worried that Moriarty is there as well. He calls Holmes, who is tracking Moriarty’s movements by satellite, and asks (1).\nThis question has three properties that are distinctive of questions involving indexicals.\nSPEAKER\nHow Holmes should answer the question depends on the speaker’s (i.e. Watson’s) environment, not his own and not Moriarty’s. It would be wrong to say “Yes” because Moriarty is where he is, or “No” because Moriarty is not in the lab with Holmes following the satellite movement.\n\nCLARIFICATION\nIt is permissible in certain contexts to ask the speaker for more information about their context before answering. If Holmes’s is unsure of Watson’s location, he can reply with “Where are you?” rather than answering directly.4\n\nDIFFERENT ANSWERS\nIf two different people, who are in different locations, ask Holmes (1), he can answer differently. Assume that Moriarty is at the party, so Holmes says “Yes” in reply to Watson. Lestrade then calls from Scotland Yard, because he is worried that Moriarty has broken in and asks Holmes (1). Holmes says “No,” and this is consistent with his previous answer.\n\nQuestions involving indexicals have a fourth property that we’ll return to from time to time in what follows.\nNEGATIVE AGREEMENT\nIt is coherent to answer the question by saying “No,” then basically repeating the question, inverting the verb-subject order so you have an indicative sentence. So Holmes could consistently (if falsely in this story) say, “No, Moriarty is here.”\n\nThe truth of Type B Pragmatism about ‘here’ explains, indeed predicts, that (1) has these properties. If a correct answer is a true answer, then Type B Pragmatism directly entails that (1) has SPEAKER. That assumption (that correctness is truth) plus the fact that you can speak to someone without knowing their location implies that (1) has CLARIFICATION, and adding the fact that different people can be in different locations implies that (1) has DIFFERENT ANSWERS. Whether Holmes can say Moriarty is here depends whether he can truthfully answer his own question Is Moriarty here? so NEGATIVE AGREEMENT is related to DIFFERENT ANSWERS. For these reasons it isn’t surprising that other terms that are agreed on all sides to be indexicals generate questions that have all four properties. For instance, because ‘me’ is an indexical, (2) has all four properties.\nThis suggests a hypothesis, that all terms for which Type B pragmatism is true generate questions with those four properties. This hypothesis seems to be false; some terms for which Type B pragmatism is true do not generate questions that have NEGATIVE AGREEMENT. But a weaker claim, that all terms for which Type B pragmatism is true generate questions with the first three properties, does seem to be true.5 Or so I will argue in the next section.\nOther Type B terms in Questions\nVarious philosophers have argued that a version of Type B pragmatism is true for each of the following four terms: ‘tall,’ ‘empty,’ ‘ready’ and ‘everyone.’ Various defenders of Type B pragmatism about ‘knows’ have argued that ‘knows’ is analogous to terms on this list. In this section I’ll argue that questions involving those four terms have the first three properties of questions listed above, though they don’t seem to have NEGATIVE AGREEMENT. In the next section I’ll argue that questions involving ‘know’ do not have those three properties. These facts combine to form an argument against the hypothesis that Type B pragmatism is true of ‘knows.’ (The argument here does not turn on supposing that Type B pragmatism is true of these four terms. All I want to argue for is the claim that any term for which Type B pragmatism is true generates questions with the first three properties. It is possible that other terms also generate these kinds of questions, but this possibility doesn’t threaten the argument.) The examples involving these four terms will be a little complicated, but the intuitions about them are clear.\nMoriarty has hired a new lackey, a twelve year old jockey. She is tall for a twelve year old girl, and tall for a jockey, but not tall for a person. Moriarty is most interested in her abilities as a jockey, so he worries that she’s tall. Holmes is also most interested in her qua jockey. Watson has noted that Moriarty never hires people who are tall for adults (he thinks this is because Moriarty likes lording over his lackeys) and is wondering whether the new hire fits this property. He asks Holmes (3).\nHolmes should say “No.” What matters is whether she is tall by the standards Watson cares about, not whether she is tall by the standards Holmes cares about (i.e. jockeys) or the standards Moriarty cares about (i.e. also jockeys). So (3) has SPEAKER. Lestrade wants information from Holmes about the lackey so his men can pick her up. They have this conversation.\n\nLestrade: Is Moriarty’s new lackey tall? My men are looking for her.\nHolmes: Where are they looking?\nLestrade: At her school.\nHolmes: Yes, she looks like she could be fourteen or fifteen.\n\nHolmes quite properly asks for a clarification of the question so as to work out what standards for tallness are in play, so (3) has CLARIFICATION. And he properly gives different answers to Watson and Lestrade, so it has DIFFERENT ANSWERS. But note that ‘tall’ doesn’t have NEGATIVE AGREEMENT. Holmes can’t answer (3) with “No, she’s tall.” I won’t repeat this observation for the other terms discussed in this section, but the point generalises.\nNext we’ll consider ‘empty.’ Holmes and Watson are stalking Moriarty at a party. Watson is mixing poisons, and Holmes is trying to slip the poison into Moriarty’s glass. Unfortunately Moriarty has just about finished his drink, and might be about to abandon it. Watson is absent-mindedly trying to concoct the next poison, but he seems to have run out of mixing dishes.\n\nWatson: Is Moriarty’s glass empty?\nHolmes: What do you want it for?\nWatson: I need something dry to mix this poison in.\nHolmes: No, it’s got a small bit of ice left in it.\n(Lestrade arrives, and sees Holmes holding a vial.)\nLestrade: Why haven’t you moved in? Is Moriarty’s glass empty?\nHolmes: Yes. He should get another soon.\n\nHolmes behaves entirely appropriately here, and his three responses show that the question Is Moriarty’s glass empty? has the CLARIFICATION, SPEAKER and DIFFERENT ANSWERS properties respectively. Note in particular that the glass is empty by the standards that matter to Moriarty and Holmes (i.e. it’s got not much more than ice left in it) doesn’t matter to how Holmes should answer until someone with the same interests, Lestrade, asks about the glass.\nThird, we’ll look at ‘ready.’ Moriarty is planning four things: to rob the Bank of England, to invade Baker St and kill Holmes, to invade Scotland Yard to free his friends, and to leave for a meeting of criminals where they will plan for the three missions. Moriarty cares most about the first plan, Holmes about the second, and Lestrade about the third, but right now Watson cares most about the fourth because it’s his job to track Moriarty to the meeting. Holmes is tracking Moriarty through an installed spycam.\n\nWatson: Is Moriarty ready?\nHolmes: Yes. You should go now.\n(Watson departs and Lestrade arrives)\nLestrade: Is Moriarty ready?\nHolmes: Who’s that? Oh, hello Inspector. No, he still has to plan the attack out.\n\nAgain, Holmes’s answers show that the question Is Moriarty ready? has the CLARIFICATION, SPEAKER and DIFFERENT ANSWERS properties. Note that in this case the issue of whether Moriarty is ready for the thing he cares most about, and the issue of whether he is ready for the thing Holmes cares most about, are not relevant to Holmes’s answers. It is the interests of the different speakers that matter, which suggests that if one of the three types of pragmatism is true about ‘ready,’ it is Type B pragmatism.\nFinally we’ll look at ‘everyone.’ Lewis (1996) suggests that ‘know’ is directly analogous to ‘every,’ so the two words should behave the same way in questions. Moriarty’s gang just robbed a department store while the royal family, along with many police, were there. Holmes is most interested in how this affected the royals, Watson in how the public reacted, Lestrade in how his police reacted, and Moriarty merely in his men and his gold. The public, and the royals, were terrified by the raid on the store, but the police reacted bravely.\n\nWatson: Did Moriarty’s men terrify everyone?\nHolmes: Her majesty and her party were quite shocked. Oh, you mean everyone. Yes, the masses there were completely stunned.\n(Lestrade enters.)\nLestrade: I just heard about the raid. How did they get through security? Did Moriarty’s men terrify everyone?\nHolmes: No, your men did their job, but they were outnumbered.\n\nAgain, Holmes’s answers show that the question Did Moriarty’s men terrify everyone? has the CLARIFICATION, SPEAKER and DIFFERENT ANSWERS properties. And again, what the quantifier domain would be if Holmes were to use the word ‘everyone,’ namely all the royal family, is irrelevant to how he should answer a question involving ‘everyone.’ That’s the distinctive feature of expressions for which Type B pragmatism is true, and it suggests that in this respect at least ‘everyone’ behaves as if Type B pragmatism is true of it.\nQuestions about Knowledge\nWe have two reasons for thinking that if Type B pragmatism is true about ‘knows,’ then questions about knowledge should have all the SPEAKER, CLARIFICATION and DIFFERENT ANSWERS properties. First, the assumption that correct answers are true answers plus trivial facts about the environment (namely that environments are not always fully known and differ between speakers) implies that the questions have these properties. Second, many words that are either uncontroversial or controversial instances where Type B pragmatism is true generate questions with these properties. So if ‘knows’ is meant to be analogous to these controversial examples, questions about knowledge should have these properties. I’ll argue in this section that knowledge questions do not have these properties. Again we’ll work through a long example to show this.\nLast week Watson discovered where Moriarty was storing a large amount of gold, and retrieved it. Moriarty is now coming to Baker St to try to get the gold back, and Holmes is planning a trap for him. Moriarty has made educated guesses that it was Watson (rather than Holmes) who retrieved the gold, and that Holmes is planning a trap at Baker St Station. But he doesn’t have a lot of evidence for either proposition. Holmes has been spying on Moriarty, so he knows that Moriarty is in just this position. Neither Moriarty nor Holmes care much about who it was who retrieved the gold from Moriarty’s vault, but this is very important to Watson, who plans to write a book about it. On the other hand, that Holmes is planning a trap at Baker St Station is very important to both Holmes and Moriarty, but surprisingly unimportant to Watson. He would prefer that he was the hero of the week for recovering the gold, not Holmes for capturing Moriarty. They have this conversation.\n\nWatson: Does Moriarty know that you’ve got a trap set up at Baker St Station?\nHolmes: No, he’s just guessing. If I set up a diversion I’m sure I can get him to change his mind.\nWatson: Does he know it was me who recovered the gold?\nHolmes: Yes, dear Watson, he figured that out.\n\nThese answers sound to me like the answers Holmes should give. Because the trap is practically important to both him and Moriarty, it seems he should say no to the first question unless Moriarty has very strong evidence.6 But because it is unimportant to Holmes and Moriarty just what Watson did, the fact that Moriarty has a true belief that’s based on the right evidence that Watson recovered the gold is sufficient for Holmes to answer the second question “Yes.” This shows that questions involving ‘knows’ do not have the SPEAKER property.\nSome might dispute the intuitions involved here, but note that on a simple Type B pragmatist theory, one that sets the context just by the speaker’s interests (as opposed to the speaker and her interlocutors) Holmes should assent to (4) and (5).\nAnd it is very intuitive that if he should assent to (4) and (5), then he should answer “No” to Watson’s first question, and “Yes” to the second.\nSome adherents of a more sophisticated Type B pragmatist theory will not say that Holmes should assent to these two, because they think he should take Watson’s interests on board in his use of ‘knows.’ This is the ‘single scoreboard view’ of Keith DeRose (2004).7 But we can, with a small addition of complexity rearrange the case so as to avoid this complication. Imagine that Watson is not talking to Holmes, but to Lestrade, and that Lestrade (surprisingly) shares Watson’s interests. Unbeknownst to the two of them, Holmes is listening in to their conversation via a bug. When listening in to conversations, Holmes has the habit of answering any questions that are asked, even if they obviously aren’t addressed to him. (I do the same thing when watching sports broadcasts, though the questions are often mind-numbingly bland. Listening to the intuitive answers one gives is a good guide to the content of the question.) So the conversation now goes as follows.\n\nWatson: Does Moriarty know that Holmes has got a trap set up at Baker St Station?\nHolmes (eavesdropping): No, he’s just guessing. If I set up a diversion I’m sure I can get him to change his mind.\nLestrade: I’m not sure. My Moriarty spies aren’t doing that well.\nWatson: Does he know it was me who recovered the gold?\nHolmes (eavesdropping): Yes, dear Watson, he figured that out.\n\nIt is important here that Holmes is talking to himself, even though he is using Watson’s questions to guide what he says. So it will be a very extended sense of context if somehow Watson’s interests guide Holmes’s context. Some speakers may be moved by empathy to align their interests with the people they are speaking about, but Holmes is not such a speaker. So Type B pragmatic theories should say that Holmes should endorse (4) and (5) in this context, and intuitively if this is true he should speak in the above interaction just as I have recorded. But this is just to say that questions about knowledge do not have SPEAKER.8\nOne might worry that we’re changing the rules, since we did not use eavesdropping situations above in arguing that if Type B pragmatism is true of ‘knows,’ then questions about knowledge should have SPEAKER. But it is a simple exercise to check that even if Holmes is eavesdropping, his answers to questions including ‘tall,’ ‘empty,’ ‘ready’ and ‘everyone’ should still have the three properties. So this change of scenery does not tilt the deck against the Type B pragmatist.\nCases like this one also suggest that questions involving ‘knows’ also lack the CLARIFICATION and DIFFERENT ANSWERS property. It would be odd of Holmes to reply to one of Watson’s questions with “How much does it matter to you?” as it would be in any case where a questioner who knows that p asks whether S also knows that p. Intuitions about cases where the questioner does not know that p are a little trickier, because then the respondent can only answer ‘yes’ if she has sufficient evidence to assure the speaker that p, and it is quite plausible that the amount of evidence needed to assure someone that p varies with the interests of the person being assured.9 But if Type B pragmatism were true, we’d expect to find cases of CLARIFICATION where it is common ground that the speaker knows p, and yet a request for standards is in order, and no such cases seem to exist. Similarly, it’s hard to imagine circumstances where Holmes would offer a different answer if Lestrade rather than Watson asked the questions. And more generally, if two questioners who each know that p ask whether S knows that p, it is hard to see how it could be apt to answer them differently.10 But if Type B pragmatism about ‘knows’ were true, knowledge questions would have these three properties, so it follows that Type B pragmatism about ‘knows’ is false.\nObjections and Replies\nObjection: The argument that Type B pragmatism implies that questions involving ‘knows’ should have the three properties assumes that correct answers are true answers. But there are good Gricean reasons to think that there are other standards for correctness.\nReply: It is true that one of the arguments for thinking that Type B pragmatism has this implication uses this assumption. But the other argument, the argument from analogy with indexicals and other terms for which Type B pragmatism is true, does not. Whatever one thinks of the theory, there is data here showing that ‘knows’ does not behave in questions like other context-sensitive terms for which Type B pragmatism is the most plausible view.\nMoreover, Type B pragmatists have to be very careful wielding this objection. If there is a substantial gap between correct answers to knowledge questions and true answers, then it is likely that there is a substantial gap between correct knowledge ascriptions and true knowledge ascriptions. As a general (though not universal) rule, truth and correctness are more tightly connected for questions than for simple statements. For example, some utterances do not generate all the scalar implicatures as answers to questions that they typically generate when asserted unprompted.11 But the primary ‘ordinary language’ argument for Type B pragmatism about knowledge ascriptions assumes that correct knowledge ascriptions are, by and large, true.\nWe can put this in more theoretical terms. If we are to use these considerations to generate a rebutting defeater for Type B pragmatism, we would need an argument that Holmes’s answers are correct iff they are true. And while that step of the argument is plausible, it is not beyond contention. But that isn’t the only use of the examples. We can also use them to undercut the argument from ordinary language to Type B pragmatism. We have a wide range of cases where ordinary usage is as if Type B pragmatism is false. And these cases are not peripheral or obscure features of ordinary usage. Answering questions is one of the most common things we do with language. So ordinary usage doesn’t provide an all things considered reason to believe that Type B pragmatism is true. If ordinary usage is (or at least was) the best reason to believe Type B pragmatism, it follows that there is no good reason to believe Type B pragmatism.\nObjection: Sometimes when we ask Does S know that p? all we want to know is whether S has the information that p. In this mood, questions of justification are not relevant. But Type B pragmatism is a theory about the interaction between the subject’s justification and knowledge ascriptions. So these questions are irrelevant to evaluating indexicalism.12\nReply: It is plausible that there is this use of knowledge questions. It seems to me that this is a usage that needs to be explained, and isn’t easily explained on current theories of knowledge.13 But I’ll leave discussion of that use of knowledge questions for another day. For now I’ll just note that even if this can be an explanation of why we sometimes assent to knowledge questions, it can’t be an explanation of why Holmes denies that Moriarty knows about the planned trap at Baker St Station. Holmes agrees that Moriarty believes there is a trap planned, but insists that because Moriarty is ‘just guessing’ that this belief does not amount to knowledge. What really needs explaining is the difference between Holmes’s two answers, and this other use of knowledge questions doesn’t seem sufficient to generate that explanation.\nObjection: There is no explanation offered here for the data, and we shouldn’t give up an explanatory theory without an explanation of why it fails.\nReply: The best way to explain the data is to look at some differences in our attitudes towards questions containing ‘knows’ and questions containing other terms for which Type B pragmatism is plausibly true. I’ll just go over the difference between ‘knows’ and ‘ready,’ the point I’m making easily generalises to the other cases.\nDifferent people may be concerned with different bits of preparation, so they may (speaker) mean different things by X is ready. But neither will regard the other as making a mistake when they focus on a particular bit of preparing to talk about by saying X is ready. And this is true even if they think that the person should be thinking about something else that X is preparing.\nKnowledge cases are not like that. Different people may have different standards for knowledge, so perhaps they may (speaker) mean different things by A knows that p, because they will communicate that A has met their preferred standards for knowledge. But in these cases, each will regard the other as making a mistake. Standards for knowledge aren’t the kind of thing we say people can differ on without making a mistake, in the way we do (within reason) say that different people can have different immediate goals (e.g. about what to have for dinner) without making a mistake. That explains why we don’t just adopt our questioners standards for knowledge when answering their knowledge questions.\nObjection: In section 2 it was argued that questions involving ‘knows’ should have the three properties because questions involving other terms for which Type B pragmatism is true have the properties. But this argument by analogy may be flawed. All those terms are ‘indexical’ in John MacFarlane’s sense (MacFarlane 2009). That is, the content of any utterance of them varies with the context. But not all Type B pragmatist theories are indexical, and this argument does not tell against a non-indexical Type B pragmatism.\nReply: The view under consideration says that all utterances of ‘S knows that p’ express the same proposition, namely that S knows that p. But the view is Type B because it says that proposition can be true relative to some contexts and false relative to other contexts, just as temporalists about propositions say that a proposition can be true at some times and false at other times, and the utterance is true iff the proposition is true in the context of the utterance. This is a Type B view, and I agree that the argument by analogy in section 2 is powerless against it.\nBut that argument was not the only argument that Type B views are committed to questions involving ‘knows’ having the three properties. There was also an argument, at the end of section 1, from purely theoretical considerations. It turns out this argument is also complicated to apply here, but it does tell against this type of Type B pragmatist as well.\nHere are two hypotheses about how one should answer a question Does NP VP? where NP is a noun phrase and VP a verb phrase. Consider the proposition expressed by the sentence NP VP in the speaker’s context. First hypothesis: the correct answer is ‘Yes’ iff that proposition is true in the speaker’s context. Second hypothesis: the correct answer is ‘Yes’ iff that proposition is true in the respondent’s context. If propositions do not change their truth value between contexts, these two hypotheses are equivalent, but on this version of Type B pragmatism that is not so, so we have to decide between them. The best way to do this is by thinking about cases where it is common ground that propositions change their truth value between contexts, namely cases where the contexts are possible worlds. Let’s consider the scenario I briefly discussed above of the television watcher answering whatever question gets asked. Above I put the questioners in the same possible world as me, but we can imagine they are fictional characters in a different possible world. Imagine a show where it has just been revealed to the audience, but not all the characters, that the Prime Minister is a space alien. The following happens.\n\nCharacter (on screen): Is the Prime Minister a space alien?\nMe (in real world): Yes!\n\nSince the proposition that the Prime Minister is a space alien is true in their world, but not in our world, the propriety of this response tells in favour of the first hypothesis above. Now this is not a conclusive argument, because it might be that there is some distinctive feature of fiction that is causing this answer, even though the second hypothesis is in general correct. But it seems we have reason to think that if this kind of Type B pragmatism were true, respondents would use the speaker’s context to work out what kind of answer to give. And as we saw above, this is not what respondents actually do, they either use their own context or (more likely) the subject’s.\nObjection: At most this shows that Type B pragmatism about ‘knows’ is not actually true of English. But there might still be good epistemological reasons to adopt it as a philosophically motivated revision, even if the data from questions shows it isn’t true. So even if hermeneutic Type B pragmatism is false, revolutionary Type B pragmatism might be well motivated.\nReply: It’s true that the philosophically most interesting concepts may not map exactly on to the meanings of words in natural language. (Though I think we should be careful before abandoning the concepts that have proven useful enough to get simple representation in the language.) And it’s true that there are reasons for having epistemological concepts that are sensitive to pragmatic factors. But what is hard to see is what interest we could have in having epistemological terms whose application is sensitive to the interests of the person using them. Hawthorne (2004) provides many reasons for thinking that terms whose applications are sensitive to the interests of the person to whom they are being applied are philosophically and epistemologically valuable. Such terms provide ways of expressing unified judgements about the person’s intellectual and practical reasoning. On the other hand, there is little to be gained by adopting Type B pragmatism. If there needs to be a revision around here, and my guess is that there does not, it should be towards Type A, not Type B, pragmatism.\n\n\nCappelen, Herman, and Ernest Lepore. 2005. Insensitive Semantics: A Defence of Semantic Minimalism and Speech Act Pluralism. Oxford: Blackwell.\n\n\nCohen, Stewart. 1986. “Knowledge and Context.” The Journal of Philosophy 83 (10): 574–83. https://doi.org/10.2307/2026434.\n\n\nDeRose, Keith. 1995. “Solving the Skeptical Problem.” Philosophical Review 104 (1): 1–52. https://doi.org/10.2307/2186011.\n\n\n———. 2004. “Single Scoreboard Semantics.” Philosophical Studies 119 (1/2): 1–21. https://doi.org/10.1023/b:phil.0000029347.01790.f2.\n\n\nEgan, Andy, John Hawthorne, and Brian Weatherson. 2005. “Epistemic Modals in Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 131–70. Oxford: Oxford University Press.\n\n\nGoldman, Alvin. 2002. “What Is Social Epistemology? A Smorgasbord of Projects.” In Pathways to Knowledge: Private and Public, by Alvin Goldman, 182–204. Oxford: Oxford University Press.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHetherington, Stephen. 2001. Good Knowledge, Bad Knowledge: On Two Dogmas of Epistemology. Oxford: Oxford University Press.\n\n\nLewis, David. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nMacFarlane, John. 2005. “The Assessment Sensitivity of Knowledge Attributions.” Oxford Studies in Epistemology 1: 197–233.\n\n\n———. 2009. “Nonindexical Contextualism.” Synthese 166 (2): 231–50. https://doi.org/10.1007/s11229-007-9286-2.\n\n\nStanley, Jason. 2000. “Context and Logical Form.” Linguistics and Philosophy 23 (4): 391–434. https://doi.org/10.1023/A:1005599312747.\n\n\n———. 2002. “Nominal Restriction.” In Logical Form and Language, edited by Georg Peter and Gerhard Preyer, 365–88. Oxford: Oxford University Press.\n\n\n———. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWeatherson, Brian. 2002. “Misleading Indexicals.” Analysis 62 (4): 308–10. https://doi.org/10.1093/analys/62.4.308.\n\n\n———. 2005. “Can We Do Without Pragmatic Encroachment?” Philosophical Perspectives 19 (1): 417–43. https://doi.org/10.1111/j.1520-8583.2005.00068.x.\n\n\nNote that the focus here is on the truth or falsity of the utterance, and not on the truth or falsity of the proposition the utterance expresses. I’m indebted to John MacFarlane for stressing to me the importance of this distinction.↩︎\nMy own view is that if any pragmatic theory is true, it is Type A pragmatism. In Weatherson (2005) I defend the view that whether an agent is sufficiently confident in p to count as believing that p depends on features of her context. In contexts where it is very important for practical deliberation whether p is true, a degree of confidence that might ordinarily count as belief that p might no longer so count. This is an odd kind of doxastic externalism, namely the view that whether a state amounts to a belief is environment-dependent. Now to know that p also requires having a certain level of confidence that p is true, and it is arguable (though I haven’t argued it) that this level is also dependent on the would be knowers environment. It is also arguable that the best theory of epistemic defeaters will contain pragmatic features.\nThough I describe Type C pragmatism as radical, I’ve been convinced by John MacFarlane’s work that it is a perfectly coherent doctrine. In Egan, Hawthorne, and Weatherson (2005) we defend a version of Type C pragmatism for sentences of the form A might be F, where might is understood epistemically.↩︎\nWhether the ‘in general’ is strictly necessary turns on tricky considerations about non-standard usages, such as answering machines. I’m generally favourable to the view that if any qualification is needed it is a very small one. See Weatherson (2002) for more discussion of this point.↩︎\nIt is important here that we restrict attention to clarificatory questions about the context, and in particular to features of the context that (allegedly) affect the truth value of utterances involving the indexical. Speakers can always ask for clarification of all sorts of features of a question, but a question only has CLARIFICATION if it is appropriate to ask for clarifying information about the nature of the questioner’s context.↩︎\nThe failure of NEGATIVE AGREEMENT for questions involving quantifiers, comparative adjectives and similar terms is interesting is interesting, and is something that a full semantic theory should account for. My feeling is that the best explanation will draw on the resources of a theory like that defended by Stanley (2000, 2002) but it would take us far away from our primary purpose to explore this here. It is certainly an argument in favour of the ‘semantic minimalism’ defended by Cappelen and Lepore (2005) that all and only the terms in their ‘basic set’ (apart from tense markers), i.e. the obvious indexicals, generate questions with NEGATIVE AGREEMENT. I think the fact that many terms generate questions with the SPEAKER, CLARIFICATION and DIFFERENT ANSWERS property tells more strongly against their view.↩︎\nThe Type A pragmatist says that it is the importance of the trap to Moriarty that makes this a high-stakes question, and the Type C pragmatist (at least as I understand that view) says that it is the importance of the trap to Holmes that matters. Since Moriarty and Holmes agree about what is important here, the Type A and Type C pragmatists can agree with each other and disagree with the Type B pragmatist.↩︎\nThe failure of NEGATIVE AGREEMENT for knowledge questions suggests that if any contextualist theory is true, it had better be a single scoreboard view.↩︎\nAs noted above, the Type A and Type C pragmatists agree with this intuition, though for very different reasons.↩︎\nWith the right supplementary assumptions, I believe this claim can be argued to be a consequence of any of our three types of pragmatism.↩︎\nThe above remarks about cases where the speakers don’t know that p also apply. I think that without a very careful story about how the norms governing assertion relate to the interests of the speaker and the audience, such cases will not tell us much about the semantics of ‘knows.’ Best then to stick with cases where it is common ground that everyone, except the subject, knows that p.↩︎\nHere are a couple of illustrations of this point. Yao Ming is seven feet six inches tall. It would be odd (at best) to use (6) to describe him, but it is clearly improper to answer (7) with ‘No.’\nFor a second case, consider an example of Hart’s that Grice (1989) uses to motivate his distinction between semantics and pragmatics. A motorist drives slowly down the street, pausing at every driveway to see if anyone or anything is rushing out. It seems extremely odd to use (8) to describe him.\nIs this oddity due to (8) being false, or it being otherwise infelicitous? Part of the argument that (8) is true, but infelicitous, is that intuitively it is correct to say ‘Yes’ in response to (9), and incorrect to say ‘No.’\nThe implicatures associated with (8) are largely absent from affirmative answers to (9), though such an answer presumably has the same truth-conditional content as (8).↩︎\nA similar view is defended by Alvin Goldman (2002).↩︎\nOne explanation, due to Stephen Hetherington (2001), is that all that is ever required for a true ascription of knowledge that p to S is that p is true and S believe it. This leaves open the question, as discussed below, of why we sometimes deny that true believers are knowers, but we can bring out familiar Gricean explanations about why we might usefully deny something that is strictly speaking true, or we could follow Hetherington and offer an explanation in terms of the gradability of knowledge claims. Another possible explanation is that ‘know’ univocally means something like what most epistemologists say that it means, but there is a good pragmatic story about why speakers sometimes attribute knowledge to those with merely true belief. (I don’t know how such an explanation would go, especially if Type B pragmatism is not true.) Finally, we might follow Goldman and say the English word ‘know’ is ambiguous between a weak and strong reading, and the strong reading has been what has concerned epistemologists, and the weak reading just requires true belief. Such an ambiguity would be a small concession to Type B pragmatism (since it is the interests of the speaker that resolve ambiguities) but we could still sensibly ask whether Type B pragmatism is true of the strong reading.\n\n↩︎\n",
    "preview": "posts/2021-03-12-questioning-contextualism/liverpool.jpg",
    "last_modified": "2021-03-12T14:35:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-08-scepticism-rationalism-and-externalism/",
    "title": "Scepticism, Rationalism, and Externalism",
    "description": "I argue that we have to accept one of the three isms in the title. Either inductive scepticism is true, or we have substantial contingent a priori knowledge, or a strongly externalist theory of knowledge is crrect.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2006-02-09",
    "categories": [
      "epistemology",
      "scepticism"
    ],
    "contents": "\n\nContents\nA Sceptical Argument\nDominance Arguments\nHunting the Bad Proposition\nHow Externalism Helps\n\nThis paper is about three of the most prominent debates in modern epistemology. The conclusion is that three prima facie appealing positions in these debates cannot be held simultaneously.\n\nPublished in Oxford Studies in Epistemology 1: 311-31.\nThe first debate is scepticism vs anti-scepticism. My conclusions apply to most kinds of debates between sceptics and their opponents, but I will focus on the inductive sceptic, who claims we cannot come to know what will happen in the future by induction. This is a fairly weak kind of scepticism, and I suspect many philosophers who are generally anti-sceptical are attracted by this kind of scepticism. Still, even this kind of scepticism is quite unintuitive. I’m pretty sure I know (1) on the basis of induction.\n\nThis paper has been presented at Cornell University and the Inland Northwest Philosophy Conference, and each time I received valuable feedback.\nIt will snow in Ithaca next winter.\nAlthough I am taking a very strong version of anti-scepticism to be intuitively true here, the points I make will generalise to most other versions of scepticism. (Focussing on the inductive sceptic avoids some potential complications that I will note as they arise.)\n\nThanks also to David Chalmers, Harold Hodes, Nicholas Sturgeon and, especially, Tamar Szabó Gendler for very helpful comments on various drafts of the paper.\nThe second debate is a version of rationalism vs empiricism. The kind of rationalist I have in mind accepts that some deeply contingent propositions can be known a priori, and the empiricist I have in mind denies this. Kripke showed that there are contingent propositions that can be known a priori. One example is Water is the watery stuff of our acquaintance. (‘Watery’ is David Chalmers’s nice term for the properties of water by which folk identify it.) All the examples Kripke gave are of propositions that are, to use Gareth Evans’s term, deeply necessary (Evans 1979). It is a matter of controversy presently just how to analyse Evans’s concepts of deep necessity and contingency, but most of the controversies are over details that are not important right here. I’ll simply adopt Stephen Yablo’s recent suggestion: a proposition is deeply contingent if it could have turned out to be true, and could have turned out to be false (Yablo 2002)1. Kripke did not provide examples of any deeply contingent propositions knowable a priori, though nothing he showed rules out their existence.\nThe final debate is a version of internalism vs externalism about epistemic justification. The internalist I have in mind endorses a very weak kind of access internalism. Say that a class of properties (intuitively, a determinable) is introspective iff any beliefs an agent has about which property in the class (which determinate) she instantiates are guaranteed to not be too badly mistaken.2 (Since ‘too badly’ is vague, ‘introspective’ will be vague too, but as we’ll see this won’t matter to the main argument.) My internalist believes the following two claims:\nWhich propositions an agent can justifiably believe supervenes in which introspective properties she instantiates, and this is knowable a priori.3\nThere exist some introspective properties and some deeply contingent propositions about the future such that it’s a priori that whoever instantiates those properties can justifiably believe those propositions.\nMy externalist denies one or other of these claims. Typically, she holds that no matter what introspective properties you have, unless some external condition is satisfied (such as the reliability of the connection between instantiating those properties and the world being the way you believe it is) you lack justification. Alternatively, she holds that the connection between introspective properties and justification is always a posteriori. (Or, of course, she might deny both.)\nMy argument will be that the combination of anti-scepticism, empiricism and internalism is untenable. Since there’s quite a bit to be said for each of these claims individually, that their combination is untenable means we are stuck with a fairly hard choice: accept scepticism, or rationalism, or externalism. Of the three, it may seem that externalism is the best, but given how weak the version of internalism is that I’m using, I think we should take the rationalist option seriously.4 In this paper I’ll just argue against the combination of anti-scepticism, empiricism and internalism, and leave it to the reader to judge which of the three to reject.\nVery roughly, the argument for the trilemma will be as follows. There are some propositions q such that these three claims are true.\nIf anti-scepticism is true, then I either know q a priori or a posteriori.\nIf internalism and empiricism is true, I do not know q a priori.5\nIf internalism is true, I do not know q a posteriori.\nMuch of the paper will be spent giving us the resources to find, and state, such a q, but to a first approximation, think of q as being a proposition like I am not a brain-in-a-vat whose experiences are as if they were a normal person.6 The important features of q are that (a) it is entailed by propositions we take ourselves to know, (b) it is possibly false and (c) if something is evidence for it, then any evidence is evidence for it. I will claim that by looking at propositions like this, propositions that say in effect that I am not being misled in a certain way, it is possible to find a value for q such that (2), (3) and (4) are all true. From that it follows that\nFor most of the paper I will assume that internalism and anti-scepticism are true, and use those hypotheses to derive rationalism. The paper will conclude with a detailed look at the role internalism plays in the argument, and this will give us some sense of what an anti-sceptical empiricist externalism may look like.\nA Sceptical Argument\nAmong the many things I know about future, one of the firmest is (1).\nIt will snow in Ithaca next winter.\nI know this on the basis of inductive evidence about the length of meteorological cycles and the recent history of Ithaca in winter. The inductive sceptic now raises the spectre of Winter Wonderland, a kind of world that usually has the same meteorological cycles as ours, and has the same history, but in which it is sunny every day in Ithaca next winter.7 She says that to know (1) we must know that (5) is false, and we do not.\nI am living in Winter Wonderland.\nJust how does reflection (5) affect my confidence that I know (1)? The sceptic might just appeal to the intuition that I don’t know that (5) is false. But I don’t think I have that intuition, and if I do it is much weaker than my intuition that I know (1) and that I can infer (5) from (1). James Pryor (2000, 527–29) has suggested the sceptic is better off using (5) in the following interesting argument.8\nEither you don’t know you’re not living in Winter Wonderland; or, if you do know that, it’s because that knowledge rests in part on your inductive knowledge that it will snow in Ithaca next winter.\nIf you’re to know (1) on the basis of certain experiences or grounds e, then for every q which is “bad” relative to e and (1), you have to be in a position to know q to be false in a non-question-begging way—i.e., you have to be in a position to know q to be false antecedently to knowing that it will snow next winter on the basis of e.\n\\(5\\) is “bad” relative to any course of experience e and (1).\nYou can’t know (1), that it will snow next winter on the basis of your current experiences.\nAn alternative hypothesis q is “bad” in the sense used here iff (to quote Pryor) “it has the special features that characterise the sceptic’s scenarios—whatever those features turn out to be.” (527) To a first approximation, q is bad relative to p and e iff you’re meant to be able to know p on the basis of e, but q is apparently compatible with e, even though it is not compatible with p.\nPryor argues that the best response to the external world sceptic is dogmatism. On this theory you can know p on the basis of e even though you have no prior reason to rule out alternatives to p compatible with e. Pryor only defends the dogmatic response to the external world sceptic, but it’s worth considering the dogmatist response to inductive scepticism. According to this response, I can come to know I’m not in Winter Wonderland on the basis of my experiences to date, even though I didn’t know this a priori. So dogmatism is a version of empiricism, and it endorses (6).9 The false premise in this argument, according to the dogmatist, is (7). We can know it will snow even though the Winter Wonderland hypothesis is bad relative to this conclusion and our actual evidence, and we have no prior way to exclude it.\nPryor notes that the sceptic could offer a similar argument concerning justification, and the dogmatist offers a similar response.\nEither you’re not justified in believing that you’re not in Winter Wonderland; or, if you are justified in believing this, it’s because that justification rests in part on your justified belief that it will snow in Ithaca next winter.\nIf you’re to have justification for believing (1) on the basis of certain experiences or grounds e, then for every q which is “bad” relative to e and (1), you have to have antecedent justification for believing q to be false—justification which doesn’t rest on or presuppose any e-based justification you may have for believing (1).\n\\(5\\) is “bad” relative to any course of experience e you could have and (1).\nYou can’t justifiably believe it will snow in Ithaca next winter on the basis of past experiences.\nThe dogmatist rejects (10), just as she rejects (7). I shall spend most of my time in the next two sections arguing for (10), returning to (7) only at the end. For it seems there are compelling reasons to accept (10), and hold that the problem with this argument is either with (9) or (11).10\nDominance Arguments\nThe primary argument for (10) will turn on a dominance principle: if you will be in a position to justifiably believe p whatever evidence you get, and you know this, then you are now justified in believing p. This kind of reasoning is perfectly familiar in decision theory: if you know that one of n states obtains, and you know that in each of those states you should do X rather than Y, then you know now (or at least you should know) that you should do X rather than Y. This is a very plausible principle, and equivalent epistemic principles are just as viable. Dominance reasoning can directly support (10) and hence indirectly support (7). (As Vann McGee (1999) showed, the dominance principle in decision theory has to be qualified for certain kinds of agents with unbounded utility functions who are faced with a decision tree with infinitely many branches. Such qualifications do not seem at all relevant here.)\nIt will be useful to start with an unsound argument for (10), because although this argument is unsound, it fails in an instructive way. Before I can present the argument I need to make an attempt at formalising Pryor’s concept of badness.\n\nq is bad relative to e and p =dfq is deeply contingent, you know p entails \\(\\neg\\)q, and for any possible evidence e\\(^\\prime\\) (that you could have had at the time your total evidence is actually e) there exists a p\\(^\\prime\\) such that you know p\\(^\\prime\\) entails \\(\\neg\\)q and you are justified in believing p\\(^\\prime\\) on the basis of e\\(^\\prime\\) if e\\(^\\prime\\) is your total evidence.\n\nRoughly, the idea is that a bad proposition is one that would be justifiably ruled out by any evidence, despite the fact that it could turn out to be true.11 Using this definition we can present an argument for rationalism. The argument will use some fairly general premises connecting justification, evidence and badness. If we were just interested in this case we could replace q with (5), r with the proposition that (5) is false, e with my current evidence, and e\\(^\\prime\\) with some evidence that would undermine my belief that (5) is false, if such evidence could exist. The intuitions behind the argument may be clearer if you make those substitutions when reading through the argument. But because the premises are interesting beyond their application to this case, I will present the argument in its more general form.\nIf you are justified in believing (1) on the basis of e, and you know (1) entails \\(\\neg\\)(5), then you are justified in believing \\(\\neg\\)(5) when your evidence is e.\nIf you are justified in believing r (at time t) on the basis of e, then there is some other possible evidence e\\(^\\prime\\) (that you could have at t) such that you would not be justified in believing r were your total evidence e\\(^\\prime\\).\nIf you are justified in believing r, and there is no evidence e such that e is part of your evidence and you are justified in believing r on the basis of e, then you are justified in believing r a priori.12\nBy definition, q is bad relative to e and p iff q is deeply contingent, you know p entails \\(\\neg\\)q, and for any possible evidence e\\(^\\prime\\) (that you could have when your evidence is e) there exists a p\\(^\\prime\\) such that you know p\\(^\\prime\\) entails \\(\\neg\\)q and you are justified in believing p\\(^\\prime\\) on the basis of e\\(^\\prime\\) if e\\(^\\prime\\) is your total evidence.\nSo, if q is bad relative to e and (1), and you are justified in believing (1) on the basis of e, then you are justified in believing \\(\\neg\\)q a priori.\n(The references to times in (13) and (15) is just to emphasise that we are talking about your current evidence, and ways it could be. That you could observe Winter Wonderland next winter doesn’t count as a relevant alternative kind of evidence now.)\nOur conclusion (16) entails (10), since (10) merely required that for every bad proposition relative to e and (1), you have ‘antecedent’ justification for believing that proposition to be false, while (16) says this justification is a priori. (‘Antecedent’ justification need not be a priori as long as it arrives before the particular evidence you have for (1). This is why (16) is strictly stronger than (10).) So if (10) is false then one of these premises must be false. I take (15) to define “bad,” so it cannot be false. Note that given this definition we cannot be certain that (5) is bad. We will return to this point a few times.\nWhich premise should the dogmatist reject? (12) states a fairly mundane closure principle for justified belief. And (13) follows almost automatically from the notion of ‘basing.’ A belief can hardly be based in some particular evidence if any other evidence would support it just as well. This does not mean that such a belief cannot be rationally caused by the particular evidence that you have, just that the evidence cannot be the rational basis for that belief. The dogmatist objects to (14). There is a prima facie argument for (14), but as soon as we set it out we see why the dogmatist is correct to stop us here.\nConsider the following argument for (14), which does little more than lay out the intuition (14) is trying to express. Assume r is such that for any possible evidence e, one would be justified in believing r with that evidence. Here’s a way to reason a priori to r. Whatever evidence I get, I will be justified in believing that q. So I’m now justified in believing that r, before I get the evidence. Compare a simple decision problem where there is one unknown variable, and it can one of two values, but whichever value it takes it is better for one to choose X rather than Y. That is sufficient to make it true now that one should choose X rather than Y. Put this way, the argument for (14) is just a familiar dominance argument.\nTwo flaws with this argument for (14) stand out, each of them arising because of disanalogies with the decision theoretic case.\nFirst, when we apply dominance reasoning in decision theory, we look at cases where it would be better to take X rather than Y in every possible case, and this is known. This point is usually not stressed, because it’s usually just assumed in decision theory problems that the players know the consequences of their actions given the value of certain unknown variables. It’s not obviously a good idea to assume this without comment in applications of decision theory, and it’s clearly a bad idea to make the same kind of assumption in epistemology. Nothing in the antecedent of (14) specifies that we can know, let alone know a priori, that if our evidence is e then we are justified in believing r. Even if this is true, even if it is necessarily true, it may not be knowable.\nSecond, in the decision theory case we presupposed it is known that the variable can take only one of two values. Again, there in nothing in the antecedent of (14) to guarantee the parallel. Even if an agent knows of every possible piece of evidence that if she gets that evidence she will be justified in believing r, she may not be in a position to justifiably conclude r now because she may not know that these are all the possible pieces of evidence. In other words, she can only use dominance reasoning to conclude r if she knows de dicto, and not merely de re, of every possible body of evidence that it justifies r.\nSo the quick argument for (14) fails. Still, it only failed because (14) left out two qualifications. If we include those qualifications, and adjust the other premises to preserve validity, the argument will work. To make this adjustment, we need a new definition of badness.\n\nq is bad relative to e and p =df\nq is deeply contingent;\np is known to entail \\(\\neg\\)q; and\nit is knowable a priori that for any possible evidence e\\(^\\prime\\) there exists a p\\(^\\prime\\) such that p\\(^\\prime\\) is known to entail \\(\\neg\\)q, and one is justified in believing p\\(^\\prime\\) on the basis of e\\(^\\prime\\).\n\nThe aim still is to find an argument for some claim stronger than (10) in sceptical argument 2. If we can do that, and if as the sceptic suggests (5) really is bad, then the only anti-sceptical response to sceptical argument 2 will be rationalism. So the fact that this looks like a sound argument for a slightly stronger conclusion than (10) is a large step in our argument that anti-scepticism plus internalism entails rationalism. (I omit the references to times from here on.)\nIf you are justified in believing (1) on the basis of e, and you know (1) entails \\(\\neg\\)(5), then you are justified in believing \\(\\neg\\)(5) when your evidence is e.\nIf you are justified in believing r on the basis of e, then there is some other possible evidence e\\(^\\prime\\) such that you would not be justified in believing r were your total evidence e\\(^\\prime\\).\nIf you know you are justified in believing r, and you know a priori that there is no evidence e you have such that you are justified in believing r on the basis of e, then you are justified in believing r a priori.13\nBy definition, q is bad relative to e and p iff q is deeply contingent, p is known to entail \\(\\neg\\)q, and it is knowable a priori that for any possible evidence e\\(^\\prime\\) there exists a p\\(^\\prime\\) such that p\\(^\\prime\\) is known to entail \\(\\neg\\)q, and one is justified in believing p\\(^\\prime\\) on the basis of e\\(^\\prime\\).\nSo, if q is bad relative to e and (1), and you are justified in believing (1) on the basis of e, then you are justified in believing \\(\\neg\\)q a priori.\nThis is a sound argument for (19), and hence for (10), but as noted on this definition of “bad” (11) may be false. If the Winter Wonderland hypothesis is to be bad it must be a priori knowable that on any evidence whatsoever, you’d be justified in believing it to be false. But as we will now see, although no evidence could justify you in believing the Winter Wonderland hypothesis to be true, it is not at all obvious that you are always justified in believing it is false.\nHunting the Bad Proposition\nA proposition is bad if it is deeply contingent but if you could justifiably believe it to be false on the basis of your current evidence, you could justifiably believe it to be false a priori. If a bad proposition exists, then we are forced to choose between rationalism and scepticism. To the extent that rationalism is unattractive, scepticism starts to look attractive. I think Pryor is right that this kind of argument tacitly underlies many sceptical arguments. The importance of propositions like (5) is not that it’s too hard to know them to be false. The arguments of those who deny closure principles for knowledge notwithstanding, it’s very intuitive that it’s easier to know (5) is false than to know (1) is true. So why does reflection on (5) provide more comfort to the inductive sceptic than reflection on (1)? The contextualist has one answer, that thinking about (5) moves the context to one where sceptical doubts are salient. Pryor’s work suggests a more subtle answer. Reflecting on (5) causes us to think about how we could come to know it is false, and prima facie it might seem we could not know that a priori or a posteriori. It’s that dilemma, and not the mere salience of the Winter Wonderland possibility, that drives the best sceptical argument. But this argument assumes that (5) could not be known to be false on the basis of empirical evidence, i.e. that it is bad. If it is not bad, and nor is any similar proposition, then we can easily deflect the sceptical argument. However, if we assume internalism, we can construct a bad proposition.\nThe prima facie case that (5) is bad (relative to (1) and our current evidence e – I omit these relativisations from now on) looks strong. The negation of (5) is (20), where H is a proposition that summarises the relevant parts of the history of the world.14\nEither \\(\\neg\\)H or it will snow in Ithaca next winter.\nNow one may argue that (5) is bad as follows. Either our evidence justifies believing \\(\\neg\\)H or it doesn’t. If it does, then it clearly justifies believing (20), for \\(\\neg\\)H trivially entails it. If it does not, then we are justified in believing H, and whenever we are justified believing the world’s history is H, we can inductively infer that it will snow in Ithaca next winter. The problem with this argument, however, is fairly clear: the step from the assumption that we are not justified in believing \\(\\neg\\)H to the conclusion we are justified in believing H is a modal fallacy. We might be justified in believing neither H nor its negation. In such a situation, it’s not obvious we could justifiably infer (20). So (5) may not be bad.\nA suggestion John Hawthorne (2002) makes seems to point to a proposition that is more plausibly bad. Hawthorne argues that disjunctions like (21) are knowable a priori, and this suggests that (22), its negation, is bad.\nEither my evidence is not e or it will snow in Ithaca next winter.\nMy evidence is e and it will not snow in Ithaca next winter.\nHawthorne does not provide a dominance argument that (21) is knowable a priori. Instead he makes a direct appeal to the idea that whatever kinds of inference we can draw now the basis of our evidence we could have drawn prior to getting e as conditional conclusions, conditional on getting e. So if I can now know it will snow in Ithaca next winter, prior to getting e I cold have known the material conditional If my evidence is e, it will snow in Ithaca, which is equivalent to (21). It’s not clear this analogy works, since when we do such hypothetical reasoning we take someone to know that our evidence is e, and this may cause some complications. Could we find a dominance argument to use instead? One might be tempted by the following argument.\nI know a priori that if my evidence is e, then I am justified in believing the second disjunct of (21).\nI know a priori that if my evidence is not e, then I am justified in believing the first disjunct of (21)\nI know a priori that if I am justified in believing a disjunct of (21) I am justified in believing the disjunction (21).\nI know a priori that my evidence is either e or not e.\nSo, I’m justified a priori in believing (21).\nThe problem here is the second premise, (24). It’s true that if my evidence is not e then the first disjunct of (21) is true. But there’s no reason to suppose I am justified in believing any true proposition about my evidence. Timothy (Williamson 2000 ch. 8) has argued that the problem with many sceptical arguments is that they assume agents know what their evidence is. I doubt that’s really the flaw in sceptical arguments, but it certainly is the flaw in the argument that (22) is bad.\nThe problem with using (22) is that the argument for its badness relied on quite a strong privileged access thesis: whenever my evidence is not e I am justified in believing it is not. If we can find a weaker privileged access thesis that is true, we will be able to find a proposition similar to (22) that is bad. And the very argument Williamson gives against the thesis that we always know what our evidence is will show us how to find such a thesis.\nWilliamson proposes a margin-of-error model for certain kinds of knowledge. On this model, X knows that p iff (roughly) p is true in all situations within X’s margin-of-error.15 The intuitive idea is that all of the possibilities are arranged in some metric space, with the distance between any two worlds being the measure of their similarity with respect to X. Then X knows all the things that are true in all worlds within some sphere centred on the actual world, where the radius of that sphere is given by how accurate she is at forming beliefs.\nOne might think this would lead to the principle B: p \\({\\rightarrow}\\) K\\(\\neg\\)K\\(\\neg\\)p, that is, if p is true then X knows that she does not know \\(\\neg\\)p. Or, slightly more colloquially, if p is true then X knows that for all she knows p is true. (I use K here as a modal operator. KA means that X, the salient subject, knows that A.) On a margin-of-error model p \\({\\rightarrow}\\) K\\(\\neg\\)K\\(\\neg\\)p is false only if p is actually true and there is a nearby (i.e. within the margin-of-error) situation where the agent knows \\(\\neg\\)p. But if nearby is symmetric this is impossible, because the truth of p in this situation will rule out the knowability of \\(\\neg\\)p in that situation.\nAs Williamson points out, that quick argument is fallacious, since it relies on a too simplistic margin-of-error model. He proposes a more complicated account: p is known at s iff there is a distance d greater than the margin-of-error and for any situation s\\(^\\prime\\) such that the distance between s and s\\(^\\prime\\) is less than d, p is true at s\\(^\\prime\\). Given this model, we cannot infer p \\({\\rightarrow}\\) K\\(\\neg\\)K\\(\\neg\\)p. Indeed, the only distinctive modal principle we can conclude is Kp \\({\\rightarrow}\\) p. However, as Delia Graff Fara (2002) has shown, if we make certain density assumptions on the space of available situations, we can recover the principle (27) within this account.16\np \\({\\rightarrow}\\) K\\(\\neg\\)KK\\(\\neg\\)p\nTo express the density assumption, let d(s1, s2) be the ‘distance’ between s1 and s2, and m the margin-of-error. The assumption then is that there is a k > 1 such that for any s1, s2 such that d(s1, s2) < km, there is an s3 such that d(s1, s3) < m and d(s3, s2) < m. And this will be made true if there is some epistemic situation roughly ‘half-way’ between s1 and s2.17 That is, all we have to assume to recover (27) within the margin-of-error model is that the space of possible epistemic situations is suitably dense. Since the margin-of-error model, and Fara’s density assumption, are both appropriate for introspective knowledge, (27) is true when p is a proposition about the agent’s own knowledge.\nTo build the bad proposition now, let G be a quite general property of evidence, one that is satisfied by everyone with a reasonable acquaintance with Ithaca’s weather patterns, but still precise enough that it is a priori that everyone whose evidence is G is justified in believing it will snow in Ithaca next winter. The internalist, remember, is committed to such a G existing and it being an introspective property. Now consider the following proposition, which I shall argue is bad.18\nI know that I know my evidence is G, and it will not snow in Ithaca next winter.\nThe negation of (28) is (29).\nIt will snow in Ithaca next winter, or I don’t know that I know my evidence is G.\nIt might be more intuitive to read (29) as the material conditional (29a), though since English conditionals aren’t material conditionals this seems potentially misleading.\nIf I know that I know that my evidence is G, then it will snow in Ithaca next winter.\nTo avoid confusions due to the behaviour of conditionals, I’ll focus on the disjunction (29). Assume for now that the margin-of-error model is appropriate for propositions about my own evidence. I will return below to the plausibility of this assumption. This assumption implies that principle (27) is always correct when p is a proposition about my evidence. Given this, we can prove (28) is bad. Note that all my possible evidential states either are, or are not, G. If they are G then by hypothesis I am justified in believing that it will snow in Ithaca next winter and hence I am justified in believing (29). If they are not, then by the principle (27) I know that I don’t know that I know my evidence is G, so I can come to know (29), so I am justified in believing (29). So either way I am justified in believing (29). It’s worth noting that at no point here did I assume that I knew whether my evidence was G, though I do assume that I know that having evidence that is G justifies belief in snow next winter.\nAll of this assumes the margin-of-error model looks appropriate for introspective properties. If it isn’t, then we can’t assume that (27) is true when p is a proposition about the introspective properties I satisfy, and hence the argument that (29) is knowable a priori fails. There’s one striking problem with assuming a priori that we can use the margin-of-error model in all situations. It is assumed (roughly) that anything that is true in all possibilities within a certain sphere with the subject’s beliefs at the centre is known. This sphere must include the actual situation, or some propositions that are actually false may be true throughout the sphere. Since for propositions concerning non-introspective properties there is no limit to how badly wrong the subject can be, we cannot set any limits a priori to the size of the sphere. So a priori the only margin-of-error model we can safely use is the sceptical model that says the subject knows that p iff p is true in all situations. For introspective properties the margin-of-error can be limited, because it is constitutive of introspective properties that the speakers beliefs about whether they possess these properties are not too far from actuality. So there seems to be no problem with using Williamson’s nice model as long as we restrict our attention to introspective properties.\nIf belief in (29) can be justified a priori, and it is true, does that mean it is knowable a priori? If we want to respect Gettier intuitions, then we must not argue directly that since our belief in (29) is justified, and it is true, then we know it. Still, being justified and true is not irrelevant to being known. I assume here, far from originally, that it is a reasonable presumption that any justified true belief is an item of knowledge. This presumption can be defeated, if the belief is inferred from a false premise, or if the justification would vanish should the subject acquire some evidence she should have acquired, or if there is a very similar situation in which the belief is false, but it is a reasonable presumption. Unless we really are in some sceptical scenario, there is no “defeater” that prevents our belief in (29) being an item of knowledge. We certainly did not infer it from a false premise, there is no evidence we could get that would undermine it, and situations in which it is false are very far from actuality.\nSince there are no such defeaters, it is reasonable to infer we can know (29) a priori. The important premises grounding this inference are an anti-sceptical premise, that we can know (1) on the basis of our current evidence, and the internalist premise that we used several times in the above argument. This completes the argument that the combination of empiricism, internalism and anti-scepticism is untenable.\nHow Externalism Helps\nIt should be obvious how the rationalist can respond to the above argument - by simply accepting the conclusion. Ultimately I think that’s the best response to this argument. As Hawthorne notes, rationalism is the natural position for fallibilists about knowledge to take, for it is just the view that we can know something a priori even though we could turn out to be wrong. In other words, it’s just fallibilism about a priori knowledge. Since fallibilism about a posteriori knowledge seems true, and there’s little reason to think fallibilism about the a priori would be false if fallibilism about the a posteriori is true, the rationalist’s position is much stronger than many have assumed.19 The inductive sceptic also has an easy response - reject the initial premise that in my current situation I know that it will snow in Ithaca next winter. There are other responses that deserve closer attention: first, the inductive sceptic who is not a universal sceptic, and in particular is not a sceptic about perception, and second the externalist.\nI said at the start that the argument generalises to most kinds of scepticism. One kind of theorist, the inductive sceptic who thinks we can nonetheless acquire knowledge through perception, may think that the argument does not touch the kind of anti-sceptical, internalist, empiricist position she adopts. The kind of theorist I have in mind says that the objects and facts we perceive are constitutive of the evidence we receive. So given we are getting the evidence we are actually getting, these objects must exist and those facts must be true. She says that if I’d started with (30), instead of (1), my argument would have ended up claiming that (31) is bad for some G.\nA hand exists.\nA hand exists, or I don’t know that I know that I’m perceiving a hand.\nShe then says that (31) is not deeply contingent, since in any situation where the first disjunct is false the second is true, so it cannot be bad. This response is correct as far as it goes, but it does not go far enough to deserve the name anti-sceptical. For it did not matter to the above argument, or to this response that (1) is about the future. All that mattered was that (1) was not entailed by our evidence. So had (1) been a proposition about the present that we cannot directly perceive, such as that it is not snowing in Sydney right now, the rest of the argument would have been unaffected. The summary here is that if one is suitably externalist about perception, so one thinks the existence of perceptual states entail the existence of the things being perceived, one can accept this argument, accept internalism, accept empiricism, and not be an external world sceptic. For it is consistent with such a position that one know the existence of the things one perceives. But on this picture one can know very little beyond that, so for most practical purposes, the position is still a sceptical one.\nThe externalist response is more interesting. Or, to be more precise, the externalist reponses are more interesting. Although I have appealed to internalism a couple of times in the above argument, it might not be so clear how the externalist can respond. Indeed, it may be worried that by exercising a little more care in various places I could have shown that everyone must accept either rationalism or scepticism. That is the conclusion Hawthorne derives in his paper on deeply contingent a priori knowledge, though as noted above he uses somewhat more contentious reasoning than I do in order to get there. To conclude, I will argue that the internalism is crucial to the argument I have presented, and I will spell out how the externalist can get out of the trap I’ve set above.\nOne easy move that’s available to an externalist is to deny that any facts about justification are a priori. That blocks the move that says we can find a G such that it’s a priori that anyone whose evidence is G can know that it will snow in Ithaca next year. This is not an essential feature of externalism. One can be an externalist about justification and still think it is a priori that if one’s evidence has the property is reliably correlated with snow in the near future then it justifies belief that it will shortly snow. But the position that all facts about justification are a posteriori fits well with a certain kind of naturalist attitude, and people with that attitude will find it easy to block the sceptical argument I’ve presented.\nCan, however, we use an argument like mine to argue against an anti-sceptic empiricist externalist who thinks some of the facts about justification can be discovered a priori? The strategy I’ve used to build the argument is fairly transparent: find a disjunctive a priori knowable proposition by partitioning the possible evidence states into a small class, and adding a disjunct for every cell of the partition. In every case, the disjunct that is added is one that is known to be known given that evidence. If one of the items of knowledge is ampliative, if it goes beyond the evidence, then it is possible the disjunction will be deeply contingent. But the disjunction is known no matter what.\nIf internalism is true, then the partition can divide up evidential states according to the introspective properties of the subject. If externalism is true, then such a partition may not be that useful, because we cannot infer much about what the subject is justified in believing from the introspective properties she instantiates. Consider, for example, the above partition of subjects into the G and the not-G, where G is some introspective property, intuitively one somewhat connected with it snowing in Ithaca next year. The subjects that are not-G know that they don’t know they know they are G, because they aren’t. Externalists need not object to this stage of the argument. They can, and should, accept that a margin-of-error model is appropriate for introspective properties. Since it’s part of the nature of introspective properties that we can’t be too badly wrong about which ones we instantiate, we’re guaranteed to satisfy some reliability clause, so there’s no ground there to deny the privileged access principle I defended above.\nThe problem is what to say about the cases where the subject is G. Externalists should say that some such subjects are justified in believing it will snow in Ithaca next winter, and some are not. For simplicity, I’ll call the first group the reliable ones and the others the unreliable ones. If I’m G and reliable, then I’m justified in believing it will snow, and hence in believing (29). But if I’m G and unreliable, then I’m not justified in believing this. Indeed, if I’m G and unreliable, there is no obvious argument that I’m justified in believing either of the disjuncts of (29). Since this is a possible evidential state, externalists should think there is no dominance argument that (29) is a priori knowable.\nCould we solve this by adding another disjunct, one that is guaranteed to be known if I’m G and unreliable? There is no reason to believe we could. If we’re unreliable, there is no guarantee that we will know we are unreliable. Indeed, we may well believe we are reliable. So there’s no proposition we can add to our long disjunction while saying to ourselves, “In the case where the subject is G and unreliable, she can justifiably believe this disjunct.” If the subject is unreliable, she may not have any justified beliefs about the external world. But this is just to say the above recipe for constructing bad propositions breaks down. Externalists should have no fear that anything like this approach could be used to construct a proposition they should find bad. This is obviously not a positive argument that anti-sceptical empiricist externalism is tenable, but it does suggest that such a position is immune to the kind of argument I have presented here.\n\n\nBonJour, Laurence. 1997. In Defense of Pure Reason. Cambridge: Cambridge University Press.\n\n\nChalmers, David. 2006. “Foundations of Two-Dimensional Semantics.” In Two-Dimensional Semantics, edited by Manuel Garcia-Carpintero and Josep Macià, 55–140. Oxford: Oxford University Press.\n\n\nEvans, Gareth. 1979. “Reference and Contingency.” Monist 62: 161–89.\n\n\nFara, Delia Graff. 2002. “An Anti-Epistemicist Consequence of Margin for Error Semantics for Knowledge.” Philosophy and Phenomenological Research 64 (1): 127–42. https://doi.org/10.1111/j.1933-1592.2002.tb00146.x.\n\n\nHawthorne, John. 2002. “Deeply Contingent a Priori Knowledge.” Philosophy and Phenomenological Research 65 (2): 247–69. https://doi.org/10.1111/j.1933-1592.2002.tb00201.x.\n\n\nMcGee, Vann. 1999. “An Airtight Dutch Book.” Analysis 59 (4): 257–65. https://doi.org/10.1093/analys/59.4.257.\n\n\nPryor, James. 2000. “The Sceptic and the Dogmatist.” Noûs 34 (4): 517–49. https://doi.org/10.1111/0029-4624.00277.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nYablo, Stephen. 2002. “Coulda, Woulda, Shoulda.” In Conceivability and Possibility, edited by Tamar Szabó Gendler and John Hawthorne, 441–92. Oxford: Oxford University Press.\n\n\nIf you prefer the ‘two-dimensional’ way of talking, a deeply contingent proposition is one that is true in some possible world ‘considered as actual.’ See Chalmers (2006) for a thorough discussion of ways to interpret this phrase, and the broader notion of so-called ‘deep’ contingency. Nothing that goes on here will turn on any of the fine distinctions made in that debate - the relevant propositions will be deeply contingent in every plausible sense.↩︎\nThat a property is introspective does not mean that whenever a subject instantiates it she is in a position to form a not too badly mistaken belief about it. Even if the subject instantiates the property she may not possess sufficient concepts in order to have beliefs about it. And even if she has the concept she may simply have more pressing cognitive needs than forming certain kinds of belief. Many agents have no beliefs about the smell in their ordinary environment much of the time, for example, and this does not show that phenomenal smell properties are not introspective. All that is required is that if she has any beliefs at all about which determinate she instantiates, the beliefs are immune to massive error.↩︎\nThere is a delicate ambiguity in this expression to which a referee drew my attention. The intended meaning is that for any two agents who instantiate the same introspective properties, belief in the same propositions is justified. What’s not intended is that if there’s an agent who justifiably believes p, and the introspective properties they instantiate are F1, …, Fn, then any agent who instantiates F1, …, Fn is justified in believing p. For there might be some other introspective property Fn+1 they instantiate that justifies belief in q, and q might be a defeater for p. The ‘unintended’ claim would be a very strong, and very implausible, claim about the subvenient basis for justification.↩︎\nRationalism is supported by BonJour (1997) and Hawthorne (2002), and my argument owes a lot to each of their discussions.↩︎\nAesthetically it would be preferable to have the antecedent of this claim be just that empiricism is true, but unfortunately this does not seem to be possible.↩︎\nI.e. I am not a brain-in-a-vat* in the sense of Cohen (1999)↩︎\nIf she is convinced that there is no possible world with the same history as ours and no snow in Ithaca next winter, the sceptic will change her story so Winter Wonderland’s past differs imperceptibly from the past in our world. She doesn’t think this issue is particularly relevant to the epistemological debate, no matter how interesting the scientific and metaphysical issues may be, and I agree with her.↩︎\nPryor is discussing the external world sceptic, not the inductive sceptic, so the premises here are a little different to those he provides.↩︎\nIt is a version of the kind of internalism discussed in footnote 2, since according to the dogmatist seeming to see that p can be sufficient justification for belief in p. Pryor’s preferred version of dogmatism is also internalist in the slightly stronger sense described in the text, but it seems possible that one could be a dogmatist without accepting that internalist thesis. One could accept, for instance, that seeming to see that p justifies a belief that p, but also think that seeming to see that q justifies a belief that p iff there is a known reliable connection between q and p. As I said, even the weaker version of internalism is sufficient to generate a conflict with anti-scepticism and empiricism, provided we just focus on the propositions that can be justifiably believed on the basis of introspective properties.↩︎\nJust which is wrong then? That depends on how “bad” is defined. On our final definition (8) will fail, but there are other sceptical arguments, using other sceptical hypotheses, on which (6) fails.↩︎\nNote that there’s a subtle shift here in our conception of badness. Previously we said that bad propositions are those you allegedly know on the basis of your actual evidence (if you know p) even though they are logically consistent with that evidence. Now we say that they are propositions you could rule out on any evidence, even though they are consistent with your actual total evidence. This is a somewhat narrower class of proposition, but focussing on it strengthens the sceptic’s case appreciably.↩︎\n{#fnt:ftn13 label=“fnt:ftn13”} David Chalmers noted that (10) and (11) entail that I exist is a priori. He thought this was a bad result, and a sufficient reason to modify these premises. I’m perfectly happy with saying, following Kaplan, that I exist is a priori. I don’t think this proves rationalism, because I think it’s also deeply necessary that I exist. (It’s not deeply necessary that Brian exists, but that’s no objection to what I just claimed, because it’s not deeply necessary that I’m Brian.)\nThis position is controversial though, so I don’t want to rest too much weight on it. If you don’t think that I exist should be a priori, rewrite (11) so that it’s conclusion is that you would be justified in believing the material conditional I exist \\({\\supset}\\) r a priori. (Note that since I’m presupposing in the dominance argument that all the salient possibilities are ones in which I have some evidence, and hence exist, it’s not surprising that I exist has a special status within the theory.)\nOn a separate point, note that I make no assumptions whatsoever here about what relationship must obtain between a justified belief and the evidence on which it is based. Depending on what the right theory of justification is, that relationship might be entailment or constitution or causation or association or reliable connection or something else or some combination of these. I do assume that a posteriori beliefs are somehow connected to evidence, and if the beliefs are justified this relation is properly called basing.↩︎\nAgain, if you don’t think I exist should be a priori, the conclusion should be that I exist \\({\\supset}\\) r is a priori.↩︎\nI assume H includes a ‘that’s all that’s relevant clause’ to rule out defeaters. That is, it summaries the relevant history of the world as such.↩︎\nThere’s a considerable amount of idealisation here. What’s really true is that X is in a position to know anything true in all situations within her margin-of-error. Since we’re working out what is a priori knowable, I’ll assume agents are idealised so they know what they are in a position to know. This avoids needless complications we get from multiplying the modalities that are in play.↩︎\nIf we translate K as \\(\\square\\) and \\(\\neg\\)K\\(\\neg\\) as \\(\\diamond\\), (24) can be expressed as the modal formula p \\({\\rightarrow}\\) \\(\\square\\)\\(\\diamond\\)\\(\\diamond\\)p.↩︎\nFara actually gives a slightly stronger principle than this, but this principle is sufficient for her purposes, and since it is weaker than Fara’s, it is a little more plausible. But the underlying idea here, that we can get strong modal principles out of margin-of-error models by making plausible assumptions about density, is taken without amendment from her paper.↩︎\nIf you preferred the amended version of (11) discussed in footnote 12, the bad proposition is I don’t exist or (28) is true.↩︎\nAs BonJour points out, rationalism has fallen into such disrepute that many authors leave it out even of surveys of the options. This seems unwarranted given the close connection between rationalism and the very plausible thesis of fallibilism.\n\n↩︎\n",
    "preview": "posts/2021-01-08-scepticism-rationalism-and-externalism/winter.jpg",
    "last_modified": "2021-02-05T20:42:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-08-can-we-do-without-pragmatic-encroachment/",
    "title": "Can We Do Without Pragmatic Encroachment?",
    "description": "I argue that interests primarily affect the relationship between credence and belief. A view is set out and defended where evidence and rational credence are not interest-relative, but belief, rational belief, and knowledge are.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2005-12-13",
    "categories": [
      "epistemology",
      "interest-relativity",
      "philosophy of mind"
    ],
    "contents": "\n\nContents\nIntroduction\nBelief and Degree of Belief\nImpractical Propositions\nDefending Closure\nToo Little Closure?\nExamples of Pragmatic Encroachment\nJustification and Practical Reasoning\nConclusions\n\nIntroduction\nRecently several authors have defended claims suggesting that there is a closer connection between practical interests and epistemic justification than has traditionally been countenanced. Jeremy Fantl and Matthew McGrath (2002) argue that there is a “pragmatic necessary condition on epistemic justification” (77), namely the following.\n\nPublished in Philosophical Perspectives 19: 417-43.\n(PC)\nS is justified in believing that p only if S is rational to prefer as if p. (77)\n\nAnd John Hawthorne (2004) and Jason Stanley (2005) have argued that what it takes to turn true belief into knowledge is sensitive to the practical environment the subject is in. These authors seem to be suggesting there is, to use Jonathan Kvanvig’s phrase “pragmatic encroachment” in epistemology. In this paper I’ll argue that their arguments do not quite show this is true, and that concepts of epistemological justification need not be pragmatically sensitive. The aim here isn’t to show that (PC) is false, but rather that it shouldn’t be described as a pragmatic condition on justification. Rather, it is best thought of as a pragmatic condition on belief. There are two ways to spell out the view I’m taking here. These are both massive simplifications, but they are close enough to the truth to show the kind of picture I’m aiming for.\n\nThanks to Michael Almeida, Tamar Szabó Gendler, Peter Gerdes, Jon Kvanvig, Barry Lam, Ishani Maitra, Robert Stalnaker, Jason Stanley, Matthew Weiner for helpful discussions, and especially to Matthew McGrath for correcting many mistakes in an earlier draft of this paper.\nFirst, imagine a philosopher who holds a very simplified version of functionalism about belief, call it (B).\n(B)\nS believes that p iff S prefers as if p\n\nOur philosopher one day starts thinking about justification, and decides that we can get a principle out of (B) by adding normative operators to both sides, inferring (JB).\n(JB)\nS is justified in believing that p only if S is justified to prefer as if p\n\nNow it would be a mistake to treat (JB) as a pragmatic condition on justification (rather than belief) if it was derived from (B) by this simple means. And if our philosopher goes on to infer (PC) from (JB), by replacing ‘justified’ with ‘rational,’ and inferring the conditional from the biconditional, we still don’t get a pragmatic condition on justification.\nSecond, Fantl and McGrath focus their efforts on attacking the following principle.\nEvidentialism\nFor any two subjects S and S\\(^\\prime\\), necessarily, if S and S\\(^\\prime\\) have the same evidence for/against p, then S is justified in believing that p iff S\\(^\\prime\\) is, too.\n\nI agree, evidentialism is false. And I agree that there are counterexamples to evidentialism from subjects who are in different practical situations. What I don’t agree is that we learn much about the role of pragmatic factors in epistemology properly defined from these counterexamples to evidentialism. Evidentialism follows from the following three principles.\nProbabilistic Evidentialism\nFor any two subjects S and S\\(^\\prime\\), and any degree of belief \\(\\alpha\\) necessarily, if S and S\\(^\\prime\\) have the same evidence for/against p, then S is justified in believing that p to degree \\(\\alpha\\) iff S\\(^\\prime\\) is, too.\n\nThreshold View\nFor any two subjects S and S\\(^\\prime\\), and any degree of belief \\(\\alpha\\), if S and S\\(^\\prime\\) both believe p to degree \\(\\alpha\\), then S believes that p iff S\\(^\\prime\\) does too.\n\nProbabilistic Justification\nFor any \\(S, S\\) is justified in believing p iff there is some degree of belief \\(\\alpha\\) such that S is justified in believing p to degree \\(\\alpha\\), and in S’s situation, believing p to degree \\(\\alpha\\) suffices for believing p.\n\n(Degrees of belief here are meant to be the subjective correlates of Keynesian probabilities. See Keynes (1921) for more details. They need not, and usually will not, be numerical values. The Threshold View is so-called because given some other plausible premises it implies that \\(S\\) believes that p iff S’s degree of belief in p is above a threshold.)\nI endorse Probabilistic Justification, and for present purposes at least I endorse Probabilistic Evidentialism. The reason I think Evidentialism fails is because the Threshold View is false. It is plausible that Probabilistic Justification and Probabilistic Evidentialism are epistemological principles, while the Threshold View is a principle from philosophy of mind. So this matches up with the earlier contention that the failure of Evidentialism tells us something interesting about the role of pragmatics in philosophy of mind, rather than something about the role of pragmatics in epistemology.\nAs noted, Hawthorne and Stanley are both more interested in knowledge than justification. So my discussion of their views will inevitably be somewhat distorting. I think what I say about justification here should carry over to a theory of knowledge, but space prevents a serious examination of that question. The primary bit of ‘translation’ I have to do to make their works relevant to a discussion of justification is to interpret their defences of the principle (KP) below as implying some support for (JP), which is obviously similar to (PC).\n(KP)\nIf S knows that p, then S is justified in using p as a premise in practical reasoning.\n\n(JP)\nIf S justifiably believes that p, then S is justified in using p as a premise in practical reasoning.\n\nI think (JP) is just as plausible as (KP). In any case it is independently plausible whether or not Hawthorne and Stanley are committed to it. So I’ll credit recognition of (JP)’s importance to a theory of justification to them, and hope that in doing so I’m not irrepairably damaging the public record.\nThe overall plan here is to use some philosophy of mind, specifically functionalist analyses of belief to respond to some arguments in epistemology. But, as you can see from the role the Threshold View plays in the above argument, our starting point will be the question what is the relation between the credences decision theory deals with, and our traditional notion of a belief? I’ll offer an analysis of this relation that supports my above claim that we should work with a pragmatic notion of belief rather than a pragmatic notion of justification. The analysis I offer has a hole in it concerning propositions that are not relevant to our current plans, and I’ll fix the hold in section 3. Sections 4 and 5 concern the role that closure principles play in my theory, in particular the relationship between having probabilistically coherent degrees of belief and logically coherent beliefs. In this context, a closure principle is a principle that says probabilistic coherence implies logical coherence, at least in a certain domain. (It’s called a closure principle because we usually discuss it by working out properties of probabilistically coherent agents, and show that their beliefs are closed under entailment in the relevant domain.) In section 4 I’ll defend the theory against the objection, most commonly heard from those wielding the preface paradox, that we need not endorse as strong a closure principle as I do. In section 5 I’ll defend the theory against those who would endorse an even stronger closure principle than is defended here. Once we’ve got a handle on the relationship between degrees of belief and belief tout court, we’ll use that to examine the arguments for pragmatic encroachment. In section 6 I’ll argue that we can explain the intuitions behind the cases that seem to support pragmatic encroachment, while actually keeping all of the pragmatic factors in our theory of belief. In section 7 I’ll discuss how to endorse principles like (PC) and (JP) (as far as they can be endorsed) while keeping a non-pragmatic theory of probabilistic justification. The interesting cases here are ones where agents have mistaken and/or irrational beliefs about their practical environment, and intuitions in those cases are cloudy. But it seems the most natural path in these cases is to keep a pragmatically sensitive notion of belief, and a pragmatically insensitive notion of justification.\nBelief and Degree of Belief\nTraditional epistemology deals with beliefs and their justification. Bayesian epistemology deals with degrees of belief and their justification. In some sense they are both talking about the same thing, namely epistemic justification. Two questions naturally arise. Do we really have two subject matters here (degrees of belief and belief tout court) or two descriptions of the one subject matter? If just one subject matter, what relationship is there between the two modes of description of this subject matter?\nThe answer to the first question is I think rather easy. There is no evidence to believe that the mind contains two representational systems, one to represent things as being probable or improbable and the other to represent things as being true or false. The mind probably does contain a vast plurality of representational systems, but they don’t divide up the doxastic duties this way. If there are distinct visual and auditory representational systems, they don’t divide up duties between degrees of belief and belief tout court, for example. If there were two distinct systems, then we should imagine that they could vary independently, at least as much as is allowed by constitutive rationality. But such variation is hard to fathom. So I’ll infer that the one representational system accounts for our credences and our categorical beliefs. (It follows from this that the question Bovens and Hawthorne (1999) ask, namely what beliefs should an agent have given her degrees of belief, doesn’t have a non-trivial answer. If fixing the degrees of belief in an environment fixes all her doxastic attitudes, as I think it does, then there is no further question of what she should believe given these are her degrees of belief.)\nThe second question is much harder. It is tempting to say that \\(S\\) believes that p iff S’s credence in p is greater than some salient number \\(r\\), where \\(r\\) is made salient either by the context of belief ascription, or the context that S is in. I’m following Mark Kaplan (1996) in calling this the threshold view. There are two well-known problems with the threshold view, both of which seem fatal to me.\nAs Robert Stalnaker (1984, 91) emphasised, any number \\(r\\) is bound to seem arbitrary. Unless these numbers are made salient by the environment, there is no special difference between believing p to degree 0.9786 and believing it to degree 0.9875. But if \\(r\\) is 0.98755, this will be the difference between believing p and not believing it, which is an important difference. The usual response to this, as found in (Foley 1993 Ch. 4) and Hunter (1996) is to say that the boundary is vague. But it’s not clear how this helps. On an epistemic theory of vagueness, there is still a number such that degrees of belief above that count, and degrees below that do not, and any such number is bound to seem unimportant. On supervaluational theories, the same is true. There won’t be a determinate number, to be sure, but there will a number, and that seems false. My preferred degree of belief theory of vagueness, as set out in Weatherson (2005) has the same consequence. Hunter defends a version of the threshold view combined with a theory of vagueness based around fuzzy logic, which seems to be the only theory that could avoid the arbitrariness objection. But as Williamson (1994) showed, there are deep and probably insurmountable difficulties with that position. So I think the vagueness response to the arbitrariness objection is (a) the only prima facie plausible response and (b) unsuccessful.\nThe second problem concerns conjunction. It is also set out clearly by Stalnaker.\n\nReasoning in this way from accepted premises to their deductive consequences (\\(P\\), also \\(Q\\), therefore \\(R\\)) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. (Stalnaker 1984, 92)\n\nIf categorical belief is having a credence above the threshold, then one can coherently do exactly this. Let \\(x\\) be a number between \\(r\\) and than \\(r\\) \\(\\nicefrac{1}{2}\\), such that for an atom of type U has probability \\(x\\) of decaying within a time \\(t\\), for some \\(t\\) and U. Assume our agent knows this fact, and is faced with two (isolated) atoms of U. Let p be that the first decays within \\(t\\), and \\(q\\) be that the second decays within \\(t\\). She should, given her evidence, believe p to degree \\(x, q\\) to degree \\(x\\), and \\(p \\wedge q\\) to degree \\(x ^2\\). If she believed \\(p \\wedge q\\) to a degree greater than \\(r\\), she’d have to either have credences that were not supported by her evidence, or credences that were incoherent. (Or, most likely, both.) So this theory violates the platitude. This is a well-known argument, so there are many responses to it, most of them involving something like appeal to the preface paradox. I’ll argue in section 4 that the preface paradox doesn’t in fact offer the threshold view proponent much support here. But even before we get to there, we should note that the arbitrariness objection gives us sufficient reason to reject the threshold view.\nA better move is to start with the functionalist idea that to believe that p is to treat p as true for the purposes of practical reasoning. To believe p is to have preferences that make sense, by your own lights, in a world where p is true. So, if you prefer A to B and believe that p, you prefer A to B given p. For reasons that will become apparent below, we’ll work in this paper with a notion of preference where conditional preferences are primary.1 So the core insight we’ll work with is the following:\n\nIf you prefer A to B given \\(q\\), and you believe that p, then you prefer A to B given \\(p \\wedge q\\)\n\nThe bold suggestion here is that if that is true for all the A, B and q that matter, then you believe p. Put formally, where Bel(p) means that the agent believes that p, and A \\(\\geq _q\\) B means that the agent thinks A is at least as good as B given \\(q\\), we have the following\nBel(p) \\(\\leftrightarrow \\forall\\)A\\(\\forall\\)B\\(\\forall q\\) (A \\(\\geq _q\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q}\\) B)\nIn words, an agent believes that p iff conditionalising on p doesn’t change any conditional preferences over things that matter.2 The left-to-right direction of this seems trivial, and the right-to-left direction seems to be a plausible way to operationalise the functionalist insight that belief is a functional state. There is some work to be done if (1) is to be interpreted as a truth though.\nIf we interpret the quantifiers in (1) as unrestricted, then we get the (false) conclusion that just about no one believes no contingent propositions. To prove this, consider a bet that wins iff the statue in front of me waves back at me due to random quantum effects when I wave at it. If I take the bet and win, I get to live forever in paradise. If I take the bet and lose, I lose a penny. Letting A be that I take the bet, B be that I decline the bet, \\(q\\) be a known tautology (so my preferences given \\(q\\) are my preferences tout court) and p be that the statue does not wave back, we have that I prefer A to B, but not A to B given p. So by this standard I don’t believe that p. This is false – right now I believe that statues won’t wave back at me when I wave at them.\nThis seems like a problem. But the solution to it is not to give up on functionalism, but to insist on its pragmatic foundations. The quantifiers in (1) should be restricted, with the restrictions motivated pragmatically. What is crucial to the theory is to say what the restrictions on A and B are, and what the restrictions on \\(q\\) are. We’ll deal with these in order.\nFor better or worse, I don’t right now have the option taking that bet and hence spending eternity in paradise if the statue waves back at me. Taking or declining such unavailable bets are not open choices. For any option that is open to me, assuming that statues do not in fact wave does not change its utility. That’s to say, I’ve already factored in the non-waving behaviour of statues into my decision-making calculus. That’s to say, I believe statues don’t wave.\nAn action A is a live option for the agent if it is really possible for the agent to perform A. An action A is a salient option if it is an option the agent takes seriously in deliberation. Most of the time gambling large sums of money on internet gambling sites over my phone is a live option, but not a salient option. I know this option is suboptimal, and I don’t have to recompute every time whether I should do it. Whenever I’m making a decision, I don’t have to add in to the list of choices bet thousands of dollars on internet gambling sites, and then rerule that out every time. I just don’t consider that option, and properly so. If I have a propensity to daydream, then becoming the centrefielder for the Boston Red Sox might be a salient option to me, but it certainly isn’t a live option. We’ll say the two initial quantifiers range over the options that are live and salient options for the agent.\nNote that we don’t say that the quantifiers range over the options that are live and salient for the person making the belief ascription. That would lead us to a form of contextualism for which we have little evidence. We also don’t say that an option becomes salient for the agent iff they should be considering it. At this stage we are just saying what the agent does believe, not what they should believe, so we don’t have any clauses involving normative concepts.\nNow we’ll look at the restrictions on the quantifier over propositions. Say a proposition is relevant if the agent is disposed to take seriously the question of whether it is true (whether or not she is currently considering that question) and conditionalising on that proposition or its negation changes some of the agents unconditional preferences over live, salient options.3 The first clause is designed to rule out wild hypotheses that the agent does not take at all seriously. If \\(q\\) is not such a proposition, if the agent is disposed to take it seriously, then it is relevant if there are live, salient A and B such that A \\(\\geq _q\\) B \\(\\leftrightarrow\\) A \\(\\geq\\) B is false. Say a proposition is salient if the agent is currently considering whether it is true. Finally, say a proposition is active relative to p iff it is a (possibly degenerate) conjunction of propositions such that each conjunct is either relevant or salient, and such that the conjunction is consistent with p. (By a degenerate conjunction I mean a conjunction with just one conjunct. The consistency requirement is there because it might be hard in some cases to make sense of preferences given inconsistencies.) Then the propositional quantifier in (1) ranges over active propositions.\nWe will expand and clarify this in the next section, but our current solution to the relationship between beliefs and degrees of belief is that degrees of belief determine an agent’s preferences, and she believes that p iff the claim (1) about her preferences is true when the quantifiers over options are restricted to live, salient actions, and the quantifier over propositions is restricted to salient propositions. The simple view would be to say that the agent believes that p iff conditioning on p changes none of her preferences. The more complicated view here is that the agent believes that p iff conditioning on p changes none of her conditional preferences over live, salient options, where the conditions are also active relative to p.\nImpractical Propositions\nThe theory sketched in the previous paragraph seems to me right in the vast majority of cases. It fits in well with a broadly functionalist view of the mind, and as we’ll see it handles some otherwise difficult cases with aplomb. But it needs to be supplemented a little to handle beliefs about propositions that are practically irrelevant. I’ll illustrate the problem, then note how I prefer to solve it.\nI don’t know what Julius Caeser had for breakfast the morning he crossed the Rubicon. But I think he would have had some breakfast. It is hard to be a good general without a good morning meal after all. Let p be the proposition that he had breakfast that morning. I believe p. But this makes remarkably little difference to my practical choices in most situations. True, I wouldn’t have written this paragraph as I did without this belief, but it is rare that I have to write about Caeser’s dietary habits. In general whether p is true makes no practical difference to me. This makes it hard to give a pragmatic account of whether I believe that p. Let’s apply (1) to see whether I really believe that p.\nBel(p) \\(\\leftrightarrow \\forall\\)A\\(\\forall\\)B\\(\\forall q\\) (A \\(\\geq _q\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q}\\) B)\nSince p makes no practical difference to any choice I have to make, the right hand side is true. So the left hand side is true, as desired. The problem is that the right hand side of (2) is also true here.\nBel(\\(\\neg p\\)) \\(\\leftrightarrow \\forall\\)A\\(\\forall\\)B\\(\\forall q\\) (A \\(\\geq _q\\) B \\(\\leftrightarrow\\) A \\(\\geq _{\\neg p \\wedge q}\\) B)\nAdding the assumption that Caeser had no breakfast that morning doesn’t change any of my practical choices either. So I now seem to inconsistently believe both p and \\(\\neg p\\). I have some inconsistent beliefs, I’m sure, but those aren’t among them. We need to clarify what (1) claims.\nTo do so, I supplement the theory sketched in section 2 with the following principles.\nA proposition p is eligible for belief if it satisfies \\(\\forall\\)A\\(\\forall\\)B\\(\\forall q\\) (A \\(\\geq _q\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q}\\) B), where the first two quantifiers range over the open, salient actions in the sense described in section 2.\nFor any proposition p, and any proposition \\(q\\) that is relevant or salient, among the actions that are (by stipulation!) open and salient with respect to p are believing that p, believing that q, not believing that p and not believing that q\nFor any proposition, the subject prefers believing it to not believing it iff (a) it is eligible for belief and (b) the agents degree of belief in the proposition is greater than \\(\\nicefrac{1}{2}\\).\nThe previous stipulation holds both unconditionally and conditional on p, for any p.\nThe agent believes that p iff \\(\\forall\\)A\\(\\forall\\)B\\(\\forall q\\) (A \\(\\geq _q\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q}\\) B), where the first two quantifiers range over all actions that are either open and salient tout court (i.e. in the sense of section 2) or open and salient with respect to p (as described above).\nThis all looks moderately complicated, but I’ll explain how it works in some detail as we go along. One simple consequence is that an agent only believes that p iff their degree of belief in p is greater than \\(\\nicefrac{1}{2}\\). Since my degree of belief in Caeser’s foodless morning is not greater than \\(\\nicefrac{1}{2}\\), in fact it is considerably less, I don’t believe \\(\\neg p\\). On the other hand, since my degree of belief in p is considerably greater than \\(\\nicefrac{1}{2}\\), I prefer to believe it than disbelieve it, so I believe it.\nThere are many possible objections to this position, which I’ll address sequentially.\nObjection: Even if I have a high degree of belief in p, I might prefer to not believe p because I think that belief in p is bad for some other reason. Perhaps, if p is a proposition about my brilliance, it might be immodest to believe that p.\nReply: Any of these kinds of considerations should be put into the credences. If it is immodest to believe that you are a great philosopher, it is equally immodest to believe to a high degree that you are a great philosopher.\nObjection: Belief that p is not an action in the ordinary sense of the term.\nReply: True, which is why this is described as a supplement to the original theory, rather than just cashing out its consequences.\nObjection: It is impossible to choose to believe or not believe something, so we shouldn’t be applying these kinds of criteria.\nReply: I’m not as convinced of the impossibility of belief by choice as others are, but I won’t push that for present purposes. Let’s grant that beliefs are always involuntary. So these ‘actions’ aren’t open actions in any interesting sense, and the theory is section 2 was really incomplete. As I said, this is a supplement to the theory in section 2.\nThis doesn’t prevent us using principles of constitutive rationality, such as we prefer to believe p iff our credence in p is over \\(\\nicefrac{1}{2}\\). Indeed, on most occasions where we use constitutive rationality to infer that a person has some mental state, the mental state we attribute to them is one they could not fail to have. But functionalists are committed to constitutive rationality (Lewis 1994). So my approach here is consistent with a broadly functionalist outlook.\nObjection: This just looks like a roundabout way of stipulating that to believe that p, your degree of belief in p has to be greater than \\(\\nicefrac{1}{2}\\). Why not just add that as an extra clause than going through these little understood detours about preferences about beliefs?\nReply: There are three reasons for doing things this way rather than adding such a clause.\nFirst, it’s nice to have a systematic theory rather than a theory with an ad hoc clause like that.\nSecond, the effect of this constraint is much more than to restrict belief to propositions whose credence is greater than \\(\\nicefrac{1}{2}\\). Consider a case where p and \\(q\\) and their conjunction are all salient, p and \\(q\\) are probabilistically independent, and the agent’s credence in each is 0.7. Assume also that \\(p, q\\) and \\(p \\wedge q\\) are completely irrelevant to any practical deliberation the agent must make. Then the criteria above imply that the agent does not believe that p or that \\(q\\). The reason is that the agent’s credence in \\(p \\wedge q\\) is 0.49, so she prefers to not believe \\(p \\wedge q\\). But conditional on p, her credence in \\(p \\wedge q\\) is 0.7, so she prefers to believe it. So conditionalising on p does change her preferences with respect to believing \\(p \\wedge q\\), so she doesn’t believe p. So the effect of these stipulations rules out much more than just belief in propositions whose credence is below \\(\\nicefrac{1}{2}\\).\nThis suggests the third, and most important point. The problem with the threshold view was that it led to violations of closure. Given the theory as stated, we can prove the following theorem. Whenever p and \\(q\\) and their conjunction are all open or salient, and both are believed, and the agent is probabilistically coherent, the agent also believes \\(p \\wedge q\\). This is a quite restricted closure principle, but this is no reason to deny that it is true, as it fails to be true on the threshold view.\nThe proof of this theorem is a little complicated, but worth working through. First we’ll prove that if the agent believes p, believes \\(q\\), and p and \\(q\\) are both salient, then the agent prefers believing \\(p \\wedge q\\) to not believing it, if \\(p \\wedge q\\) is eligible for belief. In what follows Pr(\\(x | y\\)) is the agent’s conditional degree of belief in \\(x\\) given \\(y\\). Since the agent is coherent, we’ll assume this is a probability function (hence the name).\nSince the agent believes that \\(q\\), they prefer believing that \\(q\\) to not believing that \\(q\\) (by the criteria for belief)\nSo the agent prefers believing that \\(q\\) to not believing that \\(q\\) given p (From 1 and the fact that they believe that p, and that \\(q\\) is salient)\nSo Pr(\\(q | p\\)) \\(> \\nicefrac{1}{2}\\) (from 2)\nPr(\\(q | p\\)) = Pr(\\(p \\wedge q | p\\)) (by probability calculus)\nSo Pr(\\(p \\wedge q | p\\)) \\(> \\nicefrac{1}{2}\\) (from 3, 4)\nSo, if \\(p \\wedge q\\) is eligible for belief, then the agent prefers believing that \\(p \\wedge q\\) to not believing it, given p (from 5)\nSo, if \\(p \\wedge q\\) is eligible for belief, the agent prefers believing that \\(p \\wedge q\\) to not believing it (from 6, and the fact that they believe that p, and \\(p \\wedge q\\) is salient)\nSo whenever, \\(p, q\\) and \\(p \\wedge q\\) are salient, and the agent believes each conjunct, the agent prefers believing the conjunction \\(p \\wedge q\\) to not believing it, if \\(p \\wedge q\\) is eligible. Now we have to prove that \\(p \\wedge q\\) is eligible for belief, to prove that it is actually believed. That is, we have to prove that (5) follows from (4) and (3), where the initial quantifiers range over actions that are open and salient tout court.\n\\(\\forall\\)A\\(\\forall\\)B\\(\\forall r\\) (A \\(\\geq_r\\) B \\(\\leftrightarrow\\) A \\(\\geq _p \\wedge r\\) B)\n\\(\\forall\\)A\\(\\forall\\)B\\(\\forall r\\) (A \\(\\geq_r\\) B \\(\\leftrightarrow\\) A \\(\\geq _q \\wedge r\\) B)\n\\(\\forall\\)A\\(\\forall\\)B\\(\\forall r\\) (A \\(\\geq_r\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q \\wedge r}\\) B)\nAssume that (5) isn’t true. That is, there are A, B and S such that \\(\\neg\\)(A \\(\\geq_s\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q \\wedge s}\\)B). By hypothesis S is active, and consistent with \\(p \\wedge q\\). So it is the conjunction of relevant, salient propositions. Since \\(q\\) is salient, this means \\(q \\wedge s\\) is also active. Since S is consistent with \\(p \\wedge q\\), it follows that \\(q \\wedge s\\) is consistent with p. So \\(q \\wedge s\\) is a possible substitution instance for \\(r\\) in (3). Since (3) is true, it follows that A \\(\\geq _{q \\wedge s}\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q \\wedge s}\\) B. By similar reasoning, it follows that \\(s\\) is a permissible substitution instance in (4), giving us A \\(\\geq_s\\) B \\(\\leftrightarrow\\) A \\(\\geq _{q \\wedge s}\\) B. Putting the last two biconditionals together we get A \\(\\geq_s\\) B \\(\\leftrightarrow\\) A \\(\\geq _{p \\wedge q \\wedge s}\\)B, contradicting our hypothesis that there is a counterexample to (5). So whenever (3) and (4) are true, (5) is true as well, assuming \\(p, q\\) and \\(p \\wedge q\\) are all salient.\nDefending Closure\nSo on my account of the connection between degrees of belief and belief tout court, probabilistic coherence implies logical coherence amongst salient propositions. The last qualification is necessary. It is possible for a probabilistically coherent agent to not believe the non-salient consequences of things they believe, and even for a probabilistically coherent agent to have inconsistent beliefs as long as not all the members of the inconsistent set are active. Some people argue that even this weak a closure principle is implausible. David Christensen (2005), for example, argues that the preface paradox provides a reason for doubting that beliefs must be closed under entailment, or even must be consistent. Here is his description of the case.\n\nWe are to suppose that an apparently rational person has written a long non-fiction book—say, on history. The body of the book, as is typical, contains a large number of assertions. The author is highly confident in each of these assertions; moreover, she has no hesitation in making them unqualifiedly, and would describe herself (and be described by others) as believing each of the book’s many claims. But she knows enough about the difficulties of historical scholarship to realize that it is almost inevitable that at least a few of the claims she makes in the book are mistaken. She modestly acknowledges this in her preface, by saying that she believes the book will be found to contain some errors, and she graciously invites those who discover the errors to set her straight. (Christensen 2005, 33–34)\n\nChristensen thinks such an author might be rational in every one of her beliefs, even though these are all inconsistent. Although he does not say this, nothing in his discussion suggests that he is using the irrelevance of some of the propositions in the author’s defence. So here is an argument that we should abandon closure amongst relevant beliefs.\nChristensen’s discussion, like other discussions of the preface paradox, makes frequent use of the fact that examples like these are quite common. We don’t have to go to fake barn country to find a counterexample to closure. But it seems to me that we need two quite strong idealisations in order to get a real counterexample here.\nThe first of these is discussed in forthcoming work by Ishani Maitra (Maitra 2010), and is briefly mentioned by Christensen in setting out the problem. We only have a counterexample to closure if the author believes every thing she writes in her book. (Indeed, we only have a counterexample if she reasonably believes every one of them. But we’ll assume a rational author who only believes what she ought to believe.) This seems unlikely to be true to me. An author of a historical book is like a detective who, when asked to put forward her best guess about what explains the evidence, says “If I had to guess, I’d say …” and then launches into spelling out her hypothesis. It seems clear that she need not believe the truth of her hypothesis. If she did that, she could not later learn it was true, because you can’t learn the truth of something you already believe. And she wouldn’t put any effort into investigating alternative suspects. But she can come to learn her hypothesis was true, and it would be rational to investigate other suspects. It seems to me (following here Maitra’s discussion) that we should understand scholarly assertions as being governed by the same kind of rules that govern detectives making the kind of speech being contemplated here. And those rules don’t require that the speaker believe the things they say without qualification. The picture is that the little prelude the detective explicitly says is implicit in all scholarly work.\nThere are three objections I know to this picture, none of them particularly conclusive. First, Christensen says that the author doesn’t qualify their assertions. But neither does our detective qualify most individual sentences. Second, Christensen says that most people would describe our author as believing her assertions. But it is also natural to describe our detective as believing the things she says in her speech. It’s natural to say things like “She thinks it was the butler, with the lead pipe,” in reporting her hypothesis. Third, Timothy Williamson (2000) has argued that if speakers don’t believe what they say, we won’t have an explanation of why Moore’s paradoxical sentences, like “The butler did it, but I don’t believe the butler did it,” are always defective. Whatever the explanation of the paradoxicality of these sentences might be, the alleged requirement that speakers believe what they say can’t be it. For our detective cannot properly say “The butler did it, but I don’t believe the butler did it” in setting out her hypothesis, even though believing the butler did it is not necessary for her to say “The butler did it” in setting out just that hypothesis.\nIt is plausible that for some kinds of books, the author should only say things they believe. This is probably true for travel guides, for example. Interestingly, casual observation suggests that authors of such books are much less likely to write modest prefaces. This makes some sense if those books can only include statements their authors believe, and the authors believe the conjunctions of what they believe.\nThe second idealisation is stressed by Simon Evnine (1999) in his paper “Believing Conjunctions.” The following situation does not involve me believing anything inconsistent.\nI believe that what Manny just said, whatever it was, is false.\nManny just said that the stands at Fenway Park are green.\nI believe that the stands at Fenway Park are green.\nIf we read the first claim de dicto, that I believe that Manny just said something false, then there is no inconsistency. (Unless I also believe that what Manny just said was that the stands in Fenway Park are green.) But if we read it de re, that the thing Manny just said is one of the things I believe to be false, then the situation does involve me being inconsistent. The same is true when the author believes that one of the things she says in her book is mistaken. If we understand what she says de dicto, there is no contradiction in her beliefs. It has to be understood de re before we get a logical problem. And the fact is that most authors do not have de re attitudes towards the claims made in their book. Most authors don’t even remember everything that’s in their books. (I’m not sure I remember how this section started, let alone this paper.) Some may argue that authors don’t even have the capacity to consider a proposition as long and complicated as the conjunction of all the claims in their book. Christensen considers this objection, but says it isn’t a serious problem.\n\nIt is undoubtedly true that ordinary humans cannot entertain book-length conjunctions. But surely, agents who do not share this fairly superficial limitation are easily conceived. And it seems just as wrong to say of such agents that they are rationally required to believe in the inerrancy of the books they write. (38: my emphasis)\n\nI’m not sure this is undoubtedly true; it isn’t clear that propositions (as opposed to their representations) have lengths. And humans can believe propositions that can be represented by sentences as long as books. But even without that point, Christensen is right that there is an idealisation here, since ordinary humans do not know exactly what is in a given book, and hence don’t have de re attitudes towards the propositions expressed in the book.\nI’m actually rather suspicious of the intuition that Christensen is pushing here, that idealising in this way doesn’t change intuitions about the case. The preface paradox gets a lot of its (apparent) force from intuitions about what attitude we should have towards real books. Once we make it clear that the real life cases are not relevant to the paradox, I find the intuitions become rather murky. But I won’t press this point.\nA more important point is that we believers in closure don’t think that authors should think their books are inerrant. Rather, following Stalnaker (1984), we think that authors shouldn’t unqualifiedly believe the individual statements in their book if they don’t believe the conjunction of those statements. Rather, their attitude towards those propositions (or at least some of them) should be that they are probably true. (As Stalnaker puts it, they accept the story without believing it.) Proponents of the preface paradox know that this is a possible response, and tend to argue that it is impractical. Here is Christensen on this point.\n\nIt is clear that our everyday binary way of talking about beliefs has immense practical advantages over a system which insisted on some more fine-grained reporting of degrees of confidence … At a minimum, talking about people as believing, disbelieving, or withholding belief has at least as much point as do many of the imprecise ways we have of talking about things that can be described more precisely. (96)\n\nRichard Foley makes a similar point.\n\nThere are deep reasons for wanting an epistemology of beliefs, reasons that epistemologies of degrees of belief by their very nature cannot possibly accommodate. (Foley 1993, 170, my emphasis)\n\nIt’s easy to make too much of this point. It’s a lot easier to triage propositions into TRUE, FALSE and NOT SURE and work with those categories than it is to work assign precise numerical probabilities to each proposition. But these are not the only options. Foley’s discussion subsequent to the above quote sometimes suggests they are, especially when he contrasts the triage with “indicat\\[ing\\] as accurately as I can my degree of confidence in each assertion that I defend.” (171) But really it isn’t much harder to add two more categories, PROBABLY TRUE and PROBABLY FALSE to those three, and work with that five-way division rather than a three-way division. It’s not clear that humans as they are actually constructed have a strong preference for the three-way over the five-way division, and even if they do, I’m not sure in what sense this is a ‘deep’ fact about them.\nOnce we have the five-way division, it is clear what authors should do if they want to respect closure. For any conjunction that they don’t believe (i.e. classify as true), they should not believe one of the conjuncts. But of course they can classify every conjunct as probably true, even if they think the conjunction is false, or even certainly false. Still, might it not be considered something of an idealisation to say rational authors must make this five-way distinction amongst propositions they consider? Yes, but it’s no more of an idealisation than we need to set up the preface paradox in the first place. To use the preface paradox to find an example of someone who reasonably violates closure, we need to insist on the following three constraints.\nThey are part of a research community where only asserting propositions you believe is compatible with active scholarship;\nThey know exactly what is in their book, so they are able to believe that one of the propositions in the book is mistaken, where this is understood de re; but\nThey are unable to effectively function if they have to effect a five-way, rather than a three-way, division amongst the propositions they consider.\nPut more graphically, to motivate the preface paradox we have to think that our inability to have de re thoughts about the contents of books is a “superficial constraint,” but our preference for working with a three-way rather than a five-way division is a “deep” fact about our cognitive system. Maybe each of these attitudes could be plausible taken on its own (though I’m sceptical of that) but the conjunction seems just absurd.\nI’m not entirely sure an agent subject to exactly these constraints is even fully conceivable. (Such an agent is negatively conceivable, in David Chalmers’s terminology, but I rather doubt they are positively conceivable.) But even if they are a genuine possibility, why the norms applicable to an agent satisfying that very gerrymandered set of constraints should be considered relevant norms for our state is far from clear. I’d go so far as to say it’s clear that the applicability (or otherwise) of a given norm to such an odd agent is no reason whatsoever to say it applies to us. But since the preface paradox only provides a reason for just these kinds of agents to violate closure, we have no reason for ordinary humans to violate closure. So I see no reason here to say that we can have probabilistic coherence without logical coherence, as proponents of the threshold view insist we can have, but which I say we can’t have at least when the propositions involved are salient. The more pressing question, given the failure of the preface paradox argument, is why I don’t endorse a much stronger closure principle, one that drops the restriction to salient propositions. The next section will discuss that point.\nI’ve used Christensen’s book as a stalking horse in this section, because it is the clearest and best statement of the preface paradox. Since Christensen is a paradox-mongerer and I’m a paradox-denier, it might be thought we have a deep disagreement about the relevant epistemological issues. But actually I think our overall views are fairly close despite this. I favour an epistemological outlook I call “Probability First,” the view that getting the epistemology of partial belief right is of the first importance, and everything else should flow from that. Christensen’s view, reduced to a slogan, is “Probability First and Last.” This section has been basically about the difference between those two slogans. It’s an important dispute, but it’s worth bearing in mind that it’s a factional squabble within the Probability Party, not an outbreak of partisan warfare.\nToo Little Closure?\nIn the previous section I defended the view that a coherent agent has beliefs that are deductively cogent with respect to salient propositions. Here I want to defend the importance of the qualification. Let’s start with what I take to be the most important argument for closure, the passage from Stalnaker’s Inquiry that I quoted above.\n\nReasoning in this way from accepted premises to their deductive consequences (\\(P\\), also \\(Q\\), therefore \\(R\\)) does seem perfectly straightforward. Someone may object to one of the premises, or to the validity of the argument, but one could not intelligibly agree that the premises are each acceptable and the argument valid, while objecting to the acceptability of the conclusion. (Stalnaker 1984, 92)\n\nStalnaker’s wording here is typically careful. The relevant question isn’t whether we can accept p, accept \\(q\\), accept p and \\(q\\) entail \\(r\\), and reject \\(r\\). As Christensen (2005 Ch. 4) notes, this is impossible even on the threshold view, as long as the threshold is above 2/3. The real question is whether we can accept p, accept \\(q\\), accept p and \\(q\\) entail \\(r\\), and fail to accept \\(r\\). And this is always a live possibility on any threshold view, though it seems absurd at first that this could be coherent.\nBut it’s important to note how active the verbs in Stalnaker’s description are. When faced with a valid argument we have to object to one of the premises, or the validity of the argument. What we can’t do is agree to the premises and the validity of the argument, while objecting to the conclusion. I agree. If we are really agreeing to some propositions, and objecting to others, then all those propositions are salient. And in that case closure, deductive coherence, is mandatory. This doesn’t tell us what we have to do if we haven’t previously made the propositions salient in the first place.\nThe position I endorse here is very similar in its conclusions to that endorsed by Gilbert Harman in Change in View. There Harman endorses the following principle. (At least he endorses it as true – he doesn’t seem to think it is particularly explanatory because it is a special case of a more general interesting principle.)\nRecognized Logical Implication Principle\nOne has reason to believe p if one recognizes that p is logically implied by one’s view. (Harman 1986, 17)\n\nThis seems right to me, both what it says and its implicature that the reason in question is not a conclusive reason. My main objection to those who use the preface paradox to argue against closure is that they give us a mistaken picture of what we have to do epistemically. When I have inconsistent beliefs, or I don’t believe some consequence of my beliefs, that is something I have a reason to deal with at some stage, something I have to do. When we say that we have things to do, we don’t mean that we have to do them right now, or instead of everything else. My current list of things to do includes cleaning my bathroom, yet here I am writing this paper, and (given the relevant deadlines) rightly so. We can have the job of cleaning up our epistemic house as something to do while recognising that we can quite rightly do other things first. But it’s a serious mistake to infer from the permissibility of doing other things that cleaning up our epistemic house (or our bathroom) isn’t something to be done. The bathroom won’t clean itself after all, and eventually this becomes a problem.\nThere is a possible complication when it comes to tasks that are very low priority. My attic is to be cleaned, or at least it could be cleaner, but there are no imaginable circumstances under which something else wouldn’t be higher priority. Given that, should we really leave clean the attic on the list of things to be done? Similarly, there might be implications I haven’t followed through that it couldn’t possibly be worth my time to sort out. Are they things to be done? I think it’s worthwhile recording them as such, because otherwise we might miss opportunities to deal with them in the process of doing something else. I don’t need to put off anything else in order to clean the attic, but if I’m up there for independent reasons I should bring down some of the garbage. Similarly, I don’t need to follow through implications mostly irrelevant to my interests, but if those propositions come up for independent reasons, I should deal with the fact that some things I believe imply something I don’t believe. Having it be the case that all implications from things we believe to things we don’t believe constitute jobs to do (possibly in the loose sense that cleaning my attic is something to do) has the right implications for what epistemic duties we do and don’t have.\nWhile waxing metaphorical, it seems time to pull out a rather helpful metaphor that Gilbert Ryle (1949) develops in The Concept of Mind at a point where he’s covering what we’d now call the inference/implication distinction. (This is a large theme of chapter 9, see particularly pages 292-309.) Ryle’s point in these passages, as it frequently is throughout the book, is to stress that minds are fundamentally active, and the activity of a mind cannot be easily recovered from its end state. Although Ryle doesn’t use this language, his point is that we shouldn’t confuse the difficult activity of drawing inferences with the smoothness and precision of a logical implication. The language Ryle does use is more picturesque. He compares the easy work a farmer does when sauntering down a path from the hard work he did when building the path. A good argument, in philosophy or mathematics or elsewhere, is like a well made path that permits sauntering from the start to finish without undue strain. But from that it doesn’t follow that the task of coming up with that argument, of building that path in Ryle’s metaphor, was easy work. The easiest paths to walk are often the hardest to build. Path-building, smoothing out our beliefs so they are consistent and closed under implication, is hard work, even when the finished results look clean and straightforward. Its work that we shouldn’t do unless we need to. But making sure our beliefs are closed under entailment even with respect to irrelevant propositions is suspiciously like the activity of buildings paths between points without first checking you need to walk between them.\nFor a less metaphorical reason for doubting the wisdom of this unchecked commitment to closure, we might notice the difficulties theorists tend to get into all sorts of difficulties. Consider, for example, the view put forward by Mark Kaplan in Decision Theory as Philosophy. Here is his definition of belief.\n\nYou count as believing P just if, were your sole aim to assert the truth (as it pertains to P), and you only options were to assert that P, assert that \\(\\neg\\)P or make neither assertion, you would prefer to assert that P. (109)\n\nKaplan notes that conditional definitions like this are prone to Shope’s conditional fallacy. If my sole aim were to assert the truth, I might have different beliefs to what I now have. He addresses one version of this objection (namely that it appears to imply that everyone believes their sole desire is to assert the truth) but as we’ll see presently he can’t avoid all versions of it.\nThese arguments are making me thirsty. I’d like a beer. Or at least I think I would. But wait! On Kaplan’s theory I can’t think that I’d like a beer, for if my sole aim were to assert the truth as it pertains to my beer-desires, I wouldn’t have beer desires. And then I’d prefer to assert that I wouldn’t like a beer, I’d merely like to assert the truth as it pertains to my beer desires.\nEven bracketing this concern, Kaplan ends up being committed to the view that I can (coherently!) believe that p even while regarding p as highly improbable. This looks like a refutation of the view to me, but Kaplan accepts it with some equanimity. He has two primary reasons for saying we should live with this. First, he says that it only looks like an absurd consequence if we are committed to the Threshold View. To this all I can say is that I don’t believe the Threshold View, but it still seems absurd to me. Second, he says that any view is going to have to be revisionary to some extent, because our ordinary concept of belief is not “coherent” (142). His view is that, “Our ordinary notion of belief both construes belief as a state of confidence short of certainty and takes consistency of belief to be something that is at least possible and, perhaps, even desirable” and this is impossible. I think the view here interprets belief as a state less than confidence and allows for as much consistency as the folk view does (i.e. consistency amongst salient propositions), so this defence is unsuccessful as well.\nNone of the arguments here in favour of our restrictions on closure are completely conclusive. In part the argument at this stage rests on the lack of a plausible rival theory that doesn’t interpret belief as certainty but implements a stronger closure principle. It’s possible that tomorrow someone will come up with a theory that does just this. Until then, we’ll stick with the account here, and see what its epistemological implications might be.\nExamples of Pragmatic Encroachment\nFantl and McGrath’s case for pragmatic encroachment starts with cases like the following. (The following case is not quite theirs, but is similar enough to suit their plan, and easier to explain in my framework.)\n\nLocal and Express\nThere are two kinds of trains that run from the city to the suburbs: the local, which stops at all stations, and the express, which skips the first eight stations. Harry and Louise want to go to the fifth station, so they shouldn’t catch the Express. Though if they do it isn’t too hard to catch a local back the other way, so it isn’t usually a large cost. Unfortunately, the trains are not always clearly labelled. They see a particular train about to leave. If it’s a local they are better off catching it, if it is an express they should wait for the next local, which they can see is already boarding passengers and will leave in a few minutes. While running towards the train, they hear a fellow passenger say “It’s a local.” This gives them good, but far from overwhelming, reason to believe that the train is a local. Passengers get this kind of thing wrong fairly frequently, but they don’t have time to get more information. So each of them face a gamble, which they can take by getting on the train. If the train is a local, they will get home a few minutes early. If it is an express they will get home a few minutes later. For Louise, this is a low stakes gamble, as nothing much turns on whether she is a few minutes early or late, but she does have a weak preference for arriving earlier rather than later. But for Harry it is a high stakes gamble, because if he is late he won’t make the start of his daughter’s soccer game, which will highly upset her. There is no large payoff for Harry arriving early.\n\nWhat should each of them do? What should each of them believe?\nThe first question is relatively easy. Louise should catch the train, and Harry should wait for the next. For each of them that’s the utility maximising thing to do. The second one is harder. Fantl and McGrath suggest that, despite being in the same epistemic position with respect to everything except their interests, Louise is justified in believing the train is a local and Harry is not. I agree. (If you don’t think the particular case fits this pattern, feel free to modify it so the difference in interests grounds a difference in what they are justified in believing.) Does this show that our notion of epistemic justification has to be pragmatically sensitive? I’ll argue that it does not.\nThe fundamental assumption I’m making is that what is primarily subject to epistemic evaluation are degrees of belief, or what are more commonly called states of confidence in ordinary language. When we think about things this way, we see that Louise and Harry are justified in adopting the very same degrees of belief. Both of them should be confident, but not absolutely certain, that the train is a local. We don’t have even the appearance of a counterxample to Probabilistic Evidentialism here. If we like putting this in numerical terms, we could say that each of them is justified in assigning a probability of around 0.9 to the proposition That train is a local.4 So as long as we adopt a Probability First epistemology, where we in the first instance evaluate the probabilities that agents assign to propositions, Harry and Louise are evaluated alike iff they do the same thing.\nHow then can we say that Louise alone is justified in believing that the train is a local? Because that state of confidence they are justified in adopting, the state of being fairly confident but not absolutely certain that the train is a local, counts as believing that the train is a local given Louise’s context but not Harry’s context. Once Louise hears the other passenger’s comment, conditionalising on That’s a local doesn’t change any of her preferences over open, salient actions, including such ‘actions’ as believing or disbelieving propositions. But conditional on the train being a local, Harry prefers catching the train, which he actually does not prefer.\nIn cases like this, interests matter not because they affect the degree of confidence that an agent can reasonably have in a proposition’s truth. (That is, not because they matter to epistemology.) Rather, interests matter because they affect whether those reasonable degrees of confidence amount to belief. (That is, because they matter to philosophy of mind.) There is no reason here to let pragmatic concerns into epistemology.\nJustification and Practical Reasoning\nThe discussion in the last section obviously didn’t show that there is no encroachment of pragmatics into epistemology. There are, in particular, two kinds of concerns one might have about the prospects for extending my style of argument to block all attempts at pragmatic encroachment. The biggest concern is that it might turn out to be impossible to defend a Probability First epistemology, particularly if we do not allow ourselves pragmatic concerns. For instance, it is crucial to this project that we have a notion of evidence that is not defined in terms of traditional epistemic concepts (e.g. as knowledge), or in terms of interests. This is an enormous project, and I’m not going to attempt to tackle it here. The second concern is that we won’t be able to generalise the discussion of that example to explain the plausibility of (JP) without conceding something to the defenders of pragmatic encroachment.\n(JP)\nIf S justifiably believes that p, then S is justified in using p as a premise in practical reasoning.\n\nAnd that’s what we will look at in this section. To start, we need to clarify exactly what (JP) means. Much of this discussion will be indebted to Fantl and McGrath’s discussion of various ways of making (JP) more precise. To see some of the complications at issue, consider a simple case of a bet on a reasonably well established historical proposition. The agent has a lot of evidence that supports p, and is offered a bet that returns $1 if p is true, and loses $500 if p is false. Since her evidence doesn’t support that much confidence in p, she properly declines the bet. One might try to reason intuitively as follows. Assume that she justifiably believed that p. Then she’d be in a position to make the following argument.\n\np\nIf p, then I should take the bet\nSo, I should take the bet\n\nSince she isn’t in a position to draw the conclusion, she must not be in a position to endorse both of the premises. Hence (arguably) she isn’t justified in believing that p. But we have to be careful here. If we assume also that p is true (as Fantl and McGrath do, because they are mostly concerned with knowledge rather than justified belief), then the second premise is clearly false, since it is a conditional with a true antecedent and a false consequent. So the fact that she can’t draw the conclusion of this argument only shows that she can’t endorse both of the premises, and that’s not surprising since one of the premises is most likely false. (I’m not assuming here that the conditional is true iff it has a true antecendent or a false consequent, just that it is only true if it has a false antecedent or a true consequent.)\nIn order to get around this problem, Fantl and McGrath suggest a few other ways that our agent might reason to the bet. They suggest each of the following principles.\n\nS knows that p only if, for any act A, if S knows that if p, then A is the best thing she can do, then S is rational to do A. (72)\nS knows that p only if, for any states of affairs A and B, if \\(S\\) knows that if p, then A is better for her than B, then S is rational to prefer A to B. (74)\n(PC) S is justified in believing that p only if S is rational to prefer as if p. (77)\n\nHawthorne (2004, 174–81) appears to endorse the second of these principles. He considers an agent who endorses the following implication concerning a proposed sell of a lottery ticket for a cent, which is well below its actuarially fair value.\n\nI will lose the lottery.\nIf I keep the ticket, I will get nothing.\nIf I sell the ticket, I will get a cent.\nSo I ought to sell the ticket. (174)\n\n(To make this fully explicit, it helps to add the tacit premise that a cent is better than nothing.) Hawthorne says that this is intuitively a bad argument, and concludes that the agent who attempts to use it is not in a position to know its first premise. But that conclusion only follows if we assume that the argument form is acceptable. So it is plausible to conclude that he endorses Fantl and McGrath’s second principle.\nThe interesting question here is whether the theory endorsed in this paper can validate the true principles that Fantl and McGrath articulate. (Or, more precisely, we can validate the equivalent true principles concerning justified belief, since knowledge is outside the scope of the paper.) I’ll argue that it can in the following way. First, I’ll just note that given the fact that the theory here implies the closure principles we outlined in section 5, we can easily enough endorse Fantl and McGrath’s first two principles. This is good, since they seem true. The longer part of the argument involves arguing that their principle (PC), which doesn’t hold on the theory endorsed here, is in fact incorrect.\nOne might worry that the qualification on the closure principles in section 5 mean that we can’t fully endorse the principles Fantl and McGrath endorse. In particular, it might be worried that there could be an agent who believes that p, believes that if p, then A is better than B, but doesn’t put these two beliefs together to infer that A is better than B. This is certainly a possibility given the qualifications listed above. But note that in this position, if those two beliefs were justified, the agent would certainly be rational to conclude that A is better than B, and hence rational to prefer A to B. So the constraints on the closure principles don’t affect our ability to endorse these two principles.\nThe real issue is (PC). Fantl and McGrath offer a lot of cases where (PC) holds, as well as arguing that it is plausibly true given the role of implications in practical reasoning. What’s at issue is that (PC) is stronger than a deductive closure principle. It is, in effect, equivalent to endorsing the following schema as a valid principle of implication.\n\np\nGiven p, A is preferable to B\nSo, A is preferable to B\n\nI call this Practical Modus Ponens, or PMP. The middle premise in PMP is not a conditional. It is not to be read as If p, then A is preferable to B. Conditional valuations are not conditionals. To see this, again consider the proposed bet on (true) p at exorbitant odds, where A is the act of taking the bet, and B the act of declining the bet. It’s true that given p, A is preferable to B. But it’s not true that if p, then A is preferable to B. Even if we restrict our attention to cases where the preferences in question are perfectly valid, this is a case where PMP is invalid. Both premises are true, and the conclusion is false. It might nevertheless be true that whenever an agent is justified in believing both of the premises, she is justified in believing the conclusion. To argue against this, we need a very complicated case, involving embedded bets and three separate agents, Quentin, Robby and Thom. All of them have received the same evidence, and all of them are faced with the same complex bet, with the following properties.\np is an historical proposition that is well (but not conclusively) supported by their evidence, and happens to be true. All the agents have a high credence in p, which is exactly what the evidence supports.\nThe bet A, which they are offered, wins if p is true, and loses if p is false.\nIf they win the bet, the prize is the bet B.\nS is also an historical proposition, but the evidence tells equally for and against it. All the agents regard S as being about as likely as not. Moreover, S turns out to be false.\nThe bet B is worth $2 if S is true, and worth -$1 if S is false. Although it is actually a losing bet, the agents all rationally value it at around 50 cents.\nHow much A costs is determined by which proposition from the partition {\\(q, r, s\\)} is true.\nIf \\(q\\) is true, A costs $2\nIf \\(r\\) is true, A costs $500\nIf \\(t\\) is true, A costs $1\nThe evidence the agents has strongly supports \\(r\\), though \\(t\\) is in fact true\nQuentin believes \\(q\\)\nRobby believes \\(r\\)\nThom believes \\(t\\)\nAll of the agents make the utility calculations that their beliefs support, so Quentin and Thom take the bet and lose a dollar, while Robby declines it. Although Robby has a lot of evidence in favour of p, he correctly decides that it would be unwise to bet on p at effective odds of 1000 to 1 against. I’ll now argue that both Quentin and Thom are potential counterexamples to (PC). There are three possibilities for what we can say about those two.\nFirst, we could say that they are justified in believing p, and rational to take the bet. The problem with this position is that if they had rational beliefs about the partition {\\(q, r, t\\)} they would realise that taking the bet does not maximise expected utility. If we take rational decisions to be those that maximise expected utility given a rational response to the evidence, then the decisions are clearly not rational.\nSecond, we could say that although Quentin and Thom are not rational in accepting the bet, nor are they justified in believing that p. This doesn’t seem particularly plausible for several reasons. The irrationality in their belief systems concerns whether \\(q, r\\) or \\(t\\) is true, not whether p is true. If Thom suddenly got a lot of evidence that \\(t\\) is true, then all of his (salient) beliefs would be well supported by the evidence. But it is bizarre to think that whether his belief in p is rational turns on how much evidence he has for \\(t\\). Finally, even if we accept that agents in higher stakes situations need more evidence to have justified beliefs, the fact is that the agents are in a low-risk situation, since \\(t\\) is actually true, so the most they could lose is $1.\nSo it seems like the natural thing to say is that Quentin and Thom are justified in believing that p, and are justified in believing that given p, it maximises expected utility to take the bet, but they are not rational to take the bet. (At least, in the version of the story where they are thinking about which of \\(q, r\\) and \\(t\\) are correct given their evidence when thinking about whether to take the bet they are counterexamples to (PC).) Against this, one might respond that if belief in p is justified, there are arguments one might make to the conclusion that the bet should be taken. So it is inconsistent to say that the belief is justified, but the decision to take the bet is not rational. The problem is finding a premise that goes along with p to get the conclusion that taking the bet is rational. Let’s look at some of the premises the agent might use.\nIf p, then the best thing to do is to take the bet.\nThis isn’t true (p is true, but the best thing to do isn’t to take the bet). More importantly, the agents think this is only true if S is true, and they think S is a 50/50 proposition. So they don’t believe this premise, and it would not be rational to believe it.\nIf p, then probably the best thing to do is to take the bet.\nAgain this isn’t true, and it isn’t well supported, and it doesn’t even support the conclusion, for it doesn’t follow from the fact that \\(x\\) is probably the best thing to do that \\(x\\) should be done.\nIf p, then taking the bet maximises rational expected utility.\nThis isn’t true – it is a conditional with a true antecedent and a false consequent. Moreover, if Quentin and Thom were rational, like Robby, they would recognise this.\nIf p, then taking the bet maximises expected utility relative to their beliefs.\nThis is true, and even reasonable to believe, but it doesn’t imply that they should take the bet. It doesn’t follow from the fact that doing something maximises expected utility relative to my crazy beliefs that I should do that thing.\nGiven p, taking the bet maximises rational expected utility.\nThis is true, and even reasonable to believe, but it isn’t clear that it supports the conclusion that the agents should take the bet. The implication appealed to here is PMP, and in this context that’s close enough to equivalent to (PC). If we think that this case is a prima facie problem for (PC), as I think is intuitively plausible, then we can’t use (PC) to show that it doesn’t post a problem. We could obviously continue for a while, but it should be clear it will be very hard to find a way to justify taking the bet even spotting the agents p as a premise they can use in rational deliberation. So it seems to me that (PC) is not in general true, which is good because as we’ll see in cases like this one the theory outlined here does not support it.\nThe theory we have been working with says that belief that p is justified iff the agent’s degree of belief in p is sufficient to amount to belief in their context, and they are justified in believing p to that degree. Since by hypothesis Quentin and Thom are justified in believing p to the degree that they do, the only question left is whether this amounts to belief. This turns out not to be settled by the details of the case as yet specified. At first glance, assuming there are no other relevant decisions, we might think they believe that p because (a) they prefer (in the relevant sense) believing p to not believing p, and (b) conditionalising on p doesn’t change their attitude towards the bet. (They prefer taking the bet to declining it, both unconditionally and conditional on p.)\nBut that isn’t all there is to the definition of belief tout court. We must also ask whether conditionalising on p changes any preferences conditional on any active proposition. And that may well be true. Conditional on \\(r\\), Quentin and Thom prefer not taking the bet to taking it. But conditional on \\(r\\) and p, they prefer taking the bet to not taking it. So if \\(r\\) is an active proposition, they don’t believe that p. If \\(r\\) is not active, they do believe it. In more colloquial terms, if they are concerned about the possible truth of \\(r\\) (if it is salient, or at least not taken for granted to be false) then p becomes a potentially high-stakes proposition, so they don’t believe it without extraordinary evidence (which they don’t have). Hence they are only a counterexample to (PC) if \\(r\\) is not active. But if \\(r\\) is not active, our theory predicts that they are a counterexample to (PC), which is what we argued above is intuitively correct.\nStill, the importance of \\(r\\) suggests a way of saving (PC). Above I relied on the position that if Quentin and Thom are not maximising rational expected utility, then they are being irrational. This is perhaps too harsh. There is a position we could take, derived from some suggestions made by Gilbert Harman in Change in View, that an agent can rationally rely on their beliefs, even if those beliefs were not rationally formed, if they cannot be expected to have kept track of the evidence they used to form that belief. If we adopt this view, then we might be able to say that (PC) is compatible with the correct normative judgments about this case.\nTo make this compatibility explicit, let’s adjust the case so Quentin takes \\(q\\) for granted, and cannot be reasonably expected to have remembered the evidence for \\(q\\). Thom, on the other hand, forms the belief that \\(t\\) rather than \\(r\\) is true in the course of thinking through his evidence that bears on the rationality of taking or declining the bet. (In more familiar terms, \\(t\\) is part of the inference Thom uses in coming to conclude that he should take the bet, though it is not part of the final implication he endorses whose conclusion is that he should take the bet.) Neither Quentin nor Thom is a counterexample to (PC) thus understood. (That is, with the notion of rationality in (PC) understood as Harman suggests that it should be.) Quentin is not a counterexample, because he is rational in taking the bet. And Thom is not a counterexample, because in his context, where \\(r\\) is active, his credence in p does not amount to belief in p, so he is not justified in believing p.\nWe have now two readings of (PC). On the strict reading, where a rational choice is one that maximises rational expected utility, the principle is subject to counterexample, and seems generally to be implausible. On the loose reading, where we allow agents to rely on beliefs formed irrationally in the past in rational decision making, (PC) is plausible. Happily, the theory sketched here agrees with (PC) on the plausible loose reading, but not on the implausible strict reading. In the previous section I argued that the theory also accounts for intuitions about particular cases like Local and Express. And now we’ve seen that the theory accounts for our considered opinions about which principles connecting justified belief to rational decision making we should endorse. So it seems at this stage that we can account for the intuitions behind the pragmatic encroachment view while keeping a concept of probabilistic epistemic justification that is free of pragmatic considerations.\nConclusions\nGiven a pragmatic account of belief, we don’t need to have a pragmatic account of justification in order to explain the intuitions that whether \\(S\\) justifiably believes that p might depend on pragmatic factors. My focus here has been on sketching a theory of belief on which it is the belief part of the concept of a justified belief which is pragmatically sensitive. I haven’t said much about why we should prefer to take that option than say that the notion of epistemic justification is a pragmatic notion. I’ve mainly been aiming to show that a particular position is an open possibility, namely that we can accept that whether a particular agent is justified in believing p can be sensitive to their practical environment without thinking that the primary epistemic concepts are themselves pragmatically sensitive.\n\n\nBovens, Luc, and James Hawthorne. 1999. “The Preface, the Lottery, and the Logic of Belief.” Mind 108 (430): 241–64. https://doi.org/10.1093/mind/108.430.241.\n\n\nChristensen, David. 2005. Putting Logic in Its Place. Oxford: Oxford University Press.\n\n\nEvnine, Simon. 1999. “Believing Conjunctions.” Synthese 118: 201–27. https://doi.org/10.1023/A:1005114419965.\n\n\nFantl, Jeremy, and Matthew McGrath. 2002. “Evidence, Pragmatics, and Justification.” Philosophical Review 111: 67–94. https://doi.org/10.2307/3182570.\n\n\nFoley, Richard. 1993. Working Without a Net. Oxford: Oxford University Press.\n\n\nHarman, Gilbert. 1986. Change in View. Cambridge, MA: Bradford.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHunter, Daniel. 1996. “On the Relation Between Categorical and Probabilistic Belief.” Noûs 30: 75–98. https://doi.org/10.2307/2216304.\n\n\nKaplan, Mark. 1996. Decision Theory as Philosophy. Cambridge: Cambridge University Press.\n\n\nKeynes, John Maynard. 1921. Treatise on Probability. London: Macmillan.\n\n\nLewis, David. 1994. “Reduction of Mind.” In A Companion to the Philosophy of Mind, edited by Samuel Guttenplan, 412–31. Oxford: Blackwell. https://doi.org/10.1017/CBO9780511625343.019.\n\n\nMaitra, Ishani. 2010. “Assertion, Norms and Games.” In Assertion: New Philosophical Essays, edited by Jessica Brown and Herman Cappelen, 277–96. Oxford: Oxford University Press.\n\n\nRyle, Gilbert. 1949. The Concept of Mind. New York: Barnes; Noble.\n\n\nStalnaker, Robert. 1984. Inquiry. Cambridge, MA: MIT Press.\n\n\nStanley, Jason. 2005. Knowledge and Practical Interests. Oxford University Press.\n\n\nWeatherson, Brian. 2005. “True, Truer, Truest.” Philosophical Studies 123 (1-2): 47–70. https://doi.org/10.1007/s11098-004-5218-x.\n\n\nWilliamson, Timothy. 1994. Vagueness. Routledge.\n\n\n———. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nTo say the agent prefers A to B given \\(q\\) is not to say that if the agent were to learn \\(q\\), she would prefer A to B. It’s rather to say that she prefers the state of the world where she does A and \\(q\\) is true to the state of the world where she does B and \\(q\\) is true. These two will come apart in cases where learning \\(q\\) changes the agent’s preferences. We’ll return to this issue below.↩︎\nThis might seem much too simple, especially when compared to all the bells and whistles that functionalists usually put in their theories to (further) distinguish themselves from crude versions of behaviourism. The reason we don’t need to include those complications here is that they will all be included in the analysis of preference. Indeed, the theory here is compatible with a thoroughly anti-functionalist treatment of preference. The claim is not that we can offer a functional analysis of belief in terms of non-mental concepts, just that we can offer a functionalist reduction of belief to other mental concepts. The threshold view is also such a reduction, but it is such a crude reduction that it doesn’t obviously fall into any category.↩︎\nConditionalising on the proposition There are space aliens about to come down and kill all the people writing epistemology papers will make me prefer to stop writing this paper, and perhaps grab some old metaphysics papers I could be working on. So that proposition satisfies the second clause of the definition of relevance. But it clearly doesn’t satisfy the first clause. This part of the definition of relevance won’t do much work until the discussion of agents with mistaken environmental beliefs in section 7.↩︎\nI think putting things numerically is misleading because it suggests that the kind of bets we usually use to measure degrees of belief are open, salient options for Louise and Harry. But if those bets were open and salient, they wouldn’t believe the train is a local. Using qualitative rather than quantitative language to describe them is just as accurate, and doesn’t have misleading implications about their practical environment.\n\n↩︎\n",
    "preview": "posts/2021-01-08-can-we-do-without-pragmatic-encroachment/train.jpg",
    "last_modified": "2021-02-05T20:41:59-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-epistemic-modals-in-context/",
    "title": "Epistemic Modals in Context",
    "description": "A very simple contextualist treatment of a sentence containing an  epistemic modal, e.g. *a might be F*, is that it is true iff for all the contextually salient community knows, *a* is *F*. It is widely agreed that the simple theory will not work in some cases, but the counterexamples produced so far seem amenable to a more complicated contextualist theory. We argue, however, that no contextualist theory can capture the evaluations speakers naturally make of sentences containing epistemic modals. If we want to respect these evaluations, our best option is a *relativist* theory of epistemic modals. On a relativist theory, an utterance of *a might be F* can be true relative to one context of evaluation and false relative to another. We argue that such a theory does better than any rival approach at capturing all the behaviour of epistemic modals.",
    "author": [
      {
        "name": "Andy Egan",
        "url": "https://www.andyegan.net"
      },
      {
        "name": "John Hawthorne",
        "url": "https://www.acu.edu.au/research/our-research-institutes/dianoia-institute-of-philosophy/our-people/john-hawthorne"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2005-09-29",
    "categories": [
      "language",
      "relativism"
    ],
    "contents": "\n\nContents\nA Puzzle\nContextualist Solutions\nInvariantist Solutions\nReporting Epistemic Modals\nRelativism and Centred Worlds\nObjections to Truth Relativism\nAppendix on Types of Content\n\nIn the 1970s David Lewis argued for a contextualist treatment of modals (Lewis 1976, 1979). Although Lewis was primarily interested in modals connected with freedom and metaphysical possibility, his arguments for contextualism could easily be taken to support contextualism about epistemic modals. In the 1990s Keith DeRose argued for just that position (DeRose 1991, 1998).\n\nPublished in Contextualism in Philosophy, edited by Gerhard Preyer and Georg Peter, 131-169.\nIn all contextualist treatments, the method by which the contextual variables get their values is not completely specified. For contextualist treatments of metaphysical modality, the important value is the class of salient worlds. For contextualist treatments of epistemic modality, the important value is which epistemic agents are salient. In this paper, we start by investigating how these values might be generated, and conclude that it is hard to come up with a plausible story about how they are generated. There are too many puzzle cases for a simple contextualist theory to be true, and a complicated contextualist story is apt to be implausibly ad hoc.\nWe then look at what happens if we replace contextualism with relativism. On contextualist theories the truth of an utterance type is relative to the context in which it is tokened. On relativist theories, the truth of an utterance token is relative to the context in which it is evaluated. Many of the puzzles for contextualism turn out to have natural, even elegant, solutions given relativism. We conclude by comparing two versions of relativism.\nWe begin with a puzzle about the role of epistemic modals in speech reports.\nA Puzzle\nThe celebrity reporter looked discomforted, perhaps because there were so few celebrities in Cleveland.\n“Myles,” asked the anchor, “where are all the celebrities? Where is Professor Granger?”\n“We don’t know,” replied Myles. “She might be in Prague. She was planning to travel there, and no one here knows whether she ended up there or whether she changed her plans at the last minute.”\nThis amused Professor Granger, who always enjoyed seeing how badly wrong CNN reporters could be about her location. She wasn’t sure exactly where in the South Pacific she was, but she was certain it wasn’t Prague. On the other hand, it wasn’t clear what Myles had gotten wrong. His first and third sentences surely seemed true: after all, he and the others certainly didn’t know where Professor Granger was, and she had been planning to travel to Prague before quietly changing her destination to Bora Bora.\nThe sentence causing all the trouble seemed to be the second: “She might be in Prague.” As she wiggled her toes in the warm sand and listened to the gentle rustling of the palm fronds in the salty breeze, at least one thing seemed clear: she definitely wasn’t in Prague – so how could it be true that she might be? But the more she thought about it, the less certain she became. She mused as follows: when I say something like x might be F, I normally regard myself to be speaking truly if neither I nor any of my mates know that x is not F. And it’s hard to believe that what goes for me does not go for this CNN reporter. I might be special in many ways, but I’m not semantically special. So it looks like Myles can truly say that I might be in Prague just in case neither he nor any of his mates knows that I am not. And I’m sure none of them knows that, because I’ve taken great pains to make them think that I am, in fact, in Prague – and reporters always fall for such deceptions.\nBut something about this reasoning rather confused Professor Granger, for she was sure Myles had gotten something wrong. No matter how nice that theoretical reasoning looked, the fact was that she definitely wasn’t in Prague, and he said that she might be. Trying to put her finger on just where the mistake was, she ran through the following little argument.\nWhen he says, “She might be in Prague” Myles says that I might be in Prague.1\nWhen he says, “She might be in Prague” Myles speaks truly iff neither he nor any of his mates know that I’m not in Prague.\nNeither Myles nor any of his mates know that I’m not in Prague.\nIf Myles speaks truly when he says that I might be in Prague, then I might be in Prague.\nI know I’m not in Prague.\nIt’s not the case that I know I’m not in Prague if I might be in Prague.\nThere must be a problem here somewhere, she thought – for (1) – (6) are jointly inconsistent. (Quick proof: (2) and (3) entail that Myles speaks truly when he says, “She might be in Prague.” From that and (1) it follows he speaks truly when he says Professor Granger might be in Prague. From that and (4) it follows that Professor Granger might be in Prague. And that combined with (5) is obviously inconsistent with (6).) But wherein lies the fault? Unless some fairly radical kind of scepticism is true, Professor Granger can know by observing her South Pacific idyll that she’s not in Prague – so (5) looks secure. And it seems pretty clear that neither Myles nor any of his mates know that she’s not in Prague, since they all have very good reason to think that she is – so it looks like (3) is also OK. But the other four premises are all up for grabs.\nWhich exactly is the culprit is a difficult matter to settle. While the semantic theory underlying the reasoning in (1)-(6) is mistaken in its details, something like it is very plausible. The modal ‘might’ here is, most theorists agree, an epistemic modal. So its truth-value should depend on what someone knows. But who is this someone? If it is Myles, or the people around him, then the statement “she might be in Prague” is true, and it is unclear where to block the paradox. If it is Professor Granger, or the people around her, then the statement is false, but now it is unclear why a competent speaker would ever use this kind of epistemic modal. Assuming the someone is Professor Granger, and assuming Professor Granger knows where she is, then “Granger might be in Prague” will be true iff “Granger is in Prague” is true. But this seems to be a mistake. Saying “Granger might be in Prague” is a way to weaken one’s commitments, which it could not be if the two sentences have the same truth conditions under plausible assumptions. So neither option looks particularly promising.\nTo make the problem even more pressing, consider what happens if a friend of Professor Granger’s who knows she is in the South Pacific overhears Myles’s comment. Call this third party Charles. It is prima facie very implausible that when Myles says that Professor Granger might be in Prague he means to rule out that Charles knows that she is not. After all, Charles is not part of the conversation, and Myles need not even know that he exists. So if Myles knows what he is saying, what he is saying could be true even if Charles knows Professor Granger is not in Prague. But if Charles knows this, Charles cannot regard Myles’s statement as true, else he will conclude that Professor Granger might be in Prague, and he knows she is not. So things are very complicated indeed.\nIn reasoning as we have been, we have been assuming that the following inferences are valid.\nA competent English speaker says It might be that S; and\nS, on that occasion of use, means that p; entail\nThat speaker says that it might be that p\nFurther, (9) plus\nThat speaker speaks truly; entail\nIt might be that p\nIf Charles accepts the validity of both of these inferences, then he is under considerable pressure to deny that Myles speaks truly. And it would be quite natural for him to do so – for instance, by interrupting Myles to say that “That’s wrong. Granger couldn’t be in Prague, since he left on the midnight flight to Tahiti.” But it’s very hard to find a plausible semantic theory that backs up this intervention, although such reactions are extremely common. (To solidify intuitions, here is another example: I overhear you say that a certain horse might have won a particular race. I happen to know that the horse is lame. I think: you are wrong to think that it might have won.)2\nOur solutions to this puzzle consist in proposed semantic theories for epistemic modals. We start with contextualist solutions, look briefly at invariantist solutions, and conclude with relativist solutions. Although we will look primarily at the costs and benefits of these theories with respect to intuitions about epistemic modals, it is worth remembering that they differ radically in their presuppositions about what kind of theory a semantic theory should be. Solving the puzzles to do with epistemic modals may require settling some of the deepest issues in philosophy of language\nContextualist Solutions\nIn his (1991), Keith DeRose offers the following proposal:\n\nS’s assertion “It is possible that P” is true if and only if (1) no member of the relevant community knows that P is false, and (2) there is no relevant way by which members of the relevant community can come to know that P is false. (593-4)\n\nDeRose intends ‘possible’ here to be an epistemic modal, and the proposal is meant to cover all epistemic modals, including those using ‘might.’3 We will not discuss here the issues that arise under clause (2) of DeRose’s account, since we’ll have quite enough to consider just looking at whether clause (1) or anything like it is correct.4\nIn our discussion below, we consider three promising versions of contextualist theory. What makes the theories contextualist is that they all say that Myles spoke truly when he said “She might be in Prague,” but hold that if Professor Granger had repeated his words she would have said something false.5 And the reason for the variation in truth-value is just that Myles and Professor Granger are in different contexts, which supply different relevant communities. Where the three theories differ is in which constraints they place on how context can supply the community in question.\nThe first is the kind of theory that DeRose originally proposed. On this theory, there is a side constraint that the relevant community always includes the speaker: whenever S truly utters a might be F, S does not know that a is not F. We’ll call this the speaker-inclusion constraint, or sometimes just speaker-inclusion. There is some quite compelling evidence for speaker-inclusion. Consider, for example, the following sort of case: Whenever Jack eats pepperoni pizza, he forgets that he has ten fingers, and thinks “I might only have eight fingers.” Jill (who knows full well that Jack has ten fingers) spots Jack sitting all alone finishing off a pepperoni pizza, and says, “He might have eight fingers.” Jill has said something false. And what she’s said is false because it’s not compatible with what she knows that Jack has eight fingers. But if the relevant community could ever exclude the speaker, one would think it could do so here. After all, Jack is clearly contextually salient: he’s the referent of ‘he,’ the fingers in question are on his hand, and no one else is around.6 Now, a single case does not prove a universal7 – but the case does seem to provide good prima facie vidence for DeRose’s constraint.\nOne implication of DeRose’s theory is that (1) is false, at least when Professor Granger says it. For when Professor Granger reports that Myles says “She might be in Prague,” she is reporting a claim he makes about his epistemic community – that her being in Prague is compatible with the things that they know. But when she says (in the second clause) that this means he is saying that she might be in Prague, she speaks falsely. For in her mouth the phrase “that I might be in Prague” denotes the proposition that it’s compatible with the knowledge of an epistemic community that includes Professor Granger (as the speaker) that Professor Granger is in Prague. And that is not a proposition that Myles assented to. So DeRose’s theory implies that the very intuitive (1) is false when uttered by Granger.\nWhen he says, “She might be in Prague” Myles says that I might be in Prague.\nIt is worth emphasizing how counterintuitive this consequence of speaker-inclusion is. If the speaker-inclusion constraint holds universally then in general speech involving epistemic modals cannot be reported disquotationally. But notice how natural it is, when telling the story of Jack and Jill, to describe the situation (as we ourselves did in an earlier draft of this paper) as being one where “Whenever Jack eats pepperoni pizza, he forgets that he has ten fingers, and thinks he might only have eight.” Indeed, it is an important generalization about how we use language that speakers usually do not hesitate to disquote in reporting speeches using epistemic modals. So much so that exceptions to this general principle are striking – as when the tenses of the original speech and the report do not match up, and the tense difference matters to the plausibility of the attribution.\nOne might try to explain away the data just presented by maintaining a laxity for ‘says that’ reports. A chemist might say ‘The bottle is empty’ meaning it is empty of air, while milkman might utter the same sentence, meaning in my context that it is empty of milk. Nevertheless, the milkman might be slightly ambivalent about denying:\n\nWhen the chemist says ‘The bottle is empty,’ she says that the bottle is empty.\n\nAnd this is no doubt because the overt ‘says that’ construction frequently deploys adjectives and verbs in a rather quotational way. After all, the chemist could get away with the following speech in ordinary discourse: “I know the milkman said that the bottle is empty. But he didn’t mean what I meant when I said that the bottle is empty. When he said that the bottle was empty he meant that it was empty of milk.”8 Thus the conventions of philosophers for using ‘say that’ involve regimenting ordinary use in a certain direction.9 But the disquotational facts that we are interested in cannot be explained away simply by invoking these peculiarities of ‘says that’ constructions, for the same disquotational ease surrounds the relevant belief reports. In the case just considered, while we might argue about whether it was acceptable for the chemist to say, in her conversational context, “The milkman said that the bottle was empty,” it is manifestly unacceptable for her to say “The milkman believes that the bottle is empty.” This contrasts with the case of ‘might’: If someone asked Professor Granger where Myles thought she was, she could quite properly have replied with (12).\nHe thinks that/believes that I might be in Prague.\nIndeed, we in general tend find the following inference pattern – a belief-theoretic version of (7) to (9) above – compelling:\nA competent English speaker sincerely asserts It might be that S\nS, in that context of use, means that p.; therefore,\nThat speaker believes that it might be that p\nOur puzzle cannot, then, be traced simply to a laxity in the ‘says that’ construction.10 Whatever the puzzle comes to, it certainly runs deeper than that.\nNotice that (12) does not suggest that Myles thinks that for all Professor Granger knows, she is in Prague; it expresses the thought that Myles thinks that for all he knows, that is where she is. Moreover, this is hardly a case where Granger’s utterance is of doubtful appropriateness: (12) is one of the ways canonically available for Granger to express that thought. But if we assume that what is reported in a belief report of this kind is belief in the proposition the reporter expresses by I might be in Prague, and we assume a broad-reaching speaker-inclusion constraint, we must concede that the proposition Granger expresses by uttering (12) is that Myles believes that for all Professor Granger knows, Professor Granger is in Prague.\nIf the speaker-inclusion constraint holds universally, then anyone making such a report is wrong. There are two ways for this to happen—either they know what the sentences they’re using to make the attributions mean, and they have radically false views about what other people believe, or they have non-crazy views about what people believe, but they’re wrong about the meanings of the sentences they’re using. The first option is incredibly implausible. So our first contextualist theory needs to postulate a widespread semantic blindness; in general speakers making reports are mistaken about the semantics of their own language. In particular, it requires that such speakers are often blind to semantic differences between sentence tokens involving epistemic modals. It is possible that some theories that require semantic blindness are true, but other things being equal we would prefer theories that do not assume this.11 In general the burden of proof is on those who think that the folk don’t know the meaning of their own words. More carefully: the burden of proof is on those who think that the folk are severely handicapped in their ability to discriminate semantic sameness and difference in their home language.\nSo the plausibility of (1) counts as evidence against the first contextualist theory, and provides a suggestion for our second contextualist theory. The cases that provide the best intuitive support for the speaker-inclusion constraint and the case we used above, involved unembedded epistemic modals. Perhaps this constraint is true for epistemic modals in simple sentences, but not for epistemic modals in ‘that’ clauses. Perhaps, that is, when S sincerely asserts X Vs that a might be F, she believes that X Vs that for all X (and her community) knows, a is F. (This is not meant as an account of the logical form of X Vs that a might be F, just an account of its truth conditions. We defer consideration of what hypothesis, if any, about the underlying syntax could generate those truth conditions.) To motivate this hypothesis, note how we introduced poor Jack, above. We said that he thinks he might have eight fingers. We certainly didn’t mean by that that Jack thinks something about our epistemic state.\nThe other problem with the speaker-inclusion constraint is that it does not seem to hold when epistemic modals are bound by temporal modifiers, as in the following example. A military instructor is telling his troops about how to prepare for jungle warfare. He says, “Before you walk into an area where there are lots of high trees, if there might be snipers hiding in the branches, clear away the foliage with flamethrowers.” Whatever the military and environmental merits of this tactic, the suggestion is clear. The military instructor is giving generic conditional advice: in any situation of type S, if C then do A. The situation S is easy to understand, it is when the troops are advancing into areas where there are high trees. And A, too, is clear: blaze ’em. But what about C? What does it mean to say that there might be snipers in the high branches? Surely not that it’s compatible with the military instructor’s knowledge that there are snipers in the high branches – he’s sitting happily in West Point, watching boats sail lazily along the Hudson. What he thinks about where the snipers are is neither here nor there. Intuitively, what he meant was that the troops should use flamethrowers if they don’t know whether there are snipers in the high branches. (Or if they know that there are.) So as well as leading to implausible claims about speech reports, the speaker-inclusion constraint seems clearly false when we consider temporal modifiers.\nHere is a way to deal with both problems at once. There are constraints on the application of the speaker-inclusion constraint. It does not apply when the epistemic modal is in the scope of a temporal modifier (as the flamethrower example shows) and it does not apply when the epistemic modal is in a ‘that’ clause.12 Our second contextualist theory then accepts the speaker-inclusion constraint, but puts constraints on its application.\nThis kind of theory, with a speaker-inclusion constraint only applying to relatively simple epistemic modals, allows us to accept (1). The problematic claim on this theory turns out to be (4):\nIf Myles speaks truly when he says that I might be in Prague, then I might be in Prague.\nWhen Myles said that Professor Granger might be in Prague, he was speaking truly. That utterance expressed a true proposition. So the antecedent of (4) is true. But the consequent is false: the “might” that appears there is not in a that-clause or in the scope of a temporal modifier; so the speaker-inclusion constraint requires that Professor Granger be included in the relevant community; and since she knows that she is not in Prague, it’s not true that she might be. We would similarly have to reject:\nIf Myles has a true belief that I might be in Prague, then I might be in Prague.\nBut there are reasons to be worried about this version of contextualism, beyond the uneasiness that attaches to denying (4), and, worse still, (4\\(^\\prime\\)). For one, this particular version of the speaker-inclusion constraint seems a bit ad hoc: why should there be just these restrictions on the relevant community? More importantly, the theory indicts certain inferential patterns that are intuitively valid. Suppose a bystander in our original example reasoned13:\nMyles believes that it might be that Professor Granger is in Prague.\nMyles’s belief is true; therefore,\nIt might be that Professor Granger is in Prague.\nBut this version of contextualism tells us that while (13) and (14) are true, (15) is false. In general, there are going to be counter-intuitive results whenever we reason from cases where the speaker-inclusion constraint does not apply to cases where it does.\nFinally, the theory is unable to deal with certain sorts of puzzle cases. The first kind of case directly challenges the speaker-inclusion constraint for simple sentences, although we are a little sceptical about how much such a case shows.14 Tom is stuck in a maze. Sally knows the way out, and knows she knows this, but doesn’t want to tell Tom. Tom asks whether the exit is to the left. Sally says, “It might be. It might not be.” Sally might be being unhelpful here, but it isn’t clear that she is lying. Yet if the speaker-inclusion constraint applies to unembedded epistemic modals, then Sally is clearly saying something that she knows to be false, for she knows that she knows which way is out.\nThis case is not altogether convincing, for there is something slightly awkward about Sally’s speech here. For example, if Sally knows the exit is not to the left, then even if she is prepared to utter, “It might be [to the left],” she will not normally self-ascribe knowledge that it might be to the left. And normally speakers don’t sincerely assert things they don’t take themselves to know. So it is natural to suppose that a kind of pretense or projection is going on in Sally’s speech that may well place it beyond the purview of the core semantic theory.\nThe following case makes more trouble for our second contextualist theory, though it too has complications. Ann is planning a surprise party for Bill. Unfortunately, Chris has discovered the surprise and told Bill all about it. Now Bill and Chris are having fun watching Ann try to set up the party without being discovered. Currently Ann is walking past Chris’s apartment carrying a large supply of party hats. She sees a bus on which Bill frequently rides home, so she jumps into some nearby bushes to avoid being spotted. Bill, watching from Chris’s window, is quite amused, but Chris is puzzled and asks Bill why Ann is hiding in the bushes. Bill says\nI might be on that bus.\nIt seems Bill has, somehow, conveyed the correct explanation for Ann’s dive—he’s said something that’s both true and explanatory. But in his mouth, according to either contextualist theory we have considered, it is not true (and so it can’t be explanatory) that he might have been on the bus. He knows that he is in Chris’s apartment, which is not inside the bus.\nChris’s question, like most questions asking for an explanation of an action, was ambiguous. Chris might have been asking what motivated Ann to hide in the bushes, or he might have been asking what justified her hiding in the bushes. This ambiguity is often harmless, because the same answer can be given for each. This looks to be just such a case. Bill seems to provide both a motivation and a justification for Ann’s leap by uttering (16). That point somewhat undercuts a natural explanation of what’s going on in (16). One might think that what he said was elliptical for She believed that I might be on the bus. And on our second contextualist theory, that will be true. If Bill took himself to be answering a question about motivation, that might be a natural analysis. (Though there’s the underlying problem that Ann presumably wasn’t thinking about her mental states when she made the leap. She was thinking about the bus, and whether Bill would be on it.) But that analysis is less natural if we think that Bill was providing a justification of Ann’s actions.15 And it seems plausible that he could utter (16) in the course of providing such a justification. This suggests that (16) simply means that for all Ann knew, Bill was on that bus. Alternatively, we could say that (16) is elliptical for Because I might be on that bus, and that the speaker-inclusion constraint does not apply to an epistemic modal connected to another sentence by ‘because.’ This may be right, but by this stage we imagine some will be thinking that the project of trying to find all the restrictions on the speaker-inclusion constraint is a degenerating research program, and a paradigm shift may be in order.\nSo our final contextualist theory is that DeRose’s original semantic theory, before the addition of any sort of speaker-inclusion constraint, was correct and complete. So ‘might’ behaves like ‘local’ and ‘nearby.’ If Susie says “There are snipers nearby,” the truth condition for that might be that there are snipers near Susie, or that there are snipers near us, or that there are snipers near some other contextually salient individual or group. Similarly, if she utters “Professor Granger might be in Prague” the truth condition for that might be that for all she knows Professor Granger is in Prague, or that for all we know Professor Granger is in Prague, or that for all some other community knows, Professor Granger is in Prague. There are no universal rules requiring or preventing the speaker from being included in the class of salient epistemic agents.\nAccording to the third version of contextualism, if Professor Granger does not equivocate when working through her paradox, then the problem lies with (6):\nIt’s not the case that I can know I’m not in Prague if I might be in Prague.\nAt the start of her reasoning process, Professor Granger’s use of ‘might’ means (roughly) ‘is compatible with what Myles and his friends know.’ And if it keeps that meaning to the end, then the antecedent of (6) is true, because Professor Granger might (in that sense) be in Prague, even though she knows she is not. Any attempt to show that (1) through (6) form an inconsistent set will commit a fallacy of equivocation.16\nBut (6) as uttered by Professor Granger sounds extremely plausible. And there are other, more general problems as well. It is difficult on such a theory to explain why it is so hard to get the relevant community to exclude the speaker in present tense cases: Why, for instance, can’t Jill’s statement about Jack, “He might have eight fingers,” be a statement about Jack’s epistemic state rather than her own? The third theory offers us no guidance.17\nWe’ll close this section with a discussion of the interaction between syntax and semantics in these contextualist theories. As is well known, in the last decade many different contextualist theories have been proposed for various philosophically interesting terms. Jason Stanley (2000) has argued that the following two constraints should put limits on when we posit contextualist semantic theories.\nVariable\nAny contextual effect on truth-conditions that is not traceable to an indexical, pronoun, or demonstrative in the narrow sense must be traceable to a structural position occupied by a variable. (Stanley 2000, 401)18\n\nSyntactic Evidence\nThe only good evidence for the existence of a variable in the semantic structure corresponding to a linguistic string is that the string, or another that we have reason to believe is syntactically like it, has interpretations that could only be accounted for by the presence of such a variable.\n\nIf any contextualist theory of epistemic modals is to be justifiably believed, then Variable and Syntactic Evidence together entail the existence of sentences where the ‘relevant community’ is bound by some higher operator. So ideally we would have sentences like (17) with interpretations like (18).\nEveryone might be at the party tonight.\nFor all x, it is consistent with all x knows that x will be at the party tonight.\nNow (17) cannot have this interpretation, which might look like bad news for the contextualist theory. It’s natural to think that if ‘might’ includes a variable whose value is the relevant community, that variable could be bound by a quantifier ranging over it. But if such a binding were possible, it’s natural to think that it would be manifested in (17). So Variable and Syntactic Evidence together entail that we ought not to endorse contextualism about epistemic modals.\nThis argument against contextualism fails in an interesting way, one that bears on the general question of what should count as evidence for or against a contextualist theory. The reason that any variable associated with ‘might’ in (17) cannot be bound by ‘everyone’ is that ‘might’ takes wider scope than ‘everyone.’ Note that (17) does not mean (19), but rather means (20).\nFor all x, it is consistent with what we know that x will be at the party tonight.\nIt is consistent with what we know that for all x, x will be at the party tonight.\nAs Kai von Fintel and Sabine Iatridou (2003) have shown, in any sentence of the form Every F might be G, the epistemic modal takes wide scope. For instance, (21) has no true reading if there is at most one winner of the election, even if there is no candidate that we know is going to lose.\nEvery candidate might win.\nMore generally, epistemic modals take wide scope with respect to a wide class of quantifiers.19 This fact is called the Epistemic Containment Principle by von Fintel and Iatridou. Even if there is a variable position for the relevant community in the lexical entry for ‘might,’ this might be unbindable because the epistemic modal always scopes over a quantifier that could bind it. If that’s true then the requirement imposed by Syntactic Evidence is too strong. If the evidence from binding is genuinely neutral between the hypothesis that this variable place exists and the hypothesis that it does not, because there are no instances of epistemic modals that take narrow scope with respect to quantifiers, it seems reasonable to conclude that there are these variable places on the basis of other evidence.\nHaving said all that, there still may be direct evidence for the existence of a variable position for relevant communities. Consider again our example of the military instructor, reprinted here as (22).\nBefore you walk into an area where there are lots of high trees, if there might be snipers hiding in the branches use your flamethrowers to clear away the foliage.\nAs von Fintel and Iatridou note, it is possible for epistemic modals to take narrow scope with respect to generic quantifiers. That’s exactly what happens in (22). And it seems that the best interpretation of (22) requires a variable attached to ‘might.’ Intuitively, (22) means something like (23).\nGenerally in situations where you are walking into an area where there are lots of high trees, if it’s consistent with your party’s knowledge that there are snipers hiding in the branches use your flamethrowers to clear away the foliage.\nThe italicised your party seems to be the semantic contribution of the unenunciated variable. We are not saying that the existence of sentences like (23) shows that there are such variables in the logical form of sentences involving epistemic modals.20 We just want to make two points here. First, if you are a partisan of Syntactic Evidence, then (22) should convince you not to object to semantic accounts of epistemic modals that appeal to variables, as our contextualist theories do. Second, we note a general concern that principles like Syntactic Evidence presupposes that a certain kind of construction, where the contextually variable term is bound at a level like LF, is always possible. Since there are rinciples like the Epistemic Containment Principle, we note a mild concern that this presupposition will not always be satisfied.\nInvariantist Solutions\nThe most plausible form of invariantism about epistemic modals is that DeRose’s semantics is broadly correct, but the relevant community is not set by context - it is invariably the world. We will call this position universalism. Of course when we say a might be F we don’t normally communicate the proposition that no one in the world knows whether a is F. The analogy here is to pragmatic theories of quantifier domain restriction, according to which when we say Everyone is F, we don’t communicate the proposition that everyone in the world is F, even though that is the truth condition for our utterance.\nThe universalist position denies (2) in Professor Granger’s argument. Myles did not speak truly when he said “Professor Granger might be in Prague” because someone, namely Professor Granger, knew she was not in Prague. Although (2) is fairly plausible, it probably has weaker intuitive support than the other claims, so this is a virtue of the universalist theory.\nThe big advantage (besides its simplicity) of the universalist theory is that it explains some puzzle cases involving eavesdropping. Consider the following kind of case. Holmes and Watson are using a primitive bug to listen in on Moriarty’s discussions with his underlings as he struggles to avoid Holmes’s plan to trap him. Moriarty says to his assistant,\nHolmes might have gone to Paris to search for me.\nHolmes and Watson are sitting in Baker Street listening to this. Watson, rather inexplicably, says “That’s right” on hearing Moriarty uttering (24). Holmes is quite perplexed. Surely Watson knows that he is sitting right here, in Baker Street, which is definitely not in Paris. But Watson’s ignorance is semantic, not geographic. He was reasoning as follows. For all Moriarty (and his friends) know, Holmes is in Paris searching for him. If some kind of contextualism is true, then it seems that (24) is true in Moriarty’s mouth. And, thought Watson, if someone says something true, it’s OK to say “That’s right.”\nWatson’s conclusion is clearly wrong. It’s not OK for him to say “That’s right,” in response to Moriarty saying (24). So his reasoning must fail somewhere. The universalist says that where the reasoning fails is in saying the relevant community only contains Moriarty’s gang members. If we include Holmes and Watson, as the universalist requires, then Moriarty speaks falsely when he says (24).\nThere are a number of serious (and fairly obvious) problems with the universalist account. According to universalism, the following three claims are inconsistent.\nx might be F.\nx might not be F.\nSomeone knows whether x is F.\nSince these don’t look inconsistent, universalism looks to be false.\nThe universalist’s move here has to be to appeal to the pragmatics. If (27) is true then one of (25) and (26) is false, although both might be appropriate to express in some contexts. But if we can appropriately utter sentences expressing false propositions in some contexts, then presumably we can inappropriately utter true sentences in other contexts. (Indeed, the latter possibility seems much more common.) So one could respond to the universalist’s main argument, their analysis of eavesdropping cases like Watson’s, by accepting that Watson can’t appropriately say “That’s right” but he can truly say this. The universalist will have a hard time explaining why such a theory cannot work, assuming, of course, that she can explain how her own pragmatic theory can explain all the data.\nThe major problem here is one common to all appeals to radical pragmatics in order to defend semantic theories. If universalism is true then speakers regularly, and properly, express propositions they know to be false.21 (We assume here that radical scepticism is not true, so sometimes people know some things.) Myles knows full well than someone knows whether Professor Granger is in Prague, namely Professor Granger. But if he’s a normal English speaker, this will not seem like a reason for him to not say, “Professor Granger might be in Prague.” Some might not think this is a deep problem for the universalist theory, for speakers can be mistaken in their semantic views in ever so many ways. But many ill regard it as a serious cost of the universalist claim.\nThis problem becomes more pressing when we look at what universalism says about beliefs involving epistemic modals. Myles does not just say that Professor Granger might be in Prague, he believes it. And he believes Professor Granger might not be in Prague. If he also believes that Professor Granger knows where she is, these beliefs are inconsistent given universalism. Perhaps the universalist can once again invoke pragmatics. It is not literally true in the story that Myles believes that Granger might be in Prague. But in escribing the situation we use “Myles believes that Granger might be in Prague,” to pragmatically communicate truths by a literal falsehood. This appeal to a pragmatic escape route seems even more strained than the previous universalist claims.\nIn general, the universalism under discussion here seems to run up against a constraint on semantic theorising imposed by Kripke’s Weak Disquotation Principle. The principle says that if a speaker sincerely accepts a sentence, then she believes its semantic value.22 If we have some independent information about what a speaker believes, then we can draw certain conclusions about the content of the sentences she accepts, in particular that she only accepts sentences whose content she believes. The universalist now has two options.23 First, she can say that Myles here does accept inconsistent propositions. Second, she can deny the Weak Disquotation Principle, and say that although Myles sincerely asserts, and accepts, “Professor Granger might be in Prague” he doesn’t really believe that Professor Granger might be in Prague. Generally, it’s good to have options. But it’s bad to have options as unappealing as these.24\nReporting Epistemic Modals\nOur third class of solutions will be relatively radical, so it’s worth pausing to look at the evidence for it. Consider again the dialogue between Moriarty, Holmes and Watson. Moriarty, recall, utters (24)\nHolmes might have gone to Paris to search for me.\nWatson knows that Holmes is in Baker Street, as of course does Holmes. In the above case we imagined that both Watson and Holmes heard Moriarty say this. Change the story a little so Holmes does not hear Moriarty speak, instead when he comes back into the room he asks Watson what Moriarty thinks. Watson, quite properly, replies with (30).\nHe thinks that you might have gone to Paris to search for him.\nThis is clearly not direct quotation because Watson changes the pronouns in Moriarty’s statement. It is not as if Watson said “He sincerely said, ‘Holmes might have gone to Paris to search for me.’” This might have been appropriate if Holmes suspected Moriarty was speaking in code so the proposition he expressed was very sensitive to the words he used.\nNor was Watson’s quote a ‘mixed’ quote, in the sense of what happens in (31).25 The background is that Arnold always uses the phrase ‘my little friend’ to denote his Hummer H2, despite that vehicle being neither little nor friendly. No one else, however, approves of this terminology.\nArnold: My little friend could drive up Mt Everest.\nChaz: Arnold believes his little friend could drive up Mt Everest.26\nWe’ve left off the punctuation here so as to not beg any questions, but there is a way this could be an acceptable report if the fourth and fifth word, and those two words only, are part of a quotation. This is clearly not ordinary direct quotation, for Arnold did not think, in English or Mentalese, “His little friend could drive up Mt Everest.” Nevertheless, this is not ordinary indirect quotation. In ordinary spoken English Chaz’s report will be unacceptable unless ‘little friend’ is stressed. The stress here seems to be just the same stress as is used in metalinguistic negation, as described in Horn (1989). Note the length of the pause between ‘his’ and ‘little.’ With an ordinary pause it sounds as if Chaz is using, not mentioning, ‘little friend.’ So it is possible in principle to have belief reports, like this one, that are neither strictly direct nor strictly indirect.27 Nevertheless, it does not seem like (30) need such a case. In particular, there need be no distinctive metalinguistic stress on ‘might’ in Watson’s utterance of (30), and such stress seems to be mandatory for this mixed report.\nAssuming Moriarty was speaking ordinary English, Watson’s report seems perfectly accurate. This is despite the fact that the relevant community one would naturally associate with Watson’s use of ‘might’ is quite different to the community we would associate with Moriarty’s use. When reporting speeches involving epistemic modals – and the beliefs express by sincere instances of such speeches, speakers can simply disquote the modal terms.\nAs is reasonably well known, there are many terms for which this kind of disquoting report is impermissible. In every case, Guildenstern’s report of Ophelia’s utterance is inappropriate.\nOphelia: I love Hamlet.\n…\nGuildenstern: *Ophelia thinks that I love Hamlet.\nGuildenstern: What think you of Lord Hamlet?\nOphelia: He is a jerk.\n…\nRosencrantz: What does Ophelia think of the King?\nGuildenstern: *She thinks that he is a jerk.\nGuildenstern: Are you ready to teach the class on contextualism?\nOphelia: I’m ready.\n…\nRosencrantz: Does Ophelia think she is ready to defend her dissertation?\nGuildenstern: *She thinks she is ready.\n(Guildenstern and Ophelia are on the telephone, Guildenstern is in Miami, and Ophelia is in San Francisco)\nGuildenstern: What do you like best about San Francisco?\nOphelia:There are lots of wineries nearby.\n…\nRosencrantz: Is it possible to grow wine in south Florida?\nGuildenstern: *Ophelia thinks that there are lots of wineries nearby.28\nEven when the contextualist claim is not obviously true, as with ‘local’ and ‘enemy,’ disquotational reports are unacceptable after context shifts.\n(Brian is calling from Providence, Hud and Andy are in Bellingham)\nBrian: When I get all this work done, I’ll head off to a local bar for some drinks.\nAndy: How much work is there?\nBrian: Not much. I should get to the bar in a couple of hours.\nHud: Hey, is Brian in town? Where’s he going tonight?\nAndy: *He thinks he’ll be at a local bar in a couple of hours.\nThe Enemy, speaking of us: The enemy have the advantage.\nOne of us: How are we doing?\nAnother of us: Someone just informed me that the enemy have the advantage.\n(Terrell is an NFL player, and Dennis is his coach.)\nTerrell: Why are you cutting me coach?\nDennis: Because you are old and slow.\n(After this Terrell returns to academia. Kate and Leopold are students in his department.)\nKate: Do you think Terrell would do well on our department ultimate frisbee team?\nLeopold: ??I’m not sure. Someone thinks he’s old and slow.\nThis data provides us with the penultimate argument against the contextualist theory of epistemic modals. We have already seen several such arguments.\nFirst, as seen through the difficulties with each of the options discussed in section 2, any version of contextualism faces serious problems, though by altering the version of contextualism we are using, we can alter what problems we have to face.\nSecond, there is nothing like the speaker-inclusion constraint for terms like ‘local’ and ‘enemy’ for which contextualism is quite plausible. This disanalogy tells against the contextualist theory of ‘might.’ With the right stage setting (and it doesn’t usually take very much), we can get ‘local’ and ‘enemy’ to mean local to x and enemy of x for pretty much any x we happen to be interested in talking about. At least for ‘bare’ (unembedded) epistemic modals, the situation is markedly different. We can’t, just by making Jack salient, make our own knowledge irrelevant to the truth of our utterance of, for example, “Jack might have eight fingers.” The only way we can make our knowledge irrelevant is if we are using this sentence in an explanation or justification of Jack’s actions.29\nThird, there is a difference in behaviour between embedded and unembedded occurrences of epistemic modals. When epistemic modals are embedded in belief contexts, conditionals, etc., they behave differently—the speaker inclusion constraint seems to be lifted. (Think about belief reports and that military instructor case.) ‘Local’ and ‘enemy’ don’t seem to show any analogous difference in their behaviour between their bare and embedded occurrences.\nFourth, ‘local’ and ‘enemy’ don’t generate any of the peculiar phenomena about willingness to agree. If Myles (still in Cleveland), says\nMany local bars are full of Browns fans.\nProfessor Granger (still in the South Pacific), will not hesitate to say “that’s right” (as long as she knows that many bars in Cleveland really are, as usual, full of Browns fans). The fact that the relevant bars aren’t local to her doesn’t interfere with her willingness to agree with (39) in the way that the fact that she knew that she wasn’t in Prague interfered with her willingness to agree with Myles’ claim that she might be in Prague, or in the way that Watson’s knowledge that Holmes was in London (should have) interfered with his willingness to assent to Moriarty’s claim that Holmes might be in Paris.\nFifth, when there is a context shift, we are generally hesitant to produce belief reports by disquoting sincerely asserted sentences involving contextually variable terms. This is what the examples (32) through (36) show. For a wide range of contextually variable terms, speakers will quite naturally hesitate to make disquotational reports unless they are in the same context as the original speaker. Such hesitation is not shown by speakers reporting epistemic modals.\nThe sixth argument, that there is an alternative theory that does not have these flaws, will have to wait until the next section. For now, let’s note that there are other words that seem at first to be contextually variable, but for which disquotational reports seem acceptable.\nVinny the Vulture: Rotting flesh tastes great.\nJohn: Vinny thinks that rotting flesh tastes great.\nAnt Z: He’s huge (said of 5 foot 3 141 lb NBA player Muggsy Bogues)\nAndy: Ant Z thinks that Muggsy’s huge.\nMarvin the Martian: These are the same colour (said of two colour swatches that look alike to Martians but not to humans.)\nBrian: Marvin thinks that these are the same colour.\nIn all three cases the report is accurate, or at least extremely natural. And in all three cases it would have been inappropriate for the reporter to continue “and he’s right.” But crucially, in none of the three cases is it clear that the original speaker made a mistake. In his context, it seems Vinny utters a truth by uttering, “Rotting flesh tastes great,” for rotting flesh does taste great to vultures. From Ant Z’s perspective, Muggsy Bogues is huge. We assume here, a little controversially, that there is a use of comparative adjectives that is not relativised to a comparison class, but rather to a perspective. Ant Z does not say that Muggsy is huge for a human, or for an NBA player, but just relative to him. And he’s right. Even Muggsy is huge relative to an ant. Note the contrast with (36) here. There’s something quite odd about Leopold’s statement, which intuitively means that someone said Terrell is old and slow for a graduate student, when all that was said was that he is old and slow for an NFL player.30 And, relative to the Martian’s classification of objects into colours, the two swatches are the same colour. So there’s something very odd going on here.\nThe following very plausible principle looks like it is being violated.\nTruth in Reporting\nIf X has a true belief, then Y’s report X believes that S accurately reports that belief only if in the context Y is in, S expresses a true proposition.31\n\nNot only do our three reports here seem to constitute counterexamples to Truth in Reporting, Watson’s report in (30) is also such a counterexample, if Moriarty speaks truly (and sincerely). One response here would be to give up Truth in Reporting, but that seems like a desperate measure. And we would still have the puzzle of why we can’t say “and he’s right” at the end of an accurate report.\nAnother response to these peculiar phenomena would be to follow the universalist and conclude that Moriarty, Vinny, Ant Z and Marvin all believe something false. It should be clear how to formulate this kind of position: something tastes great iff every creature thinks it tastes great; something is huge iff it is huge relative to all observers; and two things are the same colour iff they look alike (in a colour kind of way) to every observer (in conditions that are normal for them). As we saw, there are problems for the universalist move for epistemic modals. And the attractiveness of the other universal seems to dissipate when we consider the cases from a different perspective.\nBrian: Cognac tastes great.\nVinny: Brian believes that cognac tastes great.\nAndy:He’s huge (said of Buggsy Mogues, the shortest ever player in the Dinosaur Basketball Association).\nTyrone the T-Rex: Andy believes that Buggsy’s huge.\nJohn: These are the same colour (said of two colour swatches that look alike to humans but not to pigeons).\nPete the Pigeon: John believes that these are the same colour.\nAgain, every report seems acceptable, and in every case it would seem strange for the reporter to continue “and he’s right.” The universalist explanation in every case is that the original utterance is false. That certainly explains the data about reports, but look at the cost! All of our utterances about colours and tastes will turn out false, as will many of our utterances about sizes. It seems we have to find a way to avoid both contextualism and universalism. Our final suggestions for how to think about epistemic modals attempt to explain all this data.\nRelativism and Centred Worlds\nJohn MacFarlane (2003) has argued that believers in a metaphysically open future should accept that the truth of an utterance is relative to a context of evaluation.32 For example, if on Thursday Emily says, “There will be a sea battle tomorrow,” the believer in the open future wants to say that at the time her utterance is neither determinate true nor determinately false. One quick objection to this kind of theory is that if we look back at Emily’s statement while the sea battle is raging on Friday, we are inclined to say that she got it right. From Friday’s perspective, it looks like what Emily said is true. The orthodox way to reconcile these intuitions is that the only sense in which Emily’s statement is indeterminate on Thursday is an epistemic sense – we simply don’t know whether there will be a sea battle. MacFarlane argues instead that we should simply accept the intuitions as they stand. From Friday’s perspective, Emily’s statement is determinately true, from Thursday’s it is not. Hence the truth of statements is relative to a context of evaluation.\nThere is a natural extension of this theory to the cases described above. Moriarty’s statement is true relative to a context C iff it is compatible with what the people in C know that Holmes is in Paris. So in the context he uttered it, the statement is true, because it is consistent with what everyone in his context knows that Holmes is in Paris. But in the context of Watson’s report, it is false, because Watson and Holmes know that Holmes is not in Paris.\nWe will call any such theory of epistemic modals a relativist theory, because it says that the truth of an utterance containing an epistemic modal is relative to a context of evaluation. As we will see, relativist theories do a much better job than contextualist theories of handling the data that troubled contextualist theories. Relativist theories are also plausible for the predicates we discussed at the end of the last section: ‘huge,’ ‘color’ and ‘tastes.’ On such a theory, any utterance that x tastes F is true iff x tastes F to us. Similarly, an utterance x is huge that doesn’t have a comparison class, as in (41) or (44), is true iff x is huge relative to us. And Those swatches are the same color is true iff they look the same colour to us. The reference to us in the truth conditions of these sentences isn’t because there’s a special reference to us in the lexical entry for any of these worlds. Rather, the truth of any utterance involving these terms is relative to a context of evaluation, and when that is our context of evaluation, we get to determine what is true and what is false. If the sentences were being evaluated in a different context, it would be the standards of that context that mattered to their truth.\nSo far we have not talked about the pragmatics of epistemic modals, assuming that their assertability conditions are given by their truth conditions plus some familiar Gricean norms. But it is not obvious how to apply some of those norms if utterance truth is contextually relative, because one of the norms is that one should say only what is true.\nOne option is to say that utterance appropriateness is, like utterance truth, relative to a context of evaluation. This is consistent, but it does not seem to respect the data. Watson might think that Moriarty’s utterance is false, at least relative to his context of evaluation33, but if he is aware of Moriarty’s epistemic state he should think it is appropriate. So if something like truth is a norm of assertion, it must be truth relative to one or other context. But which one?\nWe could say that one should only say things that are true relative to all contexts. But that would mean John’s statement about the two swatches being the same colour would be inappropriate, and that seems wrong.\nWe could say that one should only say things that are true relative to some contexts. But then Brian could have said, “Rotting carcases taste great” and he would have said something appropriate, because that’s true when evaluated by vultures.\nThe correct norm is that one should only say something that’s true when evaluated in the context you are in. We assume here that contexts can include more than just the speaker. If Vinny the Vulture is speaking to a group of humans he arguably cannot say Rotting flesh tastes great. The reason is that rotting flesh does not taste great to the group of speakers in the conversation, most of whom are humans. This norm gives us the nice result that Myles’s statement is appropriate, as is Moriarty’s, even though in each case their most prominent audience member knows they speak falsely.34\nThis helps explain, we think, the somewhat ambivalent attitude we have towards speakers who express epistemic modals that are false relative to our context, but true relative to their own. What the speaker said wasn’t true, so we don’t want to endorse what they said. Still, there’s still a distinction between such a speaker and someone who says that the sky is green or that grass is blue. That speaker would violate the properly relativised version of the only say true things rule, and Myles and Moriarty do not violate that rule.\nAs MacFarlane notes, relativist theories deny Absoluteness of Utterance Truth, the claim that if an utterance is true relative to one context of evaluation it is true relative to all of them. It is uncontroversial of course that the truth value of an utterance type can be contextually variable, the interesting claim that relativists make is that the truth value of utterance tokens can also be different relative to different contexts. So they must deny one or more premises in any argument for Absoluteness of Utterance Truth, such as this one.\nAbsoluteness of Propositional Content: If an utterance expresses the proposition p relative to some context of evaluation, then it expresses that proposition relative to all contexts of evaluation.\nAbsoluteness of Propositional Truth Value: If a proposition p is true relative to one context in a world it is true relative to all contexts in that world; therefore,\nAbsoluteness of Utterance Truth\nThis argument provides a nice way of classifying relativist theories. One relativist approach is to say that Moriarty (or anyone else who utters an epistemic modal) says something different relative to each context of evaluation. Call this approach content relativism. Another approach is to say that there is a single proposition that he expresses with respect to every context, but the truth value of that proposition is contextually variable. Call this approach truth relativism. (So that the meaning of ‘proposition’ is sufficiently understood here, let us stipulate that we understand propositions to be the things that are believed and asserted and thus, relatedly, the semantic values of ‘that’-clauses.)\nIt might look like some of our behaviour is directly inconsistent with any sort of relativism. Consider the following dialogue.\nVinny: Rotting flesh tastes great\nVinny’s brother: That’s true.\nJohn: That (i.e. what Vinny’s brother said) is not true.\nIf what Vinny’s brother is saying is that Vinny’s utterance Rotting flesh tastes great is true in his context, then John is wrong in saying that what Vinny’s brother said isn’t true. For it is true, we claim, that Rotting flesh tastes great is true in Vinny’s context.35 But this prediction seems unfortunate, because John’s utterance seems perfectly appropriate in his context.\nThe solution here is to recognise a disquotational concept of truth, to go alongside the binary concept of truth that is at the heart of the relativist solution.36 The binary concept is a relation between an tterance and a context of evaluation. Call this trueB. So Vinny’s utterance is trueB relative to his context, and to his brother’s context, and falseB relative to John’s context. One crucial feature of the binary concept is that it is not a relativist concept. If it is true relative to one context that an utterance is trueB relative to context C, it is true relative to all contexts that the utterance is trueB relative to context C. The disquotational concept is unary. Call this trueT. As far as is permitted by the semantic paradoxes, it claims that sentences of the form S is trueT iff S will be trueB relative to any context (note here the primacy of truthB for semantic explanation) TrueT is a relative concept. An utterance can be trueT relative to C and not trueT relative to C\\(^\\prime\\). When an utterance is given the honorific true in ordinary discourse, it is the unary relative concept trueT that is being applied. That explains what is going on in (46). Vinny’s brother says that Vinny’s utterance is trueT. Relative to his context, that’s right, since Vinny’s utterance is true in his context. But relative to John’s context, that’s false, because an utterance is trueT relative to John’s context iff it is true relative to John’s context. So John spoke truly relative to his own context, so he spoke correctly. The important point is that assignments of truthT are relative rather than contextually rigid, so they might be judged true relative to some contexts and false relative to others.\nAlthough both truth relativism and content relativism can explain (46) if they help themselves to the distinction between truthB and truthT, there are four major problems for content relativism that seem to show it is not the correct theory.\nThe first problem concerns embeddings of “might” clauses in belief contexts. Suppose Watson says,\nMoriarty believes that Holmes might be in Paris.\nOn the content relativist view, (47) will say, relative to Watson, that Moriarty believes that, as far as Watson knows, Holmes is in Paris. That would be a crazy thing for Watson to assert. Suppose Watson is talking to Holmes. Then, relative to Holmes, Watson will have claimed that Moriarty believes that, as far as Holmes knows, Holmes is in Paris. That would also be a crazy thing for Watson to assert. But, given what he’s just overheard, it would be perfectly natural—and pretty clearly correct, so long as nothing funny is going on behind the scenes—for Watson to assert (47). A view that tells us that Watson’s saying something crazy relative to everybody who’s likely to be a member of his audience is in pretty serious conflict with our pretheoretical judgements about the case. (Enlarging the context to include both Holmes and Watson obviously doesn’t help, either.)\nThe second problem concerns the social function of assertion. In particular, it causes difficulties for an attractive part of the Stalnakerian story about assertion, that the central role of an assertion is to add the proposition asserted to the stock of conversational presuppositions (Stalnaker 1978). On the content relativist view, it can’t be that the essential effect of assertion is to add the proposition asserted to the stock of common presuppositions, because there’s no such thing as the proposition asserted. There will be a different proposition asserted relative to each audience member. That’s not part of an attractive theory. And it’s not terribly clear what the replacement story about the essential effect of assertion—about the fundamental role of assertion in communication—is going to be. It may be that there’s a story to be told about assertability—about when Moriarty is entitled to assert, for example, “it might be that Holmes is in Paris”—but there’s no obvious story about what he’s up to when he’s making that assertion—about what the assertion is supposed to accomplish. (And if you think that appropriateness of assertion’s got to be tied up with what your assertion’s supposed to accomplish, then you’ll be sceptical about even the first part.)\nThe third problem concerns epistemic modals in the scope of temporal modifiers. The content relativist has difficulties explaining what’s going on with sentences like (48).\nThe Trojans were hesitant in attacking because Achilles might have been with the Greek army.\nOn the content relativist view, (48) will be false relative to pretty much everybody—certainly relative to everybody alive today. It’s certainly false that the Trojans were hesitant because, as far as we know, Achilles was with the Greek army. (Or worse, because, as far as we knew then, Achilles was with the Greek army.) But, depending on how the Trojan war went, (48) could be true relative to everybody.37\nFinally, content relativism has a problem with commands. Keith’s Mom says:\nFor all days d, you should carry an umbrella on d if and only if it might rain on d.\nSuppose on Monday Keith checks the forecast and it says there’s a 50% chance of rain. So he takes an umbrella. It doesn’t rain, and on Tuesday he wonders whether what he did on Monday was what his Mom said he should. On the content relativist view, we get the following strange result: on Monday, it would have been true to say that he was doing what his Mom said he should, since at the time, the embedded clause expressed a proposition that was true relative to him. Looking back on Tuesday, though, it looks like he did what his Mom said he shouldn’t, because now the embedded clause expresses a proposition that’s false relative to him. But that’s not right. He just plain did what his Mom told him to do.\nThe same thing happens with the soldiers trying to follow the imperative issued as (22). Assume one of them attempts to follow the command by burning down some trees that seem to contain snipers. Relative to the time she is doing the burning, she will be complying with the command. But later, when it turns out the trees were sniper-free, she will not have been following the command. If we assume there’s an overarching command to not use flamethrowers unless explicitly instructed to do so, then it will turn out that, as of now, she violated her orders then. But that’s not right. She just plain followed her orders.\nThere’s a similar problem with the other terms about which relativism seems plausible. Consider the following commands:\nDon’t pick fights with huge opponents.\nStack all of the things that are the same color together.\nIf it tastes lousy, spit it out.\nIt’s possible to sensibly issue these commands, even in relevantly mixed company. And if we’re going to get the right compliance conditions, we don’t want content relativism about great-tastingness, hugeness, and same-coloredness here. When we hear a command like (52), we take (a) the same command to have been issued to everybody, and (b) everybody to be following it if we all spit out the things that taste lousy to us. On the content relativist view, we’ve each gotten different commands, and the philosopher who spits out the chunk of week-old antelope hasn’t complied with the command that Vinny was given. This seems wrong.\nSo the content relativist theory has several problems. The truth relativist theory does much better. Let us begin with the familiar notion of a function from worlds to truth values. Call any such function a Modal Profile. On the standard way of looking at things, propositions – the objects of belief and assertion, the semantic values of ‘that’-clauses – are, or at least determine a Modal Profile. The truth relativist denies this. According to the truth-relativist, the relevant propositions are true or false not relative to worlds, but relative to positions within worlds—that is, they’re true or false relative to centered worlds. (A centered world is a triple of a possible world, an individual, and a time.) There’s a few ways to formally spell out this idea. One is to replace talk of Modal Profiles with Centring Profiles, i.e. functions from centred worlds to truth values. Another is to say that a centred world and proposition combine to determine a Modal Profile, so propositions determine functions from centred worlds to Modal Profiles. Each of these proposals has some costs and benefits, and we postpone discussion of their comparative virtues to an appendix. For now we are interested in the idea, common to these proposals, that propositions only determine truth values relative to something much more fine-grained than a world. (We take no stand here on whether propositions should be identified with either Modal Profiles or Centering Profiles or functions from Centred Worlds to Modal Profiles).\nTruth relativism is not threatened by the four problems that undermine content relativism.\nAccording to truth relativism, Watson and Moriarty express the very same proposition by the words Holmes might be in Paris, so it is no surprise that Watson can report Moriarty’s assertive utterance by using the very same words. Similarly, it is no surprise that if Moriarty has a belief that he would express by saying Holmes might be in Paris, Watson can report that by (53).\nMoriarty believes that Holmes might be in Paris.\nAbove we noted that it’s unlikely that Watson could use this to express the proposition that for all Watson knows Holmes is in Paris. We used that fact to argue that DeRose’s constraint did not apply when an epistemic modal is inside a propositional attitude report. The truth relativist theory predicts not only that DeRose’s constraint should not apply, but that a different constraint should apply. When one says that a believes that b might be F, one says that a believes the proposition b might be F. And a believes that proposition iff a believes it is consistent with what they know that b is F. And that prediction seems to be entirely correct. It is impossible for Watson to use (53) to mean that Moriarty believes that for all Holmes knows he is in Paris, or that for all Watson knows Holmes is in Paris. This seems to be an interesting generalisation, and while it falls out nicely from the truth relativist theory, it needs to be imposed as a special constraint on contextualist theories.\nSince there is a proposition that is common to speakers and hearers when an epistemic modal is uttered, we can keep Stalnaker’s nice idea that the role of assertion is to add propositions to the conversational context. Since propositions are no longer identified with sets of possible worlds we will have to modify other parts of Stalnaker’s theory, but those parts are considerably more controversial.\nThe truth relativist can also explain how (48) can be true, though the explanation requires a small detour through the nature of psychological explanations involving relativist expressions go.\nThe Trojans were hesitant in attacking because Achilles might have been with the Greek army.\nAll of the following could be true, and not because the things in question are rude, huge or great tasting for us.\nMarvin the Martian dropped his pants as the Queen passed by because it would have been rude not to.\nChildren are scared of adults because they are huge.\nVultures eat rotting flesh because it tastes great.\nIn general it seems that the truth of an explanatory claim of the form, X \\(\\varphi\\)ed because p depends only on whether p is true in X’s context (plus whether the truth of p in X’s context bears the right relation to X’s \\(\\varphi\\)ing).. Whether or not p is true in our context is neither here nor there. Adults are not huge, rotting flesh does not taste great, and it is rude to drop one’s pants as the Queen passes by, but (54)-(56) could still be true, and could all count as good explanations. Similarly, (48) can be true because Achilles might have been with the Greek army could be true relative to the Trojans.\nSimilarly, what it is to comply with a command is to act in a way that makes the command true in the context of action. This is not a particular feature of epistemic modals, but just a general property of how commands involving propositions with centered-worlds truth conditions behave. If Don picks a fight with Pedro after Don has shrunk so much that Pedro is now relatively huge, he violates (50), even if Pedro was not huge when the command was issued. And he still violates it from a later perspective when Pedro and Don are the same size. The general point is that whether the command is violated depends on the applicability of the salient terms from the perspective of the person to whom the command applies. Similarly, Keith does not violate his Mom’s command if he takes an umbrella where It might rain is true in the context the action is performed. And this, of course, matches up perfectly with intuitions about the case.\nIt’s a little tricky to say just which statement in Professor Granger’s original hexalemma gets denied by the truth relativist. It all depends what we mean by spoke truly. If Myles spoke truly means that Myles said something trueT, then (2) is false (relative to Granger’s context), for its right-hand-side is true but its left-hand-side is false. If, on the other hand, it means he said something trueB relative to his own context, then (4) is false, for he did speak trulyB relative to his context, but it’s not the case that Professor Granger might be in Prague. This is awkward, but we might expect that any good solution to the paradox will be awkward.\nObjections to Truth Relativism\nIt might be thought that the truth relativist has to deny Truth in Reporting, but in fact this can be retained in its entirety provided we understand it the right way. The following situation is possible on the truth relativist theory. X has a belief that is true in her context, and Y properly reports this by saying X believes that S, where S in Y’s mouth expresses a proposition that is false in Y’s mouth in her context. But this is no violation of Truth in Reporting. What would be a violation is if X’s belief was true in Y’s context, and still Y could report it as described here. But there’s no case where, intuitively, we properly report an epistemic modal but violate that constraint. And the same holds for reports of uses of huge, color or tastes. Even if Vinny (truly) believes that rotting flesh tastes great, and the words “Rotting flesh tastes great” in John’s mouth express a false proposition, John’s report, “Vinny believes that rotting flesh tastes great” would only violate Truth in Reporting if Vinny’s belief is still true in John’s context. And it is not.\nGiven that the relativist has the concept of truthT, or as we might put it truth simpliciter, what should be done with it? The answer seems to be not much. We certainly shouldn’t restate the norms of assertion in terms of it, because that will lead to the appropriateness of assertion being oddly relativised. Whether it was appropriate for Vinny to say “Rotting flesh tastes great,” is independent of the context of evaluation, even if the truth of what he uttered is context-relative. (It would not at all be appropriate for him to have said “Rotting flesh tastes terrible” even though we should think he would have said something true by that remark, and something false by what he actually said.) And the same thing seems to hold for generalisations about truth as the end of belief. It is entirely appropriate for Myles to believe that Granger might be in Prague, because it’s trueB relative to his context. Relatedly, if knowledge is tied to truthT rather than truthB, knowledge can’t be the norm of assertion or end of belief.38 On the other hand, using truthT we can say that Truth in Reporting is true in the truth relativist theory without reinterpreting it in terms of relative truth concepts. Moreover, we can invoke truthT to explain why we got confused when thinking about the original puzzle: It is arguable that, even if we should distinguish truthT from truthB in our semantic theorizing, we aren’t unreflectively as clear about that distinction as we might be. No wonder then that we get a little confused as we think about the Granger case. We want to say Myles doesn’t make a mistake. And we also want to say “That’s wrong” speaking of the object of his assertion and belief, and what’s more, when we say that, we don’t seem to be making a binary claim about the relation between ourselves and what is believes. Once we clearly distinguish truthT from truthB things become clearly. Using the disquotational notion, we can say ‘That is falseT,’ which is a monadic claim, and not a binary one. The binary truthB explains why that claim is assertable (it is assertable because ‘That is falseT’ is truthB at my context), but doesn’t figure in the proposition believed. Meanwhile, the relevant notion of mistake – that of an agent believing a proposition that is not trueB at her context, can only be properly articulated once the distinction between the more explanatory truthT is carefully distinguished from the (arguably) conceptually more basic truthB.\nOne final expository point. In general, truth relativism makes for irresolvable disputes. Let us say that two conversational partners are in deadlock concerning a claim when the following situation arises: There is a pair of conversational participants, x and y, and a sentence S, under dispute, such that each express the same proposition (in the sense explained) by S but that S is trueB at each of the contexts x is in during the conversation, and falseB at each of the contexts y is in during the conversation. Neither speaks past one another in alternately asserting and denying the same sentence, since each expresses the same proposition by it. And each asserts what they should be asserting when each says: What I say is truthT and what the other says is falseT., since each makes a speech that is trueT at the respective contexts. In general, truth relativism about a term will lead one to predict deadlock for certain conversations, traceable to the truth relativity of the term. But in the case of ‘might,’ it is arguable that conversation tends to force a situation where, even if at the outset, a ‘might’ sentence was true relative to x and not to y (on account of the truth-relativity of the ‘might’ sentence), x and y will, in the course of engagement and dispute, be quickly put into a pair of contexts which do not differ with respect to truthB (unless the ‘might’ sentence contained other terms that themselves made for deadlock). This is not merely because the conversational participants will, through testimony, pool knowledge about the sentence embedded in the ‘might’ claim. It is in any case arguable that the relevant community whose body of knowledge determines whether a ‘might’ claim is trueB at a context always includes not just that of the person at that context but also that of his conversational partners. In the special case of ‘might,’ then, Truth Relativism may well generate far less by way of deadlock than in other cases.\nThere are two primary objections to the truth relativist theory: it doesn’t quite handle all the cases and that it is too radical.\nThere are some cases that seem to tell directly against the truth relativist position. Consider the case again of Tom and Sally stuck in a maze. Sally knows the way out, but doesn’t want to tell Tom. She says, inter alia, (57), and does not seem to violate any semantic norms in doing so, even though she knows the exit is some other way.\nThe exit might be that way.\nThis seems to directly contradict the relativist claim that the norm for assertion is speaking truly in one’s own context. We suspect that what’s going on here is that Sally is projecting herself into Tom’s context. She is, we think, merely trying to verbalise thoughts that are, or should be, going through Tom’s head, rather than making a simple assertion. As some evidence for this, note (as was mentioned above) that it would be wrong to take (57) as evidence that Sally believes the exit might be that way, whereas when a speaker asserts that p that is usually strong evidence that she believes that p. It is unfortunate for the relativist to have to appeal to something like projection, but we think it is the simplest explanation of these cases that any theorist can provide.\nThe idea that utterances have their truth value absolutely is well-entrenched in contemporary semantics, so it should only be overturned with caution. And it might be worried that once we add another degree of relativisation, it will be open to relativise in all sorts of directions. We are sensitive to these concerns, but we think the virtues of the relativist theory, and the vices of the contextualist and invariantist theories, provides a decent response to them. Invariantist theories are simply implausible, and any contextualist theory will have to include so many ad hoc conditions, conditions that seem to be natural consequences of relativism, that there are methodological considerations telling in favour of relativism. (Let us be clear: we are not recommending a general preference for relativism over contextualism in semantic theory. As we have been trying to make clear, for example, the case of ‘might’ is very different from, say, the case of ‘ready.’) It is (as always) hard to tell which way the balance tips when all these methodological considerations are weighed together, but we think the relativist has a good case.\nAppendix on Types of Content\nRobert Stalnaker has long promoted the idea that the content of an assertoric utterance is a set of possible worlds, or a function from worlds to truth values. This idea has been enormously influential in formal semantics, although it has come in for detailed criticism by various philosophers. (See especially Soames (1987) and King (1994, 1995, 1998).) But even philosophers who think that there is more to content than a set of possible worlds would agree that propositions determine a function from worlds to truth values. Some would agree that such a function exhausts the ‘discriminatory role’ of a proposition, although this depends on the (highly contestable) assumption that the role of propositions is to discriminate amongst metaphysical possibilities. Still, even philosophers who disagree with what Stalnaker says about the nature of propositions could agree that if all we wanted from a proposition was to divide up some metaphysical possibilities, propositions could be functions from worlds to truth values, but they think some propositions that divide up the metaphysical possibilities the same way should be distinguished.\nWe don’t want to take sides in that debate, because our truth relativism means we are in conflict with even the idea that a proposition determines a function from worlds to truth values. To see this, consider a sentence whose truth value is relative to a context of evaluation, such as Vegemite tastes great. The truth relativist says that this sentence should be evaluated as true from a context where people like the taste of Vegemite (call this the Australian context) and should be evaluated as false from a context where people dislike this taste (call that the American context) and both evaluations are correct (from their own perspective) even though the Australians and Americans agree about what the content of Vegemite tastes great is, and they are in the same world. So there’s just no such thing as the truth value of Vegemite tastes great in the actual world, so it does not determine a function from worlds to truth values. What kind of function does it determine then?\nOne option, inspired by Lewis’s work on de se belief, is to say that it determines a function from centred worlds to truth values. The idea is that we can identify a context of evaluation with a centred world, and then Vegemite tastes great will be true relative to a centred world iff it is properly evaluated as true within that context. Alternatively, the content of Vegemite tastes great will determine a set of centred worlds, the set of contexts from which that sentence would be evaluated as true. Just as propositions were traditionally thought to determine (or be) sets of possible worlds, properties were traditionally thought to determine (or be) functions from worlds to sets of individuals.39 Now if we identify centred worlds with \\(\\langle\\)individual, world\\(\\rangle\\) pairs, a function from worlds to sets of individuals just is a set of centred worlds.40 So the content of Vegemite tastes great could just be a property, very roughly the property of being in a context where most people are disposed to find Vegemite great-tasting.\nThis proposal has three nice features. First, even though the content of Vegemite tastes great is not, and does not even determine, a proposition as Stalnaker conceived of propositions, it does determine a property. So the proposal is not as radical as it might at first look. Second, properties are the kind of thing that divide up possibilities. The possibilities they divide are individuals, not worlds, but the basic idea that to represent is to represent yourself as being in one class of possible states rather than another is retained. The only change is that instead of representing yourself as being in one class of worlds rather than another, you represent yourself as being in one class of \\(\\langle\\)individual, world\\(\\rangle\\) pairs rather than another. Third, the proposal links up nicely with David Lewis’s account of de se belief, and offers some prospects for connecting the contents of beliefs with the contents of assertions, even when both of these contents have ceased to be propositions in Stalnaker’s sense.41\nBut there’s a problem for this account. Consider what we want to say about Possibly Vegemite tastes great, where context makes it clear that the ‘possibly’ is a metaphysical modal. There’s a trivial problem and a potentially deep problem for this account. The trivial problem is that we know what the meaning of possibly is. It’s a function that takes propositions as inputs and delivers as output a proposition that is true iff the input proposition is true at an accessible world. If the content of Vegemite tastes great is a property rather than a proposition, then we have a type-mismatch. This is a trivial problem because it’s a fairly routine exercise to convert the meanings of words like possibly so they are the right kind of things to operate on what we now take the meaning of Vegemite tastes great to be.\nThe deep problem is that when we go through that routine exercise, we get the wrong results. We don’t want Possibly Vegemite tastes great to be true in virtue of there being an accessible world where the people there like the taste of Vegemite. We want it to be true in virtue of there being a world where Vegemite’s taste is a taste that in this context we’d properly describe as great. And it’s not clear how to get that on the current story. To see how big a problem this is, consider (58), where the modal is meant to be metaphysical and have wide scope.\nPossibly everyone hates Vegemite but it tastes great.\nThat’s true, on its most natural reading. But the content of Everyone hates Vegemite but it tastes great will be the empty set of centred worlds, for there is no centred world on which this is true. Now it’s not clear just what the meaning of possibly could be that delivers the correct result that (58) is true.\nSo we are tempted to consider an alternative proposal. Start with a very natural way of thinking about why the relativist has to modify the Stalnakerian story about content. The problem is that (even given a context of utterance) tastes great does not determine a property. Rather, relative to any context of evaluation, i.e. centred world, it determines a property. That is, its content is (or at least determines) a function from centred worlds to properties. So given our actual context, it determines the property of having a taste that people around here think is great. Now properties combine with individuals to form Stalnakerian propositions. So tastes great is a function from centred worlds to functions from individuals to sets of worlds. Hence Vegemite tastes great is a function from centred worlds to sets of worlds, the previous function with the value for the ‘individual’ being fixed as Vegemite.\nOur second option then is that in general that sentences containing ‘relative’ terms like ‘tastes’ or ‘huge’ or ‘might’ determines a function from centred worlds to sets of worlds. This makes it quite easy to understand how (58) could work. Possibly type-shifts so that it is now a function from functions from centred worlds to sets of worlds to functions from centred worlds to sets of worlds. It’s fairly easy to say what this function is. If the content of p is (or determines) f, a function from centred worlds to sets of worlds, then the content of \\(\\lozenge p\\) is (or determines) g, the function such that for any centred world c, w \\({\\in}\\) g(c) iff for some w\\(^\\prime\\) accessible from w, w\\(^\\prime\\) \\({\\in}\\) f(c). The core idea is just that we ignore the role of the centred worlds until the end of our semantic evaluation, and otherwise just treat \\(\\lozenge\\) as we’d treated it in traditional semantics. This is a rather nice position in many ways, but there are two issues to be addressed.\nFirst, it is not clear that functions from centred worlds to sets of worlds are really kinds of content. They are not things that divide up intuitive possibilities, in the way that sets of individuals, and sets of \\(\\langle\\)individual, world\\(\\rangle\\) pairs do. It’s no good to say that relative to a centred world a content is determined. That would be fine if we were content relativists, and we said the content was meant to be determined relative to a centred world. But as argued in the text the content of Vegemite tastes great should be the same across various contexts of evaluation. A better response is to say functions from centred worlds to sets of worlds do determine a kind of content. For any such function f, we can determine the set of centred worlds \\(\\langle\\)i, w\\(\\rangle\\) such that w \\({\\in}\\) f(\\(\\langle\\)i, w\\(\\rangle\\)). These will be the centred worlds that the proposition is true at. It’s not necessarily a problem that the proposition does more than determine this set. (It’s not an objection to King’s account of propositions that on his theory propositions do more than determine a set of possibilities.)\nSecond, it isn’t exactly clear how to fill out these functions when we get back to our core case: epistemic modals. It’s easy to say what it is for Vegemite tastes great to be true in a world relative to our context of evaluation; indeed we did so above. It’s a lot harder to say what it is for Granger might be in Prague to be true in an arbitrary world w relative to an arbitrary context of evaluation c. As a first pass, we might say this is true in w iff for all the people in c know, it is true in w that Granger is in Prague. But the problem is that whenever c is not a centre in w, it’s very hard to say just what the people in c know about w. Under different descriptions of w they will know different things about it. If w is described as a nearby world in which Granger is in Cleveland, they will know Granger is not in Prague in w. If it is described as a nearby world in which Myles knows where Granger is they may not know anything about whether Granger is in Prague is in w, even if those descriptions pick out the same worlds. Ideally we would cut through this by talking about their de re knowledge about w, but most folks have very little de re knowledge about other possible worlds. It’s not clear this is a huge problem though. Remember that a sentence containing an epistemic modal is meant to determine a function from centred worlds to functions from worlds to truth values. Provided we have a semantics that allows for semantic indeterminacy, we can just say that the functions from worlds to truth values are partial functions, and they simply aren’t determined when it’s unclear what the people in c know about w. Or we can say there’s a default semantic rule such that w is not in f(c) (where f is the function determined by the sentence) whenever this is unclear. Since the sentences whose meanings are determined by these values of the function, like Possibly Granger might be in Prague are similarly vague, it is no harm if the function is a little vague.\nSo we have two options on the table for what kind of functions sentences might determine if they don’t determine functions from world to truth values. One option is that they determine functions from centred worlds to truth values, another that they determine functions from centred worlds to functions from worlds to truth values. Neither is free from criticism, and the authors aren’t in agreement about which is the best approach, so it isn’t entirely clear what the best way to formally implement truth relativism is. But it does not look like there are no possible moves here. Moving to truth relativism does not mean that we will have to totally abandon the fruitful approaches to formal semantics that are built on ideas like Stalnaker’s, although it does mean that those semantic theories will need to be modified in places.\n\n\nCapellen, Herman, and Ernest Lepore. 1997. “On an Alleged Connection Between Indirect Quotation and Semantic Theory.” Mind and Language 12 (3-4): 278–96. https://doi.org/10.1111/j.1468-0017.1997.tb00075.x.\n\n\nDeRose, Keith. 1991. “Epistemic Possibilities.” Philosophical Review 100 (4): 581–605. https://doi.org/10.2307/2185175.\n\n\n———. 1998. “Simple Might’s, Indicative Possibilities, and the Open Future.” The Philosophical Quarterly 48 (190): 67–82. https://doi.org/10.1111/1467-9213.00082.\n\n\nEgan, Andy. 2004. “Second-Order Predication and the Metaphysics of Properties.” Australasian Journal of Philosophy 82 (1): 48–66. https://doi.org/10.1080/713659803.\n\n\nFintel, Kai von, and Sabine Iatridou. 2003. “Epistemic Containment.” Linguistic Inquiry 34 (2): 173–98. https://doi.org/10.1162/002438903321663370.\n\n\nHawthorne, John. 2004. Knowledge and Lotteries. Oxford: Oxford University Press.\n\n\nHorn, Laurence. 1989. A Natural History of Negation. Chicago: University of Chicago Press.\n\n\nJacobson, Pauline. 1999. “Towards a Variable Free Semantics.” Linguistics and Philosophy 22 (2): 117–84. https://doi.org/10.1023/A:1005464228727.\n\n\nKing, Jeffrey. 1994. “Can Propositions Be Naturalistically Acceptable?” Midwest Studies in Philosophy 19 (1): 53–75. https://doi.org/10.1111/j.1475-4975.1994.tb00279.x.\n\n\n———. 1995. “Structured Propositions and Complex Predicates.” Noûs 29 (4): 516–35. https://doi.org/10.2307/2216285.\n\n\n———. 1998. “What Is a Philosophical Analysis?” Philosophical Studies 90 (2): 155–79. https://doi.org/10.1023/A:1004254128428.\n\n\nLewis, David. 1976. “The Paradoxes of Time Travel.” American Philosophical Quarterly 13 (2): 145–52.\n\n\n———. 1979. “Scorekeeping in a Language Game.” Journal of Philosophical Logic 8 (1): 339–59. https://doi.org/10.1007/bf00258436.\n\n\nMacFarlane, John. 2003. “Future Contingents and Relative Truth.” The Philosophical Quarterly 53 (212): 321–36. https://doi.org/10.1111/1467-9213.00315.\n\n\nPagin, Peter. 2005. “Compositionality and Context.” In Contextualism in Philosophy: Knowledge, Meaning, and Truth, edited by Gerhard Preyer and Georg Peter, 303–48. Oxford: Oxford University Press.\n\n\nSoames, Scott. 1987. “Direct Reference, Propositional Attitudes and Semantic Content.” Philosophial Topics 15 (1): 47–87. https://doi.org/10.5840/philtopics198715112.\n\n\n———. 2002. Beyond Rigidity. Oxford: Oxford University Press.\n\n\nStalnaker, Robert. 1978. “Assertion.” Syntax and Semantics 9: 315–32.\n\n\nStanley, Jason. 2000. “Context and Logical Form.” Linguistics and Philosophy 23 (4): 391–434. https://doi.org/10.1023/A:1005599312747.\n\n\nStanley, Jason, and Zoltán Gendler Szabó. 2000. “On Quantifier Domain Restriction.” Mind and Language 15 (2&3): 219–61. https://doi.org/10.1111/1468-0017.00130.\n\n\nSome of Professor Granger’s thoughts sound a little odd being in the present tense, but as we shall see, there are complications concerning the interaction of tense with epistemic modals, so for now it is easier for us to avoid those interactions.↩︎\nNote that it also seems implausible to say that this is an instance of metalinguistic negation, as discussed in Horn (1989). When Charles interrupts Myles to object, the objection isn’t that the particular form of words that Myles has chosen is inappropriate. The form of words is fine, and Myles’ utterance would be completely unobjectionable if Charles’s epistemic state were slightly different. What’s wrong is that Myles has used a perfectly acceptable form of words to say something that’s false (at least by Charles’ lights—more on this later). We also think it’s implausible to understand the ‘might’ claims in question here as claims of objective chance or objective danger.↩︎\nWe take the puzzle to be a puzzle about sentences containing epistemic modal operators, however they are identified. We are sympathetic with DeRose’s (1998) position that many sentences containing ‘might’ and ‘possible’ are unambiguously epistemic, but do not wish to argue for that here. Rather, we simply take for granted that a class of sentences containing epistemic modal operators has been antecedently identified.\nThere are two differences between ‘possible’ and ‘might.’ The first seems fairly superficial. Sentences where might explicitly takes a sentence, rather than a predicate, as its argument are awkward at best, and may be ungrammatical. It is possible that Professor Granger is in Prague is much more natural than It might be the case that Professor Granger is in Prague, but there is no felt asymmetry between Professor Granger is possibly in Prague and Professor Granger might be in Prague. We will mostly ignore these issues here, and follow philosophical orthodoxy in treating epistemic modals as being primarily sentence modifiers rather than predicate modifiers. The syntactic features of epistemic modals are obviously important, but we’re fairly confident that the assumption that epistemic modals primarily operate on sentences does not bear any theoretical load here, and could be replaced if necessary.\nThe other difference will be relevant to some arguments that follow. ‘Might’ can interact with tense operators in a way that ‘possible’ does not. It might have rained could either mean MIGHT (WAS it rains) or WAS (MIGHT it rains), while It possibly rained unambiguously means POSSIBLY (WAS it rains). It is often hard in English to tell just which meaning is meant when a sentence contains both tense operators and epistemic modals, but in Spanish these are expressed differently: Puede haber llovido; Podría haber llovido.↩︎\nThere are three kinds of cases where something like DeRose’s clause (2) could be relevant.\nFirst, Jack and Jill are in a conversation, and Jack knows p while Jill knows p \\({\\rightarrow \\neg}\\) Fa. In this case intuitively neither could truly say a might be F even though neither knows a is not F.\nSecond, there are infinitely many mathematicians discussing Fermat’s Last Theorem. The first knows just that it has no solutions for n=3, the second just that it has no solutions for n=4, and so on. Intuitions are (unsurprisingly) weaker here, but we think none of them could say Fermat’s Last Theorem might have solutions, because the group’s knowledge rules this out.\nThird, if S was very recently told that a is not F, but simply forgot this, then intuitively she speaks falsely if she says a might be F.\nFourth, if S has the materials for easily coming to know P from her current knowledge, but has not performed the relevant inference, then we might be inclined (depending on how easy the inferential steps were to see and so on) to say that she is wrong to utter ‘It might be that not P.’\nRather than try and resolve the issues these cases raise, we will stick to cases where the only thing that could make a might be F false is that someone knows that a is not F.↩︎\nShe would also have violated some pragmatic principles by knowingly using a third-person pronoun to refer to herself, but we take it those principles are defeasible, and violation of them does not threaten the truth-aptness of her utterance.↩︎\nNotice that intuitions do not change if we alter the case in such a way that Jack has a strange disorder that makes it very hard for him to come to know how many fingers he has. Thus clause (2) of Derose’s analysis cannot do the work of the relevant side constraint.↩︎\nAnd see the case of Tom and Sally in the maze below for some countervailing evidence.↩︎\nNotice that this use prohibits the inference from: The speaker said that the bottle was empty, to, The speaker expressed the proposition/said something that meant that the bottle was empty.↩︎\nWe are grateful for correspondence with John MacFarlane here.↩︎\nFor what its worth, we also note that ‘S claimed that P’ has less laxity (of the sort being discussed) than ‘S said that P.’↩︎\nNote that the negation of semantic blindness concerning some fragment of the language is not the theory that speakers know all the semantic equivalences that hold between terms in that fragment. All we mean by the denial of semantic blindness is that speakers not have false beliefs about the semantics of their terms.↩︎\n This theory looks like one in which propositional attitude operators become monsters, since the content of Jack thinks that Jill might be happy is naturally generated by applying the operator Jack thinks to the proposition that that Jill might be happy denotes when it is expressed in Jack’s context. But this is not the easiest, or obviously the best, way to look at the theory. For one thing, that way of looking at things threatens to assign the wrong content to Jack thinks that Jill might have stolen my car. The content of Jill might have stolen my car in Jack’s context is that for all Jack knows, Jill stole Jack’s car, which is not what is intended. That is to say, thinking of propositional attitude operators as monsters here ignores the special status of epistemic modals in the semantics. It is better, we think, to hold that on this theory epistemic modals are impure indexicals whose value is fixed, inter alia, by their location in the sentence as well as their location in the world. But even if this theory does not officially have monsters, the similarity to monstrous theories is worth bearing in mind as one considers the pros and cons of the theory.\nThanks to Ernest Lepore for helpful discussions here.↩︎\nWhat follows is a belief theoretic version of Charles’ reasoning.↩︎\n A similar case to the following appears in (Hawthorne 2004, 27).↩︎\nThough the theory will allow for the truth of, “I might have been on that bus” (since the epistemic modal clause doesn’t occur on its own, but in the scope of a temporal operator). So if we think that (i) that’s enough to do the justificatory and explanatory work, and (b) Bill’s utterance of “I might be on that bus” is best understood as a clumsy stab at “I might have been on that bus,” then perhaps we can account for this kind of case using our second contextualist theory. Two worries: First, it is a cost of the theory that we have to reinterpret Bill’s utterance in this way, as a clumsy attempt to say something that the theory can accommodate. Second, there might be cases where the interpretation is less plausible: As a response to, “Why is Ann getting ready to jump over the hedge?” “I might have been on that bus” sounds worse to us than “I might be on that bus.”↩︎\nThe same kind of equivocation can be seen in other arguments involving contextually variable terms. Assume that Nomar lives in Boston, Derek lives in New York, and Nomar, while talking about Fenway Park in Boston says, “I live nearby.” Derek, at home in New York, hears this on television and runs through the following argument.\nIn saying “I live nearby” Nomar says that he lives nearby. (Plausible disquotational premise about ‘nearby’)\nNomar speaks truly when he says “I live nearby” (Follows from the setup)\nIf Nomar speaks truly when he says “I live nearby” and in saying “I live nearby” he says that he lives nearby, then he lives nearby. (I.e. if he speaks truly then what he says is true.)\nIf Nomar lives nearby, then he lives in New York (Since everywhere that’s nearby to Derek’s home is in New York.); therefore\nNomar lives in New York\nThe right thing to say about this argument is that it equivocates. Every premise has a true reading. Perhaps every premise is true on its most natural reading, but the denotation of ‘nearby’ has to change throughout the argument for every premise to be true. The current view is that ‘might’ behaves like ‘nearby,’ and that Professor Granger’s argument equivocates, like Derek’s.↩︎\nThere also seems to be a past/future asymmetry about epistemic modals which the third contextualist theory will have trouble explaining. Consider this case involving past tense epistemic modals. Romeo sees Juliet carrying an umbrella home on a sunny afternoon. When he asks her why she is carrying an umbrella, she replies “It might have rained today.” There’s a scope ambiguity in Juliet’s utterance. If the epistemic modal takes wide scope with respect to the tense operator, Juliet would be claiming that she doesn’t know whether it has rained today (implicating, oddly, that this is why she now has an umbrella.) Or, as Juliet presumably intends, the temporal operator could take wide scope with respect to the epistemic modal. In that case Juliet says that it was the case at some earlier time (presumably when she left for work this morning) that it was compatible with her knowledge that it would rain today. And that seems both true and a good explanation of her umbrella-carrying.\nIt is much harder, if it is even possible, to find cases involving future tense operators where the temporal operator takes wide scope with respect to the pistemic modal. If S says, “It might rain tomorrow,” that seems to unambiguously mean that it’s compatible with S’s current knowledge (and her community’s) that it rains tomorrow. For a more dramatic case, consider a case where two people, Othello and Desdemona, have discovered that a giant earthquake next week will destroy humanity. No one else knows this yet, but there’s nothing that can be done about it. This rather depresses them, so they decide to take memory-wiping drugs so that when they wake up tomorrow, they won’t know about the earthquake. Othello can’t say, “Tomorrow, humanity might survive,” even though it is true that tomorrow, for all anyone will know, humanity will survive. If the temporal modifier could take wide scope with respect to the epistemic modal, Othello’s utterance could have a true reading. But it does not. It’s possible at this point that our policy, announced in footnote 2, of ignoring issues relating to DeRose’s second clause will come back to haunt us. One possibility here is that tomorrow it will still be false that humanity might survive because it’s not compatible with what people tomorrow know and knew that humanity survives. We don’t think that’s what is going on, but it’s possible. Here’s two quick reasons to think that the problem is not so simple. First, if Othello and Desdemona commit suicide rather than take the memory-wiping drugs, it will be compatible tomorrow with all anyone ever knew that humanity survives. But still Othello’s speech seems false. Second, it’s not obviously right that what people ever knew matters for what is epistemically possible now. Presumably at one stage Bill Clinton knew what he had for lunch on April 20, 1973. (For example, when he was eating lunch on April 20, 1973.) But unless he keeps meticulous gastronomical records, this bit of knowledge is lost to humanity forever. So there will be true sentences of the form Bill Clinton might have eaten x for lunch on April 20, 1973 even though someone once knew he did not. Now change the earthquake case so that it will happen in thirty years not a week, and no one will then know about it (because Othello and Desdemona took the memory-wiping drugs and destroyed the machines that could detect it). Still it won’t be true if Othello says, “In thirty years, humanity might survive.” This suggests to us that some kind of constraints on epistemic modals will be required. The existence of these constraints seems to refute the ‘no constraints’ version of contextualism. It also undermines the argument that the second version of contextualism is too ad hoc. Once some constraints are in place, others may be appropriate.↩︎\nWe assume here, following Stanley, a ‘traditional syntax involving variables’ [fn. 13]Stanley2000-STACAL. At least one of us would prefer a variable-free semantics along the lines of Jacobson (1999) Adopting such a semantics would involve, as Stanley says, major revisions to the presentation of this argument, but would not clearly involve serious changes to the argument. Most contextualists happily accept the existence of variables so we do not beg any questions against them, but see Pagin (2005) for an important exception.↩︎\nIt is not entirely clear what the relevant class of quantifiers is, although von Fintel and Iatridou have some intriguing suggestions about what it might be.↩︎\nAs previously noted, we are not all convinced that semantics ever needs to appeal to such variables, let alone that it does to account for the behaviour of epistemic modals.↩︎\nBy “express” we will always mean “semantically express.” We’re not concerned with, and hope not to commit ourselves to any views about, for example, what’s conveyed via various pragmatic processes.↩︎\nNote that something like this had better be true if what it is to believe p is to have a sentence that means p in one’s ‘belief box.’↩︎\nWe assume that it is not a serious option to deny that we ever accept unnegated epistemic modal sentences.↩︎\nThere also a technical problem with universalism that mirrors one of the problems Stanley and Szabó (2000) raise for pragmatic theories of quantifier domain restriction. Normally (28) would be used to express a proposition like (29).\nEvery professor enjoys every class.\nEvery salient professor enjoys every class that s/he teaches.\nIntuitively, by uttering (28) we express a proposition that contains two restricted quantifiers. Let’s accept, for the sake of the argument, that a pragmatic theory of quantifier domain restriction can sometimes explain why the quantifiers in the propositions we express are more restricted than the quantifiers in the truth conditions for the sentences we use. Stanley and Szabó argue that such an explanation will not generalise to cover embedded quantifiers where the quantifier domain in the proposition expressed is bound to the outer quantifier. One such quantifier is the quantifier over classes in (28). We will not repeat their arguments here, but simply note that if they are correct, the universalist faces a problem in explaining how we use sentences with embedded epistemic modals that are (intuitively) defined with respect to a community that is bound by a higher level quantifier. As we saw, (22) provides an example of this kind of epistemic modal.↩︎\nEarlier we used speech reports to illustrate the oddities of epistemic modals inside propositional attitude ascriptions. There are well-known difficulties with connecting appropriate speech reports to the semantic content of what is said, as opposed to merely communicated. (For some discussion of these, see Soames (2002) and Capellen and Lepore (1997).) We don’t think those difficulties affect the above arguments, where the evidence is fairly clear, and fairly overwhelming. But matters get a little more delicate in what follows, so we move to belief reports because they are more closely tied to the content of what is believed.↩︎\nIn this case, as with all the belief reports discussed below, the only evidence the reporter has for the report is given by the speech immediately preceding it. We assume there is good reason from the context to assume that the speakers are sincere.↩︎\n There are somewhat delicate questions about what a direct belief report means, but we assume the notion is well enough understood, even if we could not formally explicate what is going on in all such reports.↩︎\nWe do not say that ‘nearby’ in a speech report could never refer to the area near the location of the original speaker. Had Rosencrantz asked a question about San Francisco, and Guildenstern given the same response, that is presumably what it would have done. We just say that it does not automatically refer back to that area, and in some cases, like (35), it can refer to a quite different area. ‘Nearby’ behaves quite differently in this respect to ‘near here,’ which always refers to the area near the reporter.↩︎\nAnd then it would probably be more natural to say “He might have eight fingers,” but that’s possibly for unrelated reasons.↩︎\nOr perhaps something more specific than that, such as that he is old and slow for a player at his position.↩︎\nOne might also consider a ‘says that’ version of Truth in Reporting: If X speaks true, then Y’s report X says that S is accurate only if in the context Y is in, S expresses a true proposition. This is more questionable, since it is questionable whether ‘says that’ constructions must report what is semantically expressed by a speech, as opposed to what is merely communicated. See again the papers mentioned in footnote 25.↩︎\nWe are very grateful in this section to extensive conversations with John MacFarlane. His (2003) was one of the main inspirations for the relativist theory discussed here. His (ms), which he was kind enough to show us a copy of while we were drafting this paper, develops the argument for a relativist approach to epistemic modals in greater detail than we do here. Mark Richard also has work in progress that develops a relativist view on related matters, which he has been kind enough to show us, and which has also influenced our thinking.↩︎\nWe do not assume here that ordinary speakers, like Watson, explicitly make judgments about the truth of utterances relative to a context of evaluation, as such. They do make judgments about the truth of utterances, and those judgments are made in contexts, but they don’t explicitly makes judgments of truth relative to context of evaluation. One of the nice features, however, of the relativist account is that it is possible to do an attractive rational reconstruction of most of their views in terms of contexts.↩︎\nCan we even say that someone speaks falsely here now that truth and falsity is always relative to a context of evaluation? It turns out we can, indeed we must, although the matter is a little delicate. We return to this point below.↩︎\nWe assume here the vultures are talking mainly to other vultures, and John is talking mainly to other humans.↩︎\nWe are grateful to John Macfarlane for helpful correspondence that influenced what follows.↩︎\nWe don’t take any stand here on just how the war went, if it happened at all. The important point is that whether (48) is true when said of a particular battle is a wide-open empirical question, not one that can be settled by appeal to the semantics of might. The content relativist says, falsely, that it can be thus settled.↩︎\n Arguably, then, one will have to distinguish (and posit an ordinary conflation between) knowledgeT from knowledgeB, the latter being needed to make good on the normative importance of knowledge, the former being need to make sense of the validity of the inference from knowing that p to p. Is trouble lurking here for the truth relativist, especially given link between the truthB of ‘might’ claims and facts about knowledge? We shall not pursue the matter further here.↩︎\n Lewis preferred the theory on which properties were sets of individuals, potentially from different worlds. This theory has difficulties accounting for individuals that exist in more than one world. And since properties exist in more than one world, and properties have to be treated as individuals in some contexts (e.g. when they are the subjects of predication) this is a serious problem. Treating properties as functions from worlds to sets of individuals removes this problem without introducing any other costs. (See Egan (2004) for more details.)↩︎\nMatters are a little more complicated when we introduce times into the story. For purposes of this appendix we ignore all matters to do with tense. As you’ll see, the story is complicated enough as it is, and this omission doesn’t seriously affect the dialectic to follow.↩︎\nIt might be that propositions just are whatever things are the contents of assertions and beliefs, so we shouldn’t say that the contents of sentences like Vegemite tastes great are not propositions. But they will be very different kinds of propositions to what we are used to. Thanks here to John MacFarlane.\n\n↩︎\n",
    "preview": "posts/2021-02-03-epistemic-modals-in-context/flamethrower.jpg",
    "last_modified": "2021-03-17T10:19:31-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-13-review-of-lewisian-themes/",
    "title": "Review of “Lewisian Themes”",
    "description": "A review of Frank Jackson and Graham Priest (eds), “Lewisian Themes: The Philosophy of David K. Lewis”, Oxford University Press, 2004.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2005-08-02",
    "categories": [
      "book review",
      "on books",
      "David Lewis",
      "history of analytic"
    ],
    "contents": "\nDavid Lewis didn’t seem particularly fond of festschrifts. He didn’t participate in the one volume of papers on his work published in his lifetime (Reality and Humean Supervenience, Rowman and Littlefield 2001). He rarely contributed to festschrifts for others, in recent decades only contributing papers to volumes in honour of his very closest friends (David Armstrong and Hugh Mellor). I haven’t done a systematic study of this, but my casual reading suggests he didn’t often refer to papers published in festschrifts either. More generally, he strongly favoured published in journals with blind review to publishing in edited volumes of solicited papers.\nSo if there was to be a festschrift for David Lewis, as there surely should be, it should be one put together just the way Lewisian Themes was. Most of the papers for it were selected by blind refereeing, not by inviting the usual suspects. (And the result was that several young philosophers, even graduate students, are represented in the collection.) The festschrift did not start life as this volume, but as an edition of a journal. And not just any journal, but the journal that Lewis’s work most often graced: the Australasian Journal of Philosophy. This volume reprints the papers from that issue, and adds four more papers that were left out for space considerations.\nThe result is a marvellous collection that reflects Lewis’s work in both its quality and its breadth. Given the amount of material herein, I can do little more than summarise the papers and present a few reflections on what some of them tell us about the future directions for Lewisian philosophy. Here is the list of topics covered, each of which is followed in parentheses by the names of the authors who address that topic.\nQuantum Mechanics (David Lewis; David Papineau)\nProperties (Andy Egan; Josh Parsons)\nTruth in Fiction (Richard Hanley; Jim Mackenzie)\nRamseyan Humility (Rae Langton; Jonathan Schaffer)\nChance (Adam Elga; Ned Hall)\nMental Content (Robert Stalnaker; Barry Taylor; Alan Hájek and Philip Pettit)\nModal Realism (Kris McDaniel; Barry Taylor; Max Cresswell)\nCounterpart Theory and Essentialism (Max Cresswell; L. A. Paul)\nSimilarity (Barry Taylor; David Vander Laan)\nHoles (Roberto Casati and Achille Varzi)\nTruth (Marian David)\nVagueness (Gideon Rosen and Nicholas J. J. Smith)\nThe breadth of the papers here is astounding. Three of the papers (Papineau, Langton and Schaffer) respond to material of Lewis’s that was only published posthumously. Summaries of David Lewis’s work normally start by mentioning his modal realism, but this presents a somewhat distorted picture of his interests and importance. The list here is a better guide to the range of Lewis’s work. (Though as the editors note, it is quite incomplete because it leaves out Lewis’s work on materialism and on set theory and mereology , not to mention his work on conditionals and on value theory and many other topics.)\nThe only paper directly about modal realism is McDaniel’s interesting paper on how we might formulate and defend a version of modal realism that allows worlds to overlap. Barry Taylor’s paper develops some considerations about simplicity that suggest a worry for Lewis’s argument against linguistic ersatzism: in particular he argues against the existence of alien universals. And Max Cresswell formulates his discussion of counterpart theory using modal realist language. And that’s it for the discussion of modal realism, which seems about right given its relative importance in Lewis’s overall picture.\nMany of the papers accept the broad outlines of Lewis’s views, but suggest alternative ways of developing the details. For instance, Andy Egan’s paper “Second Order Predication and the Metaphysics of Properties” discusses Lewis’s view that properties are sets of individuals. Egan argues that Lewis should instead have said that properties are functions from worlds to sets of individuals in that world. You might think that these two views are equivalent. Let f be a function that Egan calls a property, and consider the set {x: for some world w, x ∈ f(w)}. Why wouldn’t that set be a Lewis-style property? Well, it would be if no object is in more than one world. Isn’t that true on Lewis’s metaphysics? Not quite. Most ordinary individuals are in one world only, but properties are in multiple worlds. If the property being red has some property, say being instantiated, in some worlds but not others, should it be a member of the property being instantiated or not? Egan argues, persuasively, that Lewis can give no good answer to this question. But on the view that properties are functions, we can say that the function that we identify with the property being instantiated maps worlds onto a set that includes being red in worlds where there are red things, and a set that does not include it in worlds where there are no red things. So this looks like a nice fix to a technical problem for Lewis. Egan goes on to argue that the change undermines some of Lewis’s arguments against endurantism and modal overlap.\nNed Hall discusses Lewis’s widely influential theory of objective chance. There are two disputes running through Hall’s paper: the dispute between reductionists and anti-reductionists about chance, and the dispute between the old and new versions of the Principal Principle, presented here as (O) and (N).\n\\(O\\) C0(A | E & cht(A) = x) = x\n\\(N\\) C0(A | HL) = cht(A | L)\nHere C0 is an initial credence function of a rational agent, cht is the chance function at t, H is the history of the world to t, L is the conjunction of the laws, and E is any evidence that is ‘admissible’ with respect to A at t. (Evidence is admissible, roughly, if it doesn’t provide any more information about A than we get from knowing its chance.) Lewis thought it was a cost of his reductionist position that he was forced to give up (O) and retreat to (N). Hall suggests that it isn’t a cost, it is really something we could have derived all along. Hall’s reasoning is a little hard to follow here. He shows elegantly that if the reductionist agrees to treat the chance function as an expert function, then she should accept (N) rather than (O). (Roughly speaking, to treat a function as an expert function is, as Hall says on page 101, to consider its opinion so epistemically superior to yours that you are disposed to defer to it. Before we can give a formal statement of this we need to make it precise, which may not be trivial. One of the important innovations of Hall’s paper is to clarify some importantly distinct ways of making it precise, in particular distinguishing the cases where the expert is better informed than you from the case where the expert is a better judge of evidence than you.) But it is not clear how showing that the reductionist is forced to accept (N) rather than (O), shows that it isn’t a cost of reductionism that it is incompatible with (O).\nHall makes two nice points about how (N) stands with respect to the reductionist/anti‑reductionist debate. First, he notes that it isn’t clear in general how reductionists can derive (N), i.e. derive the claim that the chance function is an expert function, from their general principles. Lewis often touted it as a virtue of reductionism that such a derivation seems possible, but Hall suggests that there will be problems with the details here. Lewis’s optimism rested on cases where there was a close connection between chance and limiting relative frequency, but this is not the only kind of case. Hall argues, persuasively, that this part of the reductionist program will have difficulties with cases where the chance of an F being a G varies continuously with some magnitude m of the F (say its mass), and the proposition A concerns whether a particular object a, which is an F such that m(a) is unique, is also a G. (The general project of trying to incorporate continuously variable quantities, like masses, into Lewis’s metaphysics appears to be very difficult, and there could be some very interesting philosophical discoveries to be made in this field.) Second, Hall notes that it is not as hard for the anti‑reductionist to explain (N) as Lewis suggests. To be sure, the anti-reductionist can’t derive (N) from, say, metaphysical necessities and logic. But we know that reasonable principles of inductive rationality won’t follow merely from metaphysical necessities and logic; in addition, we need a number of substantive synthetic principles. There is no reason why (N), or something that entails (N), cannot be among these.\nThe difficulty Lewis imagines for the anti-reductionist here arises largely from an insufficient attention to epistemology. Lewis never took epistemology (at least informal epistemology) as seriously as he took metaphysics, philosophy of mind, or philosophy of language, and occasionally this shows up in the papers here. Jonathan Schaffer’s paper “Quiddistic Knowledge” argues that there is no plausible epistemological theory that grounds Lewis’s claim in “Ramseyan Humility” that “we are irredemiably ignorant about the identities of the fundamental properties.” And Robert Stalnaker in “Lewis on Intentionality” argues that Lewis’s content internalism derives from an implausible commitment to epistemological internalism.\nThere are of course many more good ideas throughout this volume. For example, Marian David has a thorough, and thoroughly convincing, response to Lewis’s argument that the correspondence theory of truth is not a competitor to the redundancy theory of truth. Max Cresswell provides a few reasons for thinking that the counterpart relation has to be an equivalence relation, in which case we may as well do without counterpart theory. (Cresswell’s paper and the recent paper on a similar topic by Michael Fara and Timothy Williamson, “Counterparts and Actuality”, Mind 114: 1-30, complement each other nicely, and should be required reading for anyone interested in debates about counterpart theory.) Alan Hájek and Philip Pettit argue that Lewis’s argument against the view that desires are beliefs about what is good assumes that goodness is not an ‘indexical’ property, and that this might fail if what is good is partially a function of, say, our evidence, as on the view that what is good is what maximises expected utility. And there are many more gems like these.\nIt is a great compliment to Lewis’s style as a philosopher that, as we see in this collection, Lewis amassed a great number of admirers who thought the best way to honour his work was to criticise it in every way imaginable. It is sad to stop and think how much we would have learned from hearing Lewis’s responses.\n\n\n\n",
    "preview": "posts/2021-03-13-review-of-lewisian-themes/lewisian-themes.jpg",
    "last_modified": "2021-03-13T10:34:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-11-should-we-respond-to-evil-with-indifference/",
    "title": "Should We Respond to Evil With Indifference?",
    "description": "In a recent article, Adam Elga outlines a strategy for \"Defeating Dr Evil with Self-Locating Belief\". The strategy relies on an indifference principle that is not up to the task. In general, there are two things to dislike about indifference principles: adopting one normally means confusing risk for uncertainty, and they tend to lead to incoherent views in some 'paradoxical' situations. I argue that both kinds of objection can be levelled against Elga's indifference principle. There are also some difficulties with the concept of evidence that Elga uses, and these create further difficulties for the principle.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2005-05-01",
    "categories": [
      "epistemology",
      "scepticism"
    ],
    "contents": "\n\nContents\nFrom Basel to Princeton\nAdd it Up\nOut of sight, out of mind\nIt’s Evidently Intransitive\nMorgan and Morgan and Morgan and Morgan\nIntermission\nRisky Business?\nBoyfriend in a Coma\nShooting Dice can be Dangerous\n\nIn a recent article, Adam Elga (2004) outlines a strategy for “Defeating Dr Evil with Self-Locating Belief.” The strategy relies on an indifference principle that is not up to the task. In general, there are two things to dislike about indifference principles: adopting one normally means confusing risk for uncertainty, and they tend to lead to incoherent views in some ‘paradoxical’ situations. Each kind of objection can be levelled against Elga’s theory, but because Elga is more careful than anyone has ever been in choosing the circumstances under which his indifference principle applies we have to be similarly careful in focussing the objections. Even with this care the objections I put forward here will be less compelling than, say, the objections (Keynes 1921 Ch. 4) put forward in his criticisms of earlier indifference principles. But there still may be enough to make us reject Elga’s principle. The structure of this note is as follows. In and 2 I set out Elga’s theory, in and 4 I discuss some initial objections that I don’t think are particularly telling, in I discuss some paradoxes to which Elga’s theory seems to lead (this is reprised in where I discuss a somewhat different paradoxical case) and in and 8 I argue that even Elga’s careful indifference principle involves a risk/uncertainty confusion.\nPublished in Philosophy and Phenomenal Research 70: 613-35.\nThanks to Jamie Dreier, Adam Elga and an anonymous referee for helpful discussions about this paper and suggestions for improvements.\nFrom Basel to Princeton\nIn (1979) David Lewis argued that the contents of contentful mental states were not propositions, but properties. When I think that I’m a rock star, I don’t attribute truth to the proposition Brian is a rock star, but rather attribute the property of rock stardom to myself. Lewis was led to this position by considering cases where a believer is mistaken about his own identity. For example, if I believe that I’m a rock star without believing that I’m Brian, and in fact while thinking that Brian is an infamous philosopher, it is odd to attribute to me belief in the proposition Brian is a rock star. But it is perfectly natural to say I self-attribute rock stardom, and that’s just what Lewis says.\nIf we accept Lewis’s position, there are two paths we can take. First, we can try simply replacing all talk of propositional attitudes with talk of proprietal attitudes, and trusting and hoping that this won’t make a difference to our subsequent theorising. Alternatively, we can see if changing the type of entity that is the content of a contentful state has distinctive consequences, and in particular see if it gives us the conceptual resources to make progress on some old problems. That’s the approach Adam Elga has taken in a couple of papers, and whatever one thinks of his conclusions, the early returns certainly suggest that this Lewisian outlook will prove remarkably fruitful.\nOn the Lewisian approach, credences are defined over properties, and properties are sets of possibilia, i.e. centred worlds. Some properties are maximally precise, they are satisfied by exactly one possible object. Elga sometimes calls these maximally specific properties predicaments because they specify exactly what is happening to the agent that instantiates one. Say predicaments F1 and F2 are similar iff the F1 and the F2 are worldmates and their experiences are indistinguishable. Elga’s principle INDIFFERENCE says that if predicaments F1 and F2 are similar then any rational agent should assign equal credence to F1 and F2. This becomes most interesting when there are similar F1 and F2. So, for instance, consider poor O’Leary.\nO’LEARY\nO’Leary is locked in the trunk of his car overnight. He knows that he’ll wake up briefly twice during the night (at 1:00 and again at 2:00) and that the awakenings will be subjectively indistinguishable (because by 2:00 he’ll have forgotten the 1:00 awakening). At 1:00 he wakes up.\n\nElga says that when O’Leary wakes up, he should assign equal credence to it being 1:00 as to it being 2:00. So, provided O’Leary knows that one of these two hypotheses is true, INDIFFERENCE says that he should assign credence 1/2 to it being 1:00 at the wake up.\nElga has an argument for INDIFFERENCE, which we shall get to by , but for a while I will look at some immediate consequences of the position. I’ll start with two reasons to think that INDIFFERENCE needs to be strengthened to play the role he wants it to play.\nAdd it Up\nOne difficulty with INDIFFERENCE as stated so far is that it applies only to very narrow properties, predicaments, and it is not clear how to generalise to properties in which we are more interested.\nBERNOULLIUM\nDespite months of research, Leslie still doesn’t know what the half-life of Bernoullium, her newly discovered element is. It’s between one and two nanoseconds, but she can’t manufacture enough of the stuff to get a better measurement than that. She does, however, know that she’s locked in the trunk of her car, and that like O’Leary she will have two indistinguishable nocturnal awakenings. She’s having one now in fact, but naturally she can’t tell whether it is the first or the second.\n\nINDIFFERENCE says that Leslie should assign credence 1/2 to it being the first wake-up, right? Not yet. All that INDIFFERENCE says is that any two predicaments should receive equal credence. A predicament is maximally specific, so it specifies, inter alia, the half-life of Bernoullium. But for any x, Leslie assigns credence 0 to x being the half-life of Bernoullium, because there are uncountably many candidates for being the half-life, and none of them look better than any of the others. So she assigns credence 0 to every predicament, and so she satisfies INDIFFERENCE no matter what she thinks about what the time is. Even if, for no reason at all, she is certain it is her second awakening, she still satisfies INDIFFERENCE as it is written, because she assigns credence 0 to every predicament, and hence equal credence to similar predicaments.\nFortunately, we can strengthen INDIFFERENCE to cover this case. To start, note that the motivations for INDIFFERENCE suggest that if two predicaments are similar then they should receive equal credence not just in the agent’s actual state, but even when the agent gets more evidence. Leslie should keep assigning equal credence to it being her first or second wake up if she somehow learns what the half-life of Bernoullium is, for example. This suggests the following principle:\nC-INDIFFERENCE\nIf F1 and F2 are similar, and an agent does not know that she is in neither, then her conditional credence on being F1, conditional on being either F1 or F2, should be 1/2.1\n\nBut even this doesn’t quite resolve our problem. Simplifying Leslie’s situation somewhat, the live predicaments are all of the following form: this is the first/second awakening, and the half-life of Bernoullium is x. C-INDIFFERENCE requires that for any c, conditional on the half-life of Bernoullium being c, Leslie assign credence 1/2 to it being her first awakening. From this and the fact that Leslie’s credence function is a probability function it doesn’t follow that her credence in this being her first awakening is 1/2. So to get INDIFFERENCE to do the work it is meant to do in Leslie’s case (and presumably O’Leary’s case, since in practice there will be some other propositions about which O’Leary is deeply uncertain) I think we need to strengthen it to the following.\nP-INDIFFERENCE\nIf G1 and G2 are properties such that:\nFor all worlds w, there is at most one G1 in w and at most one G2 in w;\nFor all worlds w, there is a G1 in w iff there is a G2 in w; and\nFor all worlds w where there is a G1 in w, the G1 and the G2 have indistinguishable experiences; then\nG1 and G2 deserve equal credence.\n\nElga does not endorse either C-INDIFFERENCE or P-INDIFFERENCE, but I suspect he should given his starting assumptions. It is hard to believe if O’Leary is certain about everything save what time it is, then rationality imposes very strong constraints on his beliefs about time, while rationality imposes no such constraints should he (or Leslie) be uncertain about the half-life of Bernoullium. Put another way, it is hard to believe that in her current state Leslie could rationally assign credence 0.9 to this being her first awakening, but if she decided the half-life of Bernoullium is 1.415 nanoseconds, then she would be required to change that credence to 0.5. If we have INDIFFERENCE without P-INDIFFERENCE, that is possible. So I will assume in what follows that if C-INDIFFERENCE and P-INDIFFERENCE are false then INDIFFERENCE is heavily undermined.2\nOut of sight, out of mind\nElga’s discussion presupposes two kinds of internalism. First, he assumes that some internalist theory of experience is true. Second, he assumes that some internalist theory of justification is true. If the first assumption is false it threatens the applicability of the theory. If the second assumption is false it threatens the truth of the theory.\nAn externalist theory of experience says that what kind of experience S is having is determined, inter alia, by what S is experiencing. While setting out such a view, John (Campbell 2002, 124–26) says that two people sitting in duplicate prison cells looking at duplicate coffee cups will have different experiences, because one will have an experience of the coffee cup in her hand, and the other will not have an experience of that cup. This does not threaten INDIFFERENCE, but it does seem to render it trivial. On Campbell’s view, if two agents are able to make demonstrative reference to different objects, and there is no reason to think Elga’s agents in allegedly similar but not numerically identical predicaments cannot, they are having different experiences. Hence the situations are not really similar after all. Strictly speaking, this is good news for INDIFFERENCE, since it is hard given this view of experience to find counterexamples to it. But I doubt that Elga will be happy with this defence.\nThe second kind of internalist assumption is more threatening. Many externalists about justification think whether a particular experience justifies a belief for an agent depends not just on intrinsic features of that experience, but on the relationship between experiences of that kind and the world around the agent. In some versions of this, especially the version defended by Timothy Williamson (1998), whether an experience either constitutes or produces evidence depends on whether it constitutes or produces knowledge. Since it is not clear that any two similar agents know the same thing, since it is clear that they do not have the same true beliefs, on Williamson’s theory it seems that the agents will not have the same evidence. In particular, it is possible that part of one agent’s evidence is inconsistent with her being the other agent. If part of her evidence is that she has hands, then she is not a brain-in-a-vat having experiences like hers, and she should not assign high credence to the claim that she is one, no matter what INDIFFERENCE says. So Elga needs to reject this kind of externalism about evidence. This is not a devastating objection. I am sure that Elga does reject Campbell’s and Williamson’s theories, so just raising them against him without argument would be question-begging. But this does mean that the target audience for INDIFFERENCE is smaller than for some philosophical claims, since adherents of Campbell’s or Williamson’s views will be antecedently disposed to think INDIFFERENCE is useless or false.\nIt’s Evidently Intransitive\nDakota is sitting in a bright green room. She is trying to reconstruct how she got there when Dr Evil informs her just what happened. An epistemology student, not coincidentally called Dakota, was snatched out of her study and duplicated 999 times over. The duplicates were then numbered (though we’ve lost which number was given to the original) each put in a coloured cell. The thousand coloured cells rotated slowly through the colour sphere, starting with cell 0 (the new home of Dakota number 0) being green, going blueish until cell 250 (for Dakota number 250) is just blue, then reddish until cell 500 is just red, swinging through the yellows with pure yellow reached at 750, and then back to the greens, with 999 being practically identical to 1000. For any n, cells number n and n+1 are indistinguishable. That means that Dakota number n is similar, in Elga’s sense, to Dakota number n+1, for their (apparent) experiences before being in the rooms are identical, and their experiences in the rooms are indistinguishable. Hence our Dakota, sitting in the bright green room, should assign equal credence to being Dakota number n and Dakota number n+1 for any n. But this is absurd. Since she can see that her walls are green, she should assign high credence to being Dakota number 0, and credence 0 to being Dakota number 500.\nThe problem here is that Elga wants to define an equivalence relation on predicaments, the relation deserving the same credence as, out of an intransitive relation, being indistinguishable from. There are two possible responses, each of them perfectly defensible.\nFirst, Elga could deny the premise that the adjacent cells are indistinguishable. Although there is some prima facie plausibility to the claim that some different colours are indistinguishable, Delia Graff Fara (2001) has argued that this is false. It would mean committing to yet another controversial philosophical position, but if Elga endorsed Graff’s claims, he could easily deal with Dakota.\nSecondly, he could tinker with the definition of similarity. Instead of saying that possibilia represent similar predicaments iff they are indistinguishable worldmates, he could say that they represent similar predicaments iff they are worldmates that are indistinguishable from the same predicaments. (This kind of strategy for generating an equivalence relation from an intransitive relation is borrowed from Goodman (1951).) Even if adjacent cells are indistinguishable from each other, they will not be indistinguishable from the same cells. This delivers the plausible result that the duplicate Dakotas stuck in the cells do not instantiate similar predicaments. Some might object that this move is ad hoc, but once we realise the need to make similar an equivalence relation, it seems clear enough that this is the most natural way to do that.\nMorgan and Morgan and Morgan and Morgan\nI think I outdid myself this time, said Dr Evil. I was just going along duplicating you, or at least someone like you, and the duplication process was taking less and less time. So I thought, I wonder what is the lower bound here? How quick can we make the duplication process? So I tried a few things to cut down the time it took, and I got a little better with practice, and, well, it turns out that the time taken can be made arbitrarily small. Before I knew it, there were infinitely many of you. Oops.\nMorgan was a little shocked. She could cope with having a duplicate or two around, but having infinitely many duplicates was a little hard to take. On the other hand, and this was hard to think about, perhaps she should be grateful. Maybe she was one of the later ones created, and she wouldn’t have existed if not for Evil’s irrational exuberance. She started to ponder how likely that was, but she was worried that it required knowing more about Evil than any mortal could possibly know.\nWell, continued Dr Evil, I did one thing right. As each duplicate was created I gave it a serial number, 0 for the original Morgan, 1 for the first duplicate and so on, so the bookkeeping will be easier. Don’t go looking for it, it’s written on your left leg in ectoplasmic ink, and you won’t be able to see it.\nNow that makes things easier, thought Morgan. By INDIFFERENCE the probability that my serial number is x is 1/n, where n is the number of duplicates created. So dividing 1 by infinity, that’s zero. So the probability that my serial number is less than x is the probability that it’s zero plus the probability that it’s one plus … plus the probability that it’s x, that’s still zero. So if he had stopped after x for any x, I would not exist with probability one. I’m liking Evil more and more, though something bothers me about that calculation.\nMorgan was right to worry. She’s just talked herself, with Elga’s help, into a violation of the principle of countable additivity. The additivity axiom in standard probability theory says that for any two disjoint propositions, the probability of their disjunction is the sum of their probabilities. The countable additivity axiom says that for any countable set of disjoint propositions, the probability that at least one of them is true is the sum of each of their probabilities. (It follows from the axioms of probability theory that this sum is always defined.) Here we have to alter these axioms slightly so they apply to properties rather than propositions, but still the principle of countable additivity seems plausible. But Morgan has to violate it. The probability she assigns to having some serial number or other is not zero, in fact it is one as long as she takes Evil at his word. But for each x, the probability that her serial number is x is zero. In symbols, we have\nPr(\\({\\exists}\\)x (Serial number = x)) = 1\n\\({\\Sigma}\\)Pr(Serial number = x) = 0\nBut countable additivity says that these values should be equal.\nOrthodoxy endorses countable additivity, but there are notable dissenters that are particularly relevant here. Bruno (deFinetti1974?) argued that countable additivity should be rejected because it rules out the possibility of an even distribution across the natural numbers. DeFinetti thought, as Morgan does, that we could rationally be in a position where we know of a particular random variable only that its value is a non-negative integer, and for every x, we assign equal probability to each hypothesis that its value is x. Since that is inconsistent with countable additivity, all the worse for countable additivity. This is a decent argument, though as de Finetti himself noted, it has some counterintuitive consequences.\nI decided, Dr Evil continued, to do something fairly spectacular with all these people. By some small tinkering with your physiology I found a way to make you immortal. Unfortunately, a quick scan of your psychology revealed that you weren’t capable of handling eternity. So every fifty years I will wipe all your memories and return you to the state you were in when duplicated. I will write, or perhaps I did write, on your right leg the number of times that your memories have been thus wiped. Don’t look, it’s also in ectoplasmic ink. Just to make things fun, I made enough duplicates of myself so that every fifty years I can tell you what happened. Each fifty-year segment of each physical duplicate will be an epistemic duplicate of every other such segment. How cool is that?3\nMorgan was not particularly convinced that it was cool, but an odd thought crossed her mind once or twice. She had one number L written on her left leg, and another number R written on her right leg. She had no idea what those numbers were, but she thought she might be in a position to figure out the odds that L \\({\\geq}\\) R. So she started reasoning as follows, making repeated appeals to C-INDIFFERENCE. (She must also appeal to P-INDIFFERENCE at every stage if there are other propositions about which she is uncertain. Assume that appeal made.)\nLet’s say the number on my left leg is 57. Then L \\({\\geq}\\) R iff R < 58. But since there are 58 ways for R < 58 to be true, and infinitely many ways for R < 58 to be false, and by C-INDIFFERENCE each of these ways deserve the same credence conditional on L = 57, we get Pr(L \\({\\geq}\\) R  L = 57) = 0. But 57 was arbitrary in this little argument, so I can conclude \\({\\forall}\\)l: Pr(L \\({\\geq}\\) R  L = l) = 0. This seems to imply that Pr(L \\({\\geq}\\) R) = 0, especially since I know L takes some value or other, but let’s not be too hasty.\nLet’s say the number on my right leg is 68. Then L \\({\\geq}\\) R iff L \\({\\geq}\\) 68. And since there are 68 ways for L \\({\\geq}\\) 68 to be false, and infinitely many ways for it to be true, and by C-INDIFFERENCE each of these ways deserve the same credence conditional on R = 68, we get Pr(L \\({\\geq}\\) R  R = 68) = 1. But 68 was arbitrary in this little argument, so I can conclude \\({\\forall}\\)r: Pr(L \\({\\geq}\\) R  R = r) = 1. This seems to imply that Pr(L \\({\\geq}\\) R) = 1, especially since I know R takes some value or other, but now I’m just confused.\nMorgan is right to be confused. She has not quite been led into inconsistency, because as she notes the last step, from \\({\\forall}\\)l: Pr(L \\({\\geq}\\) R  L = l) = 0 to Pr(L \\({\\geq}\\) R) = 0 is not forced. In fact, the claim that this is always a valid inferential step is equivalent to the principle of countable additivity, which we have already seen a proponent of INDIFFERENCE in all its variations must reject. But it would be a mistake to conclude from this that we just have a standoff. What Morgan’s case reveals is that accepting the indifference principles that Elga offers requires giving up on an intuitively plausible principle of inference. That principle says that if the probability of p conditional on any member of a partition is x, then the probability of p is x. If we think that principle of inference is prima facie more plausible than Elga’s principle of indifference, as I think we should, that is pretty good prima facie evidence that Elga’s principle is wrong.\nThe next three sections will be devoted to determining whether we can convert this persuasive argument into a knockdown argument (we cannot) and whether Elga’s arguments in favour of INDIFFERENCE do enough to overcome this prima facie argument that INDIFFERENCE is flawed (they do not). A concluding section notes how to redo this argument so it appeals only to potential rather than actual infinities.\nIntermission\nCHARYBDIS: I know how to make that argument stronger. Just get Evil to offer Morgan a bet on whether L \\({\\geq}\\) R. Ask how much she’ll pay for a bet that pays €1 if L \\({\\geq}\\) R and nothing otherwise. If she pays anything for it, tell her the value of L, whatever it is, and ask her if she’d like to sell that bet back for half what she paid for it. Since she now assigns probability zero to L \\({\\geq}\\) R she’ll happily do that, and then she’ll have lost money. If she won’t pay anything for the bet to start with, offer her the reverse bet. She should pay €1 for that, and now apply the same tactics except tell her the value of R rather than L. Either way the stupid person will lose money.\nSCYLLA:Very practical Charybdis, but we’re not sure it gets to the heart of the matter. Not sure. Well, let us say why rather than leaving it like that. For one thing, Morgan might not like playing dice with Evil, even if Evil is the source of her life. So she might have a maximum price of 0 for either bet.\nCHARYBDIS:But then surely she’ll be turning down a sure win. I mean between the bets she has a sure gain of at least €1.\nSCYLLA:And if she is offered both bets at once we’re sure she would take that gain, but as we heard your story she wasn’t.4\nCHARYBDIS:So does this mean her degree of belief in both R \\({\\geq}\\) L and L \\({\\geq}\\) R is 0?\nSCYLLA:It might mean that, and of course some smart people have argued that that is coherent, much to the chagrin of your Bayesian friends we’re sure.5 But more likely it means that she just isn’t following the patterns of practical reasoning that you endorse.6 Also, we’re not so sure about the overall structure of the argument. We think your reasoning is as follows. Morgan ends up doing something silly, giving up money. (Well, we’re not sure that’s always silly, but let’s say it is here.) So something went wrong. So she has silly beliefs. That last step goes by fairly fast we think. From her making some mistake or other, we can only conclude that, well, she made some mistake or other, not that she made some particular mistake in the composition of her credences.7\nCHARYBDIS:What other mistake might she have made?\nSCYLLA:There are many hidden premises in your chains of reasoning to conclusions about how Morgan should behave. For instance, she only values a €1 bet on L \\({\\geq}\\) R at Pr(L \\({\\geq}\\) R) if she knows she can’t buy that bet more cheaply elsewhere, or sell it for a larger price elsewhere. Even if those assumptions are true, Morgan may unreasonably believe they are false, and that might be her mistake.8 But even that isn’t our main concern. Our main concern is that you understate how bad Morgan’s position is.\nCHARYBDIS:What’s worse for a mortal than assured loss of money?\nSCYLLA:Morgan is not a mortal any more, you know. And immortals we’re afraid are almost bound to lose money to clever enough tricksters. Indeed, a so-called Dutch Book can be made against any agent that (a) has an unbounded utility function and (b) is not overly opinionated, so there are still infinitely many ways the world could be consistent with their knowledge.9 That includes us, and you dear Charybdis. And yet we are not as irrational as that Morgan. I don’t think analogising her position to ours really strengthens the case that she is irrational.\nCHARYBDIS:Next you might say that making money off her, this undeserving immortal, is immoral.\nSCYLLA:Perish the thoughts.\nRisky Business?\nThere are two kinds of reasons to dislike indifference principles, both of them developed most extensively in Keynes (1921). The first, which we have been exploring a bit so far, is that such principles tend to lead to incoherence. The second is that such principles promote confusion between risk and uncertainty.\nOften we do not know exactly what the world is like. But not all kinds of ignorance are alike. Sometimes, our ignorance is like that of a roulette player facing a fair wheel about to be spun. She knows not what will happen, but she can provide good reasons for assigning equal credence to each of the 37 possible outcomes of the spin. Loosely following Frank Knight (1921), we will say that a proposition like The ball lands in slot number 18 is risky. The distinguishing feature of such propositions is that we do not know whether they are true or false, but we have good reason to assign a particular probability to their truth. Other propositions, like say the proposition that there will be a nuclear attack on an American city this century, are quite unlike this. We do not know whether they are true, and we aren’t really in a position to assign anything like a precise numerical probability to their truth. Again following Knight, we will say such propositions are uncertain. In (1937) Keynes described a number of other examples that nicely capture the distinction being drawn here.\n\nBy ‘uncertain’ knowledge, let me explain, I do not mean merely to distinguish what is known for certain from what is only probable. The game of roulette is not subject, in this sense, to uncertainty; nor is the prospect of a Victory bond being drawn. Or, again, the expectation of life is only slightly uncertain. Even the weather is only moderately uncertain. The sense in which I am using the term is that in which the prospect of a European war is uncertain, or the price of copper and the rate of interest twenty years hence, or the obsolescence of a new invention, or the position of private wealth owners in the social system in 1970. About these matters there is no scientific basis on which to form any calculable probability whatever. We simply do not know. Nevertheless, the necessity for action and decision compels us as practical men to do our best to overlook this awkward fact and to behave exactly as we should if we had behind us a good Benthamite calculation of a series of prospective advantages and disadvantages, each multiplied by its appropriate probability, waiting to be summed. (Keynes 1937, 114–15)\n\nNote that the distinction between risky and uncertain propositions is not the distinction between propositions whose objective chance we know and those that we don’t. This identification would fail twice over. First, as Keynes notes, whether a proposition is risky or uncertain is a matter of degree, but whether we know something is, I presume, not a matter of degree.10 Second, there are risky propositions with an unknown chance. Assume that our roulette player turns away from the table at a crucial moment, and misses the ball landing in a particular slot. Now the chance that it lands in slot 18 is 1 (if it did so land) or 0 (otherwise), and she does not know which. Yet typically, the proposition The ball lands in slot 18 is still risky for her, for she has no reason to change her attitude towards the proposition that it did land in slot 18.\nMy primary theoretical objection to INDIFFERENCE is that the propositions it purports to provide guidance on are really uncertain, but it treats them as risky. Once we acknowledge the risk/uncertainty distinction, it is natural to think that our default state is uncertainty. Getting to a position where we can legitimately treat a proposition as risky is a cognitive achievement. Traditional indifference principles fail because they trivialise this achievement. An extreme version of such a principle says we can justify assigning a particular numerical probability, 0.5, to propositions merely on the basis of ignorance of any evidence telling for or against it. This might not be an issue to those who think that “probability is a measure of your ignorance.” (Poole, Mackworth, and Goebel 1998, 348) But to those of us who think probability is the very guide to life, such a position is unacceptable. It seems to violate the platitude ‘garbage in, garbage out’ since it takes ignorance as input, and produces a guide to life as output. INDIFFERENCE is more subtle than these traditional indifference principles, but this theoretical objection remains. The evidence that O’Leary or Morgan or Leslie has does not warrant treating propositions about their location or identity as risky rather than uncertain. When they must make decisions that turn on their identity or location, this ignorance provides little or no guidance, not a well-sharpened guide to action.\nIn this section I argue that treating these propositions as uncertain lets us avoid the traps that Morgan falls into. In the next section I argue that the case Elga takes to support INDIFFERENCE says nothing to the theorist who thinks that the INDIFFERENCE principle conflates risk and uncertainty. In fact, some features of that case seem to support the claim that the propositions covered by INDIFFERENCE are uncertain, not risky.\nIn (1921), Keynes put forward a theory of probability that was designed to respect the distinction between risky propositions and uncertain propositions. He allowed that some propositions, the risky ones and the ones known to be true or false, had a numerical probability (relative to a body of evidence) while other propositions have non-numerical probabilities. Sometimes numerical and non-numerical probabilities can be compared, sometimes they cannot. Arithmetic operations are all assumed to be defined over both numerical and non-numerical probabilities. As Ramsey (1926) pointed out, in Keynes’s system it is hard to know what \\({\\alpha}\\) + \\({\\beta}\\) is supposed to mean when \\({\\alpha}\\) and \\({\\beta}\\) are non-numerical probabilities, and it is not even clear that ‘+’ still means addition in the sense we are used to.\nOne popular modern view of probability can help Keynes out here. Following Ramsey, many people came to the view that the credal states of a rational agent could be represented by a probability function, that function being intuitively the function from propositions into the agent’s degree of belief in that proposition. In the last thirty years, there has been a lot of research on the theory that says we should represent rational credal states not by a single probability function, but by a set of such probability functions. Within philosophy, the most important works on this theory are by Henry Kyburg (1974), Isaac Levi (1974, 1980), Richard Jeffrey (1983) and Bas Bas Fraassenvan Fraassen (1990). What is important here about this theory is that many distinctive features of Keynes’s theory are reflected in it.\nLet S be the set of probability functions representing the credal states of a rational agent. Then for each proposition p we can define a set S(p) = {Pr(p): Pr \\({\\in}\\) S}. That is, S(p) is the set of values that Pr(p) takes for Pr being a probability function in S. We will assume here that S(p) is an interval. (See the earlier works cited for the arguments in favour of this assumption.) When p is risky, S(p) will be a singleton, the singleton of the number we have compelling reason to say is the probability of p. When p is a little uncertain, S(p) will be a fairly narrow interval. When it is very uncertain, S(p) will be a wide interval, perhaps as wide as [0, 1]. We say that p is more probable than q iff for all Pr in S, Pr(p) > Pr(q), and as probable as q iff for Pr in S, Pr(p) = Pr(q). This leaves open the possibility that Keynes explicitly left open, that for some uncertain proposition p and some risky proposition q, it might be the case that they are not equally probable, but neither is one more probable than the other. Finally, we assume that when an agent whose credal states are represented by S updates by learning evidence e, her new credal states are updated by conditionalising each of the probability functions in S on e. So we can sensibly talk about S(p  e), the set {Pr(p  e): Pr \\({\\in}\\) S}, and this represents her credal states on learning e.\n(It is an interesting historical question just how much the theory sketched here agrees with the philosophical motivations of Keynes’s theory. One may think that the agreement is very close. If we take Keynes’s entire book to be a contextual definition of his non-numerical probabilities, a reading encouraged by Lewis (1970), then we should conclude he was talking about sets like this, with numerical probabilities being singleton sets.)\nThis gives us the resources to provide good advice to Morgan. Pick a monotone increasing function f from integers to [0, 1] such that as n \\({\\rightarrow}\\) \\({\\infty}\\), f(n) \\({\\rightarrow}\\) 1. It won’t really matter which function you pick, though different choices of f might make the following story more plausible. Say that S(L \\({\\geq}\\) R  L = l) = [0, f(l)]. The rough idea is that if L is small, then it is quite improbable that L  \\({\\geq}\\) R, although this is a little uncertain. As l gets larger, L \\({\\geq}\\) R gets more and more uncertain. The overall effect is that we simply do not know what S(L \\({\\geq}\\) R) will look like after conditionalising on the value of L, so we cannot apply the kind of reasoning Morgan uses to now come to some conclusions about the probability of L \\({\\geq}\\) R.\nIf we view the situations described by INDIFFERENCE as involving uncertainty rather than risk, this is exactly what we should expect. And note that in so doing, we need not undermine the symmetry intuition that lies behind INDIFFERENCE. Assume that F and G are similar predicaments, and I know that I am either F or G. INDIFFERENCE says I should assign equal probability to each, so S(I am F) = S(I am G) = {0.5}. But once we’ve seen how attractive non-numerical probabilities can be, we should conclude that all symmetry gives us is that S(I am F) = S(I am G), which can be satisfied if each is [0.4, 0.6], or [0.2, 0.8] or even [0, 1]. (I think that for O’Leary, for example, S(It is 1 o’clock) should be a set somehow like this.) Since I would not be assigning equal credence to I am F and I am G if I satisfied symmetry using non-numerical probabilities, so I will violate INDIFFERENCE without treating the propositions asymmetrically. Such a symmetric violation of INDIFFERENCE has much to recommend it. It avoids the incoherence that INDIFFERENCE leads to in Morgan’s case. And it avoids saying that ignorance about our identity can be a sharp guide to life.11\nA referee noted that the intuitive characterisation here doesn’t quite capture the idea that we should treat similar predicaments alike. The requirement that if F and G are similar then S(I am F) = S(I am G) does not imply that there will be a symmetric treatment of F and G within S if there are more than two similar predicaments. What we need is the following condition. Let T be any set of similar predicaments, g any isomorphism from T onto itself, and Pr any probability function in S. Then there exists a Pr\\(^\\prime\\) in S such that for all A in T, Pr(A) = Pr\\(^\\prime\\)(g(A)). When there are only two similar predicaments A and B this is equivalent to the requirement that S(A) = S(B), but in the general case it is a much stricter requirement. Still, it is a much weaker constraint than INDIFFERENCE, and not vulnerable to the criticisms of INDIFFERENCE set out here.\nBoyfriend in a Coma\nElga argues for INDIFFERENCE by arguing it holds in a special case, and then arguing that the special case is effectively arbitrary, so if it holds there it holds everywhere. The second step is correct, so we must look seriously at the first step. Elga’s conclusions about the special case, DUPLICATION, eventually rest on treating an uncertain proposition as risky.\nDUPLICATION\nAfter Al goes to sleep researchers create a duplicate of him in a duplicate environment. The next morning, Al and the duplicate awaken in subjectively indistinguishable states.\n\nAssume (in all these cases) that before Al goes to sleep he knows the relevant facts of the case. In that case INDIFFERENCE12 dictates that when Al wakes up his credence in I am Al should be 0.5. Elga argues this dictate is appropriate by considering a pair of related cases.\nTOSS-and-DUPLICATION\nAfter Al goes to sleep, researchers toss a coin that has a 10% chance of landing heads. Then (regardless of the toss outcome) they duplicate Al. The next morning, Al and the duplicate awaken in subjectively indistinguishable states.\n\nElga notes, correctly, that the same epistemic norms apply to Al on waking in DUPLICATION as in TOSS-and-DUPLICATION. So if we can show that when Al wakes in TOSS-and-DUPLICATION his credence in I am Al should be 0.5, that too will suffice to prove INDIFFERENCE correct in this case. The argument for that claim has three premises. (I’ve slightly relabeled the premises for ease of expression.)\nPr(H) = 0.1\nPr(H (H \\({\\wedge}\\) A) \\({\\vee}\\) (T \\({\\wedge}\\) A)) = 0.1\nPr(H (H \\({\\wedge}\\) A) \\({\\vee}\\) (T \\({\\wedge}\\) D)) = 0.1\nHere Pr is the function from de se propositions to Al’s degree of belief in them, H = The coin lands heads, T = The coin lands tails, A = I am Al and D = I am Al’s duplicate. From (1), (2) and (3) and the assumption that Pr is a probability function it follows that Pr(A) = 0.5, as required. This inference goes through even in the Keynesian theory that distinguishes risk from uncertainty. Premise (1) is uncontroversial, but both (2) and (3) look dubious. Since the argument for (3) would, if successful, support (2), I’ll focus, as Elga does, on (3). The argument for it turns on another case.\nCOMA\nAs in TOSS-and-DUPLICATION, the experimenters toss a coin and duplicate Al. But the following morning, the experimenters ensure that only one person wakes up: If the coin lands heads, they allow Al to wake up (and put the duplicate into a coma); if the coin lands tails, they allow the duplicate to wake up (and put Al into a coma).\n\n(It’s important that no one comes out of this coma, so assume that the victim gets strangled.)\nElga then argues for the following two claims. If in COMA Al gets lucky and pulls through, his credence in H should be 0.1, as it was before he entered the dream world. Al’s credence in H in COMA should be the same as his conditional credence in H should be the same as his conditional credence in H given (H \\({\\wedge}\\) A) \\({\\vee}\\) (T \\({\\wedge}\\) D) in TOSS-and-DUPLICATION. The second premise looks right, so the interest is on what happens in COMA. Elga argues as follows (notation slightly changed):\n\nBefore Al was put to sleep, he was sure that the chance of the coin landing heads was 10%, and his credence in H should have accorded with this chance: it too should have been 10%. When he wakes up, his epistemic situation with respect to the coin is just the same as it was before he went to sleep. He has neither gained nor lost information relevant to the toss outcome. So his degree of belief in H should continue to accord with the chance of H at the time of the toss. In other words, his degree of belief in H should continue to be 10%.\n\nAnd this, I think, is entirely mistaken. Al has no evidence that his evidence is relevant to H, but absence of evidence is not evidence of absence. Four considerations support this conclusion.\nFirst, Al gets some evidence of some kind or other on waking. Certain colours are seen, certain pains and sensations are sensed, certain fleeting thoughts fleet across his mind. Before he sleeps Al doesn’t knows what these shall be. Maybe he thinks of the money supply, maybe of his girlfriend, maybe of his heroine, maybe of kidneys. He doesn’t know that the occurrence of these thoughts is probabilistically independent of his being Al rather than Dup, so he does not know they are probabilistically independent of H. So perhaps he need not retain the credence in H he has before he was drugged. Even if this evidence looks like junk, we can’t rule out that it has some force.\nSecondly, the kind of internalism about evidence needed to support Elga’s position is remarkably strong. (This is where the concerns raised in become most pressing.) Elga notes that he sets himself against both an extreme externalist position that says that Al’s memories and/or perceptions entail that he is Al and against an “intermediate view, according to which Al’s beliefs about the setup only partially undermine his memories of being Al. According to such a view, when Al wakes up his credence in H ought to be slightly higher than 10%.” But matters are worse than that. Elga must also reject an even weaker view that says that Al might not know whether externalism about evidence is true, so he does not know whether his credence in H should change. My view is more sympathetic to that position. When Al wakes, he does not know which direction is credences should move, or indeed whether there is such a direction, so his credence in H should be a spread of values including 0.1.\nThirdly, Al’s position looks like cases where new evidence makes risky propositions uncertain. Mack’s betting strategy for the Gold Cup, a horse race with six entrants, is fairly simple. He rolls a fair die, and bets on whatever number comes up. Jane knows this is Mack’s strategy, but does not how the die landed this time. Nor does she know anything about horses, so the propositions Horse n wins the Gold Cup are uncertain for Jane for each n. Call these propositions wn, and the proposition that Mack’s die landed n dn. Right now, d2 is risky, but h2 is uncertain. Jane hears a party starting next door. Mack’s won. Jane has learned, inter alia, d2 \\(\\leftrightarrow\\) h2. Now it seems that d2, Mack’s die landed 2, inherits the uncertainty of h2, Horse number 2 won the Gold Cup. The formal theory of uncertainty I sketched allows for this possibility. It is possible that there be p, e such that S(p) is a singleton, while S(p  e) is a wide interval, in theory as wide as [0, 1]. This is what happens in Jane’s case, and it looks like it happens in Al’s case too. H used to be risky, but when he wakes he comes to learn H \\({\\leftrightarrow}\\) A, just as Jane learned d2 \\(\\leftrightarrow\\) h2. In each case, the left-hand clause of the biconditional inherits the uncertainty of the right-hand clause.\nFinally, H being uncertain for Al when he wakes in COMA is consistent with the intuition that Al has no reason to change his credences in H in one direction or another when he says goodbye to his duplicate. (Or, for all he knows, to his source.) Perhaps externalist theories of evidence provide some reason to raise these credences, as suggested above, but I do not rely on such theories. What I deny is that the absence of a reason to move one way or the other is a reason to stay put. Al’s credence in H might change in a way that reflects the fact H is now uncertain, just like A is in COMA, just like A is in TOSS-and-DUPLICATION, and, importantly, just like A is in DUPLICATION. I think the rest of Elga’s argument is right. DUPLICATION is a perfectly general case. In any such case, Al should be uncertain, in Keynes’s sense, whether he is the original or the duplicate.\nShooting Dice can be Dangerous\nThe good news, said Dr Evil, is that you are still mortal. Odysseus was not as upset as Dr Evil had expected. The bad news is that I’m thinking of torturing you. I’m going to roll this fair die, and if it lands 6 you will be tortured. If it does not, you will be (tentatively) released, and I’ll create two duplicates of you as you were when you entered this room, repeat this story to both them. Depending on another roll of this fair die, I will either torture them both, or create two duplicates of each of them, and repeat the process until I get to torture someone.13\nOdysseus thought through this for a bit. So I might be a duplicate you’ve just created, he said. I might not be Odysseus.\nYou might not be, said Dr Evil, although so as to avoid confusion if you’re not him I’ll use his name for you.\nWhat happens if the die never lands 6, asked Odysseus. I’ve seen some odd runs of chance in my time.\nI wouldn’t be so sure of that, said Dr Evil. Anyway, that’s why I said I would tentatively release you. I’ll make the die rolls and subsequent duplication quicker and quicker so we’ll get through the infinite number of rolls in a finite amount of time. If we get that far I’ll just bring everyone back and torture you all. Aren’t I fair?\nFairness wasn’t on Odysseus’s mind though. He was trying to figure out how likely it was that he would be tortured. He was also a little concerned about how likely it was that he was the original Odysseus, and if he was not whether Penelope too had been duplicated. As it turns out, his torturous computations would assist with the second question, though not the third. Two thoughts crossed his mind.\nI will be tortured if that die lands 6, which has a chance of 1 in 6, or if it never lands 6 again, which has a chance of 0. So the chance of my being tortured is 1 in 6. I have no inadmissible evidence, so the probability I should assign to torture is 1 in 6.\nLet’s think about how many Odysseuses there are in the history of the world. Either there is 1, in which case I’m him, and I shall be tortured. Or there are 3, in which case two of them shall be tortured, so the probability that I shall be tortured is 2 in 3. Or there are 7, in which case four of them shall be tortured, so the probability that I shall be tortured is 4 in 7. And so on, it seems like the probability that I shall be tortured approaches 1 in 2 from above as the number of Odysseuses approaches infinity. Except, of course, in the case where it reaches infinity, when it is again certain that I shall be tortured. So it looks like the probability that I will be tortured is above 1 in 2. But I just concluded it is 1 in 6. Where did I go wrong?\nIn his second thought, Odysseus appeals frequently to INDIFFERENCE. He then appeals to something like the conglomerability principle that tripped up Morgan. The principle Odysseus uses is a little stronger than the principle Morgan used. It says that if there is a partition and conditional on each member of the partition, the probability of p is greater than x, then the probability of p is greater than x. As we noted, this principle cannot be accepted in its full generality by one who rejects countable additivity. And one who accepts INDIFFERENCE must reject countable additivity. So where Odysseus goes wrong is in appealing to this inference principle after previously adopting an indifference principle inconsistent with it.\nThis does not mean the case has no interest. Morgan’s case showed that when we have an actual infinity of duplicates, INDIFFERENCE can lead to counterintuitive results, and that the best way out might be to say that Morgan faced a situation of uncertainty, not one of risk. But it might have been thought that something special about Morgan’s case, that she has infinitely many duplicates, might be responsible for the problems here. So it may be hoped that INDIFFERENCE can at least be accepted in more everyday cases. Odysseus shows that hope is in vain. All we need is the merest possibility of there being infinitely many duplicates, here a possibility with zero probability, to create a failure of conglomerability. This suggests that the problems with INDIFFERENCE run relatively deep.\nThe details of how Odysseus’s case plays out given INDIFFERENCE are also interesting, especially to those readers not convinced by my refutation of INDIFFERENCE. For their benefit, I will close with a few observations about how the case plays out.\nAs in Morgan’s case, we can produce two different partitions of the possibility space that seem to support different conclusions about Odysseus’s prospects. Assume for convenience that Dr Evil makes a serial number for each Odysseus he makes, the Homeric hero being number 1, the first two duplicates being 2 and 3, and so on. Let N stand for the number of our hero, M for the number of Odysseuses that are made, and T for the property of being tortured. Then given INDIFFERENCE it behoves Odysseus to have his credences governed by the following Pr function.\n\\({\\forall}\\)k Pr(T  M = 2k - 1) = 2k-1/(2k - 1)\nPr(T  M = \\({\\infty}\\)) = 1\n\n\\({\\forall}\\)n Pr(T  N = n) = 1/6\nBetween 4a and 4b we cover all possible values for M, and in every case Pr(T) is greater than 1/2. More interesting are Odysseus’s calculations about whether he is the Homeric hero, i.e. about whether N = 1. Consider first a special case of this, what the value of Pr(N = 1 N < 8) is. At first glance, it might seem that this should be 1/7, because there are seven possible values for N less than 8. But this is too quick. There are really eleven possibilities to be considered.\nF1: N = 1 and M = 1\nF2: N = 1 and M = 3\nF5: N = 1 and M > 3\n \nF3: N = 2 and M = 3\nF6: N = 2 and M > 3\n \nF4: N = 3 and M = 3\nF7: N = 3 and M > 3\n \n \nF8: N = 4 and M > 3\n \n \nF9: N = 5 and M > 3\n \n \nF10: N = 6 and M > 3\n \n \nF11: N = 7 and M > 3\nBy INDIFFERENCE, each of the properties in each column should be given equal probability. So we have\n\\[\\begin{aligned}\nx &= Pr(F_1 | N < 8)  \\\\\ny &= Pr(F_2 | N < 8) = Pr(F_3 | N < 8) = Pr(F_4 | N < 8)  \\\\\nz &= Pr(F_5 | N < 8) = \\dots = Pr(F_11 | N < 8)  \\end{aligned}\\]\nWe just have to solve for x, y and z. By the Principal Principle we get\nPr(M = 1  N = 1) = 1/6\\({\\therefore}\\) x = (x + y + z) / 6\nPr(M = 3 N = 1 and M \\({\\geq}\\) 3) = 1/6\\({\\therefore}\\) y = (y + z) / 6\nAnd since these 11 possibilities are all the possibilities for N < 8, we have\nx + 3y + 7z = 1\nSolving for all these, we get x = 3/98, y = 5/196 and z = 25/196, so Pr(N = 1  N < 8) = x + y + z = 9/49. More generally, we have the following (the proof of this is omitted): \\[Pr(N = 1 | N < 2^{k+1}) = \\frac{6^k}{\\sum_{i=0}^{k}6^i10^{k-i}}\\]\nSince the RHS \\({\\rightarrow}\\) 0 as k \\({\\rightarrow}\\) \\({\\infty}\\), Pr(N = 1) = 0. Our Odysseus is probably not the real hero. Similar reasoning shows that Pr(N = n) = 0 for all n. So we have another violation of countable additivity. But we do not have, as in Morgan’s case, a constant distribution across the natural numbers. In a sense, this distribution is still weighted towards the bottom, since for any n > 1, Pr(N = 1  N = 1 \\({\\vee}\\) N = n) > 1/2. Of course, I don’t think INDIFFERENCE is true, so these facts about what Odysseus’s credence function will look like under INDIFFERENCE are of purely mathematical interest to me. But it might be possible that someone more enamoured of INDIFFERENCE can use this ‘unbalanced’ distribution to explain some of the distinctive features of the odd position that Odysseus is in.\n\n\n\nBartha, Paul, and Christopher Hitchcock. 1999. “The Shooting-Room Paradox and Conditionalizing on Measurably Challenged Sets.” Synthese 118 (3): 403–37. https://doi.org/10.1023/a:1005100407551.\n\n\nCampbell, John. 2002. Reference and Consciousness. Oxford: Oxford University Press.\n\n\nChambers, Robert, and John Quiggin. 2000. Uncertainty, Production, Choice, and Agency: The State-Contingent Approach. Cambridge: Cambridge University Press.\n\n\nDreier, James. 2001. “Boundless Good.”\n\n\nElga, Adam. 2000. “Self-Locating Belief and the Sleeping Beauty Problem.” Analysis 60 (2): 143–47. https://doi.org/10.1093/analys/60.2.143.\n\n\n———. 2004. “Defeating Dr. Evil with Self-Locating Belief.” Philosophy and Phenomenological Research 69 (2): 383–96. https://doi.org/10.1111/j.1933-1592.2004.tb00400.x.\n\n\nFara, Delia Graff. 2001. “Phenomenal Continua and the Sorites.” Mind 110 (440): 905–36. https://doi.org/10.1093/mind/110.440.905.\n\n\nFraassen, Bas Fraassenvan. 1990. “Figures in a Probability Landscape.” In Truth or Consequences, edited by J. M. Dunn and A. Gupta, 345–56. Amsterdam: Kluwer.\n\n\nFraassen, Bas van. 1995. “Belief and the Problem of Ulysses and the Sirens.” Philosophical Studies 77 (1): 7–37. https://doi.org/10.1007/bf00996309.\n\n\nGoodman, Nelson. 1951. The Structure of Appearance. Cambridge, MA: Harvard University Press.\n\n\nHájek, Alan. 2005. “Scotching Dutch Books.” Philosophical Perspectives 19: 139–51. https://doi.org/10.1111/j.1520-8583.2005.00057.x.\n\n\nHetherington, Stephen. 2001. Good Knowledge, Bad Knowledge: On Two Dogmas of Epistemology. Oxford: Oxford University Press.\n\n\nJeffrey, Richard. 1983. “Bayesianism with a Human Face.” In Testing Scientific Theories, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.\n\n\nKeynes, John Maynard. 1921. Treatise on Probability. London: Macmillan.\n\n\n———. 1937. “The General Theory of Employment.” Quarterly Journal of Economics 51 (2): 209–23. https://doi.org/10.2307/1882087.\n\n\nKnight, Frank. 1921. Risk, Uncertainty and Profit. Chicago: University of Chicago Press.\n\n\nKyburg, Henry. 1974. The Logical Foundations of Statistical Inference. Dordrecht: Reidel.\n\n\nLevi, Isaac. 1974. “On Indeterminate Probabilities.” Journal of Philosophy 71 (13): 391–418. https://doi.org/10.2307/2025161.\n\n\n———. 1980. The Enterprise of Knowledge. Cambridge, MA.: MIT Press.\n\n\nLewis, David. 1970. “How to Define Theoretical Terms.” Journal of Philosophy 67 (13): 427–46. https://doi.org/10.2307/2023861.\n\n\n———. 1979. “Attitudes de Dicto and de Se.” Philosophical Review 88 (4): 513–43. https://doi.org/10.2307/2184646.\n\n\nMcGee, Vann. 1999. “An Airtight Dutch Book.” Analysis 59 (4): 257–65. https://doi.org/10.1093/analys/59.4.257.\n\n\nMilne, Peter. 1991. “Scotching the Dutch Book Argument.” Erkenntnis 32 (1): 105–26. https://doi.org/10.1007/bf00209558.\n\n\nPoole, David, Alan Mackworth, and Randy Goebel. 1998. Computational Intelligence: A Logical Approach. Oxford: Oxford University Press.\n\n\nRamsey, Frank. 1926. “Truth and Probability.” In Philosophical Papers, edited by D. H. Mellor, 52–94. Cambridge: Cambridge University Press.\n\n\nSchick, Frederick. 1986. “Dutch Bookies and Money Pumps.” Journal of Philosophy 83 (2): 112–19. https://doi.org/10.2307/2026054.\n\n\nShafer, Glenn. 1976. A Mathematical Theory of Evidence. Princeton: Princeton University Press.\n\n\nWilliamson, Timothy. 1998. “Conditionalizing on Knowledge.” British Journal for the Philosophy of Science 49 (1): 89–121. https://doi.org/10.1093/bjps/49.1.89.\n\n\nINDIFFERENCE entails C-INDIFFERENCE given the following extra assumptions. First, if INDIFFERENCE is true it is indefeasible, so it must remain true whatever one’s evidence is. Secondly, rational agents should update by conditionalisation. Thirdly, it is always possible for an agent to get evidence that tells her she is in F1 or F2 and no more. The third premise is at best an idealisation, but it is hard to see how or why that should tell against C-INDIFFERENCE.↩︎\nNote also that if P-INDIFFERENCE is false, then Dr Evil has an easy way out of the ‘brain race’ that comes up at the end of Elga’s paper. He just need be told about some new element without being told its half-life, and magically he is free to assign credence 1 to his being on the spaceship rather than on Earth. This would reduce the interest of the puzzle somewhat I fear.↩︎\nEvil’s plan resembles in many respects a situation described by Jamie Dreier (2001) in his “Boundless Good.” The back story is a little different, but the situation is closely (and intentionally) modelled on his sphere of pain/sphere of pleasure example.↩︎\nCompare the objection to Dutch Book arguments in Schick (1986).↩︎\nFor example, Shafer (1976).↩︎\nCompare the state-dependent approach to decision-making discussed in Chambers and Quiggin (2000).↩︎\nThis point closely resembles an objection to Dutch Book reasoning made in Hájek (2005), though Scylla is much more sceptical about how much we can learn from these pragmatic arguments than Hájek is.↩︎\nScylla’s reasoning here is based on Milne (1991), though of course Milne’s argument is much less condensed than that.↩︎\nThis is proven in McGee (1999).↩︎\nThough see Hetherington (2001) for an argument to the contrary.↩︎\nBradley (monton2002?) discusses using sets of probability functions to solve another problem proposed by Elga, the Sleeping Beauty problem (Elga 2000). Monton notes that if Beauty’s credence in The coin landed heads is [0, 0.5] when she wakes up on Monday, then she doesn’t violate van Fraassen’s General Reflection Principle (Bas van Fraassen 1995). (I assume here familiarity with the Sleeping Beauty problem.) Monton has some criticisms of this move, in particular the consequences it has for updating, that don’t seem to carry across to the proposal sketched here. But his discussion is noteworthy as a use of this approach to uncertainty as a way to solve problems to do with similar predicaments.↩︎\nAs with earlier cases, strictly speaking we need C-INDIFFERENCE and P-INDIFFERENCE to draw the conclusions suggested unless Al is somehow certain about all other propositions. I will ignore that complication here, and in .↩︎\nDr Evil’s plans create a situation similar to the well known ‘shooting room’ problem. For the best analysis of that problem see Bartha and Hitchcock (1999). Dr Evil has changed the numbers involved in the puzzle a little bit to make the subsequent calculations a little more straightforward. He’s not very good at arithmetic you see.↩︎\n",
    "preview": "posts/2021-01-11-should-we-respond-to-evil-with-indifference/evilcat.jpg",
    "last_modified": "2021-02-05T20:42:36-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-04-true-truer-truest/",
    "title": "True, Truer, Truest",
    "description": "My theory of vagueness.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2005-01-01",
    "categories": [
      "language",
      "logic",
      "vagueness"
    ],
    "contents": "\n\nContents\nTruer\nFurther Reflections on Truer\nConstraints on Truer and Classical Logic\nSemantics and Proof Theory\nSexy Sorites\nLinearity Intuitions\n\nWhat the world needs now is another theory of vagueness. Not because the old theories are useless. Quite the contrary, the old theories provide many of the materials we need to construct the truest theory of vagueness ever seen. The theory shall be similar in motivation to supervaluationism, but more akin to many-valued theories in conceptualisation. What I take from the many-valued theories is the idea that some sentences can be truer than others. But I say very different things to the ordering over sentences this relation generates. I say it is not a linear ordering, so it cannot be represented by the real numbers. I also argue that since there is higher-order vagueness, any mapping between sentences and mathematical objects is bound to be inappropriate. This is no cause for regret; we can say all we want to say by using the comparative truer than without mapping it onto some mathematical objects. From supervaluationism I take the idea that we can keep classical logic without keeping the familiar bivalent semantics for classical logic. But my preservation of classical logic is more comprehensive than is normally permitted by supervaluationism, for I preserve classical inference rules as well as classical sequents. And I do this without relying on the concept of acceptable precisifications as an unexplained explainer.\nPublished in Philosophical Studies 123: 47-70.\nThanks to Juan Comesaña, John Hawthorne, Allan Hazlett, Alyssa Ney, and audiences at the 2002 APA Central, the University of Edinburgh and especially the 2003 BSPC for helpful comments. From the latter I’m especially grateful to Jonathan Bennett, Alex Byrne, Cian Dorr, Andy Egan, Elizabeth Harman, Robin Jeshion, Mike Nelson, Jonathan Schaffer, Ted Sider and Gabriel Uzquiano. I’m also grateful to two very helpful referee reports and to Restall (2000), without which I would never have known enough about lattices to write this paper.\nThe world does not need another guide to varieties of theories of vagueness, especially since Timothy Williamson (1994) and Rosanna Keefe (2000) have already provided quite good guides. I assume throughout familiarity with popular theories of vagueness.\nTruer\nThe core of my theory is that some sentences involving vague terms are truer than others. I won’t give an analysis of truer, instead I will argue that we already tacitly understand this relation. The main argument for this will turn on a consideration of two ‘many-valued’ theories of vagueness, one of which will play a central role (as the primary villain) in what follows.\nThe most familiar many-valued theory, call it \\(M\\), says there are continuum many truth values, and they can be felicitously represented by the interval \\([0,~1]\\). The four main logical connectives: and, or, if and not are truth-functional. The functions are:\n\\[\n\\begin{align}\nV(A \\wedge B) &= min(V(A), V(B))   \\\\\nV(A \\vee B) &= max(V(A), V(B))   \\\\\nV(A \\rightarrow B) &= max(1, 1 - V(A) + V(B)) \\\\\nV(\\neg A) &= 1 - V(A)\n\\end{align}\n\\]\nwhere \\(V\\) is the valuation function on sentences, \\(min(x, y)\\) is the smaller of \\(x\\) and y and \\(max(x, y)\\) is the larger of \\(x\\) and y.\nAdopting these rules for the connectives commits us to adopting the logic ŁC. \\(M\\) is the theory that this semantic model, under its most natural interpretation, is appropriate for vague natural languages. (We’ll discuss less natural interpretations presently.)\n\\(M\\) tells a particularly nice story about the Sorites. A premise like If she’s rich, someone with just a little less money is also rich will have a very high truth value. If we make the difference in money between the two subjects small enough, this conditional will have a truth value arbitrarily close to 1.\n\\(M\\) also tells a nice story about borderline cases and determinateness. An object \\(a\\) is a borderline case of being an \\(F\\) just in case the sentence a is F has a truth value between 0 and 1 exclusive. Similarly, \\(a\\) is a determinate \\(F\\) just in case the truth value of a is F is 1. (It is worthwhile comparing how simple this analysis of determinateness is to the difficulties supervaluationists have in providing an analysis of determinateness. On this topic, see Williamson (1995), McGee and McLaughlin (1998) and Williamson (2004).)\nBut \\(M\\) tells a particularly implausible story about contradictions. Here is how Timothy Williamson (1994) makes this problem vivid.\n\nMore disturbing is that the law of non-contradiction fails …. \\(\\neg(p \\wedge \\neg p)\\) always has the same degree of truth as \\(p \\vee \\neg p\\), and thus is perfectly true only when \\(p\\) is either perfectly true or perfectly false. When \\(p\\) is half-true, so are both \\(p \\wedge \\neg p\\) and \\(\\neg(p \\wedge \\neg p)\\). (Williamson 1994, 118)\nAt some point [in waking up] ‘He is awake’ is supposed to be half-true, so ‘He is not awake’ will be half-true too. Then ‘He is awake and he is not awake’ will count as half-true. How can an explicit contradiction be true to any degree other than 0? (Williamson 1994, 136)\n\nThere is a way to keep the semantic engine behind \\(M\\) while avoiding this consequence. (The following few paragraphs are indebted pretty heavily to the criticisms of Strawson’s theory of descriptions in Dummett (1959))\nConsider an interpretation of the above semantics on which there are only two truth values: True and False. Any sentence that gets truth value 1 is true, all the others are false. The numbers in [0, 1) represent different ways of being false. (As Tolstoy might have put it, all true sentences are alike, but every false sentence is false in its own unique way.) Which way a sentence is false can affect the truth value of compounds containing that sentence. In particular, if \\(A\\) and \\(B\\) are false, then the truth values of Not A and If A then B will depend on the ways \\(A\\) and \\(B\\) take their truth values. If \\(V\\)(\\(A\\)) = 0 and \\(V\\)(\\(B\\)) = 0.3, then Not A and If A then B will be true, but if \\(V\\)(\\(A\\)) becomes 0.6, and remember this is just another way of being false, both Not A and If A then B will be false.\nThe new theory we get, one I’ll call \\(M_D\\), is similar to \\(M\\) in some respects. For example, it agrees about what the axioms should be for a logic for natural language. But it has several philosophical differences. In particular, it has none of the three characteristics of \\(M\\) we noted above.\nIt cannot tell as plausible story as \\(M\\) does about the Sorites. If any sentence with truth value below 1 is false, then many of the premises in a Sorites argument are false. This is terrible – it was bad enough to be told that one of the premises were false, but now we find many thousands of them are false. I doubt that being told they are false in a distinctive way will improve our estimation of the theory. Similarly, it is hard to see just how the new theory has anything interesting to say about the concept of a borderline case.\nOn the other hand, according to \\(M_D\\), contradictions are always false. To be sure, a contradiction might be false in some obscure new way, but it is still false. Recall Williamson’s objection that an explicit contradiction should be true to degree 0 and nothing more. This objection only works if being true to degree 0.5 is meant to be semantically significant. If being ‘true to degree 0.5’ is just another way of being false, then there is presumably nothing wrong with contradictions are true to degree 0.5. This is not to say Williamson’s objection is no good, since he intended it as an objection to \\(M\\), but just to say that re-interpreting the semantic significance of the numbers in \\(M\\) makes a philosophical difference.\nDespite \\(M_D\\)’s preferable treatment of contradictions, I think \\(M\\) is overall a better theory because it has a much better account of borderline cases. But for now I want to stress a simpler point: \\(M\\) and \\(M_D\\) are different theories of vagueness, and that we grasp the difference between these theories. One crucial difference between the two theories is that in \\(M\\), but not \\(M_D\\), \\(S_1\\)is truer than \\(S_2\\) if \\(V\\)(\\(S_1\\)) is greater than \\(V\\)(\\(S_2\\)). In \\(M_D\\), if \\(S_1\\)is truer than \\(S_2\\), \\(V\\)(\\(S_1\\)) must be one and \\(V\\)(\\(S_2\\)) less than one. And that is the only difference between the two theories. So if we understand this difference, we must grasp this concept truer than. Indeed, it is in virtue of grasping this concept that we understand why saying each of the Sorites conditionals is almost true is a prima facie plausible response to the Sorites, and why having a theory that implies contradictions are truer than many other sentences is a rather embarrassing thing.\nI have implicitly defined truer by noting its theoretical role. As David Lewis (1972) showed, terms can be implicitly defined by their theoretical role. There is one unfortunate twist here in that truer is defined by its role in a false theory, but that does not block the implicit definition story. We know what phlogiston and ether mean because of their role in some false theories. The meaning of truer can be extracted in the same way from the false theory \\(M\\).\nFurther Reflections on Truer\nAs noted, I won’t give a reductive analysis of truer. The hopes for doing that are no better than the hopes of giving a reductive analysis of true. But I will show that we pre-theoretically understand the concept.\nMy primary argument for this has already been given. Intuitively we do understand the difference between \\(M\\) and its \\(M_D\\), and this is only explicable by our understanding truer. Hence we understand truer.\nSecond, it’s noteworthy that truer is morphologically complex. If we understand true, and understand the modifier -er, then we know enough in principle to know how they combine. Not every predicate can be turned into a comparative. But most can, and our default assumption should be that true is like the majority.\nI have heard two arguments against that assumption. First, it could be argued that most comparatives in English generate linear orderings, but truer generates a non-linear ordering. I reject the premise of this argument. Cuter, Smarter, Smellier, and Tougher all generate non-linear orderings over their respective domains, and they seem fairly indicative of large classes. Second, it could be argued that it’s crucial to understanding comparatives that we understand the interaction of the underlying adjectives with comparison classes. Robin Jeshion and Mike Nelson made this objection in their comments on my paper at BSPC 2003. Again, the premise is not obviously true. We can talk about some objects being straighter or rounder despite the fact that it’s hard to understand round for an office building or straight for a line drive. (Jonathan Bennett made this point in discussion at BSPC.) Straight and round either don’t have or don’t need comparison classes, but they form comparatives. So true, which also does not take comparison classes, could also form a comparative.\nFinally, if understanding the inferential role of a logical operator helps know its meaning, then it is notable that truer has a very clear inferential role. It is the same as a strict material implication \\(\\square (q \\supset p)\\) defined using a necessity operator whose logic is KT. Since many operators have just this logic, this doesn’t individuate truer, but it helps with inferential role semantics aficionados.\nI claim that the concept truer, and the associated concept as true as, are the only theoretical tools we need to provide a complete theory of vagueness. It is simplest to state the important features of my theory by contrasting it with \\(M\\). I keep the following good features of \\(M\\).\nG1\nThere are intermediate sentences, i.e. sentences that are truer than some sentences and less true than others. For definiteness, I will say \\(S\\) is intermediate iff \\(S\\) is truer than 0=1 and less true than 0=0.\n\nG2\n\\(a\\) is a borderline \\(F\\) iff a is F is intermediate, and \\(a\\) is determinately \\(F\\) iff \\(a\\) is \\(F\\) and \\(a\\) is not a borderline \\(F\\).\n\nI won’t repeat the arguments here, but I take G1 to be a large advantage of theories like \\(M\\) over epistemicist theories. (See J. Burgess (2001), Sider (2001) and Weatherson (2003a) for more detailed arguments to this effect.) And as noted G2 is a much simpler analysis of determinacy and borderline than supervaluationists have been able to offer.\nI drop the following bad features of \\(M\\).\nB1 Some contradictions are intermediate sentences.\n\nOn my theory all contradictions are determinately false, and determinately determinately false, and so on. The argument for this has been given above.\n\nB2 Some classical tautologies are intermediate sentences.\n\nOn my theory all classical tautologies are determinately true, and determinately determinately true, and so on. We will note three arguments for this being an improvement in the next section.\n\nB3 Some classical inference rules are inadmissible.\n\nOn my theory all classical inference rules are admissible. As Williamson (1994) showed, the most prominent version of supervaluationism is like \\(M\\) in ruling some classical rules to be inadmissible, and this is clearly a cost of those theories.\n\nB4 Sentences of the form S is intermediate are never intermediate\n\nI will argue below this is a consequence of \\(M\\), and it means it is impossible to provide a plausible theory of higher-order vagueness within \\(M\\). In my theory we can say that there is higher-order vagueness by treating truer as an iterable operator, so we can say that S is intermediate is intermediate. If \\(S\\) is a is F, that’s equivalent to saying that \\(a\\) is a borderline case of a borderline case of an \\(F\\). Essentially we get out theory of higher-order vagueness by simply iterating our theory of first-order vagueness, which is what Williamson does in his justly celebrated treatment of higher-order vagueness. Note it’s not just \\(M\\) that has troubles with higher-order vagueness. See Williamson (1994) and Weatherson (2003b) for the difficulties supervaluationists have with higher-order vagueness. The treatment of higher-order vagueness here is a substantial advantage of my theory over supervaluationism.\n\nB5 Truer is a linear relation.\n\nOn my theory it need not be the case that \\(S_1\\)is truer than \\(S_2\\), or \\(S_2\\) is truer than \\(S_1\\), or they are as true as each other. In the last section I will argue that this is a substantial advantage of my theory. I claim that truer generates a Boolean lattice on possible sentences of the language. (For a familiar example of a Boolean lattice, think of the subsets of \\(\\mathbb{R}\\) ordered by the subset relation.)\n\nI also provide a very different, and much more general, treatment of the Sorites than is available within \\(M\\). The biggest technical difference between my theory and \\(M\\) concerns the relationship between the semantics and the logic. In \\(M\\) the logic falls out from the truth-tables. Since I do not have the concept of an intermediate truth value in my theory, I could not provide anything like a truth-table. Instead I posit several constraints on the interaction of truer with familiar connectives, posit an analysis of validity in terms of truer, and note that those two posits imply that all and only classically admissible inference rules are admissible.\nConstraints on Truer and Classical Logic\nThe following ten constraints on truer seem intuitively compelling. I’ve listed here both the philosophically important informal claim, and the formal interpretation of that claim. (I use \\(A \\geqslant _T B\\) as shorthand for \\(A\\) is at least as true as B. Note all the quantifiers over sentences here are possibilist quantifiers, we quantify over all possible sentences in the language.)\n(A1)\n\\(\\geqslant _T\\) is a weak ordering (i.e. reflexive and transitive)\nIf \\(A \\geqslant _T B\\) and \\(B \\geqslant _T C\\) then \\(A \\geqslant _T C\\)\\(A \\geqslant _T A\\)\n\n(A2)\n\\(\\wedge\\) is a greatest lower bound with respect to \\(\\geqslant _T\\)\\(A \\wedge B \\geqslant _T C\\) iff \\(A \\geqslant _T C\\) and \\(B \\geqslant _T C\\)\\(C \\geqslant _T\\) A\\(\\wedge B\\) iff for all \\(S\\) such that \\(A \\geqslant _T S\\) and \\(B \\geqslant _T\\)S it is also the case that \\(C \\geqslant _T S\\)\n\n(A3)\n\\(\\vee\\) is a least upper bound with respect to \\(\\geqslant _T\\)\\(A \\vee B \\geqslant _T C\\) iff for all \\(S\\) such that \\(S \\geqslant _T A\\) and \\(S \\geqslant _T B\\), it is also the case that \\(S \\geqslant _T C\\)\\(C \\geqslant _T\\) A\\(\\vee B\\) iff \\(C \\geqslant _T A\\) and \\(B \\geqslant _T C\\)\n\n(A4)\n\\(\\neg\\) is ordering inverting with respect to \\(\\geqslant _T\\)\\(A \\geqslant _T B\\) iff \\(\\neg B \\geqslant _T \\neg A\\)\n\n(A5)\nDouble negation is redundant\\(\\neg \\neg A =_T A\\)\n\n(A6)\nThere is an absolutely false sentence \\(S_F\\) and an absolutely true sentence \\(S_T\\)\nThere are sentences \\(S_F\\)and \\(S_T\\)such that \\(S_F =_T \\neg S_T\\) and \\(\\neg S_F =_T S_T\\)  and for all \\(S\\): \\(S_T \\geqslant _T S \\geqslant _T S_F\\)\n\n(A7)\nContradictions are absolutely false\\(A \\wedge \\neg A =_T S_F\\)\n\n(A8)\n\\(\\forall\\) is a greatest lower bound with respect to \\(\\geqslant _T\\)\\(A \\geqslant _T \\forall x\\)(\\(\\phi x\\)) iff for all \\(S\\) such that for all \\(o\\), if \\(n\\) is a name of \\(o\\) then \\(\\phi n \\geqslant _T S\\), it is the case that \\(A \\geqslant _T S\\)\\(\\forall x\\)(\\(\\phi x\\)) \\(\\geqslant _T A\\) iff for all \\(o\\), if \\(n\\) is a name of \\(o\\) then \\(\\phi n \\geqslant _T A\\)\n\n(A9)\n\\(\\exists\\) is a least upper bound with respect to \\(\\geqslant _T\\)\\(A \\geqslant _T \\exists x\\)(\\(\\phi x\\)) iff for all \\(o\\), if \\(n\\) is a name of \\(o\\) then \\(A \\geqslant _T \\phi\\)n\\(\\exists x\\)(\\(\\phi x\\)) \\(\\geqslant _T A\\) iff for all \\(S\\) such that for all \\(o\\), if \\(n\\) is a name of \\(o\\) then \\(S \\geqslant _T \\phi n\\), \\(S \\geqslant _T A\\)\n\n(A10)\nA material implication with respect to \\(\\geqslant _T\\) can be defined.\nThere is an operative \\(\\rightarrow\\) such that\n\\(B \\rightarrow A \\geqslant _T S_T\\)iff \\(A \\geqslant _T B\\)\n(\\(A \\wedge B\\))\\(\\rightarrow C =_T A \\rightarrow\\)(\\(B \\rightarrow C\\))\n\nApart from (A10) these are fairly straightforward. We can’t argue for (A10) by saying English if…then is a material implication, because that leads directly to the paradoxes of material implication. Assuming that \\(\\neg A \\vee B\\) is a material implication is equivalent to assuming (inter alia) that \\(A \\vee \\neg A\\) is perfectly true. I believe this, but since it is denied by many I want that to be a conclusion, not a premise. So the argument for (A10) must be a little indirect. In particular, we will appeal to the behaviour of quantifiers. We can formally represent All Fs are Gs in two ways: using restricted or unrestricted quantifiers. In the first case the formal representation will look like:\n\n\\(\\forall x\\)(Fx ? Gx)\n\nwith some connective in place of ‘?’ But it seems clear that whatever connective goes in there must be a material implication. In the second case, the formal representation will look like:\n\n[\\(\\forall x\\): Fx] Gx\n\nIn that case, we can define a connective \\(\\nabla\\) that satisfies the definition of a material implication:\n\n\\(A \\nabla B\\) =df [\\(\\forall x\\): \\(A \\wedge x\\)=\\(x\\)] (\\(B \\wedge x\\)=\\(x\\))\n\nThis is equivalent to the odd (but intelligible) sentence Everything such that A is such that B. Again, considerations about what should be logical truths involving quantifiers suggests that \\(\\nabla\\) must be a material implication. So either way there should be a material implication present in the language, as (A10) says.\nGiven (A1) to (A10) it follows that this material implication is equivalent to \\(\\neg A \\vee B\\), and hence \\(A \\vee \\neg A\\) is a logical truth. This is a surprising conclusion, since intuitively vagueness poses problems for excluded middle, but I think it is more plausible that vague instances of excluded middle are problematic for pragmatic reasons than that any of (A1) to (A10) are false.\nWhat is interesting about these ten constraints is that they suffice for classical logic, with just one more supposition. I assume that an argument is valid iff it is impossible for the premises taken collectively to be truer than the conclusion, i.e. iff it is impossible for the conjunction of the premises to be truer than the conclusion. Given that, we get:\n\n\\(\\forall A_1\\), …,\\(A_n\\),\\(B\\): \\(A_1\\), …, \\(A_n \\vdash\\)T \\(B\\) iff, according to classical logic, \\(A_1\\), …, \\(A_n\\) \\(\\vdash\\) \\(B\\)\n\n(I use \\(\\Gamma\\) \\(\\vdash\\)T\\(A\\) to mean that in all models for \\(\\geqslant _T\\) that satisfy the constraints, here (A1) to (A10), the conclusion is at least as true as the greatest lower bound of the premises.) I won’t prove this result, but the idea is that (A1) to (A10) imply that \\(\\geqslant _T\\) defines a Boolean lattice over equivalence classes of sentences with respect to \\(=_T\\). And all Boolean lattices are models for classical logic, from which our result follows. Indeed, Boolean lattices are models for classical logic in the strong sense that classical inference rules, such as conditional proof and reductio, are admissible in logics defined on them, so we also get the admissibility of classical inference rules in this theory. (Note that this result only holds in the right-to-left direction for languages that contain the \\(\\geqslant _T\\) operator. Once this operator is added, some arguments that are not classically valid, such as \\(B \\geqslant _T A\\), \\(A \\vdash\\)T \\(B\\), will valid. But the addition of this operator is conservative: if we look at the \\(\\geqslant _T\\)-free fragment of such languages, the above result still holds in both directions.)\nThere are three reasons for wanting to keep classical logic in a theory of vagueness. First, as Williamson has stressed, classical logic is at the heart of many successful research programs. Second, non-classical theories of vagueness tend to abandon too much of classical logic. For instance, \\(M\\) abandons the very plausible schema (\\(A \\wedge A \\rightarrow B\\))\\(\\rightarrow B\\). The third reason is the one given here - these ten independently plausible constraints on truer entail that the logic for a language containing truer should be classical. These three arguments add up to a powerful case that non-classical theories like \\(M\\) are mistaken, and we should prefer a theory that preserved classical logic.\nSemantics and Proof Theory\nIn this section I will describe a semantics and proof theory for a language containing truer as an iterable operator. This is important for the theory of higher-order vagueness. I say that \\(a\\) is a borderline borderline \\(F\\) just in case the sentence a is a borderline F is intermediate, where ‘borderline’ is analysed as in section 2. It might not be obvious that it is consistent with (A1) to (A10) that any sentence a is a borderline F could be consistent. One virtue of the model theory outlined here is that it shows this is consistent.\nFor comparison, note that \\(M\\) as it stands has no way of dealing with higher-order vagueness, i.e. with borderline cases of borderline cases of \\(F\\)-ness. If every sentence a is a borderline F either does or does not receive an integer truth value, then this intuitive possibility is ruled out. We cannot solve the problem simply by iterating \\(M\\). (This is a point stressed by (Williamson 1994 Ch. 4).) We cannot say that it is true to degree 0.5 than (2) is true to degree 1, and true to degree 0.5 that it is true to degree 0.8. For then it is only true to degree 0.5 that (2) has some truth value or other. And the use of truth-tables to generate a logic presupposes that every sentence has some truth values or other. If this is not determinately true, \\(M\\) is not a complete theory. So the model theory will show that our theory is substantially better than \\(M\\) in this respect.\nConsider the following (minor) variant on KT. Vary the syntax so \\(\\square A\\) is only well-formed if \\(A\\) is of the form \\(B \\rightarrow C\\). Call the resulting logic KTR, with the R indicating the syntactic restriction. The restriction makes very little difference. Since \\(A\\) is equivalent to (\\(A \\rightarrow A\\)) \\(\\rightarrow A\\), even if \\(\\square A\\) is not well-formed in KTR, the KT-equivalent sentence \\(\\square\\)((\\(A \\rightarrow A\\)) \\(\\rightarrow A\\)) will be well-formed. The Kripke models for KTR are quite natural. \\(\\square\\)(\\(B \\rightarrow C\\)) is true at a point iff all accessible points at which \\(B\\) is true are points at which \\(C\\) is true. (There is no restriction on the accessibility relation other than reflexivity.)\nSince KTR is so similar to KT, we can derive most of its formal properties by looking at the derivations of similar properties for KT. (The next few paragraphs owe a lot to (Goldblatt 1992, Chs.1–3).) Let’s start with an axiomatic proof theory. The axioms for KTR are:\nAll classical tautologies\nAll well-formed instances of K: \\(\\square\\)(\\(A \\rightarrow B\\)) \\(\\rightarrow\\) (\\(\\square A \\rightarrow \\square B\\))\nAll well-formed instances of T: \\(\\square A \\rightarrow A\\)\nThe rules for KTR are\nModus Ponens\nIf \\(A \\rightarrow B\\) is a theorem and \\(A\\) is a theorem, then \\(B\\) is a theorem\n\nRestricted Necessitation\nIf \\(A\\) is a theorem and \\(\\square A\\) is well-formed, then \\(\\square A\\) is a theorem.\n\nGiven these, we can now define a maximal consistent set for KTR. It is a set \\(S\\) of sentences with the following three properties:\nAll theorems of KTR are in \\(S\\).\nFor all \\(A\\), either \\(A\\) is in S or \\(\\neg A\\) is in \\(S\\).\n\\(S\\) is closed under modus ponens.\nThe existence of Kripke models for KTR show that some maximal consistent sets exist: the set of truths at any point will be a maximal consistent set. The canonical model for KTR is \\(\\langle W, R, V \\rangle\\) where\nW is the set of maximal consistent sets for KTR\nR is a subset of W \\(\\times\\) W such that w1Rw2 iff for all \\(A\\) such that \\(\\square A \\in\\) w1, \\(A \\in\\) w2\n\\(V\\) is the valuation such that \\(V\\)(\\(A\\)) = {w: \\(A \\in\\) w}\nSince all instances of T are theorems, it can be easily shown that R is reflexive, and hence that this is a frame for KTR and hence that KTR is canonically complete.\nWe can translate all sentences of KTR into a language that contains \\(\\geqslant _T\\) but not \\(\\square\\). Just replace \\(\\square\\)(\\(B \\rightarrow A\\)) with \\(A \\geqslant _T B\\) wherever \\(\\square\\) occurs including inside sentences. (We appeal here and here alone to the restriction in KTR.) Translating the axioms for KTR, we get the following axioms for the logic of \\(\\geqslant _T\\).\nAll classical tautologies\nAll instances of: (\\(B\\)  \\(\\geqslant _T A\\)) \\(\\rightarrow\\) ((\\(A \\geqslant _T\\) (\\(A \\rightarrow A\\))) \\(\\rightarrow\\) (\\(B \\geqslant _T\\) (\\(B \\rightarrow B\\))))\nAll instances of: (\\(B \\geqslant _T A\\)) \\(\\rightarrow\\) (\\(A \\rightarrow B\\))\nThe rules are\nModus ponens\nIf \\(A \\rightarrow B\\) is a theorem and \\(A\\) is a theorem, then \\(B\\) is a theorem.\n\nDetermination\nIf \\(A \\rightarrow B\\) is a theorem, then \\(B \\geqslant _T A\\) is a theorem.\n\nWe can simplify somewhat by replacing the second axiom schema with\nAll instances of: \\(A \\geqslant _T B \\rightarrow\\) (\\(B \\geqslant _T C \\rightarrow A \\geqslant _T C\\))\nA Kripke model for this logic is just a Kripke model for KT, except we say \\(B \\geqslant _T A\\) is true at a point iff \\(B\\) is true at all accessible points at which \\(A\\) is true. This leads to a semantic definition of validity. An argument is valid iff it preserves truth at any point in all such models.\nMaximal consistent sets with respect to \\(\\geqslant _T\\) and a canonical model for \\(\\geqslant _T\\) can be easily constructed by parallel with the maximal consistent sets and canonical models for KTR. These constructions show that if \\(A\\) is a theorem of the logic for \\(\\geqslant _T\\), then it is true at all points in all models. More generally, they can be used to show that this logic is canonically complete, though the details of the proof are omitted. The maximal consistent sets for \\(\\geqslant _T\\), i.e. the points in the canonical model, just are the results of applying the translation rule \\(\\square\\)(\\(B \\rightarrow A\\)) \\(\\Rightarrow A \\geqslant _T B\\) to the (sentences in the) maximal consistent sets for KTR.\nThat’s important because the points in the canonical model for \\(\\geqslant _T\\) are useful for understanding the relationship between truer and true, and for understanding what languages are. The set of true sentences in English is one of the points in the canonical model for \\(\\geqslant _T\\). For semantic purposes, languages just are points in this canonical model. It is indeterminate just which such point English is, but it is one of them. For many purposes it is useful to think of the theory based on truer as a variant on \\(M\\). But considering the canonical model for \\(\\geqslant _T\\) highlights the similarities with supervaluationism rather than the similarities with \\(M\\), for the points in the canonical model look a lot like precisifications. It is, however, worth noting the many differences between my theory and supervaluationism. I identify languages with a single point rather than with a set of points, which leads to the smoother treatment of higher-order vagueness on my account. Also, I don’t start with a set of acceptable points/precisifications. The canonical model contains all the points that are formally consistent, and I identify particular languages, like English, by vaguely saying that the point that represents English is (roughly) there. (Imagine my vaguely pointing at some part of the model when saying this.) The most important difference is that I take the points, with the truer than relation already defined, to be primitive, and the accessibility/acceptability relation to be defined in terms of them. This reflects the fact that I take the truer relation to be primitive, and determinacy to be defined in terms of it, whereas typically supervaluationists do things the other way around. None of these differences are huge, but they all favour my theory over supervaluationism.\nTo return to the point about higher order vagueness, note that all of the following sentences are consistent in KT, and hence their ‘equivalents’ using >T are also consistent.\nAnd obviously this pattern can be extended indefinitely. In general, any claim of the form that \\(a\\) is an \\(n\\)-th order borderline case of an \\(F\\) is consistent in this theory, as can be seen by comparison with KT.\nTo close this section, I will note that we can also provide a fairly straightforward natural deduction system for the logic of \\(\\geqslant _T\\). There are two philosophical benefits to doing this. First, it proves my earlier claim that I can keep all inference rules of classical logic. Second, it helps justify (A1) to (A10). Most rules correspond directly to one of the constraints. For that reason I’ve set all the rules, even though you’ve probably seen most of them before.\n(\\(\\wedge\\) In)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A\\), \\(\\Delta\\) \\(\\vdash\\) \\(B \\Rightarrow \\Gamma \\cup\\Delta \\vdash A \\wedge B\\)\n\n(\\(\\wedge\\) Out-left)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A \\wedge B \\Rightarrow\\) \\(\\Gamma\\) \\(\\vdash\\) \\(A\\)\n\n(\\(\\wedge\\) Out-right)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A \\wedge B \\Rightarrow\\) \\(\\Gamma\\) \\(\\vdash\\) \\(B\\)\n\n(\\(\\vee\\) In-left)\n\\(\\Gamma\\) \\(\\vdash\\) \\(B \\Rightarrow\\)  \\(\\Gamma\\) \\(\\vdash\\) \\(A \\vee B\\)\n\n(\\(\\vee\\) In-right)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A \\Rightarrow\\)  \\(\\Gamma\\) \\(\\vdash\\) \\(A \\vee B\\)\n\n(\\(\\vee\\) Out)\n\\(\\Gamma \\cup\\){\\(A\\)} \\(\\vdash\\) \\(C\\), \\(\\Delta \\cup\\){\\(B\\)} \\(\\vdash\\) \\(C\\), \\(\\Lambda\\) \\(\\vdash\\)  \\(A \\vee B \\Rightarrow\\) \\(\\Gamma \\cup \\Delta \\cup \\Lambda \\vdash C\\)\n\n(\\(\\rightarrow\\) In)\n\\(\\Gamma \\cup\\){\\(A\\)} \\(\\vdash\\) \\(B \\Rightarrow\\)  \\(\\Gamma\\) \\(\\vdash\\) \\(A \\rightarrow B\\)\n\n(\\(\\rightarrow\\) Out)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A \\rightarrow B\\), \\(\\Delta\\) \\(\\vdash A \\Rightarrow \\Gamma \\cup \\Delta \\vdash B\\)\n\n(\\(\\neg\\) In)\n\\(\\Gamma \\cup\\){\\(A\\)} \\(\\vdash\\) \\(B \\wedge \\neg B \\Rightarrow\\)  \\(\\Gamma\\) \\(\\vdash\\) \\(\\neg A\\)\n\n(\\(\\neg\\) Out)\n\\(\\Gamma\\) \\(\\vdash \\neg \\neg A \\Rightarrow\\) \\(\\Gamma\\) \\(\\vdash\\) \\(A\\)\n\n(\\(\\geqslant _T\\) In)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A \\Rightarrow\\) {\\(B \\geqslant _T C\\): \\(B \\in\\)  \\(\\Gamma\\)} \\(\\vdash\\)  \\(A \\geqslant _T C\\)\n\n(\\(\\geqslant _T\\) Convert)\n\\(\\Gamma\\) \\(\\vdash\\)  \\(A \\geqslant _T B \\Rightarrow\\)  \\(\\Gamma\\) \\(\\vdash\\) (\\(B \\rightarrow A\\))\\(\\geqslant _T C\\)\n\n(\\(\\geqslant _T\\) Out)\n\\(\\Gamma\\) \\(\\vdash\\) \\(A \\geqslant _T B \\Rightarrow\\)  \\(\\Gamma\\) \\(\\vdash\\)  \\(B \\rightarrow A\\)\n\n(Thanks to Gabriel Uzquiano for several probing questions that led to this section being written.)\nSexy Sorites\nA good theory of vagueness should tell us two things about the Sorites. The easy part is to say what is wrong with Sorites arguments: not all premises are perfectly true. The hard part is to say why the premises looked plausible to start with. The \\(M\\) theorist has the beginnings of a story, though not the end of a story. The beginning is that all the premises in a typical Sorites argument are nearly true, and they look plausible because we confuse near truth for truth. Can I say the same thing, since my theory is like \\(M\\)? No, for two reasons. First, since my theory explicitly gets rid of numerical representations of intermediate truth values, I don’t have any way to analyse almost true. Second, since I say that one of the Sorites premises is false, I’d be committed to the odd view that some false sentence is almost perfectly true. Thanks to Cian Dorr for pointing out this consequence.\nThe story the \\(M\\) theorist tells does not generalise. The problem is that not all Sorites arguments involve conditionals. A typical Sorites situation involves a chain from a definite \\(F\\) to a definite not-\\(F\\). Let \\(^\\prime\\) denote the successor relation in this sequence, so if \\(F\\) is is tall and \\(a\\) is 178cm tall, then \\(a ^\\prime\\) will be 177.99cm tall, assuming the sequence progresses 0.1mm at a time. According to \\(M\\), every premise like (SI) is almost true.\n(SI)\nIf \\(a\\) is tall, then \\(a ^\\prime\\) is tall.\n\nBut we could have built a Sorites argument with premises like (SA).\n(SA)\nIt is not the case that \\(a\\) is tall and \\(a ^\\prime\\) is not tall.\n\nAnd premises of this form are not, in general, almost true. Indeed, some will have a truth value not much about 0.5. So \\(M\\) has no explanation for why premises like (SA) look persuasive. This is quite bad, because (SA) is more plausible than (SI) as I’ll now show. Consider the following thought experiment. You are trying to get a group of (typically non-responsive) undergraduates to appreciate the force of the Sorites paradox. If they don’t feel the force of (SI), how do you persuade them? My first instinct is to appeal to something like (SA). If that doesn’t work, I appeal to theoretical considerations about how our use of tall couldn’t possibly pick a boundary between \\(a\\) and \\(a ^\\prime\\). I think I find (SI) plausible because I find (SA) plausible, and I would try to get the students to feel likewise. There’s an asymmetry here. I wouldn’t defend (SA) by appealing to (SI), and I don’t find (SA) plausible because it follows from (SI). (This is not to endorse universally quantified versions of either (SA) or (SI). They are like Axiom V - claims that remain intuitively plausible even when we know they are false.)\nSadly, many theories have little to say about why (SA) seems true. The official epistemicist story is that speakers only accept sentences that are determinately, i.e. knowably, true. But some instances of (SA) are actually false, and many many more are not knowably true. The supervaluationist story about (SA) is no better.\nHere’s a surprising fact about the Sorites that puts an unexpected constraint on explanations of why (SA) is plausible. In the history of debates about it, I don’t think anyone has put forward a Sorites argument where the major premises are like (SO).\n(SO)\nEither \\(a\\) is not tall, or \\(a ^\\prime\\) is tall.\n\n(This point is also noticed in Braun and Sider (2007).) There’s a good reason for this: (SO) is not intuitively true, unless perhaps one sees it as a roundabout way of saying (SA). In this respect it conflicts quite sharply with (SA), which is intuitively true. But hardly any theory of vagueness (certainly not \\(M\\) or supervaluationism or epistemicism) provide grounds for distinguishing (SA) from (SO), since most theories of vagueness endorse DeMorgan’s laws. Further, none of the many and varied recent solutions to the Sorites that do not rely on varying the underlying logic (e.g. Fara (2000; Sorensen 2001; Eklund 2002)) seem to do any better at distinguishing (SA) from (SO). As far as I can tell none of these theories could, given their current conceptual resources, tell a story about why (SA) is intuitively plausible that does not falsely predict (SO) is intuitively plausible. That is, none of these theories could solve the Sorites paradox with their current resources.\nThere is, however, a simple theory that does predict that (SA) will look plausible while (SO) will not. Kit Fine (1975) noted that if we assume that speakers systematically confuse \\(p\\) for Determinately p, even when \\(p\\) occurs as a constituent of larger sentences rather than as a standalone sentence, then we can explain why speakers may accept vague instances of the law of non-contradiction, but not vague instances of the law of excluded middle. (That speakers do have these differing reactions to the two laws has been noted in a few places, most prominently J. A. Burgess and Humberstone (1987) and Tappenden (1993).) It’s actually rather remarkable how many true predictions one can make using Fine’s hypothesis. It correctly predicts that (5) should sound acceptable.\nNow (5) is a contradiction, so both the fact that it sounds acceptable if I am a borderline case of vagueness, and the fact that some theory predicts this, are quite remarkable. This is about as good as it gets in terms of evidence for a philosophical claim.\n(We might wonder just why Fine’s hypothesis is true. One idea is that there really isn’t any difference in truth value between \\(p\\) and Determinately p. This leads to the absurd position that some contradictions, like (5), are literally true. I prefer the following two-part explanation. The first part is that when one utters a simple subject-predicate sentence, one implicates that the subject determinately satisfies the predicate. This is a much stronger implicature than conversational implicature, since it is not cancellable. And it does not seem to be a conventional implicature. Rather, it falls into the category of nonconventional nonconversational implicatures Grice suggests exists on pg. 41 of his (1989). The second part is that some implicatures, including determinacy implicatures, are computed locally and the results of the computations passed up to whatever system computes the intuitive content of the whole sentence. This implies that constituents of sentences can have implicatures. This theme has been studied quite a bit recently; see Levinson (2000) for a survey of the linguistic data and Sedivy et al. (1999) for some empirical evidence supporting up this claim. Just which, if any, implicatures are computed locally is a major research question, but there is some evidence that Fine’s hypothesis is the consequence of a relatively deep fact about linguistic processing. This isn’t essential to the current project - really all that matters is that Fine’s hypothesis is true - but it does suggest some interesting further lines of research and connections to ongoing research projects.)\nIf Fine’s hypothesis is true, then we have a simple explanation for the attractiveness of (SA). Speakers regularly confuse (SA) for (6), which is true, while they confuse (SO) for (7), which is false.\nThis explanation cannot directly explain why speakers find (SI) attractive. My explanation for this, however, has already been given. The intuitive force behind (SI) comes from the fact that it follows, or at least appears to follow, from (SA), which looks practically undeniable.\nSo Fine’s hypothesis gives us an explanation of what’s going on in Sorites arguments that is available in principle to a wide variety of theorists. Fine proposed it in part to defend a supervaluationist theory, and Keefe (2000) adopts it for a similar purpose. Patrick Greenough (2003) has recently adopted a similar looking proposal to provide an epistemicist explanation of similar data. (Nothing in the explanation of the attractiveness of Sorites premises turns on any analysis of determinacy, so the story can be told by epistemicists and supervaluationists alike.) And the story can be added to the theory of truer sketched here. It might be regretted that we don’t have a distinctive story about the Sorites in terms of truer. But the hypothesis that some sentences are truer than others is basically a semantic hypothesis, and if the reason Sorites premises look attractive is anything like the reason (5) looks prima facie attractive, then that attractiveness should receive a pragmatic explanation. What is really important is that there be some story about the Sorites we can tell.\nLinearity Intuitions\nThe assumption that truer is a non-linear relation is the basis for most of the distinctive features of my theory, so it should be defended. There are two reasons to believe it.\nOne is that we can’t simultaneously accept all of the following five principles.\nTruer is a linear relation.\n(A2), that conjunction is a greatest lower bound.\n(A4), that negation is order inverting.\n(A7), that contradictions are determinately false.\nThere are indeterminate sentences.\nI think by far the least plausible of these is the first, so it must go.\nLinearity (or at least determinate linearity) also makes it difficult to tell a plausible story about higher order vagueness. Linearity is the claim that for any two sentences \\(A\\) and \\(B\\), the following disjunction holds. Either \\(A\\) >T \\(B\\), or \\(B\\) >T \\(A\\), or \\(A =_T B\\). If truer is determinately linear, that disjunction is determinately true. And if truer is linear, and if that disjunction is determinately true, then one of its disjuncts must be determinately true, for linearity rules out the possibility of a determinately true disjunction with no determinately true disjunct. Now take a special case of that disjunction, where \\(B\\) is 0=0. In that case we can rule out \\(A\\) >T \\(B\\). So the only options are \\(B\\) >T \\(A\\) or \\(A =_T B\\). We have concluded that given linearity, one of these disjuncts must be determinately true. That is, \\(A\\) is either determinately intermediate or determinately determinate. But intuitively neither of these need be true, for \\(A\\) might be in the ‘penumbra’ between the determinately intermediate and the determinately determinate. This argument is only a problem if we assume determinate linearity, but it’s hard to see the theoretical motivation for believing in linearity but not determinate linearity.\nStill, it is very easy to believe in linearity. Even for comparatives that are clearly non-linear, like more intelligent than, there is a strong temptation to treat them as linear. (Numerical measurements of intelligence are obviously inappropriate given that more intelligent than is non-linear, but there’s a large industry involved in producing such measurements.) And this temptation leads to some prima facie plausible objections to my theory. (All of these objections arose in the discussion of the paper at BSPC.)\nTrue and Truer (due to Cian Dorr)\nHere’s an odd consequence of my theory plus the plausible assumption that If S then S is true is axiomatic. We can’t infer from \\(A\\) is true and \\(B\\) is false that \\(A\\) is truer than \\(B\\). But this looks like a reasonably plausible inference.\nIf we added this as inference rule, we would rule out all intermediate sentences. To prove this assume, for reductio, that \\(A\\) is intermediate. Since we keep classical logic, we know \\(A \\vee \\neg A\\) is true. If A, then \\(A\\) is true, and hence \\(\\neg A\\) is false. Then the new this inference rule implies \\(A \\geqslant _T \\neg A\\), hence \\(A\\)\\({\\wedge}{\\neg}\\)\\(A \\geqslant _T \\neg A\\), since \\(\\neg A \\geqslant _T \\neg A\\), and hence 0=1\\(\\geqslant _T \\neg A\\), since 0=1 \\(\\geqslant _T A \\wedge \\neg A\\), and \\(\\geqslant _T\\) is transitive. So \\(A\\) is determinately true, not intermediate. A converse proof shows that if \\(\\neg A\\), then \\(A\\) is determinately false, not intermediate. So by (\\(\\vee\\)-Out) it follows that \\(A\\) is not intermediate, but since \\(A\\) was arbitrary, there are no intermediate truths. So this rule is unacceptable, despite its plausibility.\nComparing Negative and Positive (due to Jonathan Schaffer)\nLet \\(a\\) be a regular borderline case of genius, somewhere near the middle of the penumbra. Let b be someone who is not a determinate case of genius, but is very close. Let \\(A\\) be a is a genius and \\(B\\) be b is a genius. It seems plausible that \\(A \\geqslant _T \\neg B\\), since \\(a\\) is right around the middle of the borderline cases of genius, but b is only a smidgen short of clear genius. But since b is closer to being a genius than \\(a\\), we definitely have \\(B \\geqslant _T A\\). By transitivity, it follows that \\(B \\geqslant _T \\neg B\\), hence \\(B\\) is determinately true (by the reasoning of the last paragraph). Since \\(\\neg B\\) is not determinately false, it follows that \\(B \\wedge \\neg B\\) is not determinately false, contradicting (A7).\nSince I accept (A7) I must reject the initial assumption that \\(A \\geqslant _T \\neg B\\). But it’s worth noting that this case is quite general. Similar reasoning could be used to show that for any indeterminate propositions of the form \\(x\\) is a genius and y is not a genius, the first is not truer than the second. This seems odd, since intuitively these could both be indeterminate while the first is very nearly true and the second very nearly false.\nComparing Different Predicates (due to Elizabeth Harman)\nOne intuitive way to understand the behaviour of truer is that \\(A\\) is truer than \\(B\\) iff \\(A\\) is true on every admissible precisification on which \\(B\\) is true and the converse does not hold. This can’t be an analysis of truer, since it assumes we can independently define what is an admissible precisification, and this seems impossible. But it’s a useful heuristic. And reflecting on it brings up a surprising consequence of my theory. If we assume that precisifications of predicates from different subject areas (e.g. hexagonal and honest) are independent, it follows that subject-predicate sentences involving those predicates and indeterminate instances of them are incomparable with respect to truth. But this seems implausible. If France is a borderline case of being hexagonal that is close to the lower bound, and George Washington is a borderline case of being honest who is close to the upper bound, then we should think George Washington is honest is truer than France is hexagonal.\nAll three of these objections seem to me to turn on an underlying intuition that truer should be a linear relation. If we are given this, then the inference principle Dorr suggests looks unimpeachable, and the comparisons Schaffer and Harman suggested look right. But once we drop the idea that truer is linear, I think the plausibility of these claims falls away. So the arguments against linearity are ipso facto arguments that we should simply drop the intuitions Dorr, Schaffer and Harman are relying upon.\nTo conclude, it’s worth noting that a very similar inferential rule to the rule Dorr suggests is admissible. From the fact that \\(A\\) is determinately true, and \\(B\\) is determinately false, it follows that \\(A\\) is truer than B. If we assume, as seems reasonable, that we’re only in a position to say that \\(A\\) is true when it is determinately true, then whenever we’re in a position to say \\(A\\) is true and \\(B\\) is false, it will be true that \\(A\\) is truer than \\(B\\). This line of defence is obviously similar to the explanation I gave in the previous section of why Sorites premises look plausible, and to the argument Rosanna Keefe gives that the failure of classical inference rules is no difficulty for supervaluationism because it admits very similar inference rules (Keefe 2000).\n\n\n\nBraun, David, and Theodore Sider. 2007. “Vague, so Untrue.” Noûs 41 (2): 133–56. https://doi.org/10.1111/j.1468-0068.2007.00641.x.\n\n\nBurgess, J. A., and I. L. Humberstone. 1987. “Natural Deduction Rules for a Logic of Vagueness.” Erkenntnis 27 (2): 197–229. https://doi.org/10.1007/bf00175369.\n\n\nBurgess, John. 2001. “Vagueness, Epistemicism and Response-Dependence.” Australasian Journal of Philosophy 79 (4): 507–24. https://doi.org/10.1080/713659306.\n\n\nDummett, Michael. 1959. “Truth.” Proceedings of the Aristotelian Society 59 (1): 141–62. https://doi.org/10.1093/aristotelian/59.1.141.\n\n\nEklund, Matti. 2002. “Inconsistent Languages.” Philosophy and Phenomenological Research 64 (2): 251–75. https://doi.org/10.1111/j.1933-1592.2002.tb00001.x.\n\n\nFara, Delia Graff. 2000. “Shifting Sands: An Interest-Relative Theory of Vagueness.” Philosophical Topics 28 (1): 45–81. https://doi.org/10.5840/philtopics20002816.\n\n\nFine, Kit. 1975. “Vagueness, Truth and Logic.” Synthese 30 (3-4): 265–300. https://doi.org/10.1007/bf00485047.\n\n\nGoldblatt, Robert. 1992. Logics of Time and Computation. Palo Alto: CSLI.\n\n\nGreenough, Patrick. 2003. “Vagueness: A Minimal Theory.” Mind 112 (446): 235–81. https://doi.org/10.1093/mind/112.446.235.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nKeefe, Rosanna. 2000. Theories of Vagueness. Cambridge: Cambridge University Press.\n\n\nLevinson, Stephen. 2000. Presumptive Meanings. Cambridge, MA: MIT Press.\n\n\nLewis, David. 1972. “Psychophysical and Theoretical Identifications.” Australasian Journal of Philosophy 50 (3): 249–58. https://doi.org/10.1080/00048407212341301.\n\n\nMcGee, Vann, and Brian McLaughlin. 1998. “Review of Timothy Williamson’s Vagueness.” Linguistics and Philosophy 21: 221–31.\n\n\nSedivy, Julie, Michael. Tanenhaus, Craig Chambers, and Gregory Carlson. 1999. “Achieving Incremental Semantic Interpretation Through Contextual Representation.” Cognition 71 (2): 109–47. https://doi.org/10.1016/s0010-0277(99)00025-6.\n\n\nSider, Theodore. 2001. “Maximality and Intrinsic Properties.” Philosophy and Phenomenological Research 63 (2): 357–64. https://doi.org/10.1111/j.1933-1592.2001.tb00109.x.\n\n\nSorensen, Roy. 2001. Vagueness and Contradiction. Oxford: Oxford University Press.\n\n\nTappenden, Jamie. 1993. “The Liar and Sorites Paradoxes: Toward a Unified Treatment.” Journal of Philosophy 90 (11): 551–77. https://doi.org/10.2307/2940846.\n\n\nWeatherson, Brian. 2003a. “Epistemicism, Parasites, and Vague Names.” Australasian Journal of Philosophy 81 (2): 276–79. https://doi.org/10.1093/ajp/jag209.\n\n\n———. 2003b. “Review of Rosanna Keefe, Theories of Vagueness.” Philosophy and Phenomenological Research 67: 491–94.\n\n\nWilliamson, Timothy. 1994. Vagueness. Routledge.\n\n\n———. 1995. “Definiteness and Knowability.” Southern Journal of Philosophy 33 (Supp) (S1): 171–91. https://doi.org/10.1111/j.2041-6962.1995.tb00769.x.\n\n\n———. 2004. “Reply to McGee and McLaughlin.” Linguistics and Philosophy 27 (1): 113–22. https://doi.org/10.1023/b:ling.0000010847.78827.d0.\n\n\n\n\n",
    "preview": "https://brian.weatherson.org/images/butterfly.jpg",
    "last_modified": "2021-02-04T09:25:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-chopping-up-gunk/",
    "title": "Chopping up Gunk",
    "description": "We raise an objection to the idea that the world is gunky. Certain plausible sounding supertasks have implausible consequences if the world is made of gunk.",
    "author": [
      {
        "name": "John Hawthorne",
        "url": "https://www.acu.edu.au/research/our-research-institutes/dianoia-institute-of-philosophy/our-people/john-hawthorne"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2004-11-01",
    "categories": [
      "metaphysics"
    ],
    "contents": "\nAtomism, the view that indivisible atoms are the basic building blocks of physical reality, has a distinguished history. But it might not be true. The history of physical science certainly gives many of us pause. Every time some class of objects appeared to be the entities that Newton had described as “solid, massy, hard, impenetrable, movable Particles” out of which “God in the Beginning formed Matter” (Newton 1952, 400), further research revealed that these objects were divisible after all. One might be tempted to see that history as confirming Leibniz’ dismissal of atomism as a “youthful prejudice” .1 Perhaps material objects and their parts are always divisible. There are no extended atoms; nor are there point particles which compose material beings.2\nWhen first presented with this hypothesis, our imaginations are quickly drawn to picturing the process whereby a quantity of such matter – call it gunk -- is chopped up into smaller and smaller pieces. Prima facie, there is nothing problematic here: insofar as such a process continues without end, the initial quantity gets resolves into smaller and smaller chunks with no limit to the diminution. But suppose this process is packed into an hour, as imagined by Jose Bernadete (1964) in his 1964 monograph Infinity:\n\nTake a stick of wood. In 1/2 minute we are to divide the stick into two equal parts. In the next 1/4 minute we are to divide each of the two pieces again into two equal parts. In the next 1/8 minute we are to divide each of the four pieces (for there are now four equal pieces) again into two equal parts, &c. ad infinitum (Bernadete 1964, 184).\n\nIf matter is divisible without end there seems to be no conceptual obstacle to each of the divisions. Yet how are we to imagine the situation at the end of the hour, when the super-task (call it ‘super-cutting’) has been performed on a quantity of gunk?3\nIf there were extended atoms that were never annihilated, it is clear enough what would happen if some super-being undertook to perform super-cutting: the process would grind to a halt when insurmountably hard particles resisted the chopper.\nIf, meanwhile, there were point-sized particles that composed planes that were as thin as a line, it would be natural to picture the limit of the process as a sea of separated slivers, each devoid of finite extent along one dimension. As Benardete, notes, one might then redo super-cutting in order to finally resolve the original stick into a sea of “metaphysical motes” devoid of finite extent in any direction:\n\nAt the end of the minute how many pieces of wood will we have laid out before us? Clearly an infinite number. If the original stick was twenty inches in length, one inch in width, and one inch in depth, what are the dimensions of the metaphysical chips into which the stick has been decomposed? Each chip will be one inch by one inch by one inch by – what? So prodigiously thin must each chip be that its value is certifiably less then any rational (or irrational) quantity. Let us now take up one of the metaphysical chips and decompose it further into an infinite number of metaphysical splinters. In 1/2 minute we shall divide the chip into two equal parts. Each pieces will be one inch by 1/2 inch. In the next 1/4 minute we shall divide each of the two pieces again into two equal parts, yielding four pieces each being one inch by 1/4 inch. In the next 1/8 minute we shall divide each of the four pieces again into two equal parts, &c ad infinitum. At the end of the mute we shall have composed the metaphysical chip into metaphysical splinters. Each splinter will be one inch in length. Let us now take up one of the metaphysical splinters and break it down into an infinite number of metaphysical motes (Bernadete 1964, 184–85)\n\nThe number of cuts made on the stick, the chip and the splinter respectively is aleph zero. The number of chips, splinters and motes left at the end of each cutting process, meanwhile, is aleph one. (Think of numbering each piece in a super-cutting process by an infinite expansion of one’s and zero’s as follows: if it lay on the left of the first cut, the first numeral is a zero, if to the right, the first numeral is a one; it if lay on the left of one of the pieces that was divided at the second round of cutting its second numeral is a zero, if to the right a one; and so on. For each decimal expansion of one’s and zero’s there is a bit and the end with that expansion.). This result is surprising to some, but poses no deep conceptual confusion. With an ontology of chips, splinters and motes available to us, there is a natural description available to us of the limit to the super-cutting processes described.\nBut what to say when gunk is subjected to super-cutting? If each quantity of matter has proper parts, then a sea of metaphysical motes is not, it would seem, an available outcome. In what follows, we unpack this puzzle, providing along the way some a priori physics for gunk-lovers. The problem is only well formed when we make explicit some of the assumptions that drive it. We do so below:\n\nGunk\nEvery quantity of matter has proper parts.\nConservation of Matter:\nNo mereological sum of quantities of matter can be destroyed by any sequence of cuts (though it may be scattered)4.\nOccupation\nIf a quantity of matter occupies a point of space, then there is some volume, extended in all dimensions, to which that point belongs which that quantity of matter occupies.\nSuper-cutting\nThe laws of the world permit super-cutting.\n\nNote that (1), the thesis that every quantity of matter has parts does not, by itself, entail any of the other theses. One might also think that matter sometimes vanishes as a result of some sequence of cuts, denying (2). One might hold that there are metaphysical splinters (and perhaps even chips), denying (3). One might hold that any given quantity of matter does have point sized pieces but that those pieces themselves have parts (the parts being of the same size as the whole in this case), denying (3). One might hold that some pieces of gunk can occupy, say, a spherical region and also a single isolated point at some considerable distance from the spherical region (while maintaining that no part of it merely occupies the point), also denying (3). One might imagine that while always having parts, the parts below a certain thickness are inseparable, denying (4). One might think there is a minimum amount of time that any event of separation takes, also denying (4) and so on.\nIf the gunk hypothesis is maintained, but one or more of (2) to (4) is jettisoned, there is no problem left to solve. For example: If we are allowed to suppose that gunk may vanish, then it will be perfectly consistent to say that nothing is left at the limit of super-cutting. If we are allowed parts that lack finite extent, then it will be consistent to adopt Benardete’s picture of the outcome. And so on. Our puzzle, properly formulated is: What would happen if super-cutting occurred in a world where (1) to (4) are true?\nIn order to answer that question, we need to supplement Bernadete’s brief discussion of the super-cutting process. It is not immediately clear from what he says that super-cutting a piece of wood will turn an object into chips, even assuming the wood to be composed of point particles. That is a natural description of the limit of the process, but it is hardly one that is forced upon us by the barebones description of the process that Benardete provides. When we divide the stick into two pieces, and then into four pieces, where are we to put these pieces? Presumably we must ensure that they are separated. If not, it will not be clear that we really have splinters left at the end. If the stick is cut into four, but the four pieces are then stored so closely together that they are not scattered any more, then we will not have four scattered objects after two rounds of cutting. By extension, unless we separate the pieces sufficiently after each round (or at least after sufficiently many of them) then even in a world where matter is composed of point particles, it is not clear that there will be infinitely many chips left at the end. Note in this connection that there are limits as to how far we can separate the objects. In a world where super-cutting produces chips, we could not, from left to right, put a one inch gap between each chip and any other, since there are aleph one chips and not aleph one inches of space along any vector. Nor is it even clear what kind of spacing will do the trick: how we are to keep aleph one chips separated from each other? What we need is a formal model showing how super-cutting is to be performed. Only then can we answer with any precision what would happen were super-cutting to be performed on gunk.\nAssume, for simplicity, that we have a stick that is exactly one inch long. At the first stage, cut the stick into two 1/2 inch long pieces, move the left-hand one 1/4 inch leftwards and the right hand one 1/4 inch rightwards. This can be accomplished in 1/2 second without moving the objects at a speed of faster than 1 inch per second, or accelerating or decelerating the objects at a rate higher than 4 inches per second per second.5 At the second stage, cut each piece into two, and move each of the left-hand pieces 1/16 of an inch leftwards, and each of the right-hand pieces 1/16 of an inch rightwards. So if the original piece occupied the interval [0, 1) on a particular axis, the four pieces will now occupy the intervals: [-5/16, -1/16), [1/16, 5/16), [11/16, 15/16), [17/16, 21/16). (The reason we are using these half-open intervals is to avoid questions about whether the objects that are separated by the cut used to overlap.) This cutting and moving can be accomplished in 1/4 of a second, without any piece attaining a velocity higher than 1/2 inch per second, or an acceleration higher than 4 inches per second per second.\nThe third stage of the cutting is to take each of these four pieces, cut them in two, move the left-hand part of each of the four 1/64 of an inch to the left, and the right-hand part 1/64 of an inch to the right. So the eight pieces now occupy the intervals: [-21/64, -13/64), [-11/64, -3/64), [3/64, 11/64), [13/64, 21/64), [43/64, 51/64), [53/64, 61/64), [67/64, 75/64), [77/64, 85/64). Again, this cutting and moving can be accomplished within 1/8 of a second, without any piece attaining a velocity higher than 1/4 inch per second, or an acceleration higher than 4 inches per second per second.6\nIn general, at stage n, we take the 2n pieces, divide each of them in two, move the left-hand piece 1/22n inches leftward, and the right-hand piece 1/22n inches rightward. This can all be done in 1/2n seconds without any piece attaining a velocity higher than 1/2n-1 inches per second, or an acceleration higher than 4 inches per second per second. So the whole super-cut can be performed in 1 second: the first stage in 1/2 second, the second stage in 1/4 second, the third stage in 1/8 second, and so on. Note, moreover, that the whole super-cut can be performed in a second without the pieces ever moving at any huge velocity. If readers doubted the possibility of super-cutting because they believed it to be a necessary truth that no matters travels at or beyond the speed of light, their doubts were misplaced: no piece of matter in the super-cutting process approaches a superluminous velocity.\nFurther, in this kind of procedure, a quantity of matter that is scattered during the super-cutting process remains scattered during the process. To see this, first consider a particular example. We noted above that at the second stage there were pieces occupying the intervals [-5/16, -1/16) and [1/16, 5/16). Before this, the point 0 had been occupied; at this stage a gap of 1/8 inch around 0 had been opened. This gap keeps being closed at each stage. After the third stage there were pieces occupying the intervals [/64, -3/64), [3/64, 11/64), so the gap is now only 3/32 inch. After the fourth stage, there will be pieces at [-27/256, -11/256), [11/256, 27/256), so the gap is now only 11/128 inch. This process will make the gap ever smaller, but will not lead to its closure. As the process continues, the size of the gap will approach 1/12 of an inch, but never cross that boundary. To see this, note that the size of the gap in inches after stage n (n \\({\\geq}\\) 3) is 1/8 - 1/25 - 1/27 - … - 1/22n. The sum of the series 1/25 + 1/27 + … is 1/24. Hence the gap at stage n is greater than 1/8 - 1/24 = 1/12. So once the pieces around 0 have been separated, they will never be rejoined.\nThis result applies generally to all of the separated pieces in the super-cut. Once a gap is created, parts of pieces from either side of the gap are moved ever closer to the centre of the gap at every subsequent stage of the super-cut. But since we decrease the distance moved by each piece at each stage of the cut, and in particular decrease it by a factor greater than 2, the pieces, once disjointed, will never be united.\nHow is the matter arranged at the end of the super-cut? To answer this question we need to assume that motion is continuous. For each part of the object we can calculate its position function, the function from the length of time the super-cut has been in progress to the position of the part. At least, we can calculate this for all times until the end of the super-cut. With the continuity assumption in place we can infer that its position at the end of the cut is the limiting value of its position function. So we make this assumption.\nWe assumed above that there is a Cartesian axis running along the object; say that a part a covers a point x just in case a occupies some region [y, z), and y \\({\\leq}\\) x and z > x. When we say a occupies [y, z), we do not mean to imply it occupies only that region, just that it occupies at least that region. Assume then that a part a occupies a point x (0 \\({\\leq}\\) x < 1), and that the binary representation of x is 0.x1x2…xn…, where for each xi, xi equals 0 or 1, and for all i, there exists a j > i such that xj equals zero.7 If x1 = 1, then x \\({\\geq}\\) 1/2, so the some part of a, a small part that originally covered x, will be moved rightward at the first stage. It is possible that a itself may be split by the cut, but there will be a small part around x that is not split, and it will move rightward. If x1 = 0, then x < 1/2, so some part of a, a small part that originally covered x, will be moved leftward at the first stage. Indeed, in general some part of a, a small part that originally covered x, will be moved rightward at the n’th stage if xn = 1, and some part of a, a small part that originally covered x, will be moved leftward at the n’th stage if xn = 0.\nUsing the fact that a part gets moved 2-2n inches at stage n, we can infer that after n stages, a small part that originally covered x and has not been split by the cuts will cover the following point after n cuts. \\[x + \\frac{(-1)^{x_1 + 1}}{4} + \\frac{(-1)^{x_2 + 1}}{16} + \\dots + \\frac{(-1)^{x_n + 1}}{2^{2n}}\\]\nAssuming continuity of motion, we can assume that a will end up with a part that eventually covers the following point, which we will call f(x). \\[f(x) = x + \\sum_{i=1}^{\\infty}\\frac{(-1)^{x_i + 1}}{2^{2i}}\\]\nFrom this, it follows immediately that for all x in [0, 1), f(x) will end up being occupied. It turns out that these are the only points that are occupied at the end of the super-cut.\nAssume that a point y is occupied at the end of the super-cut. We will construct a number c such that y = f(c). Recall that we noted above that whenever two pieces were separated, a gap was created between them that would never be completely filled. While parts of the stick would move closer and closer to the centre of that gap during the super-cut, the middle two-thirds of the gap would never be reoccupied. That interval, that would never be reoccupied, would be liberated. The interval [1/3, 2/3) is liberated at the first stage, the intervals [-1/24, 1/24) and [23/24, 25/24) are liberated at the second stage, the intervals [-37/192, -35/192), [35/192, 37/192), [155/192, 157/192) and [227/192, 229/192) are liberated at the third stage, and so on. If y is occupied, then y must not be in any liberated interval. Therefore it must be either to the left or to the right of any interval that is liberated.\nLet c1 equal 0 if y is to the left of the first liberated interval, [1/3, 2/3), and 1 otherwise. Given the value of c1, it is already determined which side y is of one of the intervals liberated at the second stage. If y is to the left of [1/3, 2/3), for example, then it is to the left of [23/24, 25/24). But the value of c1 does not determine which side y is of the other interval. Let c2 equal 0 if y is to the left of that interval, and 1 otherwise. The values of c1 and c2 determine which side y is of three of the four intervals liberated at the fourth stage, but leave open which side it is of one of these four. Let c3 equal 0 if y is to the left of that interval, 1 otherwise. If we repeat this procedure for all stages, we will get values of ci for all i. Let c be the number whose binary expansion is 0.c1c2…cn…. It follows that y = f(c). The reason once it is determined which side y is of each of the liberated intervals, y has been determined to fall in an interval that is exactly one point wide, and f(c) is in that interval, so f(c) must equal y. So y is occupied iff for some x, y = f(x). Say S = {y: \\({\\exists}\\)x (y = f(x))}; the conclusion is that all and only the points in S are occupied.\nCould a piece of gunk occupy the points in S? Not given the assumptions we have made so far. S has two properties that might not seem consistent at first glance. It is dense in the sense that for any point y in S, and any distance \\({\\delta}\\), there is another point z in S such that y - z < \\({\\delta}\\). But it is disconnected in the sense that for any two points y and z in S, there is an extended region r between y and z that is wholly unoccupied. The proofs of density and disconnectedness are given in the appendix.\nGiven (3), disconnectedness is inconsistent with gunk occupying S. If a material object occupies S, it must occupy the points in S. Let y be any one of these points. By (3), S must occupy some extended region containing y, say, [y1, y2). Two cases to consider. First case: y1 < y. If [y1, y2) \\({\\subset}\\) S, then y1 and y are in S, and so are all the points in between them. Since the object occupies S, it follows that these points are occupied. Hence there is no extended region between y1 and y that is wholly unoccupied, which is inconsistent with disconnectedness. Second case: y1 = y. Again, [y1, y2) \\({\\subset}\\) S, and since this interval is non-empty, y2 > y1. Hence (y1 + y2) / 2 is greater than y1, and all the points between it and y1 are occupied. This is also inconsistent with disconnectedness. So given (3), no material object could occupy S.\nIn summary, (1) through (4) plus continuity of motion cannot be true together. From (1), (2), and (4), we inferred that our super-cutting process was possible, and that it would not destroy any quantity of matter (though of course it would scatter it). Assuming continuity of motion, we calculated which points would be occupied after the super-cut. By (3) we concluded that no piece of gunk could occupy those points, or indeed any subset of them, yielding an inconsistent result. Suppose that the continuity of motion thesis is dropped. We can then maintain (1) to (4) with consistency. One should note, however, that a world where (1) to (4) holds would be a strange world indeed: if super-cutting is performed, the pieces of gunk would have to jump location at the limit. The gunk cannot occupy S: but in order to occupy a different set of points, various quantities of matter would have to jump position at the limit.\nIf one believes in gunk one has a choice: Abandon one or more of (2) to (4) or else deny that it is nomologically necessary that motion be continuous. Which assumption should be dropped? We leave it to the gunk lover to select the most tolerable package. The choice for the gunk lover is a little unenviable. Those who are attracted to the view that the actual world is gunky are very much wedded to (1) and (3). When philosophers take seriously the idea that that matter has parts all the way down8, they do not imagine conjoining that thesis with point sized parts, or else immaterial parts9, or else quantities of matter that are as thin as a plane, and so on. With a commitment to (1) and (3) in place, super-cutting will be loaded with physical significance. Accept that the laws of nature permits super-cutting and one will be committed to either denying the conservation of matter or the continuity of motion.\nAppendix\nTo prove density, note that if y is occupied, there is a point x with binary representation 0.x1x2… such that y = f(x). For any positive \\({\\delta}\\), there is an integer n such that \\({\\delta}\\) > 2-2n. Let v be the number represented by 0.x1x2…xnxn+1\\(^\\prime\\)xn+2xn+3…, where xn+1\\(^\\prime\\) = 1 iff xn+1 = 0, and xn+1\\(^\\prime\\) = 0 otherwise. The difference between f(x) and f(v) will be exactly 2-2n-1. Since f(v) is occupied, and y = f(x), there is an occupied point exactly 2-2n-1 inches from y, so there is a point less than \\({\\delta}\\) inches from y, as required.\nTo prove disconnectedness, let y and z be any two distinct occupied points. So for some distinct v, x, y = f(x) and z = f(v). Say that the binary representation of x is 0.x1x2…, and the binary representation of v is 0.v1v2… Let j be the lowest number such that xj \\({\\neq}\\) vj. (Since x and v are distinct, there must be at least one value j.) Without loss of generality, assume that xj = 0 and vj = 1. (There is no loss of generality because we are just trying to show that between any two occupied points there is a gap, so it does not matter which of the two points is the rightward one.) Let k be the number with binary representation 0.x1x2…xj1, and let l2 be f(k). Finally, define l1 by the following equation: \\[l_i = \\sum_{i=1}^j \\frac{(-1)^{x_i+1}}{2^{2i}} + \\sum_{i = j+1}^\\infty \\frac{1}{2^{2i}}\\]\nIt is easy enough to see that f(x), that is y, must be less that l1. For l1 is the value that f(x) would take were every digit in the binary expansion of x after j be 1. But by definition there must be some value j\\(^\\prime\\) > j such that xj\\(^\\prime\\) = 0. From this it follows that: \\[\\sum_{i = j+1}^\\infty \\frac{1}{2^{2i}} > \\sum_{i = j+1}^\\infty \\frac{(-1)^{x_i+1}}{2^{2i}}\\]\nAnd from that it follows that l1 > f(x). Indeed, by similar reasoning, it follows that for all u < k, f(u) < l1. Since f is monotone increasing, it also follows that for all u \\({\\geq}\\) k, f(u) \\({\\geq}\\) l2. And from those facts, it follows that there does not exist a u such that f(u) \\({\\in}\\) [l1, l2). And since y < l1 < l2 \\({\\leq}\\) z, this implies that there is an extended unoccupied region between y and z, as required.\n\n\nBernadete, Jose. 1964. Infinity: An Essay in Metaphysics. Oxford: Clarendon Press.\n\n\nLeibniz, Gottfried Wilhelm. 1998. Philosophical Texts. Translated by R. S. Woolhouse and Richard Francks. Oxford: Oxford University Press.\n\n\nNewton, Isaac. 1952. Opticks. New York: Dover Press.\n\n\nZimmerman, Dean. 1996. “Could Extended Objects Be Made Out of Simple Parts: An Argument for Atomless Gunk.” Philosophy and Phenomenological Research 56 (1): 1–29. https://doi.org/10.2307/2108463.\n\n\nSee ‘Nature Itself’ in (Leibniz 1998, 220).↩︎\nCf Leibniz: ‘I hold that matter is essentially an aggregate, and consequently that it always has actual parts,’ in ‘Third Explanation of The New System,’ (Leibniz 1998, 193).↩︎\nWhat is important, of course, is that the sequence of separations occur: it does not matter whether some kind of super-sharp knife is responsible for them. In what follows, descriptions of cutting sequences can be replaced without loss of content by descriptions of separation sequences, leaving it open whether repulsive forces or chance events or knives or … are responsible for the separation sequence.↩︎\nThe ‘can’ here is one of nomological possibility.↩︎\nThe idea is that in the first quarter second we accelerate the object at 4 inches per second per second. This will raise its velocity to 1 inch per second, and move the object 1/8 of an inch. In the second quarter second we decelerate it at 4 inches per second per second, so its velocity ends up at zero, and it ends up having moved 1/4 of an inch.↩︎\nNote that, interestingly, if we moved the pieces 1/2 inch after the first round, 1/4 inch after the second round, 1/8 inch after the third round and so on then at the limit, each left and right edge that was once attached will have moved back together again. The process we have chosen preserves separation in a way that the aforementioned process does not.↩︎\nThe final condition is important to rule out numbers having two possible representations. For example, we have to choose whether the representation of 1/2 should be 0.1000… or 0.0111…, and we somewhat arbitrarily, choose the former.↩︎\nSee, for example, (Zimmerman 1996).↩︎\nLeibniz, with his monads, is an exception of course. No contemporary gunk lover wants a monadology, however.\n\n↩︎\n",
    "preview": "posts/2021-01-05-chopping-up-gunk/knife.jpg",
    "last_modified": "2021-02-04T22:20:02-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-morality-fiction-and-possibility/",
    "title": "Morality, Fiction and Possibility",
    "description": "Authors have a lot of leeway with regard to what they can make true in their story. In general, if the author says that p is true in the fiction we're reading, we believe that p is true in that fiction. And if we're playing along with the fictional game, we imagine that, along with everything else in the story, p is true. But there are exceptions to these general principles. Many authors, most notably Kendall Walton and Tamar Szabó Gendler, have discussed apparent counterexamples when p is \"morally deviant\". Many other statements that are conceptually impossible also seem to be counterexamples. In this paper I do four things. I survey the range of counterexamples, or at least putative counterexamples, to the principles. Then I look to explanations of the counterexamples. I argue, following Gendler, that the explanation cannot simply be that morally deviant claims are impossible. I argue that the distinctive attitudes we have towards moral propositions cannot explain the counterexamples, since some of the examples don't involve moral concepts. And I put forward a proposed explanation that turns on the role of 'higher-level concepts', concepts that if they are satisfied are satisfied in virtue of more fundamental facts about the world, in fiction, and in imagination.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2004-11-01",
    "categories": [
      "ethics",
      "fiction",
      "metaphysics"
    ],
    "contents": "\n\nContents\nFour Puzzles\nThe Range of the Puzzles\nAn Impossible Solution\nSome Ethical Solutions\nGrok\nVirtue\nMore Solutions\nThe Phenomenological Puzzle\nThe Imaginative Puzzle\nTwo Thoughts Too Many?\n\nWhy Virtue Matters\nTwo Hard Cases\n\nFour Puzzles\nSeveral things go wrong in the following story.\n\nPublished in Philosophers’ Imprint 4:3.\n\nDeath on a Freeway\nJack and Jill were arguing again. This was not in itself unusual, but this time they were standing in the fast lane of I-95 having their argument. This was causing traffic to bank up a bit. It wasn’t significantly worse than normally happened around Providence, not that you could have told that from the reactions of passing motorists. They were convinced that Jack and Jill, and not the volume of traffic, were the primary causes of the slowdown. They all forgot how bad traffic normally is along there. When Craig saw that the cause of the bankup had been Jack and Jill, he took his gun out of the glovebox and shot them. People then started driving over their bodies, and while the new speed hump caused some people to slow down a bit, mostly traffic returned to its normal speed. So Craig did the right thing, because Jack and Jill should have taken their argument somewhere else where they wouldn’t get in anyone’s way.\n\n\nPicture via Creative Commons.\nThe last sentence raises a few related puzzles. Intuitively, it is not true, even in the story, that Craig’s murder was morally justified. What the narrator tells us here is just false. That should be a little surprising. We’re being told a story, after all, so the storyteller should be an authority on what’s true in it. Here we hearers get to rule on which moral claims are true and false, not the author. But usually the author gets to say what’s what. The action takes place in Providence, on Highway 95, just because the author says so. And we don’t reject those claims in the story just because no such murder has ever taken place on Highway 95. False claims can generally be true in stories. Normally, the author’s say so is enough to make it so, at least in the story, even if what is said is really false. The first puzzle, the alethic puzzle, is why authorial authority breaks down in cases like Death on the Freeway. Why can’t the author just make sentences like the last sentence in Death true in the story by saying they are true? At this stage I won’t try and give a more precise characterisation of which features of Death lead to the break down of authorial authority, for that will be at issue below.\n\nI’ve spoken to practically everyone I know about the issues here, and a full list of thanks for useful advice, suggestions, recommendations, criticisms, counterexamples and encouragement would double the size of the paper. If I thank philosophy departments rather than all the individuals in them it might cut the size a little, so thanks to the departments at Brown, UC Davis, Melbourne, MIT and Monash. Thanks also to Kendall Walton, Tamar Gendler and two referees for Philosophers’ Imprint. The most useful assistance came from Wolfgang Schwarz and especially Tyler Doggett, without whose advice this could never have been written, and to George Wilson, who prevented me from (keeping on) making a serious error of over-generalisation.\nThe second puzzle concerns the relation between fiction and imagination. Following Kendall Walton (1990), it is common to construe fictional works as invitations to imagine. The author requests, or suggests, that we imagine a certain world. In Death we can follow along with the author for most of the story. We can imagine an argument taking place in peak hour on Highway 95. We can imagine this frustrating the other drivers. And we can imagine one of those drivers retaliating with a loaded gun. What we cannot, or at least do not, imagine is that this retaliation is morally justified. There is a limit to our imaginative ability here. We refuse, fairly systematically, to play along with the author here. Call this the imaginative puzzle. Why don’t we play along in cases like Death? Again, I won’t say for now which cases are like Death.\nThe third puzzle concerns the phenomenology of Death and stories like it. The final sentence is striking, jarring in a way that the earlier sentences are not. Presumably this is closely related to the earlier puzzles, though I’ll argue below that the cases that generate this peculiar reaction are not identical with cases that generate alethic or imaginative puzzles. So call this the phenomenological puzzle.\nFinally, there is a puzzle that David Hume (1757) first noticed. Hume suggested that artistic works that include morally deviant claims, moral claims that wouldn’t be true were the descriptive aspects of the story true, are thereby aesthetically compromised. Why is this so? Call that the aesthetic puzzle. I will have nothing to say about that puzzle here, though hopefully what I have to say about the other puzzles will assist in solving it.\nI’m going to call sentences that raise the first three puzzles puzzling sentences. Eventually I’ll look at the small differences between those three puzzles, but for now we’ll focus on what they have in common. The puzzles, especially the imaginative puzzle, have become quite a focus of debate in recent years. The aesthetic puzzle is raised by David Hume (1757), and is discussed by Kendall Walton (1994) and Richard Moran (1995). Walton and Moran also discuss the imaginative and alethic puzzles, and they are the focus of attention in recent work by Tamar Szabó Gendler (2000), Gregory Currie (2002) and Stephen Yablo (2002). My solution to the puzzles is best thought of as a development of some of Walton’s ‘sketchy story’ (to use his description). Gendler suggests one way to develop Walton’s views, and shows it leads to an unacceptable solution, because it leads to mistaken predictions. I will argue that there are more modest developments of Walton’s views that don’t lead to so many predictions, and in particular don’t lead to mistaken predictions, but which still say enough to solve the puzzles.\nThe Range of the Puzzles\nAs Walton and Yablo note, the puzzle does not only arise in connection with thin moral concepts. But it has not been appreciated how widespread the puzzle is, and getting a sense of this helps us narrow the range of possible solutions.\nSentences in stories attributing thick moral concepts can be puzzling. If my prose retelling of Macbeth included the line “Then the cowardly Macduff called on the brave Macbeth to fight him face to face,” the reader would not accept that in the story Macduff was a coward. If my retelling of Hamlet frequently described the young prince as decisive, the reader would struggle to go along with me imaginatively. Try imagining Hamlet doing exactly what he does, and saying exactly what he says, and thinking what he thinks, but always decisively. For an actual example, it’s easy to find the first line in Bob Dylan’s Ballad of Frankie Lee and Judas Priest, that the titular characters ‘were the best of friends’ puzzling in the context of how Frankie Lee treats Judas Priest later in the song. It isn’t too surprising that the puzzle extends to the thick moral concepts, and Walton at least doesn’t even regard these as a separate category.\nMore interestingly, any kind of evaluative sentence can be puzzling. Walton and Yablo both discuss sentences attributing aesthetic properties. (Yablo 2002, 485) suggests that a story in which the author talks about the sublime beauty of a monster truck rally, while complaining about the lack of aesthetic value in sunsets, is in most respects like our morally deviant story. The salient aesthetic claims will be puzzling. Note that we are able to imagine a community that prefers the sight of a ‘blood bath death match of doom’ (to use Yablo’s evocative description) to sunsets over Sydney Harbour and it could certainly be true in a fiction that such attitudes were commonplace. But that does not imply that those people could be right in thinking the trucks are more beautiful. (Walton 1994, 43–44) notes that sentences describing jokes that are actually unfunny as being funny will be puzzling. We get to decide what is funny, not the author.\nWalton and Yablo’s point here can be extended to epistemic evaluations. Again it isn’t too hard to find puzzling examples when we look at attributions of rationality or irrationality.\n\nAlien Robbery\nSam saw his friend Lee Remnick rushing out of a bank carrying in one hand a large bag with money falling out of the top and in the other hand a sawn-off shotgun. Lee Remnick recognised Sam across the street and waved with her gun hand, which frightened Sam a little. Sam was a little shocked to see Lee do this, because despite a few childish pranks involving stolen cars, she’d been fairly law abiding. So Sam decided that it wasn’t Lee, but really a shape-shifting alien that looked like Lee, that robbed the bank. Although shape-shifting aliens didn’t exist, and until that moment Sam had no evidence that they did, this was a rational belief. False, but rational.\n\nThe last two sentences of Alien Robbery are fairly clearly puzzling.\nSo far all of our examples have involved normative concepts, so one might think the solution to the puzzle will have something to do with the distinctive nature of normative concepts, or with their distinctive role in fiction. Indeed, Gendler’s and Currie’s solutions have just this feature. But sentences that seem somewhat removed from the realm of the normative can still be puzzling. (It is of course contentious just where the normative/non-normative barrier lies. Most of the following cases will be regarded as involving normative concepts by at least some philosophers. But I think few people will hold that all of the following cases involve normative concepts.)\nAttributions of mental states can, in principle, be puzzling. If I retell Romeo and Juliet, and in this say ‘Although he believed he loved Juliet, and acted as if he did, Romeo did not really love Juliet, and actually wanted to humiliate her by getting her to betray her family,’ that would I think be puzzling. This example is odd, because it is not obviously impossible that Romeo could fail to love Juliet even though he thought he loved her (people are mistaken about this kind of thing all the time) and acted as if he did (especially if he was trying to trick her). But given the full detail of the story, it is impossible to imagine that Romeo thought he had the attitudes towards Juliet he is traditionally thought to have, and he is mistaken about this.\nAttributions of content, either mental content or linguistic content, can be just as puzzling. The second and third sentences in this story are impossible to imagine, and false even in the story.\n\nCats and Dogs\nRhodisland is much like a part of the actual world, but with a surprising difference. Although they use the word ‘cat’ in all the circumstances when we would (i.e. when they want to say something about cats), and the word ‘dog’ in all the circumstances we would, in their language ‘cat’ means dog and ‘dog’ means cat. None of the Rhodislanders are aware of this, so they frequently say false things when asked about cats and dogs. Indeed, no one has ever known that their words had this meaning, and they would probably investigate just how this came to be in some detail, if they knew it were true.\n\nA similar story can be told to demonstrate how claims about mental content can be puzzling. Perhaps these cases still involve the normative. Loving might be thought to entail special obligations and Kripke (1982) has argued that content is normative. But we are clearly moving away from the moral, narrowly construed.\nStephen Yablo recently suggested that certain shape predicates generate imaginative resistance. These predicates are meant to be special categories of a broader category that we’ll discuss further below. Here’s Yablo’s example.\n\nGame Over\nThey flopped down beneath the giant maple. One more item to find, and yet the game seemed lost. Hang on, Sally said. It’s staring us in the face. This is a maple tree we’re under. She grabbed a five-fingered leaf. Here was the oval they needed! They ran off to claim their prize. (Yablo 2002, 485, title added)\n\nThere’s a potential complication in this story in that one might think that it’s metaphysically impossible that maple trees have ovular leaves. That’s not what is meant to be resisted, and I don’t think is resisted. What is resisted is that maple leaves have their distinctive five-fingered look, that the shape of the leaf Sally collects is like that (imagine I demonstrate a maple leaf here) and that its shape be an oval.\nFewer people may care about the next class of cases, or have clear intuitions about them, but if one has firm ontological beliefs, then deviant ontological claims can be puzzling. I’m a universalist about mereology, at least with respect to ordinary concrete things, so I find many of the claims in this story puzzling.\n\nWiggins’ World\nThe Hogwarts Express was a very special train. It had no parts at all. Although you’d be tempted to say that it had carriages, an engine, seats, wheels, windows and so on, it really was a mereological atom. And it certainly had no temporal parts - it wholly was wherever and whenever it was. Even more surprisingly, it did not enter into fusions, so when the Hogwarts Local was linked to it for the first few miles out of Kings Cross, there was no one object that carried all the students through north London.\n\nI think that even in fictions any two concrete objects have a fusion. So the Hogwarts Express and the Hogwarts Local have a fusion, and when it is a connected object it is commonly called a train. I know how to describe a situation where they have no fusion (I did so just above) but I have no idea how to imagine it, or make it true in a story.\nMore generally, there are all sorts of puzzling sentences involving claims about constitution. These I think are the best guide to a solution to the puzzle.\n\nA Quixotic Victory\n–What think you of my redecorating Sancho?\n–It’s rather sparse, said Sancho.\n–Sparse. Indeed it is sparse. Just a television and an armchair.\n–Where are they, Señor Quixote? asked Sancho. All I see are a knife and fork on the floor, about six feet from each other. A sparse apartment for a sparse mind. He said the last sentence under his breath so Quixote would not hear him.\n–They might look like a knife and fork, but they are a television and an armchair, replied Quixote.\n–They look just like the knife and fork I have in my pocket, said Sancho, and he moved as to put his knife and fork besides the objects on Quixote’s floor.\n–Please don’t do that, said Quixote, for I may be unable to tell your knife and fork from my television and armchair.\n–But if you can’t tell them apart from a knife and fork, how could they be a television and an armchair?\n–Do you really think being a television is an observational property? asked Quixote with a grin.\n–Maybe not. OK then, how do you change the channels? asked Sancho.\n–There’s a remote.\n–Where? Is it that floorboard?\n–No, it’s at the repair shop, admitted Quixote.\n–I give up, said Sancho.\nSancho was right to give up. Despite their odd appearance, Quixote’s items of furniture really were a television and an armchair. This was the first time in months Quixote had won an argument with Sancho.\n\nQuixote is quite right that whether something is a television is not determined entirely by how it looks. A television could be indistinguishable from a non-television. Nonetheless, something indistinguishable from a knife is not a television. Not in this world, and not in the world of Victory either, whatever the author says. For whether something is a television is determined at least in part by how it looks, and while it is impossible to provide a non-circular constraint on how a television may look, it may not look like a common knife.\nIn general, if whether or not something is an F is determined in part by ‘lower-level’ features, such as the shape and organisation of its parts, and the story specifies that the lower-level features are incompatible with the object being an F, it is not an F in the fiction. Suitably generalised and qualified, I think this is the explanation of all of the above categories. To understand better what the generalisations and qualifications must be, we need to look at some cases that aren’t like Death, and some alternative explanations of what is going on in Death.\nSentences that are intentional errors on the part of storytellers are not puzzling in our sense. We will use real examples for the next few pages, starting with the opening line of Joyce’s most famous short story.\n\nThe Dead\nLily, the caretaker’s daughter, was literally run off her feet.(Joyce 1914/2000, 138)\n\nIt isn’t true that Lily is literally run off her feet. She is run off her feet by the incoming guests, and if you asked her she may well say she was literally run off her feet, but this would reveal as much about her lack of linguistic care as about her demanding routine. Is this a case where the author loses authority over what’s true in the story? No, we are not meant to read the sentence as being true in the story, but being a faithful report of what Lily (in the story) might say to herself. In practice it’s incredibly difficult to tell just when the author intends a sentence to be true in the story, as opposed to being a report of some character’s view of what is true. (See Holton (1997) for an illustration of the complications this can cause.) But since we are operating in theory here, we will assume that problem solved. The alethic puzzle only arises when it is clear that the author intends that p is true in her story, but we think p is not true. The imaginative puzzle only arises when the author invites us to imagine p, but we can not, or at least do not. Since Joyce does not intend this sentence to be true in The Dead, nor invites us to imagine it being true, neither puzzle arises. What happens to the phenomenological puzzle in cases like these is a little more interesting, and I’ll return to that in .\nJust as intentional errors are not puzzling, careless errors are not puzzling. Writing a full length novel is a perilous business. Things can go wrong. Words can be miswritten, mistyped or misprinted at several different stages. Sometimes the errors are easily detectable, sometimes they are not, especially when they concern names. In one of the drafts of Ulysses, Joyce managed to write “Connolly Norman” in place of “Conolly Norman.” Had that draft being used for the canonical printing of the work, it would be tempting to say that we had another alethic puzzle. For the character named here is clearly the Superintendent of the Richmond District Lunatic Asylum, and his name had no double-‘n,’ so in the story there is no double-‘n’ either.1\nHere we do have an instance where what is true in the story differs from the what is written in the text. But this is not a particularly interesting deviation. To avoid arcane discussions of typographical errors, we will that in every case we possess an ideal version of the text, and are comparing it with the author’s intentions. Slip-ups that would be detected by a careful proof-reader, whether they reveal an unintended divergence between word and world, as here, or between various parts of the text, as would happen if Dr Norman were not named after a real person but had his name spelled differently in parts of the text, will be ignored.2\nNote two ways in which the puzzles as I have stated them are narrower than they first appear. First, I am only considering puzzles that arise from a particular sentence in the story, intentionally presented in the voice of an authoritative narrator. We could try and generalise, asking why it is that we sometimes (but not always) question the moral claims that are intended to be tacit in a work of fiction. For instance, we might hold that for some Shakespearean plays there are moral propositions that Shakespeare intended to be true in the play, but which are not in fact true. Such cases are interesting, but to keep the problem of manageable proportions I won’t explicitly discuss them here. (I believe the solution I offer here generalises to those cases, but I won’t defend that claim here.) Second, all the stories I have discussed are either paragraph-long examples, or relatively detachable parts of longer stories. For all I’ve said so far, the puzzle may be restricted to such cases. In particular, it might be the case that a suitably talented author could make it true in a story that killing people for holding up traffic is morally praiseworthy, or that a television is phenomenally and functionally indistinguishable from a knife. What we’ve seen so far is just that an author cannot make these things true in a story simply by saying they are true.3 I leave open the question of whether a more subtle approach could make those things true in a fiction. Similarly, I leave it open whether a more detailed invitation to imagine that these things are true would be accepted. All we have seen so far is that simple direct invitations to imagine these things are rejected, and it feels like we could not accept them.\nAn Impossible Solution\nHere’s a natural solution to the puzzles, one that you may have been waiting for me to discuss. The alethic puzzle arises because only propositions that are possibly true can be true in a story, or can be imagined. The latter claim rests on the hypothesis that we can imagine only what is possible, and that we resist imagining what is impossible.\nThis solution assumes that it is impossible that killing people for holding up freeway traffic is the right thing to do. Given enough background assumptions, that is plausible. It is plausible, that is, that the moral facts supervene on the non-moral facts. And the supervenience principle here is quite a strong one - in every possible world where the descriptive facts are thus and so, the moral facts are the same way.4 If we assume the relevant concept of impossibility is truth in no possible worlds, we get the nice result that the moral claims at the core of the problem could not possibly be true.\nSeveral authors have discussed solutions around this area. Kendall Walton (1994) can easily be read as endorsing this solution, though Walton’s discussion is rather tentative. Tamar Szabó Gendler rejects the theory, but thinks it is the most natural idea, and spends much of her paper arguing against this solution. As those authors, and Gregory Currie (2002), note, the solution needs to be tidied up a little before it will work for the phenomenological and imaginative puzzles. (It is less clear whether the tidying matters to the alethic puzzle.) For one thing, there is no felt asymmetry between a story containing, “Alex proved the twin primes theorem,” and one containing, “Alex found the largest pair of twin primes,” even though one of them is impossible. Since we don’t know which it is, the impossibility of the false one cannot help us here. So the theory must be that it is believed impossibilities that matter, for determining what we can imagine, not just any old impossibilities. Presumably impossibilities that are not salient will also not prevent imagination.\nEven thus qualified, the solution still overgenerates, as Gendler noted. There are stories that are not puzzling in any way that contain known salient impossibilities. Gendler suggests three kinds of cases of this kind, of which I think only the third clearly works. The first kind of case is where we have direct contradictions true in the story. Gendler suggests that her Tower of Goldbach story, where seven plus five both does and does not equal twelve, is not puzzling. Graham Priest (1997) makes a similar point with a story, Sylvan’s Box, involving an empty box with a small statue in one corner. These are clear cases of known, salient impossibility, but arguably are not puzzling in any respect. (There is a distinction between the puzzles though. It is very plausible to say that it’s true in Priest’s story that there’s an empty box with a small statue in one corner. It is less plausible to say we really can imagine such a situation.) Opinion about such cases tends to be fairly sharply divided, and it is not good I suspect to rest too much weight on them one way or the other.\nThe second kind of case Gendler suggests is where we have a distinctively metaphysical impossibility, such as a singing snowman or a talking playing card. Similar cases as discussed by Alex Byrne (1993) who takes them to raise problems for David Lewis’s (1978) subjunctive conditionals account of truth in fiction. If we believe a strong enough kind of essentialism, then these will be impossible, but they clearly do not generate puzzling stories. For a quick proof of this, note that Alice in Wonderland is not puzzling, but several essentialist theses are violated there. It is true in Alice in Wonderland, for example, that playing cards plant rose trees.\nBut these examples don’t strike me as particularly convincing either. For one thing, the essentialism assumed here may be wrong. For another, the essentialism might not be both salient and believed to be right, which is what is needed. And most importantly, we can easily reinterpret what the authors are saying in order to be make the story possibly true. We can assume, for example, that the rosebush planting playing cards are not playing cards as we know them, but roughly human-shaped beings with playing cards for torsos. Gendler and Byrne each say that this is to misinterpret the author, but I’m not sure this is true. As some evidence, note that the authorised illustrations in Alice tend to support the reinterpretations.5\nGendler’s third case is better. There are science fiction stories, especially time travel stories, that are clearly impossible but which do not generate resistance. Here’s two such stories, the first lightly modified from a surprisingly popular movie, and the second lifted straight from a very popular source.\n\nBack to the Future\\(^\\prime\\)\nMarty McFly unintentionally travelled back in time to escape some marauding Libyan terrorists. In doing so he prevented the chance meeting which had, in the timeline that had been, caused his father and mother to start dating. Without that event, his mother saw no reason to date the unattractive, boring nerdy kid who had been, in a history that no longer is, Marty’s father. So Marty never came into existence. This was really a neat trick on Marty’s part, though he was of course no longer around to appreciate it. Some people manage to remove themselves from the future of the world by foolish actions involving cars. Marty managed to remove himself from the past as well.\nThe Restaurant at the End of the Universe\nThe Restaurant at the End of the Universe is one of the most extraordinary ventures in the entire history of catering.\nIt is built on the fragmented remains of an eventually ruined planet which is enclosed in a vast time bubble and projected forward in time to the precise moment of the End of the Universe.\nThis is, many would say, impossible.\n…\nYou can visit it as many times as you like … and be sure of never meeting yourself, because of the embarrassment this usually causes.\nThis, even if the rest were true, which it isnt, is patently impossible, say the doubters.\nAll you have to do is deposit one penny in a savings account in your own era, and when you arrive at the End of Time the operation of compound interest means that the fabulous cost of your meal has been paid for.\nThis, many claim, is not merely impossible but clearly insane. (Adams 1980, 213–14)\n\nNeither of these are puzzling. Perhaps it’s hard to imagine the last couple of sentences of the McFly story, but everything the respective authors say is true in their stories. So the impossibility theory cannot be right, because it overgenerates, just as Gendler said.\nRecently Kathleen Stock (2003) has argued that one of the assumptions that Gendler makes, specifically that it isn’t true that “a judgement of conceptual impossibility renders a scenario unimaginable” (Gendler 2000, 66) is false. Even if Stock is right, this doesn’t threaten the kind of response that I have (following Gendler) offered to the puzzles. But actually there are a few reasons to doubt Stock’s reply. I’ll discuss these points in order.\nIt isn’t entirely clear from Stock’s discussion what she is taking a conceptual impossibility to be. I think it is a proposition of the form Some F is a G (or That F is a G, or something of this sort) where it is constitutive of being an F that the F is not a G. There is no positive characterisation of conceptual impossibility in Stock’s paper, but it is clearly meant to be something stronger than mere impossibility, or a priori falsehood. In any case, most of the core arguments turn on worries about allegedly deploying a concept while refusing to draw inferences that are constitutive of that concept, so the kind of definition I’ve offered above seems to be on the right track.\nNow if this is the case then Stock has no objection to the imaginability of the two stories I offered that involve known and salient impossibilities. For neither of these stories includes a conceptual impossibility in this sense. So even if conceptual impossibilities cannot be imagined, some impossibilities can be imagined. (And at this point what holds for imagination also holds for truth in fiction.)\nWhile this suffices as a response to the particular claims Stock makes, it might be thought it undercuts the objection I have made to the impossible solution. For it might be thought that what is wrong with the puzzling sentences just is that they represent conceptual impossibilities in this sense, and we have no argument that these can be imagined, or true in fiction. This is not too far removed from the actual solution I will offer, so it is a serious worry. The problem with this line is that not all of our puzzles are conceptual impossibilities. It isn’t constitutive of being a television that a thing is phenomenally or functionally distinguishable from a knife, but the claim in Victory that some television is not phenomenally or functionally distinguishable from a knife is puzzling. Even in our core cases, of morally deviant claims in fiction, there need not be any conceptual impossibilities. As R. M. Hare (1951) pointed out long ago, people with very different moral beliefs could have in common the concept GOOD. Arguably, someone who thinks that what Craig does in Death is good is morally confused, not conceptually confused. So whether Gendler or Stock is right about the imaginability of conceptual impossibility is neither here nor there with respect to these puzzles.\nHaving said that, there are some reasons to doubt Stock’s argument. One of her moves is to argue that we couldn’t imagine conceptual impossibilities because we can’t believe conceptual impossibilities. But as Sorensen (2001) persuasively argues, we can believe conceptual impossibilities. One of Sorensen’s arguments, lightly modified, helps us respond to another of Stock’s arguments. Stock notes, rightly, that we shouldn’t take the fact that it seems we can imagine impossibilities to be conclusive evidence we can do so. After all, we are wrong about whether things are as they seem all the time. But this might be a special case. I think that if it seems to be the case that p then we can imagine that p. And Stock agrees it seems to be the case that we can imagine conceptual impossibilities. So we can imagine that we can imagine conceptual impossibilities. Hence it can’t be a conceptual impossibility that we can imagine at least one conceptual impossibility. This doesn’t tell against the claim that it is some other kind of impossibility, though as we’ll see Stock’s main argument rests on considerations about the conceptual structure of imagination, so it isn’t clear how she could argue for this.\nThe main argument Stock offers is that no account of how concepts work are compatible with our imagining conceptual impossibilities. Her argument that atomist theories of concepts (as in Fodor (1998)) are incompatible with imagining conceptual impossibilities isn’t that persuasive. She writes that “clearly it is not the case that imagining”the cow jumped over the moon\" stands in a lawful relation to the property of being a cow (let alone the property of \\[being\\] a cow jumping over the moon. Imagining by its very nature is resistant to any attempt to incorporate it into an externalist theory of content\" (2003, 114). But this isn’t clear at all. When I imagine going out drinking with Bill Clinton there is, indeed there must be, some kind of causal chain running back from my imagining to Bill Clinton himself. If there was not, I’d at most be imagining going out drinking with a guy who looks a lot like Bill Clinton. Perhaps it isn’t as clear, but when I imagine that a cow (and not just a zebra disguised to look like a cow) is jumping over the moon it’s nomologically necessary that there’s a causal chain of the right kind stretching back to actual cows. And it’s arguable that the concept I deploy in imagining that a cow (a real cow) is jumping over the moon just is the concept whose content is fixed by the lawful connections between various cows and my (initial) deployment of it. So I don’t see why a conceptual atomist should find this kind of argument convincing.\nStock’s response to Gendler was presented at a conference on Imagination and the Arts at Leeds in 2001, and at the same conference Derek Matravers (2003) offered an alternative solution to the alethic puzzle. Although it does not rest on claims about impossibility, it also suffers from an overgeneration problem. Matravers suggests that in at least some fictions, we treat the text as a report by a (fictional) narrator concerning what is going on in a faraway land. Now in reality when we hear reports from generally trustworthy foreign correspondents, we are likely to believe their descriptive claims about the facts on the ground. Since they have travelled to the lands in question, and we have not, the correspondent is epistemologically privileged with respect to those facts on the ground. But when the correspondent makes moral evaluations of those facts, she is not in a privileged position, so we don’t just take her claims as the final word. Matravers suggests there are analogous limits to how far we trust a fictional narrator.\nThe problem with this approach is that there are several salient disanalogies between the position of the correspondent and the fictional narrator. The following case, which I heard about from Mark Liberman, illustrates this nicely. On March 5, 2004, the BBC reported that children in a nursery in England had found a frog with three heads and six legs. Many people, including Professor Liberman, were sceptical, notwithstanding the fact that the BBC was actually in England and Professor Liberman was not. The epistemological privilege generated by proximity doesn’t extend to implausible claims about three-headed frogs. The obvious disanalogy is that if a fictional narrator said that there was a three-headed six-legged frog in the children’s nursery then other things being equal we would infer it is true in the fiction that there was indeed a three-headed six-legged frog in the children’s nursery.6 So there isn’t an easy analogy between when we trust foreign correspondents and fictional narrators. Now we need an explanation of why the analogy does hold when either party makes morally deviant claims, even though it doesn’t when they both make biologically deviant claims. But it doesn’t seem any easier to say why the analogy holds then than it is to solve the original puzzle.\nTwo other quick points about Matravers’s solution. It’s going to be a little delicate to extend this solution to all the cases I have discussed above, for normally we do think fictional narrators are privileged with respect to where the televisions and windows are. What matters here is that how far narratorial privilege extends depends on what other claims the narrator makes. Perhaps the same is true of foreign correspondents, though we’d need to see an argument for that. Second, it isn’t clear how this solution could possibly generalise to cover cases, such as frequently occurs in plays, where the deviant moral claim is clearly intended by the author to be true in the fiction but the reader (or watcher) does not agree even though the author’s intention is recognised. As I mentioned at the start, these cases aren’t our concern here, though it would be nice to see how a generalisation to these cases is possible. But the primary problem with Matravers’s solution is that as it stands it (improperly) rules out three-headed frogs in fiction, and it is hard to see how to remedy this problem without solving the original puzzle.\nSome Ethical Solutions\nIf one focuses on cases like Death, it is natural to think the puzzle probably has something to do with the special nature of ethical predicates, or perhaps of ethical concepts, or perhaps of the role of either of these in fiction. I don’t think any such solution can work because it can’t explain what goes wrong in Victory, and this will recur as an objection in what follows.\nThe most detailed solution to the puzzles has been put forward by Tamar Szabó Gendler. She focuses on the imaginative puzzle, but she also makes valuable points about the other puzzles. My solution to the phenomenological puzzle is basically hers plus a little epicycle.\nShe says that we do not imagine morally deviant fictional worlds because of our “general desire to not be manipulated into taking on points of view that we would not reflectively endorse as our own.” How could we take on a point of view by accepting something in a fiction? Because of the phenomena noted above that some things become true in a story because they are true in the world. If this is right, its converse must be true as well. If what is true in the story must match what is true in the world, then to accept that something is true in the story just is to accept that it is true in the world. Arguably, the same kind of ‘import/export’ principles hold for imagination as for truth in fiction. Some propositions become part of the content of an imagining because they are true. So, in the right circumstances, they will only be part of an imagining if they are true. Hence to imagine them (in the right circumstances) is to commit oneself to their truth. Gendler holds that we are sensitive to this phenomena, and that we refuse to accept stories that are morally deviant because that would involve accepting that morally deviant claims are true in the world.\nThat’s a relatively rough description of Gendler’s theory, but it says enough to illustrate what she has in mind, and to show where two objections may slip in. First, it is not clear that it generalises to all the cases. Gendler is aware of some of these cases and just bites the relevant bullets. She holds, for instance, that we can imagine that actually lame jokes are funny, and it could be true in a story that such a joke is funny. It would be a serious cost to her theory if she had to say the same thing about all the examples discussed above.\nThe second problem is more serious. The solution is only as good as the claim that moral claims are more easily exported than descriptive claims, and more generally that the types of claims we won’t imagine are more easily exported than those we don’t resist. Gendler has two arguments for why the first of these should be true, but neither of them sounds persuasive. First, she says that the moral claims are true in all possible worlds if true at all. But this won’t do on its own, because as she proved, we don’t resist some necessarily false claims. (This objection is also made by (Matravers 2003, 94).)\nSecondly, she claims that in other cases where there are necessary falsehoods true in a story, as in Alice in Wonderland, or the science fiction cases, the author makes it clear that unusual export restrictions are being imposed. But this is wrong for two reasons. First, I don’t think that any particularly clear signal to this effect occurs in my version of Back to the Future. Secondly, even if I had explicitly signalled that I had intended to make some of the facts in the story available for export, and you didn’t believe that, that isn’t enough reason to resist imagining the story. For my intent as to what can and cannot be exported is not part of the story.\nTo see this, consider one relatively famous example. At one stage B. F. Skinner tried to promote behaviourism by weaving his theories into a novel (of sorts): Walden Two. Now I’m sure Skinner (1948) intended us to export some psychological and political claims from the story to the real world. But it is entirely possible to read the story with full export restrictions in force without rejecting that what Skinner says is true in that world. (It is dreadfully boring, since there’s nothing but propagandising going on, but possible.) If exporting was the only barrier here, we should be able to impose our own tariff walls and read the story along, whatever the intent of the author, as we can with Walden Two. One can accept it is true in Walden Two that behaviourism is the basis of a successful social policy, even though Skinner wants us to accept this as true in the story iff it is true in the world, and it isn’t true in the world. We cannot read Death or Victory with the same ironic detachment, and Gendler’s theory lacks the resources to explain this.\nCurrie’s theory attacks the problem from a quite different direction. He relies on the motivational consequences of accepting moral claims. Assume internalism about moral motivation, so to accept that \\({\\phi}\\)-ing is right is to be motivated to \\({\\phi}\\), at least ceteris paribus. So accepting that \\({\\phi}\\)-ing is right involves acquiring a desire to \\({\\phi}\\), as well, perhaps, as beliefs about \\({\\phi}\\)-ing. Currie suggests that there is a mental state that stands to desire the way that ordinary imagination stands to belief. It is, roughly, a state of having an off-line desire, in the way that imagining that p is like having an off-line belief that p, a state like a belief that p but without the motivational consequences. Currie suggests that imagining that \\({\\phi}\\)-ing is right involves off-line acceptance that \\({\\phi}\\)-ing is right, and that in part involves having an off-line desire (a desire-like imagination) to \\({\\phi}\\). Finally, Currie says, it is harder to alter our off-line desires at will than it is to alter our off-line beliefs, and this explains the asymmetry. The argument for this last claim seems very hasty, but we’ll let that pass. For even if it is true, Currie’s theory does little to explain the later cases of imaginative resistance, from Alien Robbery to Victory. It cannot explain, why we have resistance to claims about what is rational to believe, or what is beautiful, or what attitudes other people have. The idea that there is a state that stands to desire as imagination stands to belief is I suspect a very fruitful one, but I don’t think its fruits include a solution to these puzzles.\nGrok\nStephen Yablo has suggested that the puzzles, or at least the imaginative puzzle, is closely linked to what he calls response-enabled concepts, or grokking concepts. (I’ll also use response-enabled (grokking) as a property of the predicates that pick out these concepts.) These are introduced by examples, particularly by the example ‘oval.’\nHere are meant to be some platitudes about OVAL. It is a shape concept - any two objects in any two worlds, counterfactual or counteractual, that have the same shape are alike in whether they are ovals. But which shape concept it is is picked out by our reactions. They are the shapes that strike us as being egg-like, or perhaps more formally, like the shape of all ellipses whose length/width ratio is the golden ratio. In this way the concept OVAL meant to be distinguished on the one hand from, say, PRIME NUMBER, which is entirely independent of us, and from WATER, which would have picked out a different chemical substance had our reactions to various chemicals been different. Note that what ‘prime number’ picks out is determined by us, like all semantic facts are. So the move space into which OVAL is meant to fit is quite tiny. We matter to its extension, but not the way we matter to ‘prime number’ (or that we don’t matter to PRIME NUMBER), and not the way we matter to ‘water.’ I’m not sure there’s any space here at all. To my ear, Yablo’s grokking predicates strike me as words that have associated egocentric descriptions that fix their reference without having egocentric reference fixing descriptions, and such words presumably don’t exist. But for present purposes I’ll bracket those general concerns and see how this idea can help solve the puzzles. For despite my disagreement about what these puzzles show about the theory of concepts, Yablo’s solution is not too dissimilar to mine.\nThe important point for fiction about grokking concepts is that we matter, in a non-constitutive way, for their extension. Not we as we might have been, or we as we are in a story, but us. So an author can’t say that in the story squares looked egg-shaped to the people, so in the story squares are ovals, because we get to say what’s an oval, not some fictional character. Here’s how Yablo puts it:\n\nWhy should resistance \\[meaning, roughly, unimaginability\\] and grokkingness be connected in this way? It’s a feature of grokking concepts that their extension in a situation depends on how the situation does or would strike us. ‘Does or would strike us’ as we are: how we are represented as reacting, or invited to react, has nothing to do with it. Resistance is the natural consequence. If we insist on judging the extension ourselves, it stands to reason that any seeming intelligence coming from elsewhere is automatically suspect. This applies in particular to being ‘told’ about the extension by an as-if knowledgeable narrator. (2002, 485)\n\nIt might look at first as if Victory will be a counterexample to Yablo’s solution, just as it is to the Ethical solutions. After all, the concept that seems to generate the puzzles there is TELEVISION, and that isn’t at all like his examples of grokking concepts. (The examples, apart from evaluative concepts, are all shape concepts.) On the other hand, if there are any grokking concepts, perhaps it is plausible that TELEVISION should be one of them. Indeed, the platitudes about TELEVISION provide some support for this. (The following two paragraphs rely heavily on Fodor (1998).)\nThree platitudes about TELEVISION stand out. One is that it’s very hard to define just what a television is. A second is that there’s a striking correlation between people who have the concept TELEVISION and people who have been acquainted with a television. Not a perfect correlation - some infants have acquaintance with televisions but not as such, and some people acquire TELEVISION by description - but still strikingly high. And a third is that conversations about televisions are rarely at cross purposes, even when they consist of people literally talking different languages. TELEVISION is a shared concept.\nCan we put these into a theory of the concept TELEVISION? Fodor suggests we can, as long as we are not looking for an analysis of TELEVISION. Televisions are those things that strike us, people in general, as being sufficiently like the televisions we’ve seen, in a televisual kind of way. This isn’t an account of the meaning of the word ‘television’ - there’s no reference to us in that word’s dictionary entry, and rightly so. Nor is it an analysis of what constitutes the concept television. There’s no reference to us there either. But it does latch on to the right concept, or at least the right extension, in perhaps the only way we could. And this proposal certainly explains the platitudes well. The epistemic necessity of having a paradigm television to use as a basis for similarity judgments explains the striking correlation between televisual acquaintance and concept possession. The fact that the only way of picking out the extension uses something that is not constitutive of the concept, namely our reactions to televisions, explains why we can’t reductively analyse the concept. And the use of people’s reactions in general rather than idiosyncratic reactions explains why its a common concept. These look like good reasons to think something like Fodor’s theory of the concept TELEVISION is right, and if it is then TELEVISION seems to be response-enabled in Yablo’s sense. So unlike the Ethical solutions, Yablo’s solution might yet predict that Victory will be puzzling.\nStill, I have three quibbles about his solution, and that’s enough to make me think a better solution may still to be found.\nFirst, there’s a missing antecedent in a key sentence in his account, and it’s hard to see how to fill it in. What does he mean when he says ‘how the situation does or would strike us?’ Does or would strike us if what? If we were there? But we don’t know where there is. There, in Victory, is allegedly a place where televisions look like knifes and forks. What if the antecedent is If all the non-grokking descriptions were accurate? The problem now is that this will be too light. If TELEVISION is grokking, then there is a worry that many concepts, including perhaps all artefact concepts, will be grokking. Fodor didn’t illustrate his theory with TELEVISION, he always used DOORKNOB. But the theory was meant to be rather general. If we take out all the claims involving grokking concepts, there may not be much left.\nSecond, despite the generality of Fodor’s account, it isn’t clear that mental concepts, and content concepts, are grokking. We would need another argument that LOVE is grokking, and that so is BELIEVING THAT THERE ARE SPACE ALIENS. Perhaps such an argument can be given, but it will not be a trivial exercise.\nFinally, I think this Yablo’s solution, at least as most naturally interpreted, overgeneralises. Here’s a counterexample to it. The following story is not, I take it, puzzling.\n\nFixing a Hole\nDQ and his buddy SP leave DQ’s apartment at midday Tuesday, leaving a well-arranged lounge suite and home theatre unit, featuring DQ’s prized oval television. They travel back in time to Monday, where DQ has some rather strange and unexpected adventures. He intended to correct something that happened yesterday, that had gone all wrong the first time around, and by the time the buddies reunite and leave for Tuesday (by sleeping and waking up in the future) he’s sure it’s all been sorted. When DQ and his buddy SP get back to his apartment midday Tuesday, it looks for all the world like there’s nothing there except an ordinary knife and fork.\n\nNow this situation would not strike us, were we to see it, as one where there is a lounge suite and home theatre unit in DQ’s apartment midday Tuesday, for it looks as if there’s an ordinary knife and fork there. But still, the author gets to say that what’s in DQ’s apartment as the story opens includes an oval television. And this despite the fact that the two concepts, TELEVISION and OVAL, are grokking. Perhaps some epicycles could be added to Yablo’s theory to solve this problem, but for now the solution is incomplete.\nVirtue\nThe content cases may remind us of one of Fodor’s most famous lines about meaning.\n\nI suppose that sooner or later the physicists will complete the catalogue they’ve been compiling of the ultimate and irreducible properties of things. When they do, the likes of spin, charm, and charge will perhaps appear on the list. But aboutness surely won’t; intentionality doesn’t go that deep … If the semantic and the intentional are real properties of things, it must be in virtue of their identity with (or maybe their supervenience on?) properties that are themselves neither intentional nor semantic. If aboutness is real, it must really be something else. (Fodor 1987, 97)\n\nIf meaning doesn’t go that deep, but there are meaning facts, then those facts must hold in virtue of more fundamental facts. “Molino de viento” means windmill in Spanish in virtue of a pattern of usage of those words by Spanish speakers, for instance.\nIt seems that many of the stories above involve facts that hold, if they hold at all, in virtue of other facts. Had Fodor other interests than intentionality, he may have written instead that beauty doesn’t go that deep, and neither does television. If an event is to be beautiful, this is a fact that must obtain in virtue of other facts about it, perhaps its integrity, wholeness, symmetry and radiance as Aquinas says (Joyce 1944/1963, 212), and that event being a monster truck death match of doom probably precludes those facts from obtaining.7 If Quixote’s favourite item of furniture is to be a television, this must be in virtue of it filling certain functional roles, and being indistinguishable from a common knife probably precludes that.\nWhat is it for a fact to obtain in virtue of other facts obtaining? A good question, but not one we will answer here. Still, the concept seems clear enough that we can still use it, as Fodor does. What we have in mind by ‘virtue’ is understandable from the examples. One thing to note from the top is that it is not just supervenience: whether x is good supervenes on whether it is good, but it is not good in virtue of being good. How much our concept differs from supervenience is a little delicate, but it certainly differs.\nReturning to our original example, moral properties are also less than perfectly fundamental. It is not a primitive fact that the butcher or the baker is generous, but a fact that obtains in virtue of the way they treat their neighbours. It is not a primitive fact that what Craig does is wrong, but a fact that obtains in virtue of the physical features of his actions.\nHow are these virtuous relations relevant to the puzzles? To a first approximation, these relations are always imported into stories and into imagination. The puzzles arise when we try to tell stories or imagine scenes where they are violated. The rest of the paper will be concerned with making this claim more precise, motivating it, and arguing that it solves the puzzles. In making the claim precise, we will largely be qualifying it.\nThe first qualification follows from something we noted at the end of section 2. We don’t know whether puzzles like the ones with which we started arise whenever there is a clash between real-world morality (or epistemology or mereology) and the morality (or epistemology or mereology) the author tries to put in the story. We do know they arise for simple stories and direct invitations to imagine. So if we aren’t to make claims that go beyond our evidence, we should say there is a default assumption that these relations are imported into stories or imaginations, and it is not easy to overcome this assumption. (I will say for short there is a strong default assumption, meaning just that an author cannot cancel the assumption by saying so, and that we cannot easily follow invitations to imagine that violate the relations.)\nThe second qualification is that sometimes we simply ignore, either in fiction or imagination, what goes on at some levels of detail. This means that sometimes, in a sense, the relations are not imported into the story. For instance, for it to really be true that in a language that “glory” means a nice knockdown argument, this must be true in virtue of facts about how the speakers of that language use, or are disposed to use, “glory.” But we can simply say in a story that “glory” in a character’s language means a nice knockdown argument without thereby making any more general facts about usage or disposition to use true in the story.8 More generally, we can simply pick a level of conceptual complexity at which to write our story or conduct our imaginings. Even if those concepts apply, when they do, in virtue of more basic facts, no more basic facts need be imported into the story. For a more vivid, if more controversial, example, one might think that cows are cows in virtue of their DNA having certain chemical characteristics. But when we imagine a cow jumping over the moon, we need not imagine anything about chemistry. Those facts are simply below the radar of our imagining. What do we mean then when we say that these relations are imported into the story? Just that if the story regards both the higher-level facts and the lower-level facts as being within its purview, then they must match up. This does not rule out the possibility of simply leaving out all lower-level facts from the story. In general the same thing is true for imagining, though we will look at some cases below where we it seems there is a stronger constraint on imagining.\nThe third qualification is needed to handle an example pressed on me by a referee. Recall our example Fixing a Hole.\n\nFixing a Hole\nDQ and his buddy SP leave DQ’s apartment at midday Tuesday, leaving a well-arranged lounge suite and home theatre unit, featuring DQ’s prized oval television. They travel back in time to Monday, where DQ has some rather strange and unexpected adventures. He intended to correct something that happened yesterday, that had gone all wrong the first time around, and by the time the buddies reunite and leave for Tuesday (by sleeping and waking up in the future) he’s sure it’s all been sorted. When DQ and his buddy SP get back to his apartment midday Tuesday, it looks for all the world like there’s nothing there except an ordinary knife and fork.\n\nIn this story it seems that on Tuesday there is a television that looks exactly like a knife. If we interpret the claim about the relations between higher-level facts and the lower-level facts as a kind of impossibility claim, e.g. as the claim that a conjunction p \\({\\wedge}\\) q is never true in a story if the conditional If q, then p is false in virtue of q being true is true, then we have a problem. Let p be the claim that there is a television, and let q be the claim that the only things in the apartment looked life a knife and fork. If that’s how the more basic phenomenal and functional facts are, then there isn’t a television in virtue of those facts. (That is, this relation between phenomenal and functional facts and facts about where the televisions are really holds.) So this rule would say p \\({\\wedge}\\) q could not be true in the story. But in fact p \\({\\wedge}\\) q is true in the story.\nThe difficulty here is that Fixing a Hole is a contradictory story, and contradictory stories need care. First, here’s how we should interpret the rule\n\nVirtue\nIf p is the kind of claim that if true must be true in virtue of lower-level facts, and if the story is about those lower-level facts, then it must be true in the story that there is some true proposition r which is about those lower-level facts such that p is true in virtue of r.\n\nIn Fixing a Hole there are some true lower-level claims that are inconsistent with there being a television. But there is also in the story a true proposition about how DQ’s television looked before his time-travel misadventure. And it is true (both in reality and in the story) that something is a television in virtue of looking that way. (Note that we don’t say there must be some proposition r that is true in the story in virtue of which p is true. For there is no fact of the matter in Fixing a Hole about how DQ’s television looked before he left. So in reality we could not find such a proposition. But it is true in the story that his television looks some way or other, so as long as we talk about what in the story is true, and don’t quantify over propositions that are (in reality) true in the story, we avoid this pitfall.)\nSo my solution to the alethic puzzle is that Virtue is a strong default principle of fictional interpretation. I haven’t done much yet to motivate it, apart from noting that it seems to cover a lot of the cases that have been raised without overgenerating in the manner of the impossible solution. A more positive motivation must wait until I have presented my solutions to the phenomenological and imaginative puzzles. I’ll do that in the next section, then in tell a story about why we should believe Virtue.\nMore Solutions\nThe Phenomenological Puzzle\nMy solution here is essentially the same as Gendler’s. She think that when we strike a sentence that generated imaginative resistance we respond with something like, “That’s what you think!” What makes this notable is that it’s constitutive of playing the fiction game that we not normally respond that we way, that we give the author some flexibility in setting up a world. I think that’s basically right, but a little more is needed to put the puzzle to bed.\nSometimes the “That’s what you think!” response does not constitute abandoning the fiction game. At times it is the only correct way to play the game. It’s the right thing to say to Lily when reading the first line of The Dead. (Maybe it would be rude to say it aloud to poor Lily, the poor girl is run off her feet after all, but it’s appropriate to think it.) This pattern recurs throughout Dubliners. When in Eveline the narrator says that Frank has sailed around the world, the right reaction is to say to Eveline (or whoever is narrating then), “That’s what you think!” There’s a cost to playing the game this way. We end up knowing next to nothing about Frank. But it is not as if making the move stops us playing, or even stops us playing correctly. It’s part of the point of Eveline that we know next to nothing about Frank.\nWhat makes cases like Death and Victory odd is that our reaction is directed at someone who isn’t in the story. One of Alex Byrne’s (1993) criticisms of Lewis was that on Lewis’s theory it is true in every story that the story is being told. Byrne argued that in many fictions it is not true that in the fictional world there is someone sufficiently knowledgeable to tell the story. In these fictions, we have a story without a storyteller. If there are such stories, then presumably Death and Victory are amongst them. It is not a character in the story who ends by saying that Craig’s action was right or that Quixote’s apartment contains a television. The author says that, and hence deserves our reproach, but the author isn’t in the story. Saying “That’s what you think!” directly to him or her breaks the fictional spell for suddenly we have to recognise a character not in the fictional world.\nThis proposal for the phenomenological puzzle yields a number of predictions which seem to be true and interesting. First, a story that has a narrator should not generate a phenomenological puzzle, even when outlandish moral claims are made. The more prominent the narrator, the less striking the moral claim. Imagine, for example, a version of Death where the text purports to be Craig’s diary, and it includes naturally enough his own positive evaluation of what he did. We wouldn’t believe him, of course, but we wouldn’t be struck by the claim the same way we are in the actual version of Death.\nOne might have thought that what is shocking is what we discover about the author. But this isn’t right, as can be seen if we reflect on stories that contain Craig’s diary. It is possible, difficult but possible, to embed the diary entry corresponding to Death in a longer story where it is clear that the author endorses Craig’s opinions. (Naturally I won’t do this. Examples have to come to an end somewhere.) Such a story would, in a way, be incredibly shocking. But it wouldn’t make the final line shocking in just the way that the final line of Death is shocking. Our reactions to these cases suggest that the strikingness of the last line of Death is not a function of what it reveals about the author, but of how it reveals it.\nThe final prediction my theory makes is somewhat more contentious. Some novels announce themselves as works of fiction. They go out of their way to prevent you ignoring the novel’s role as mediation to a fictional world. (For an early example of this, consider the sudden appearance of newspaper headlines in the ‘Aeolus’ episode of Ulysses.) In such novels we already have to recognise the author as a player in the fictional game, if not a character in the story. I predict that sentences where we do not take what is written to really be true in the story, even though this is what the author intended, should be less striking in these cases because we are already used to reacting to the author as such rather than just to the characters. Such books go out of their way to break the fictional spell, so spell breaking should matter less in these cases. I think this prediction is correct, although the works in question tend to be so complicated that it is hard to generate clear intuitions about them.\nThe Imaginative Puzzle\nImagine, if you will, a chair. Have you done so? Good. Let me make some guesses about what you imagined. First, it was a specific kind of chair. There is a fact of the matter about whether the chair you imagined is, for example, an armchair or a dining chair or a classroom chair or an airport lounge chair or an outdoor chair or an electric chair or a throne. We can verbally represent something as being a chair without representing it as being a specific kind of chair, but imagination cannot be quite so coarse.9\nSecondly, what you imagined was incomplete in some respects. You possibly imagined a chair that if realised would contain some stitching somewhere, but you did not imagine any details about the stitching. There is no fact of the matter about how the chair you imagined holds together, if indeed it does. If you imagined a chair by imagining bumping into something chair-like in the dead of night, you need not have imagined a chair of any colour, although in reality the chair would have some colour or other.10\nWere my guesses correct? Good. The little I needed to know about imagination to get those guesses right goes a long way towards solving the puzzle.\nChairs are not very distinctive. Whenever we try to imagine that a non-fundamental property is instantiated the content of our imagining will be to some extent more specific than just that the object imagined has the property, but not so much more specific as to amount to a complete description of a possibilia. It’s the latter fact that does the work in explaining how can imagine impossible situations. If we were, foolishly, to try to fill in all the details of the impossible science fiction cases it would be clear they contained not just impossibilities, but violations of Virtue, and then we would no longer be able to imagine them. But we can imagine the restaurant at the end of the universe without imagining it in all its glorious gory detail. And when we do so our imagining appears to contain no such violations.\nBut why can’t we imagine these violations in fictions? It is primarily because we can only imagine the higher-level claim some way or another, just as we only imagine a chair as some chair or other, and the instructions that go along with the fiction forbid us from imagining any relevant lower-level facts that would constitute the truth of the higher-level claim. We have not stressed it much above, but it is relevant that fictions understood as invitations to imagine have a “That’s all” clause.11 We are not imagining Death if we imagine that Jack and Jill had just stopped arguing with each other and were about to shoot everyone in sight when Craig shot them in self-defence. The story does not explicitly say that wasn’t about to happen. It doesn’t include a “That’s all” clause. But such clauses have to be understood. So not only are we instructed to imagine something that seems incompatible with Craig’s action being morally acceptable, we are also instructed (tacitly) to not imagine anything that would make it the case that his action is morally acceptable. But we can’t simply imagine moral goodness in the abstract, to imagine it we have to imagine a particular kind of goodness.\nTwo Thoughts Too Many?\nI have presented three solutions to the three different puzzles with which we started. Might it not be better to have a uniform solution? No, because although the puzzles are related, they are not identical. Three puzzles demand three solutions.\nWe saw already that the phenomenological puzzle is different to the other two. If we rewrite Death as Craig’s diary there would be nothing particularly striking about the last sentence, certainly in the context of the story as so told. But the last sentence generates alethic and imaginative puzzles. Or at least it could generate these puzzles if the author has made it clear elsewhere in the story that Craig’s voice is authoritative. So we shouldn’t expect the same solution to that puzzle as the other two.\nThe alethic puzzle is different to the other two because ultimately it depends on what the moral and conceptual truths are not on what we take them to be. Consider the following story.\n\nThe Benefactor\nSmith was a very generous, just and in every respect moral man. Every month he held a giant feast for the village where they were able to escape their usual diet of gains, fruits and vegetables to eat the many and varied meats that Smith provided for them.\n\nConsider in particular, what should be easy to some, how Benefactor reads to someone who believes that we are morally required to be vegetarian if this is feasible. In Benefactor it is clear in the story that most villagers can survive on a vegetarian diet. So it is morally wrong to serve them the many and varied meats that Smith does. Hence such a reader should disagree with the author’s assessment that Smith is moral ‘in every respect.’ Such a reader will think that in fact in the story Smith is quite immoral in one important respect.\nNow for our final assumption. Assume it is really true that we morally shouldn’t eat meat if it is avoidable. Since the ethical vegetarians have true ethical beliefs about the salient facts here, it seems plausible that their views on what is true in the story should carry more weight than ours. (I’m just relying on a general epistemological principle here: other things being equal trust the people who have true beliefs about the relevant background facts.) So it seems that it really is false in the story that Smith is in every respect moral. Benefactor raises an alethic puzzle even though for non-vegetarians it does not raise a phenomenological or imaginative puzzle.\nThis point generalises, so we need not assume for the general point that vegetarianism is true or that our typical reader is not vegetarian. We can be very confident that some of our ethical views will be wrong, though for obvious reasons it is hard to say which ones. Let p be a false moral belief that we have. And let S be a story in which p is asserted by the (would-be omniscient) narrator. For reasons similar to what we said about Benefactor, p is not true in S. But S need not raise any imaginative or phenomenological puzzles. Hence the alethic puzzle is different to the other two puzzles.\nWhy Virtue Matters\nI owe you an argument for why authors should be unable to easily generate violations of Virtue, though there is no general bar on making impossibilities true in a story. My general claims here are not too dissimilar to Yablo’s solution to the puzzles, but there are a couple of distinctive new points. Before we get to the argument, it’s time for another story.\nThree design students walk into an furniture showroom. The new season’s fashions are all on display. The students are all struck by the piece de resistance, though they are all differently struck by it. Over drinks later, it is revealed that while B and C thought it was a chair, A did not. But the differences did not end there. When asked to sketch this contentious object, A and B produced identical sketches, while C’s recollections were drawn somewhat differently. B clearly disagrees with both A and C, but her differences with each are quite different. With C she disagrees on some simple empirical facts, what the object in question looked like. With A she disagrees on a conceptual fact, or perhaps a semantic fact, whether the concept CHAIR, or perhaps just the term ‘chair,’ applies to the object in question. As it turns out, A and B agree that ‘chair’ means CHAIR, and agree that CHAIR is a public concept so one of them is right and the other wrong about whether this object falls under the concept. In this case, their disagreement will have a quite different feel to B’s disagreement with C. It may well be that there is no analytic/synthetic distinction, and that questions about whether an object satisfies a concept are always empirical questions, but this is not how it feels to A and B. They feel that they agree on what the world is like, or at least what this significant portion of it is like, and disagree just on which concepts apply to it.\nThe difference between these two kinds of disagreement is at the basis of our attitudes towards the alethic puzzle. It may look like we are severely cramping authorial freedom by not permitting violations of Virtue.12 From A and B’s perspective, however, this is no restriction at all. Authors, they think, are free to stipulate which world will be the site of their fiction. But as their disagreement about whether the piece de resistance was a chair showed, we can agree about which world we are discussing and disagree about which concepts apply to it. The important point is that the metaphysics and epistemology of concepts comes apart here.\nThere can be no difference in whether the concept CHAIR applies without a difference in the underlying facts. But there can be a difference of opinion about whether a thing is a chair without a difference of opinion about the underlying facts. The fact that it’s the author’s story, not the reader’s, means that the author gets to say what the underlying facts are. But that still leaves the possibility for differences of opinion about whether there are chairs, and on that question the author’s opinion is just another opinion.\nAuthorial authority extends as far as saying which world is fictional in their story, it does not extend as far as saying which concepts are instantiated there. Since the main way that we specify which world is fictional is by specifying which concepts are instantiated at it, authorial authority will usually let authors get away with any kind of conceptual claim. But once we have locked onto the world being discussed, the author has no special authority to say which concepts, especially which higher-level concepts like RIGHT or FUNNY or CHAIR are instantiated there.\n(Does it matter much that the distinction between empirical disagreements and conceptual disagreements with which I started might turn out not to rest on very much? Not really. I am trying to explain why we have the attitudes towards fiction that we do, which in turn determines what is true in fiction generally. All that matters is that people generally think that there is something like a conceptual truth/empirical truth distinction, and I think enough people would agree that A and B’s disagreement is different in kind from B and C’s disagreement to show that is true. If folks are generally wrong about this, if there is no difference in kind between conceptual truths and empirical truths, then our communal theory of truth in fiction will rest on some fairly untenable supports. But it will still be our theory, although any coherent telling of it will have to be in terms of things that are taken to be conceptual truths and things that are taken to be empirical truths.)\nThis explanation of why authorial authority collapses just when it does yields one fairly startling, and I think true, prediction. I argued above that authors could not easily generate violations of Virtue. That this is impossible is compatible with any number of hypotheses about how readers will resolve those impossibilities that authors attempt to slip in. The story here, that authors get to say which world is at issue but not which concepts apply to it, yields the prediction that readers will resolve the tension in favour of the lower-level claims. When given a physical description of a world and an incompatible moral description, we will take the physical description to fix which world is at issue and reduce the moral description to a series of questionable claims about the world. Compare what happens with A, B and C. We take A and B to agree about the world and disagree about concepts, rather than say taking B and C to agree about what the world is like (there’s a chair at the heart of the furniture show) and say that A and B disagree about the application of some recognitional concepts. This prediction is borne out in every case discussed in . We do not conclude that Craig did not really shoot Jack and Jill, because after all the world at issue is stipulated to be one where he did the right thing. Even more surprisingly, we do not conclude that Quixote’s furniture does not look like kitchen utensils, because it consists of a television and an armchair. This is surprising because in Victory I never said that the furniture looked like kitchen utensils. The tacit low-level claim about appearances is given precedence over the explicit high-level claims about which objects populate Quixote’s apartment. The theory sketched here predicts that, and supports the solution to the alethic puzzle sketched in , which is good news for both the theory and the solution.\nIt’s been a running theme here that the puzzles do not have anything particularly to do with normativity. But some normative concepts raise the kind of issues about authority mentioned here in a particularly striking way. There is always some division of cognitive labour in fiction. The author’s role is, among other things, to say which world is being made fictional. The audience’s role is, among other things, to determine the artistic merit of the fictional work. On other points there may be some sharing of roles, but this division is fairly absolute. The division threatens to collapse when authors start commenting on the aesthetic quality of words produced by their characters. At the end of Ivy Day in the Committee Room Joyce has one character describe a poem just recited by another character as “A fine piece of writing” (Joyce 1914/2000, 105). Most critics seem to be happy to accept the line, because Joyce’s poem here really is, apparently, a fine piece of writing. But to me it seems rather jarring, even if it happens to be true. It’s easy to feel a similar reaction when characters in a drama praise the words of another character.13 This is a special, and especially vivid, illustration of the point I’ve been pushing towards here. The author gets to describe the world at whichever level of detail she chooses. But once it has been described, the reader has just as much say in which higher-level concepts apply to parts of that world. When the concepts are evaluative concepts that directly reflect on the author, the reader’s role rises from being an equal to having more say than the author, just as we normally have less say than others about which evaluative concepts apply to us.\nThis idea is obviously similar to Yablo’s point that we get to decide when grokking concepts apply, not the author. But it isn’t quite the same. I think that if any concepts are grokking, most concepts are, so it can’t be the case that authors never get to say when grokking concepts apply in their stories. Most of the time authors will get to say which grokking concepts apply, because they have to use them to tell us about the world. What’s special about the kind of concepts that cause puzzles is that we get to decide when they apply full stop, but that we get to decide how they apply given how more fundamental concepts apply. So the conciliatory version of the relation between my picture here and Yablo’s is that I’ve been filling in, in rather laborious detail, his missing antecedent.\nTwo Hard Cases\nThe first hard case is suggested by Kendall Walton (1994). Try to imagine a world where the over-riding moral duty is to maximise the amount of nutmeg in the world. If you are like me, you will find this something of a challenge. Now consider a story Nutmeg that reads (in its entirety!): “Nobody ever discovered this, but it turned out all along their over-riding moral duty was to maximise the amount of nutmeg in the world.” What is true in Nutmeg? It seems that there are no violations of Virtue here, but it is hard to imagine what is being described.\nThe second hard case is suggested by Tamar Szabó Gendler (2000). (I’m simplifying this case a little, but it’s still hard.) In her Tower of Goldbach, God decrees that 12 shall no longer be the sum of two primes, and from this it follows (even in the story) that it is not the sum of 7 and 5. (It is not clear why He didn’t just make 5 no longer prime - say the product of 68 and 57. That may have been simpler.) Interestingly, this has practical consequences. When a group of seven mathematicians from one city attempts to join a group of five from another city, they no longer form a group of twelve. Again, two questions. Can we imagine a Goldbachian situation, where 7 and 5 equal not 12? Is it true in Gendler’s story that 7 and 5 equal not 12? If we cannot imagine Goldbach’s tower, where is the violation of Virtue?\nFirst a quick statement of my responses to the two cases then I’ll end with my detailed responses. To respond properly we need to tease apart the alethic and imaginative puzzles. I claim that the alethic puzzle only arises when there’s a violation of Virtue. There’s no violation in either story, so there is no alethic puzzle. I think there are independent arguments for this conclusion in both cases. We can’t imagine either (if we can’t) because any way of filling in the more basic facts leads to violations.\nIt follows from my solution to the alethic puzzle that Nutmegism (Tyler Doggett’s name for the principle that we must maximise quantities of nutmeg) could be true in a story. There is no violation in Nutmeg, since there are no lower level claims made. Still, the story is very hard to imagine. The reason for this is quite simple. As noted, we cannot just imagine a chair, we have to imagine something more detailed that is a chair in virtue of its more basic properties. (There is no particular more basic property we need imagine, as is shown by the fact that we can imagine a chair just by imagining something with a certain look, or we can imagine a chair in the dark with no visual characteristics. But there is always something more basic.) Similarly to imagine a duty, we have to imagine something more detailed, in this case presumably a society or an ecology, in virtue of which the duty exists. But no such possible, or even impossible, society readily springs to mind. So we cannot imagine Nutmegism is true.\nBut it is hard to see how, or why, this inability should be raised into a restriction on what can be true in a story. One might think that what is wrong with Nutmeg is that the fictional world is picked out using high-level predicates. If we extend the story any way at all, the thought might go, we will generate a violation of Virtue. And that is enough to say that Nutmegism is not true in the story. But actually this isn’t quite right. If we extend the story by adding more moral claims, there is no duty to minimise suffering, there is no duty to help the poor etc, there are still no violations in the story. The restriction we would have to impose is that there is no way of extending the story to fill out the facts in virtue of which the described facts obtain, without generating a violation. But that looks like too strong a constraint, mostly because if we applied it here, to rule out Nutmegism being true in Nutmeg, we would have to apply it to every story written in a higher level language than that of microphysics. It doesn’t seem true that we have to be able to continue a story all the way to the microphysical before we can be confident that what the author says about, for instance, where the furniture in the room is. So there’s no reason to not take the author’s word in Nutmeg, and since the default is always that what the author says is true, Nutmegism is true in the story.\nThe mathematical case is more difficult. The argument that 7 and 5 could fail to equal 12 in the story turns on an example by Gregory Currie (1990). (The main conclusions of this example are also endorsed by Byrne (1993).) Currie imagines a story in which the hero refutes Gödel’s Incompleteness Theorem. Currie argues that the story could be written in such a way that it is true in the story not merely that everyone believes our hero refuted Gödel, but that she really did. But if it could be true in a story that Gödel’s Incompleteness Theorem could be false, then it’s hard to see just why it could not be true in a story that a simpler arithmetic claim, say that 7 and 5 make 12, could also be false. Anything that can’t be true in a story can’t be true in virtue of some feature it has. The only difference between Gödel’s Incompleteness Theorem and a simple arithmetic statement appears to be the simplicity of the simple statement. And it doesn’t seem possible, or advisable, to work that kind of feature into a theory of truth in fiction.\nThe core problem here is that how simple a mathematical impossibility is very much a function of the reader’s mathematical knowledge and acumen. Some readers probably find the unique prime factorisation theorem so simple and evident that for them a story in which it is false is as crashingly bad as a story in which 7 and 5 do not make 12. For other readers, it is so complex that a story in which it has a counterexample is no more implausible than a story in which Gödel is refuted. I think it cannot be true for the second reader that the unique prime factorisation theorem fails in the story and false for the first reader. That amounts to a kind of relativism about truth in fiction that seems preposterous. But I agree with Currie that some mathematical impossibilities can be true in a fiction. So I conclude that, whether it is imaginable or not, it could be true in a story that 7 and 5 not equal 12.\nI think, however, that it is impossible to imagine that 7 plus 5 doesn’t equal 12. Can we explain that unimaginability in the same way we explained why Nutmeg couldn’t be imagined? I think we can. It seems that the sum of 7 and 5 is what it is in virtue of the relations between 7, 5 and other numbers. It is not primitive that various sums take the values they take. That would be inconsistent with, for example, it being constitutive of addition that it’s associative, and associativity does seem to be constitutive of addition. We cannot think about 7, 5, 12 and addition without thinking about those more primitive relations. So we cannot imagine 7 and 5 equally anything else. Or so I think. There’s some rather sophisticated, or at least complicated, philosophy of mathematics in the story here, and not everyone will accept all of it. So we should predict that not everyone will think that these arithmetic claims are unimaginable. And, pleasingly, not everyone does. Gendler, for instance, takes it as a data point that Tower of Goldbach is imaginable. So far so good. Unfortunately, if the story is true we should also expect that whether people find the story imaginable links up with the various philosophies of mathematics they believe. And the evidence for that is thin. So there may be more work to do here. But there is clearly a story that we can tell that handles the case.\n\n\nAdams, Douglas. 1980. The Restaurant at the End of the Universe. London: Pan Macmillan.\n\n\nByrne, Alex. 1993. “Truth in Fiction - the Story Continued.” Australasian Journal of Philosophy 71 (1): 24–35. https://doi.org/10.1080/00048409312345022.\n\n\nCurrie, Gregory. 1990. The Nature of Fiction. Cambridge: Cambridge University Press.\n\n\n———. 2002. “Desire in Imagination.” In Conceivability and Possibility, edited by Tamar Szabó Gendler and John Hawthorne, 201–21. Oxford: Oxford University Press.\n\n\nFodor, Jerry A. 1987. Psychosemantics. Cambridge, MA: MIT Press.\n\n\n———. 1998. Concepts: Where Cognitive Science Went Wrong. Oxford: Oxford University Press.\n\n\nGendler, Tamar Szabó. 2000. “The Puzzle of Imaginative Resistance.” Journal of Philosophy 97 (2): 55–81. https://doi.org/10.2307/2678446.\n\n\nHare, R. M. 1951. The Language of Morals. Oxford: Oxford University Press.\n\n\nHolton, Richard. 1997. “Some Telling Examples: Reply to Tsohatzidis.” Journal of Pragmatics 28 (5): 625–28. https://doi.org/10.1016/s0378-2166(96)00081-1.\n\n\nHume, David. 1757. “On the Standard of Taste.” In Essays: Moral, Political and Legal, 227–49. Indianapolis: Liberty Press.\n\n\nJackson, Frank. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nJoyce, James. 1914/2000. Dubliners. Oxford: Oxford University Press.\n\n\n———. 1944/1963. Stephen Hero. New Directions: Norfolk, CT.\n\n\n———. 1922/1993. Ulysses. Oxford: Oxford University Press.\n\n\nKidd, John. 1988. “The Scandal of ‘Ulysses’.” The New York Review of Books 35 (11): 32–39.\n\n\nKripke, Saul. 1982. Wittgenstein on Rules and Private Language. Oxford: Basil Blackwell.\n\n\nLewis, David. 1978. “Truth in Fiction.” American Philosophical Quarterly 15 (1): 37–46.\n\n\nMatravers, Derek. 2003. “Fictional Assent and the (so-Called) ‘Puzzle of Imaginative Resistance’.” In Imagination, Philosophy and the Arts, edited by Matthew Kieran and Dominic McIver Lopes, 91–108. London. Routledge.\n\n\nMoran, Richard. 1995. “The Expression of Feeling in Imagination.” Philosophical Review 103 (1): 75–106. https://doi.org/10.2307/2185873.\n\n\nPriest, Graham. 1997. “Sylvan’s Box: A Short Story and Ten Morals.” Notre Dame Journal of Formal Logic. 38 (4): 573–82. https://doi.org/10.1305/ndjfl/1039540770.\n\n\nSkinner, B. F. 1948. Walden Two. New York: Macmillan.\n\n\nSorensen, Roy. 2001. Vagueness and Contradiction. Oxford: Oxford University Press.\n\n\nStock, Kathleen. 2003. “The Tower of Goldbach and Other Impossible Tales.” In Imagination, Philosophy and the Arts, edited by Matthew Kieran and Dominic McIver Lopes, 107–24. London. Routledge.\n\n\nWalton, Kendall. 1990. Mimesis as Make Believe. Cambridge, MA: Harvard University Press.\n\n\n———. 1994. “Morals in Fiction and Fictional Morality.” Aristotelian Society 68(Supp) (1): 27–50. https://doi.org/10.1093/aristoteliansupp/68.1.27.\n\n\nYablo, Stephen. 2002. “Coulda, Woulda, Shoulda.” In Conceivability and Possibility, edited by Tamar Szabó Gendler and John Hawthorne, 441–92. Oxford: Oxford University Press.\n\n\nFor details on the spelling of Dr Norman’s name, and the story behind it, see Kidd (1988). The good doctor appears on page 6 of Joyce (1922/1993).↩︎\nAt least, they will be ignored if it is clear they are errors. If there seems to be a method behind the misspellings, as in Ulysses there frequently is, the matter is somewhat different, and somewhat more difficult.\nTyler Doggett has argued that these cases are more similar to paradigm cases of imaginative resistance than I take them to be. Indeed, I would not have noticed the problems they raise without reading his paper. It may be a shortcoming of my theory here that I have to set questions about whether these sentences are puzzling to one side and assume an ideal proof-reader.↩︎\nThanks here to George Wilson for reminding me that we haven’t shown anything stronger than that.↩︎\nArguably the relevant supervenience principle is even stronger than that. To use some terminology of Stephen Yablo’s, there’s no difference in moral facts without a difference in non-moral facts between any two counteractual worlds, as well as between any two counterfactual worlds. This might be connected to some claims I will make below about the relationship between the normative and the descriptive.↩︎\nDetermining whether this is true in all such stories would be an enormous task, I fear, and somewhat pointless given the next objection. If anyone wants to say all clearly impossible statements in fiction are puzzling, I suspect the best strategy is to divide and conquer. The most blatantly impossible claims are most naturally fit for reinterpretation, and the other claims rest on an essentialism that is arguably not proven. I won’t try such a massive defence of a false theory here.↩︎\nThere is a complication here in that such a sentence might be evidence that the fictional work is not to be understood as this kind of report, and instead understood as something like a recording of the children’s thoughts. I’ll assume we’re in a story where it is clear that the sentences are not to be so interpreted.↩︎\nAlthough it isn’t obvious just which of the Thomistic properties the death match lacks.↩︎\nDo we make facts about the actual speaker’s usage true in the story? No. The character might have idiosyncratic reasons for not using the word “glory,” and for ignoring all others who use it. That’s consistent with the word meaning a nice knockdown argument.↩︎\nThis relates to another area in which my solution owes a debt to Gendler’s solution. Supposing can be coarse in a way that imagining cannot. We can suppose that Jack sold a chair without supposing that he sold an armchair or a dining chair or any particular kind of chair at all. Gendler concludes that what we do in fiction, where we try and imagine the fictional world, is very different to what we do, say, in philosophical argumentation, where we often suppose that things are different to the way they actually are. We can suppose, for the sake of argument as it’s put, that Kantian or Aristotelian ethical theories are entirely correct, even if we have no idea how to imagine either being correct. Thanks to Tyler Doggett for pointing out the connection to Gendler here.↩︎\nThanks to Kendall Walton for pointing out this possibility.↩︎\n“That’s all” clauses play a distinct, but related, role in (Jackson 1998 Ch. 1). It’s also crucial to my solution to the alethic puzzle that there be a “That’s all” clause in the story. What’s problematic about these cases is that the story (implicitly) rules out there being the lower-level facts that would make the expressed higher-level claims true.↩︎\nAgain, it is worth noting that I am not ruling out any violation of Virtue, just easy violations of it. The point being made in the text is that even a blanket ban on violations would not be a serious restriction on authorial freedom.↩︎\nFor a while this would happen frequently on the TV series The West Wing. President Bartlett would deliver a speech, and afterwards his staffers would congratulate themselves on what a good speech it was. The style of the congratulations was clearly intended to convey the author’s belief that the speech they themselves had written was a good speech, not just the characters’ beliefs to this effect. When in fact it was a very bad speech, this became very jarring. In later series they would often not show the speeches in question and hence avoid this problem.\n\n↩︎\n",
    "preview": "posts/2021-01-05-morality-fiction-and-possibility/freeway.jpg",
    "last_modified": "2021-02-05T15:24:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-05-pranksters-ethics/",
    "title": "Prankster's Ethics",
    "description": "We raise an objection to a very weak form of consequentialism. A world with only moral saints would be improved by adding a few mostly harmless pranksters. This result is not dependent on a particular way of thinking about value; it is resilient across a lot of measures of the value of worlds. But these pranksters would be doing things that are morally wrong. So we cannot identify rightness with making the world a better place.",
    "author": [
      {
        "name": "Andy Egan",
        "url": "https://www.andyegan.net"
      },
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2004-11-01",
    "categories": [
      "ethics"
    ],
    "contents": "\n\nContents\nA Quick Argument for Boorishness\nThe Good, the Right, and the Saintly\nA Problem about Quantifier Scope?\nValue, Desire and Advice\n\nA Quick Argument for Boorishness\nDiversity is a good thing. Some of its value is instrumental. Having people around with diverse beliefs, or customs, or tastes, can expand our horizons and potentially raise to salience some potential true beliefs, useful customs or apt tastes. Even diversity of error can be useful. Seeing other people fall away from the true and the useful in distinctive ways can immunise us against similar errors. And there are a variety of pleasant interactions, not least philosophical exchange, that wouldn’t be possible unless some kinds of diversity existed. Diversity may also have intrinsic value. It may be that a society with diverse views, customs and tastes is simply thereby a better society. But we will mostly focus on diversity’s instrumental value here.\n\nPublished in Philosophical Perspectives 18: 45-52.\nWe think that what is true of these common types of diversity is also true of moral diversity. By moral diversity we mean not only diversity of moral views, though that is no doubt valuable, but diversity of moral behaviour. In a morally diverse society, at least some people will not conform as tightly to moral norms as others. In short, there will be some wrongdoers. To be sure, moral diversity has some costs, and too much of it is undoubtedly a bad thing. Having rapists and murderers adds to moral diversity (assuming, as we do, that most people are basically moral) but not in a way that is particularly valuable. Still, smaller amounts of moral diversity may be valuable, all things considered. It seems particularly clear that moral diversity within a subgroup has value, but sometimes society as a whole is better off for being morally diverse. Let us consider some examples.\nMany violations of etiquette are not moral transgressions. Eating asparagus spears with one’s fork is not sinful, just poor form. But more extreme violations may be sinful. Hurtful use of racial epithets, for example, is clearly immoral as well as a breach of etiquette. Even use of language that causes not hurt, but strong discomfort, may be morally wrong. Someone who uses an offensive term in polite company, say at a dinner party or in a professional philosophical forum, may be doing the wrong thing. But having the wrongdoer around may have valuable consequences. For example, they generate stories that can be told, to great amusement, at subsequent dinner parties. They also prompt us to reconsider the basis for the standards we ourselves adopt in such matters. The reconsideration may cause us to abandon useless practices, and it may reinforce useful practices. These benefits seem to outweigh the disutility of the discomfort felt by those in attendance when the fateful word drops from the speaker’s lips. These side benefits do not make the original action morally permissible. Indeed, it is precisely because the action is not morally permissible that the benefits accrue.\nWhile we think that case is one of valuable moral diversity, some may question the immorality of the act in question. So let us try a more clearly immoral case: the mostly harmless prankster. Sam is a pie-thrower. Sam doesn’t just throw pies at the rich and infamous. No, Sam’s pies land on common folk like you and I, often for no reason beyond Sam’s amusement. Causing gratuitous harm for one’s own amusement is immoral. And a pie in the face, while better than a poke in the eye with a burnt stick, is harmful. But it may, in some circumstances, have side benefits. There will be the (guilty) pleasure occasioned in the unharmed bystanders, though it would be wrong to put too much weight on that. Other more significant benefits may accrue if Sam’s society is otherwise saintly. Sam’s existence will prompt people to take some simple, and worthwhile, precautions against perpetrators of such attacks. Even if society currently contains no malfeasants, such precautions will be useful against future wrongdoers. This benefit will increase if Sam graduates from pie-throwing to more varied pranks. (As may the entertainment value of Sam’s pranks.) Many computer hackers perform just this function in the present world. Malicious hackers on the whole cause more harm than good. But other hackers, who hack without gratuitously harming, provide a protective benefit by informing us of our weaknesses. These are the pie-throwers of the virtual world. Sam’s actions have other benefits. If Sam’s pranks are harmless enough, some will mistakenly think that they are morally acceptable, and we can have enjoyable, valuable, philosophical discussions with them. (Note that this benefit also increases if Sam varies the pranks.) The upshot is that Sam’s pranks can make the world a better place, all things considered, despite being immoral. Indeed, in some ways they make the world a better place because they are immoral.\nThe philosophical point, or points, here may be familiar. One point certainly is familiar: we have here an example of a Moorean organic unity. The goodness of the whole is no simple function of the goodness of the parts. It might be thought that this follows simply from the familiar counterexamples to utilitarianism, and that our examples have no more philosophical interest than those old counterexamples. Both of these thoughts would be mistaken.\nThe familiar counterexamples we have in mind include, for example, the case of the doctor who kills a healthy patient to harvest her organs, or the judge who executes an innocent man to prevent a riot. Importantly, those examples do not refute consequentialism in general, but only a version of consequentialism that adopts a particular kind of reductive analysis of the good. The details of the analysis won’t matter here, but it may be an analysis of goodness in terms on happiness, or preference satisfaction. If we give up the reductive analysis of goodness, we can say that the doctor and the judge do not make for a better society. A familiar heuristic supports that claim. (We take no stand here on whether this heuristic can be turned into an analysis.) Behind the Rawlsian veil of ignorance, we would prefer that there not be such doctors or judges in society. We think that most of us would agree, even in full appreciation of the possibility that we will be saved by the doctor, or possibly the judge. On the other hand, we think we’d prefer a society with the occasional boorish dinner guest, or a rare pie-thrower, to a society of moral saints. We say this in full appreciation of the possibility that we may get a pie in the face for our troubles. Possibly if we knew we would be the pie-throwee we would change our minds, but fortunately pies cannot penetrate the veil of ignorance.\nAlthough it isn’t much discussed in the literature, we think this form of consequentialism is interesting for several reasons beyond its capacity to avoid counterexamples. For one thing, it is not easy to say whether this counts as an agent-neutral ethical theory. On the one hand, we can say what everyone should do in neutral terms: for each person it is better if they do things that create a better world from the perspective of those behind the veil of ignorance. On the other hand this rule leads to obligations on agents that do not seem at all neutral. From behind the veil of ignorance we’d prefer that parents love their children and hence privilege their interests, and that they love them because they are their children not because this creates a better world, so parents end up with a special obligation to their children. Having this much (or more importantly this little) neutrality in a moral theory sounds quite plausible to us, and although we won’t develop the point here there is possibly an attractive answer to the ‘nearest and dearest’ objection to consequentialism (Jackson 1991). More generally, because we have preferences from behind the veil of ignorance about why people act and not just about how they act – we prefer for instance that people visit sick friends in hospital because they are friends not because of an abstract sense of duty – this form of consequentialism is not particularly vulnerable to objections that claim consequentialists pay too little attention to motives.\nSo we think a consequentialist can avoid the standard objections to utilitarianism by being less ambitious and not trying to provide a reductive analysis of goodness. The most natural retreat is to behind the veil of ignorance, but our examples can reach even there. This is far from the only interesting consequence of the examples.\nThe Good, the Right, and the Saintly\nWe think that the cases of the curser and the pie-thrower are examples of situations in which (a) an agent ought not to \\(\\varphi\\), and (b) it’s best that the agent does \\(\\varphi\\). Our judgements about the cases are not based on any theoretical analysis of the right and the good. They’re simply intuitions about cases—it just seems to us that the right thing to say about the pie thrower is that she ought not to do what she does, but that it’s still best if she does it. To the extent that these intuitions are puzzling or theoretically problematic (and we think that they are at least a little bit puzzling, and at least potentially problematic), it’s open to us to reject one or the other intuition about the cases, and either deny that the curser and the pie thrower ought not to curse or throw pies, or deny that it’s best that they do curse and throw pies. This is an option, but we think it’s not a very attractive one. Suppose that instead we take the intuitions at face value, and accept our judgements about the cases. What follows?\nOur analysis of the examples is incompatible with two attractive views about the connection between goodness (that is, the property of things—in particular worlds—in virtue of which some of them stand in the better than relation to others) and rightness, and between goodness and good character:\nIt’s better if everyone does what’s right.\nIt’s better if everyone has good character.1\nNow, neither of these will do as a philosophical thesis. But it’s probably not worth spending the time and effort on patching them up, since even the patched-up versions will be false.\nIf the pie-thrower ought not to throw her pies, but it’s nonetheless best that she does, no patched-up version of (1) that captures the intuition behind it can be right. Any patched-up version of (1) will still be claiming that there’s a very tight connection between what it would be right for us to do (what we ought to do) and what it would be best for us to do. Any plausible elaboration on (1) will include a commitment to the thesis that, if we ought not to do something, then it’s best if we don’t do it. But if our analyses of the cases of the curser and the pie-thrower are right, then these are counterexamples.\nWhat about (2)? Well, it’s not better if the cursing dinner guest has good character. What happens if we suppose that the curser does have good character? One of two things: (i) He’ll no longer curse at dinner parties, and we’ll lose the benefits that come from his cursing. This would be bad. (ii) He’ll still curse at dinner parties, but he’ll be cursing in a studied way. He’ll be cursing because he’s seen that things will be better if somebody uses foul language in inappropriate circumstances, and he’s taken it upon himself to fill the unfilled functional role. This would also be bad. This sort of studied bad conduct doesn’t have the same value as bad conduct that springs from bad character. Here is some evidence for this: We value the curser’s breaching of societal norms, even though he ought not to do it. Were we to find out that every expletive had been studied, produced either to produce these important social goods, or to create a familiar bad-boy image, we would stop valuing his breachings of the moral order. They would, instead, become merely tiresome and annoying. Since we value spontaneous cursings which are products of less-than-optimal character, but we do not value studied cursings which are products of exemplary character, it’s very plausible to conclude (though admittedly not quite mandatory) that the spontaneous curses are much more valuable than the studied ones. We’re inclined to say, in fact, that while having a few spontaneous cursers around makes things better, having studied cursers around makes things worse. Since you have to have less-than-perfect character in order to be a spontaneous curser, it follows that you can’t get the benefits of having cursers around without having some people with less-than-perfect character around. And since it’s better to have the cursers than not, it’s better to have some people with less-than-perfect character around than not. This will be incompatible with almost any plausible way of cashing out (2).2\nA Problem about Quantifier Scope?\nBut isn’t there a sense in which (for example) the pie-thrower ought to throw his pies? After all, if nobody was throwing pies, we might think to ourselves, “gosh, it would be better if there were a few—not many, but a few—pie throwers around.” Then it would be natural to conclude, “somebody ought to start throwing pies at strangers.” And then it would be natural to infer that at least the first person to start throwing pies at strangers would be doing what they ought. It would be natural, but it would be wrong. The plausible reading of “someone ought to start throwing pies at strangers” is, “it ought to be that somebody starts throwing pies at strangers,” not, “there’s somebody out there such that they ought to start throwing pies at strangers.” So we haven’t gotten anybody a moral license to throw pies yet. And in fact it’s very plausible that we ought to understand assertions that it ought to be that P as claiming that it would be better if it were the case that P; that is, as making claims about what would be good, not about what would be right.\nThere’s a puzzle about what to make of cases where we’re inclined to say that it ought to be that somebody \\(\\varphi\\)s—that is, that somebody ought to \\(\\varphi\\); but also that there’s nobody such that they ought to \\(\\varphi\\)—in fact, that everybody is such that they ought not to \\(\\varphi\\).3 Maybe the fact that our intuitions about the examples give rise to these kinds of puzzling cases is evidence that one or the other of our intuitions ought to be rejected. The move we suggested above is that the reason this seems so puzzling is that we’ve been punning on “ought.” The “ought” in “somebody ought to start throwing pies” doesn’t have anything much to do with what moral obligations anybody has—doesn’t have anything much to do with what’s right—but has a great deal to do with what’s good. And if that’s the case, then all we have is more evidence against the tight connection between the right and the good: it would be better if somebody started throwing pies, but everybody has a moral obligation not to. So it would be better if somebody did what they oughtn’t.\nValue, Desire and Advice\nAlthough the “ought” in “somebody ought to throw pies” has little to do with what’s right, it might have a lot to do with what we find desirable. And this will cause problems for some familiar meta-ethical theories. Quite naturally, Jack does not desire to throw pies at strangers for amusement in the actual world. Jack’s a very civic minded fellow in that respect. In fact, his concern for others goes deeper than that. He’d be quite prepared to risk his body for the sake of his fellow citizens. As it turns out, he’s been a volunteer fire fighter for years now. And Jack likes to think that if need be, he would be prepared, to use an old fashioned phrase, to risk his soul for the community. He hopes he would be morally depraved if what the society needed was depravity. Jack agrees with the discussion of character in section 2, so he hopes that when society needs a pie-thrower, he will step up with the plate, and do so directly because he wants to throw pies at innocent bystanders. Letting C stand for the circumstances described above, where it would be good for there to be more wrongdoing, Jack’s position can be summarised by saying that he desires that in C he desires that he throws pies at innocents.\nDoes this all mean Jack values his throwing pies at innocents in C? Not necessarily. Does it mean that if we were all like Jack, and we are subjectivists about what is right, it would be right to throw pies at innocents in C? Definitely not. David Lewis (1989) equates what we value with what we desire to desire.4 And he equates what is valuable with what we value. The text is not transparent, but it seems Lewis wants valuable to subsume both what we call the ‘right’ and the ‘good.’ And this he cannot have. Assume that everyone in Jack’s community desires to (de se) desire that (s)he throw pies at innocents in C. That does not make it right that pies are thrown at innocents. We take no stand here on whether the flaw is in the equation of personal value with second-order desire, or in the reduction of both rightness and goodness to personal value, but there is a problem for Lewis’s dispositional theory of value.5\nThis point generalises to cause difficulties for several dispositional theories of value. For example, Michael Smith (1994) holds that right actions are what our perfectly rational selves would advise us to do. This assumes that when the good and the right come apart, our perfectly rational selves would choose the right over the good. And it’s far from clear that Smith has the resources to argue for this assumption. Smith’s argument that our perfectly rational selves will advise us to do what is right relies on his earlier argument that anyone who does not do what she judges to be right is practically irrational, unlike presumably our perfectly rational selves. And the main argument for that principle is that it is the best explanation of why actually good people are motivated to do what they judge to be right, even when they change their judgements about what is right. But now we should be able to see that there’s an alternative explanation available. Actually good people might be motivated to do what they judge to be good rather than right. We have seen no reason to believe that the right and the good actually come radically apart, so this is just as good an explanation of the behaviour actual moral agents as Smith’s explanation. So for all Smith has argued, one might judge \\(\\varphi\\)ing to be right, also judge it not to be good, hence be not motivated to \\(\\varphi\\), and not be practically irrational. Indeed, our perfectly rational self might be just like this.6 Hence we cannot rely on our perfectly rational self to be a barometer of what is right, as opposed to what is good.\n\n\nDriver, Julia. 2001. Uneasy Virtues. Cambridge: Cambridge University Press.\n\n\nHurka, Thomas. 2001. “Vices as Higher-Level Evils.” Utilitas 13 (2): 195–212. https://doi.org/10.1017/s0953820800003137.\n\n\nJackson, Frank. 1991. “Decision Theoretic Consequentialism and the Nearest and Dearest Objection.” Ethics 101 (3): 461–82. https://doi.org/10.1086/293312.\n\n\nLewis, David. 1989. “Dispositional Theories of Value.” Aristotelian Society Supplementary Volume 63 (1): 113–37. https://doi.org/10.1093/aristoteliansupp/63.1.89.\n\n\nSmith, Michael. 1994. The Moral Problem. Oxford: Blackwell.\n\n\nProposition (2) is quite a natural position to hold if one is trying to capture the insights of virtue ethics in a consequentialist framework, as in Driver (2001) or Hurka (2001). But if we take ‘better’ in a more neutral way, so (2) does not mean that there are better consequences if everyone has good character, but simply that the world is a better place if this is so, even if this has few consequences, or even negative consequences, then it will be a position common to most virtue ethicists.↩︎\nSpecifically, it will be incompatible with any maximizing version of (2). There are ‘threshold’ versions of (2) that don’t fall afoul of this kind of problem because they don’t claim it would be best for everyone to have perfect character, but only that it would be best for everyone to have pretty good character, or at least for nobody to have really bad character.↩︎\nIt’s actually the second part that makes it puzzling. Compare the familiar and unproblematic situation in which we ought to give you a horse, but there’s no horse such that we ought to give you that one, and the more troubling situation in which we ought to give you a horse, but every horse is such that we ought not to give you that one.↩︎\nMore precisely, with what we desire to desire in circumstances of appropriate imaginative acquaintance. We can suppose that Jack, and everyone else under discussion in this paragraph, is suitably imaginatively acquainted with the salient situations. Jack knows full well what it is like to get a pie in the face.↩︎\nSomeone might think it obvious that Lewisian value can’t be used in an analysis of both rightness and goodness, since it is one concept and we are analysing two concepts. But Lewisian value bifurcates in a way that one might think it is suitable for analysing both rightness and goodness. Since there are both de dicto and de se desires, one can easily draw out both de dicto and de se values. And it is prima facie plausible that the de dicto values correspond to what is good, and the de se values to what is right. Indeed, given a weak version of consequentialism where these two can be guaranteed to not directly conflict, this correspondence may well hold. But we think the pie-thrower threatens even those consequentialists. The net philosophical conclusion is that the pie-thrower is a problem for Lewis’s meta-ethics, but only because (a) she is a problem for Lewis’s consequentialism, and, surprisingly, (b) Lewis’s meta-ethics depends on his consequentialism being at least roughly right.↩︎\nWe have glossed over a technical point here that is irrelevant to the current discussion. What matters is not whether our perfectly rational selves are motivated to \\(\\varphi\\), it matters whether they desire that we \\(\\varphi\\), and hence whether they are motivated to advise us to \\(\\varphi\\). Keeping this point clear matters for all sorts of purposes, but not we think the present one.\n\n↩︎\n",
    "preview": "posts/2021-01-05-pranksters-ethics/pie.jpg",
    "last_modified": "2021-02-05T15:22:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-review-of-the-realm-of-reason/",
    "title": "Review of “The Realm of Reason”",
    "description": "Review of Christopher Peacocke, “The Realm of Reason”. Oxford: Clarendon Press, 2004",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2004-10-15",
    "categories": [
      "book review",
      "on books",
      "epistemology",
      "metaphysics"
    ],
    "contents": "\nSome of what we know we know by experience and some by reason. It’s experience not reason that teaches that Arsenal ended last season with 90 points and Chelsea with 79, it’s reason not experience that teaches 90 is greater than 79, and, arguably, it’s the two together that teach that Arsenal ended with more points than Chelsea. One useful classification of philosophers is by the relative importance they assign to experience and reason in grounding what we know. Empiricists (on one reading of that term) play down reason, sometimes going so far as to declare that anything known by a means other than experience must be a mere matter of definition. Rationalists play reason up.\n\nPublished in the Times Literary Supplement\nChristopher Peacocke is firmly in the rationalist camp, and The Realm of Reason is an attempt to lay out what he takes rationalism to be. It gives his preferred version of rationalism and some arguments in its favour. It’s much too much to attempt in a short book and it isn’t entirely persuasive on any of the applications, but it is a grand vision for what a global rationalism might look like, one that might prove attractive even if the details need work. Given the length of the book a surprising amount of time is spent on relatively abstruse details. Peacocke provides a particularly careful account of what distinguishes rationalists from empiricists and does a lot of work classifying and adjudicating between rationalisms of various strengths. These are the best parts of the book, but also the least accessible.\nPeacocke’s preferred version of rationalism has two distinctive components. First, he focuses not on beliefs, as is usual, but on the “transitions” between representational states that occur in thought, as when we move to a new belief on the basis of one we already have. Mental representational states are often beliefs, but they also include things, like perceptions, that have representational content without necessarily being believed. Peacocke’s rationalist claim is that for any justified transition, there’s an a priori explanation of why it is justified. Second, he insists that this explanation rely crucially on the contents of the states involved in the transition.\nSo we get a quite strong “foundationalist” epistemology. Experience provides the foundations for empirical knowledge, but how we get from there to what we know is entirely in the domain of reason. It is famously difficult to justify many steps by reason alone, and the most pressing is the very first: How do we justify the transition from appearances to reality, such as the transition from That looks crooked to That is crooked? Some philosophers have thought that we need to link appearance and reality so closely that the link is infallible. Peacocke doesn’t take that line, so he has to justify the transition some other way.\nDescartes faced a similar problem when trying to get over his radical doubt about the existence of the material world, and solved it by appeal to God. We can tell a priori, he thought, that a benevolent God exists, and a benevolent God will not let us be deceived about this matter, at least when we are careful enough to rely on clear and distinct perceptions. Now Descartes had to be careful here to only appeal to a priori reasons for belief in God. He couldn’t, for instance, argue from the apparent design of the universe to the existence of a designer, because we can’t tell at this stage whether the apparent design is merely an artifact of our defective perceptual faculties. Indeed, we can’t rely on any apparent fact about the external world until we’ve determined that appearances are a good guide to reality. So we need to argue for the existence of God without appeal to perception, and then use God’s existence to justify future reliance on perception.\nIn keeping with the spirit of the age, Peacocke updates Descartes’s strategy by replacing God with Darwin. Very roughly, Peacocke argues that the best explanation of our having representative capacities at all is that we are the products of a long process of natural selection. And if we are the products of a long process of selection, then we probably have accurate representations. If those two claims can be justified a priori we have an a priori argument to the (prima facie, probable) accuracy of our representations.\nLess roughly, Peacocke argues for a “Complexity Reduction Principle”. We are entitled, on a priori grounds, to believe that complex phenomena have explanations, and we are entitled to regard simpler explanations as more probably true than more complex ones. That we have representations at all is a complex matter. How might it be explained? One explanation is via Divine creation. Another is that we are “brains-in-vats” living in a virtual reality world dreamt up by some quirky scientist (cf The Matrix). But neither of these explanations really reduces the complexity, since in each case we need to appeal to a thing (God, the scientist) that already has representational capacities. A simpler explanation, allegedly, is that we are the product of natural selection and having accurate representations is selected for. This is certainly a novel argument for Darwinism. It isn’t why they teach natural selection to biology students. And of course it has flaws. Peacocke does little to show that there are no better explanations of our having representations. Nor does he address the question of how complicated hereditary mechanisms must be if they are to support natural selection. Arguably they are much more complicated than is needed for representation, so Darwin doesn’t help reduce complexity here.\nSo it’s not clear Peacocke’s rationalism can get past step one; but let’s see what would happen next. To go beyond particular perceptions, in acquiring knowledge, we need induction. Peacocke takes the basic form of enumerative induction to be the (defeasible) inference from All the (many and varied) observed Fs have been Gs to All Fs are Gs. The observation of only Gs, and no non-Gs, amongst these many and varied Fs is a complex fact, and its best (ie simplest) explanation is sometimes that all the Fs are Gs. Peacocke argues that in these cases this explanation is the a priori justification of the transition, and in only these cases is the transition justified; he concludes that induction is acceptable by rationalist lights.\nThe chapter on induction is only fifteen pages long, and it really needs to be much longer. Peacocke sets out the position just outlined, and compares it in some detail to a similar position advocated by Gilbert Harman, and that’s it. There is no discussion of what we do when most, rather than all, the observed Fs have been Gs, even though that’s surely the more important practical case. There’s no discussion of the case that’s frequently central to modern discussions on induction—the case in which a certain (stable) ratio of the Fs are Gs. Peacocke only talks about the special case when all Fs are Gs, and it isn’t obvious that the discussion generalizes. There is no discussion of rationalist alternatives, such as Keynes’s justification of enumerative induction in terms of analogical inference, or D. C. Williams’s probabilistic defence of induction. And there’s no discussion of empiricist attempts to justify induction a posteriori, or to do without it. Even if Peacocke’s suggested justification works, and it is at least a serious contender, a persuasive treatment of induction should have dealt with at least some of these points.\nThe final two chapters discuss moral beliefs. Again, Peacocke thinks that all the inferences we make in order to get from our perceptual beliefs to our moral beliefs can be justified a priori. His view is that we can come to know a priori some moral principles. And we can know contingent moral facts, such as that someone’s giving £1000 to Oxfam is morally praiseworthy, by carrying out the following inference. The person, say Joe, helped other people in need. (We learn this by experience.) Helping those in need is morally praiseworthy. (We learn this moral principle by deploying our reason.) Hence what Joe did is morally praiseworthy. But there’s a problem here, and Peacocke never fully addresses it. It’s only prima facie true that helping those in need is morally praiseworthy. There are always exceptions to the principle. If Joe’s children starved to death because that donation was the last money Joe had to buy them food, the donation wasn’t morally praiseworthy. Moreover, it is just about impossible to state the exceptions without using moral language. So it is far from clear how we are meant to come to know that this case is not one of the exceptions, because knowing this requires both empirical knowledge and moral sensitivity. From a “principleist” position like Peacocke’s, knowing this is not one of the exceptions seems just as hard as the original problem of coming to know that the action was praiseworthy. So it seems the rationalist still has work to do here.\nOne can easily get the feeling from this book that rationalism runs into problems as soon as we try to apply it to real-world cases. But it isn’t obvious these are deep problems with rationalism, and in particular it isn’t clear that the problems can’t be fixed with relatively minor adjustments. Even if there are difficulties in application throughout The Realm of Reason, there is a lot of important philosophical work going on beneath the surface. Peacocke’s best work is done in classifying the various types of rationalist position that are available, and motivating the kind of view he wants to defend. This material remains valuable, highly valuable to anyone wanting to draw a plausible rationalist picture, even if his real-world applications are not yet perfect.\n\n\n\n",
    "preview": "posts/2021-01-04-review-of-the-realm-of-reason/peacocke.jpg",
    "last_modified": "2021-03-04T11:24:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-07-luminous-margins/",
    "title": "Luminous Margins",
    "description": "Timothy Williamson has recently argued that few mental states are luminous, meaning that to be in that state is to be in a position to know that you are in the state. His argument rests on the plausible principle that beliefs only count as knowledge if they are safely true. That is, any belief that could easily have been false is not a piece of knowledge. I argue that the form of the safety rule Williamson uses is inappropriate, and the correct safety rule might not conflict with luminosity.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2004-07-01",
    "categories": [
      "epistemology",
      "notes"
    ],
    "contents": "\n\nContents\nLuminosity\nWilliamson’s Example\nReliability\nSafety\nRetractions\n\nLuminosity\nIn Knowledge and Its Limits Timothy Williamson argues that few conditions are luminous.1 A condition is luminous iff we know we are in it whenever we are. Slightly more formally, Williamson defines\nPublished in  83: 373-383.\nThanks to Tamar Szabó Gendler, John Hawthorne, Chris Hill, Ernest Sosa and the ’s referees.\n\nA condition C is defined to be luminous if and only if (L) holds:\n\\(L\\)\nFor every case \\({\\alpha}\\), if in \\({\\alpha}\\) C obtains, then in \\({\\alpha}\\) one is in a position to know that C obtains (95).\n\n\nIntuitively, the argument against this is as follows. The following three conditions are incompatible.\nGradual Change\nThere is a series of cases, each very similar to adjacent cases, that starts with a case where C clearly obtains, and ends with a case where C clearly doesn’t obtain.\n\nLuminosity\nWhenever C obtains you can know it does.\n\nSafety\nOnly safe beliefs count as knowledge, so whenever you can know that C obtains, C obtains in all very similar cases.\n\nLuminosity and Safety entail\nTolerance\nWhenever C obtains, it obtains in all very similar cases.\n\nBut Tolerance is incompatible with Gradual Change, since Tolerance entails that if the first member of the series is a case where C obtains, then every successive member is also a case where C obtains. Williamson argues that for any interesting epistemic condition, Gradual Change is a clear possibility. And he argues that Safety is a general principle about knowledge. So Luminosity must be scrapped. The counterexamples to Luminosity we get from following this proof through are always borderline cases of C obtaining. In these cases Luminosity fails because any belief that C did obtain would be unsafe, and hence not knowledge.\nI will argue, following Sainsbury (1995), that Williamson has misinterpreted the requirement that knowledge be safe. The most plausible safety condition might be compatible with Gradual Change and Luminosity, if we make certain plausible assumptions about the structure of phenomenal beliefs.\nOne consequence of the failure of Luminosity is that a certain historically important kind foundationalist analysis of knowledge fails. This kind of foundationalist takes the foundations to be luminous. Although I think Williamson’s argument against Luminosity does not work, my objections are no help to the foundationalist. As I said, my objection to Williamson rests on certain assumptions about the structure of phenomenal beliefs. It is a wide open empirical and philosophical question whether these assumptions are true. If this kind of foundationalism provided a plausible analysis of knowledge, then it would be a wide open question whether our purported knowledge rested on any foundations, and hence a wide open question whether we really had any knowledge. But this is a closed question. It is a Moorean fact that we know many things. So while I object to Williamson’s claim that we have no luminous mental states, I do not object to the weaker claim that we might not have any luminous mental states, and this claim is enough to do much of the philosophical work to which Williamson puts Luminosity.\nWilliamson’s Example\nWilliamson suggests that (L), the formal rendition of Luminosity, fails for all interesting conditions even if we restrict the quantifier to those that are ‘physically and psychologically feasible’ [94], and I will assume that is what we are quantifying over. To argue that (L) fails for any interesting C, Williamson first argues that it fails in a special case, when C is the condition feeling cold, and then argues that the conditions that lead to failure here are met for any other interesting C. So I will also focus on the special case.\nMr Davis’s apartment faces southwest, so while it is often cold in the mornings it always warms up as the midday and afternoon sun streams in. This morning Mr Davis felt cold when he awoke, but now at noon he is quite warm, almost hot. But the change from wake-up time to the present is rather gradual. Mr Davis does not take a hot bath that morning, nor cook a hot breakfast, but sits reading by the window until the sun does its daily magic. Assume, for the sake of the argument, that feeling cold is luminous, so whenever Mr Davis feels cold, he knows he feels cold. Williamson argues this leads to a contradiction as follows. (I’ve changed names and pronouns to conform with my example.)\n\nLet t0, t1, …, tn be a series of times at one millisecond intervals from dawn to noon. Let \\({\\alpha}\\)i be the case at ti (0 \\({\\leq}\\) i \\({\\leq}\\) n). Consider a time ti between t0 and tn, and suppose that at tn Mr Davis knows that he feels cold. … Now at ti+1 he is almost equally confident that he feels cold, by the description of the case. So if he does not feel cold at ti+1, then his confidence at ti that he feels cold is not reliably based, for his almost equal confidence on a similar basis one millisecond earlier that he felt cold was misplaced … His confidence at ti was reliably based in the way required for knowledge only if he feels cold at ti+1. In the terminology of cases…:\n(ii) If in \\({\\alpha}\\)i he knows that he feels cold, then in \\({\\alpha}\\)i+1 he feels cold. (97)\n\nGiven (L), all instances of (ii), and the fact that Mr Davis feels cold when he awakes, we get the false conclusion that he now feels cold. So if we accept all instances of (ii), we must conclude that (L) is false when C is feeling cold and ‘one’ denotes Mr Davis. Why, then, accept (ii)? One move Williamson makes here is purely defensive. He notes that (ii) is different from the conditionals that lead to paradox in the Sorites argument. The antecedent of (ii) contains the modal operator knows that absent from its consequent, so we cannot chain together instances of (ii) to produce an implausible conditional claim. If that operator were absent then from all the instances of (ii) it would follow that if Mr Davis feels cold at dawn he feels cold at noon, which is false. But by strengthening the antecedent, Williamson weakens (ii) to avoid that conclusion. But the fact that (ii) is not paradoxical is not sufficient reason to accept it.\nReliability\nIt is useful to separate out two distinct strands in Williamson’s argument for (ii). One strand sees Williamson arguing for (ii) by resting on the principle that beliefs constitute knowledge only if they are reliably based. The idea is that if Mr Davis’s belief that he feels cold is a bit of knowledge, it is reliable, and if it is reliable it is true in all similar situations, and hence it is true in \\({\\alpha}\\)i+1. The other strand sees him appealing to a vague but undoubtedly real requirement that beliefs must be safely true in order to be knowledge. Neither argument is successful, though the second kind of argument is better than the first.\nWilliamson acknowledges Conee and Feldman’s arguments that no reliabilist epistemologist has yet solved the generality problem (100). But he takes this to be reason to abandon not the concept of reliability, but the hope of providing a reductive analysis of it. Williamson thinks we can get a long way by just resting on the intuitive concept of reliability. This seems to be a mistake. There are two ordinary ways of using ‘reliable’ in the context of discussing beliefs, and neither provides support for (ii).\nFirst, and this is clearly not what is needed, sometimes ‘reliable’ just means true. This is the sense of the word in which we can consistently say, “It turned out the information that old Ronnie provided us about where the gov’nor was eating tonight was reliable, which was plenty surprising since Ronnie hadn’t been right about anything since the Nixon administration.” This is the sense in which reliable means just what the etymology suggests it means, something that can be relied upon. And that means, in practice, true. But that won’t help at all, for if ‘reliable’ just means true, then nothing follows from the fact that knowledge is reliable that does not follow from the fact that it is factive.\nSecond, there is a distinctively philosophical sense in which reliable means something more like true in a wide range of circumstances. This is the sense in which a stopped clock is not even reliable twice a day. At first, this might look to help Williamson a little more. But if philosophical usage is to be key, the second look is more discouraging. For in its philosophical usage, reliability does not even entail truth. And if reliability does not entail truth in the actual situation, it surely does not entail truth in nearby situations. But Williamson’s argument for (ii) requires that reliability in \\({\\alpha}\\)i entails truth in \\({\\alpha}\\)i+1. So on neither of its natural readings does the concept of reliability seal the argument here, and since we have no unnatural reading to fall back upon, the argument from reliability for (ii) fails. To be fair, by chapter 5 of Williamson’s book the concept of reliability that seems to be employed is little distinguishable from the concept of safety. So let us turn to those arguments.\nSafety\nWilliamson at times suggests that the core argument for (ii) is a straight appeal to intuition. “[E]ven when we can appeal to rigorous rules, they only postpone the moment at which we must apply concepts in particular cases on the basis of good judgement. … The argument for (ii) appeals to such judgement.” (101) The appeal to intuition is the royal road to scepticism, so we would be justified in being a little wary of it. Weinberg, Stich, and Nichols (2001) discovered that undergraduates from the same social class as Williamson, Mr Davis and I would frequently judge that a subject could not know that mule was a mule unless he could tell it apart from a cleverly painted zebra. The judgements of that class are not obviously the basis for a sane epistemology.\nWilliamson undersells his argument by making it an appeal to judgement. For there is a principle here, if not a rigorous rule, that grounds the judgement. The principle is something like Ernest Sosa’s safety principle. The idea is that a belief does not constitute knowledge if it is false in similar situations. “[N]ot easily would S believe that p without it being the case that p.” (Sosa 1999, 142) There is much to be said here about what is a similar situation. (David Lewis (1996) discusses a concept of similarity in the context of saying that worlds can be salient, in his sense, in virtue of being similar to salient worlds.) It might turn out that there is no account of similarity that makes it plausible that this is a constraint on knowledge. But for present purposes I am prepared to grant (a) that only safe beliefs count as knowledge, and (b) that \\({\\alpha}\\)i+1 is a similar situation to \\({\\alpha}\\)i.\nThis might seem like too much of a concession to Williamson, for it already conflicts with some platitudes about knowledge. Consider a case that satisfies the following three conditions. Some light reflects off a leopard some distance away and strikes our eyes. The impact of that light causes, by the normal processes, a belief that a leopard is nearby to appear in our belief box. Beliefs, including leopard-related beliefs, that we form by this kind of process are on the whole very reliable. You might think these conditions are sufficient for our belief to count as knowledge that a tiger is present. The proponent of Safety denies this. She says that if, for example, there are several cheetahs with a particularly rare mutation that make the look much like leopards around, and if we saw them at similar distance we would have mistaken them for leopards. Since we could easily have had the belief that a leopard is nearby while there were no leopards, only cheetahs, nearby, the belief is not safe and so does not count as knowledge.\nThere are two reasons to think that safety is too strong here, neither of which strike me as completely compelling. (I’m still conceding things to Williamson here. If there’s a general objection to Safety then his argument against Luminosity does not get off the ground. That’s not my position. As I’ll soon argue, I think Williamson has misinterpreted Safety.) The first reason is a worry that if we deny knowledge in a case of reliable veridical perception, we are conceding too much to the sceptic. But the proponent of Safety has a very good reason to distinguish this case from my current veridical perception of a table - my perception is safe and the perception of a leopard is not. So there is no slippery slope to scepticism here. The second is that the allegedly similar case is not really that similar, because in that case the belief is caused by a cheetah, not a leopard. But to regard cases where the evidence is different in this way as being dissimilar is to make the safety condition impotent, and Sosa has shown that we need some version of Safety to account for our intuitions about different cases.2\nSo I think some version of Safety should be adopted. I don’t think this gives us (ii), for reasons related to some concerns first raised by Mark Sainsbury (1995). The role for Safety condition in a theory of knowledge is to rule out knowledge by lucky guesses. This includes lucky guesses in mathematics. If Mr Davis guesses that 193 plus 245 is 438, he does not thereby know what 193 plus 245 is. Can Safety show why this is so? Yes, but only if we phrase it in a certain way. Assume that we have a certain belief B with content p. (As it might be, Mr Davis’s belief with content 193 + 245 = 438.) Then the following two conditions both have claims to being the correct analysis of ‘safe’ as it appears in Safety.\nContent-safety\nB is safe iff p is true in all similar worlds.\n\nBelief-safety\nB is safe iff B is true in all similar worlds.\n\nIf we rest with content-safety, then we cannot explain why Mr Davis’s lucky guess does not count as knowledge. For in all nearby worlds, the content of the belief he actually has is true. If we use belief-safety as our condition though, I think we can show why Mr Davis has not just got some mathematical knowledge. The story requires following Marian David’s good advice for token physicalists and rejecting content essentialism about belief (David (2002); see also Gibbons (1993). The part of Mr Davis’s brain that currently instantiates a belief that 193 plus 245 is 438 could easily have instantiated a belief that 193 plus 245 is 338, for Mr Davis is not very good at carrying hundreds while guessing. If, as good physicalists, we identify his belief with the part of the brain that instantiates it, we get the conclusion that this very belief could have had the false content that 193 plus 245 is 338. So the belief is not safe, and hence it is not knowledge.\nThis lends some credence to the idea that it’s belief-safety, not content-safety, that’s the important safety criteria. When talking about Mr Davis’s mathematical hunches, belief-safety is a stronger condition than content-safety. But when talking about his feelings, things may be reversed.\nLet me tell you a little story about how Mr Davis’s mind is instantiated. Mr Davis’s phenomenal beliefs do not arise from one part of his brain, his belief box or mind’s eye, tracking another part, the part whose states constitute his feeling cold. Rather, when he is in some phenomenal state, the very same brain states constitute both the phenomena and a belief about the phenomena. Mr Davis’s brain is so wired that he could not have any sensation of radiant heat (or lack thereof) without his thereby believing that he is having just that sensation, because he could not have felt cold without that feeling itself being a belief that he felt cold. In that case, belief-safety will not entail (ii). Imagine that at \\({\\alpha}\\)i Mr Davis feels cold, but at \\({\\alpha}\\)i+1 he does not. (I assume here, with Williamson, that there is such an i.) At \\({\\alpha}\\)i he thereby believes that he feels cold. The content of that belief is a de se proposition that is false at \\({\\alpha}\\)i+1, so it violates content-safety. But in \\({\\alpha}\\)t+1 that part of his brain does not constitute his feeling cold (for he does not feel cold), and thereby does not constitute his believing that he feels cold. By hypothesis, by that time no part of his brain constitutes feeling cold. So the belief in \\({\\alpha}\\)i that he feels cold is not false in \\({\\alpha}\\)i+1; it either no longer exists, or now has the true content that Mr Davis does not feel cold. So belief-safety does not prevent this belief of Mr Davis’s from being knowledge. And indeed, it seems rather plausible that it is knowledge, for he could not have had just this belief without it being true. This belief violates content-safety but not belief-safety, and since we have no reason to think that content-safety rather than belief-safety is the right form of the safety constraint, we have no reason to reject the intuition that this belief, this more or less infallible belief, counts as a bit of knowledge.\nThis story about Mr Davis’s psychology might seem unbelievable, so let me clear up some details. Mr Davis has both phenomenal and judgemental beliefs about his phenomenal states. The phenomenal beliefs are present when and only when the phenomenal states are present. The judgemental beliefs are much more flexible, they are nomically independent of the phenomena they describe. The judgemental beliefs are grounded in ‘inner perceptions’ of his phenomenal states. The phenomenal beliefs are not, they just are the phenomenal states. The judgemental beliefs can be complex, as in a belief that I feel cold iff it is Monday, while the phenomenal beliefs are always simple. It is logically possible that Mr Davis be wired so that he feel cold without believing he feels cold, but it is not an accident that he is so wired. Most of his conspecifics are similarly set up. It is possible that at a particular time Mr Davis has both a phenomenal belief and a judgemental belief that he feels cold, with the beliefs being instantiated in different parts of his brain. If he has both of these beliefs in \\({\\alpha}\\)i, then Williamson’s argument may well show that the judgemental belief does not count as knowledge, for it could be false in \\({\\alpha}\\)i+1. If he has the judgemental belief that he is not cold in \\({\\alpha}\\)i, then the phenomenal belief that he is cold may not be knowledge, for it is plausible that the existence of a contrary belief defeats a particular belief’s claim to knowledge. But that does not mean that he is not in a position to know that he is cold in \\({\\alpha}\\)i.\nSome may object that it is conceptually impossible that a brain state that instantiates a phenomenal feel should also instantiate a belief. And it is true that Mr Davis’s phenomenal states do not have some of the features that we typically associate with beliefs. These states are relatively unstructured, for example. Anyone who thinks that it is a conceptual truth that mental representations are structured like linguistic representations will think that Mr Davis could not have the phenomenal beliefs I have ascribed to him. But it is very implausible that this is a conceptual truth. The best arguments for the language of thought hypothesis rest on empirical facts about believers, especially the facts that mental representation is typically productive and systematic. If there are limits to how productive and systematic Mr Davis’s phenomenal representations are, then it is possible that his phenomenal states are beliefs. Certainly those states are correlated with inputs (external states of affairs) and outputs (bodily movements, if not actions) to count as beliefs on some functionalist conceptions of belief.\nA referee noted that we don’t need the strong assumption that phenomenal states can be beliefs to make the argument here, though it probably is the most illumination example. Either of the following stories about Mr Davis’s mind could have done. First, Mr Davis’s phenomenal belief may be of the form “I feel \\({\\phi}\\),” where “I” and “feel” are words in Mr Davis’s language of thought, and \\({\\phi}\\) is the phenomenal state, functioning as a name for itself. As long as the belief arises whenever Mr Davis is \\({\\phi}\\), and it has the phenomenal state as a constituent, it can satisfy belief-safety even when content-safety fails. The second option involves some more contentious assumptions. The phenomenal belief may be of the form “I feel thus,” where the demonstrative picks out the phenomenal state. As long as it is essential to the belief that it includes a demonstrative reference to that phenomenal state, it will satisfy belief-safety. This is more contentious because it might seem plausible that a particular demonstrative belief could have picked out a different state. What won’t work, of course, is if the phenomenal belief is “I feel F,” where F is an attempted description of the phenomenal state. That certainly violates every kind of safety requirement. I think it is plausible that phenomenal states could be belief states, but if you do not believe that it is worth noting the argument could possibly go through without it, as illustrated in this paragraph.\nMr Davis is an interesting case because he shows just how strong a safety assumption we need to ground (ii). For Mr Davis is a counterexample to (ii), but his coldness beliefs satisfy many plausible safety-like constraints. For example, his beliefs about whether he feels cold are sensitive to whether he feels cold. Williamson (Ch. 7) shows fairly conclusively that knowledge does not entail sensitivity, so one might have thought that in interesting cases sensitivity would be too strong for what is needed, not too weak as it is here. From this it follows that any safety condition that is strictly weaker than sensitivity, such as the condition that the subject could not easily believe p and be wrong, is not sufficient to support (ii). Williamson slides over this point by assuming that the subject will be almost as confident that he feels cold at \\({\\alpha}\\)i+1 as he is at \\({\\alpha}\\)i. This is no part of the description of the case, as Mr Davis shows.\nMy argument above rests on the denial of content essentialism, which might look like a relatively unsafe premise. So to conclude this section, let’s see how far the argument can go without that assumption. Sainsbury responds to his example, the lucky arithmetic guess, by proposing a different version of safety: mechanism-safety.\nMechanism-safety\nB is safe iff the mechanism that produced B produces true beliefs in all similar worlds.\n\nI didn’t want to rest on this too much because I think it’s rather hard to say exactly what the mechanism is that produces Mr Davis’s belief that he feels cold. But if it’s just his sensory system, then I think it is clear that even at \\({\\alpha}\\)i, Mr Davis’s belief that he feels cold satisfies mechanism-safety. The bigger point here is that content-safety is a very distinctive kind of safety claim, but it’s the only kind that justifies (ii).\nRetractions\nTo close, let me stress how limited my criticisms of Williamson here are. Very briefly, the argument is that there can be some self-presenting mental states, states that are either token identical with the belief that they exist or are constituents of (the contents of) beliefs that they exist, and these beliefs will satisfy all the safety requirements we should want, even in borderline cases. If some conditions are invariably instantiated by self-presenting states, then those conditions will be luminous. And I think it is a live possibility, relative at least to the assumptions Williamson makes, that there are such self-presenting states. But there aren’t very many of them. There is a reason I picked feels cold as my illustration. It’s not laughable that it is self-presenting.\nOn the other hand, it is quite implausible that, say, knowing where to buy the best Guinness is self-presenting. And for states that are not self-presenting, I think Williamson’s anti-luminosity argument works. That’s because it is very plausible (a) that for a belief to be knowledge it must satisfy either belief-safety or mechanism-safety, (b) a non-self-presenting state satisfies belief-safety or mechanism-safety only if it satisfies content-safety, and (c) as Williamson showed, if beliefs about a state must satisfy content-safety to count as knowledge, then that state is not luminous. So epistemic states, like the state of knowing where to buy the best Guinness, are not luminous. That is to say, one can know where to buy the best Guinness without knowing that one knows this. And saying that (for these reasons) is to just endorse Williamson’s arguments against the KK principle. Those arguments are an important special case of the argument against luminosity, and I don’t see how any of my criticisms of the general argument touch the special case.\nWilliamson describes his attacks on luminosity as an argument for cognitive homelessness. If a state was luminous, that state would be a cognitive home. Williamson thinks we are homeless. I think we may have a small home in our phenomenal states. This home is not a mansion, perhaps just a small apartment with some afternoon sun, but it may be a home.\nDon’t be fooled into thinking this supports any kind of foundationalism about knowledge, however. It is true that if we have the kind of self-presenting states that Mr Davis has (under one of the three descriptions I’ve offered), then we have the self-justifying beliefs that foundationalism needs to get started. But it is at best a wide-open philosophical and scientific question whether we have any such states, while it is not a wide-open question whether we have any knowledge, or any justified beliefs. If these states are the only things that could serve as foundations, it would be at least conceptually possible that we could have knowledge without self-justifying foundations. So the kind of possibility exemplified by Mr Davis cannot, on its own, prop up foundationalism.\n\n\nDavid, Marian. 2002. “Content Essentialism.” Acta Analytica 17: 103–14. https://doi.org/10.1007/bf03177510.\n\n\nGibbons, John. 1993. “Identity Without Supervenience.” Philosophical Studies 70 (1): 59–79. https://doi.org/10.1007/bf00989662.\n\n\nLewis, David. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nSainsbury, Mark. 1995. “Vagueness, Ignorance and Margin for Error.” British Journal for the Philosophy of Science 46: 589–601. https://doi.org/10.1093/bjps/46.4.589.\n\n\nSosa, Ernest. 1999. “How to Defeat Opposition to Moore.” Philosophical Perspectives 13: 141–53. https://doi.org/10.1111/0029-4624.33.s13.7.\n\n\nWeatherson, Brian. 2003. “What Good Are Counterexamples?” Philosophical Studies 115 (1): 1–31. https://doi.org/10.1023/A:1024961917413.\n\n\nWeinberg, Jonathan, Stephen Stich, and Shaun Nichols. 2001. “Normativity and Epistemic Intuitions.” Philosophical Topics 29 (1): 429–60. https://doi.org/10.5840/philtopics2001291/217.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nWilliamson (2000) Ch. 4; all references to this book unless otherwise specified.↩︎\nI assume here a relatively conservative epistemological methodology, one that says we should place a high priority on having our theories agree with our intuitive judgments. I’m in favour of a more radical methodology that makes theoretical virtues as important as agreement with particular intuitions Weatherson (2003). On the radical view Safety might well be abandoned. But on that view knowledge might be merely true belief, or merely justified true belief, so the argument for Luminosity will be a non-starter. But the argument of this paper does not rest on these radical methodological principles. The position I’m defending is that, supposing a standard methodological approach, we should accept a Safety principle. But as I’ll argue, the version of Safety Williamson adopts is not appropriate, and the appropriate version does not necessarily support the argument against Luminosity.\n\n↩︎\n",
    "preview": "posts/2021-01-07-luminous-margins/luminous.jpg",
    "last_modified": "2021-02-05T20:41:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-nine-objections-to-steiner-and-wolff-on-land-disputes/",
    "title": "Nine Objections to Steiner and Wolff on Land Disputes",
    "description": "Nine objections to Steiner and Wolff on land disputes.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-10-01",
    "categories": [
      "political philosophy",
      "philosophy of economics"
    ],
    "contents": "\nIn the July 2003 Analysis, Hillel Steiner and Jonathan Wolff (2003) propose a framework for “resolving disputed land claims between competing nations or ethnic groups.” The idea is that we should auction off the land, with the loser of the auction getting the money. While this might mean that the richer party will normally end up with the land, and this is normally not thought to be a good thing, if the auction is conducted as they specify “it will turn out that the other party ends up with something which, in the circumstances, it prefers to the land: lots of money.”\nPublished in Analysis 63: 321-327.\nPicture by Mustang Joe via Creative Commons.\nActually, it isn’t so clear that this is what will result. Let’s say we have a particular parcel of land that groups A and B want. They each want it quite strongly, but B has deeper pockets than A, so while A would be prepared to pay 8 for the land, B would be prepared to pay 12. For the auction process to function, there must be a minimum bid increment, I’ll say it is \\(\\frac{1}{2}\\). Assume that B has just bid 4, A must now choose whether to bid 4\\(\\frac{1}{2}\\) or accept B’s bid. And assume for now that A is not bidding tactically, it only makes a bid if it would prefer to win the auction with that bid than accept B’s bid. This assumption will be relaxed below.\nSo for now, A must decide whether it prefers to be given 4, or to get the land for 4\\(\\frac{1}{2}\\). Since it values the land at 8, and since it will give up 8\\(\\frac{1}{2}\\) to buy the land (the 4\\(\\frac{1}{2}\\) it will pay, plus the 4 it would have received from B) it may well decide to just accept the bid. But now it has ended up with something it definitely does not prefer to the land, since it just accepted a bid for 4. There are two assumptions at play here. One is that A doesn’t bid tactically, which I shall return to a bit. The other is that how much A will pay for the land is not affected by receiving B’s 4. That is, I assume that the marginal utility of money is relatively constant for A over the ranges of money at play in the auction. This assumption might be false if we’re dealing with a very large or valuable body of land, but it’s not unreasonable in most circumstances. (Space prevents a complete study of what happens if we take the declining marginal utility of money completely into account. Roughly, the effect is that some of my criticisms are slightly vitiated.) Now while these assumptions might be false, Steiner and Wolff give us no reason to be certain they are false. So for all they’ve said we could have a situation just like this one, where the poorer party ends up with something it wants much less than the land. Hence\n\nObjection 1. There is no guarantee that the losing party will end up with something they prefer to the land.\n\nWhile this contradicts an alleged benefit of Steiner and Wolff’s plan, it might not be thought to be a deep problem. After all, A gets half as much as they wanted, and if they are only one of two equal claimants to the land, then this is a fair result. This may be true, but note that the assumption that each party has an equal claim to the land is doing a lot of work here. If A’s claim is stronger, then only getting half of the value of the land is quite unfair. If the two claims are incommensurable, there may be no fact of the matter whether it is fair that A receives 4. If we cannot tell which of the moral claims is stronger, which is very often the case in land disputes, it may be impossible to tell whether A’s receiving 4 is fair or not. Hence\n\nObjection 2. The proposal is only appropriate where each party has a genuinely equal moral claim to the land. This doesn’t happen often, and it is quite rare that we know it happens.\n\nWhile Steiner and Wolff note that they are leaving questions about enforcement and compliance to another place, so it isn’t fair to press them too strongly on these topics, it is worth noting how this feature of their proposal makes compliance harder to enforce. If by participating in the auction both parties are tacitly agreeing that the other party has an equal claim to the land, and I think the above suggests they are doing just this, that will reduce the legitimacy of the auction process in the eyes of members of the losing group. And that will lead to enforcement difficulties down the line.\nThere is an administrative problem lurking around here. Since each party will end up with something from this process once the auction begins, we must have a way of determining whether the competing claims warrant an auction, or whether one party should receive the land, or whether some kind of negotiation is possible. And once we set up a process to do that, it could easily encourage relatively spurious land claims. Unless there is a serious cost to suggesting that one should be party to an auction of some block of land, there is a large incentive to get into these auctions wherever and whenever possible. Perhaps some method could be designed to offset this incentive, and perhaps even the desire groups have to be approved by the court of public opinion will offset it at times, but it seems to be a problem with the proposal as formulated.\nTo be sure, if A accepts B’s bid, then both parties do end up with something from the auction. A gets 4, and B gets some land that it values at 12 for 4, a gain of 8. Note that B does much better out of the auction than A. If the auction stops when the richer party makes a bid at or above half the price the poorer party would pay, then the richer party will always end up with a higher ‘utility surplus.’ Hence\n\nObjection 3. If there’s no tactical bidding the utility surplus is given entirely to the richer party.\n\nLet’s relax the assumption that A does not bid tactically. Indeed, let’s make things as good as could be realistically expected for A. It knows that B values the land at 12 and does not bid tactically, so B will make bids up to 6, and accept any bid over 6. Hence the auction proceeds as follows: A bids 4\\(\\frac{1}{2}\\), B bids 5, A bids 5\\(\\frac{1}{2}\\), B bids 6, A accepts. Now things could go better for A, but it would require some luck and courage. A could bid 6\\(\\frac{1}{2}\\) and B could reply with a bid of 7, but since this requires B acting against its own interests (it is better off accepting the bid of 6\\(\\frac{1}{2}\\) after all), and hence also requires A making a risky move that will only yield dividends only if B acts against its own interests in just this way, such an outcome seems unlikely. So in practice the best case scenario for A is that B pays 6 for the land. In this case A ends up with 6, and B ends up paying 6 for land it values at 12, a gain of 6. Hence\n\nObjection 4. Among the realistic outcomes, the best case scenario for the poorer party is that it ends up with as large a utility surplus as the richer party.\n\nBest cases don’t often happen, so in practice we should normally expect a result somewhere between the ‘no tactical bargaining’ option, where B receives a larger share of the surplus, and this ‘best case scenario’ where the two parties get an equal share of the surplus. Hence in almost all cases, the richer party will get a larger surplus than the poorer party. This seems like a flaw in the proposal, but worse is to come. Most of the ways in which B can realistically increase its share of the surplus involve behaviour that we should not want to encourage.\nConsider again A’s decision to reject the bid of 5 and bid 5\\(\\frac{1}{2}\\). Assume, for simplicity, that A plans to accept a bid of 6, but drop the assumption that A knows that B will reject a bid of 5\\(\\frac{1}{2}\\), if it is made. So before A makes its decision, there are three possible outcomes it faces:\nIn this case it receives 5.\nIn this case it gets the land (value 8) for 5\\(\\frac{1}{2}\\), net gain 2\\(\\frac{1}{2}\\).\nIn this case B bids 6, and A accepts, so it gets 6.\nA’s expected utility is higher if it bids 5\\(\\frac{1}{2}\\) rather than accepts B’s bid iff its degree of belief that B will bid 6 is over \\(\\frac{5}{7}\\). If it is less confident than that that B will bid 6, it should accept the bid of 5. As it happens, B is going to reject a bid of 5\\(\\frac{1}{2}\\) and bid 6, so it is better off if A accepts the bid of 5. If A knows B’s plans, this will not happen. But if A is ignorant of B’s intentions, it is possible it will accept the bid of 5. Indeed, since A’s confidence that B will decline must be as high as \\(\\frac{5}{7}\\) before it makes the bid of 5\\(\\frac{1}{2}\\), it might be quite likely in this case that A will just accept the bid.\nNot surprisingly, we get the result that B is better off if its bargaining plans are kept secret than if they are revealed to A. That in itself may not be objectionable. But remember that the agents here are not individuals, they are states. And the decisions about how to bid involve policy questions that will often be the most important issue the state in question faces for many a year. Ideally, decisions about how to approach the auction should be decided as democratically as possible. But democratic decision making requires openness, and it is impossible that all the stakeholders in B, including one imagines the citizens, can participate in the decision about how to approach the auction without B tipping its hand. In the modern world it’s impossible to involve everyone in B without opening the debate to agents of A. And this, as we’ve seen, probably has costs. Since B is better off if it does not make decisions about how to approach the auction in the open, we have\n\nObjection 5. The proposal favours secretive governments over open democratic governments.\n\nAssume that B has been somewhat secretive, but A is still fairly confident that B will not accept a bid of 5\\(\\frac{1}{2}\\). Its degree of belief that such a bid will be rejected is \\(\\frac{3}{4}\\), let’s say, so it is disposed to gamble and make that bid. But now B starts making some noises about what it will do with any money it gets from A. The primary beneficiary of this windfall will be B’s military. And the primary use of this military is to engage in military conflicts with A. While some of these engagements will be defensive, if A gets the land under dispute many will be offensive. (I don’t think these assumptions are particularly fanciful in many of the land disputes we see in the modern world.) A must take this into account when making its decisions. It seems reasonable to say that every 1 that A gives B has a disutility of 1.2 for A, 1 for the cost of the money it gives up, and 0.2 for the extra damage it may suffer when that money is turned into weaponry turned back against A. Now the utility calculations are quite different. If B accepts A’s bid of 5\\(\\frac{1}{2}\\), A’s balance sheet will look like this:\nThe land, value 8.\n5\\(\\frac{1}{2}\\) paid to B, value 5\\(\\frac{1}{2}\\)\nB’s extra military capability, value (a little over) 1.\nRoughly 1\\(\\frac{1}{2}\\).\nSo now the expected utility of bidding 5\\(\\frac{1}{2}\\) is:\n\nProb(Bid Accepted) Utility(Bid Accepted) + Prob(Bid Rejected) Utility(Bid Rejected)\\(\\approx \\frac{1}{4} \\times 1\\frac{1}{2} + \\frac{3}{4} \\times 6\\)\n= \\(4\\frac{7}{8}\\)\n\nHence A’s expected utility for accepting B’s bid of 5, i.e. 5, is higher than its expected utility of bidding 5\\(\\frac{1}{2}\\), so it will accept the bid, just as B wanted it to do. So if B indicates that it will use any payments from A to attack A, it may well be able to get the land for less. Hence\n\nObjection 6. The proposal favours belligerent governments over peaceful governments.\n\nOne qualification to this objection is that what matters here is what A thinks B will do, not what B actually does. So the objection is not that the proposal rewards offensive behaviour, but that it rewards belligerence, or indications of offensive behaviour. This isn’t as bad as rewarding military action, but it is still objectionable.\nThroughout I have used a particular example to make the points clearer, none of the arguments turns on the details of this example. What matters is that in any case where one party is able to spend more for the land in question simply because they are richer, the richer party will almost inevitably have a higher utility surplus, and this party can increase their expected utility surplus by being more secretive about their plans, and by being adopting a more belligerent tone towards their rivals before and during the auction. So it seems the proposal systematically rewards behaviour we should be discouraging.\nThe remaining objections concern the implementation of Steiner and Wolff’s proposal. While I don’t have a demonstrative proof that any of these concerns present insurmountable difficulties, they all suggest ways in which the proposal must be qualified if it is to be just.\nThe proposal seems to assume that the parties to the dispute agree over whether the land in question can be divided. As Steiner and Wolff put it, “The auction can thus be viewed as a device for achieving a fair settlement for the disposition of a good when neither division nor joint ownership is acceptable to the parties.” In some conflicts at least part of what is at issue is whether the land can be divided. For instance, if we were applying this proposal as a way of settling the war between Britain and Ireland in 1921, would we say that all of Ireland should be auctioned off, or just that the six counties that became Northern Ireland should be auctioned? Assuming the British had decided that governing southern Ireland had become too much trouble and were only interested in retaining the north, they may not have wanted to pay for the whole country just to protect their interests in the north. But at least some of the Irish would have been unwilling to accept a process that may have led to the division of the country, as would have obtained had the south been granted Home Rule, but the north left subject to an auction. (The historical facts are, obviously, somewhat more complicated than I’ve sketched here, but even when those complications are considered the difficulties that must be overcome before we know how to apply the proposal to a real situation are formidable.) Hence\n\nObjection 7. The proposal assumes a mechanism for determining which land is indivisible, and in some cases developing such a process is no easier than settling the dispute.\n\nSteiner and Wolff assume that the groups, A and B, are easily identifiable. In practice, this may not be so easy. For example, at least some people in Scotland would prefer that Scotland was independent. For now most people prefer devolution to independence (and some would prefer rule from Westminster) but we can easily imagine circumstances in which the nationalist support would rise to a level where it became almost a majority. If a majority in Scotland wants to secede, and the British government is willing to do this, then presumably they will just secede. But what are we to do if a narrow majority in Scotland wants to secede, and the British government (or people) do not want them to go? Presumably Steiner and Wolff’s proposal is that some sort of auction should be held to determine who should be in charge of the land. But who exactly are meant to be the parties? On the Westminster side, is the party Britain as a whole, or Britain except for Scotland? On the Scottish side, is it the Scottish people? The Scottish government, which for now is a creature that exists at the pleasure of the British Parliament? Those people who support Scottish independence? If the last, how shall we determine just who these people are? Perhaps some one or other of these answers can be defended, but the proposal is seriously incomplete, hence\n\nObjection 8. There is no mechanism for determining who shall count as a member of the groups in question.\n\nFinally, the proposal simply assumes that we can agree upon the currency in which the auction shall be conducted, but it is not ever so clear that this can be done. Usually, the two parties to a dispute will use different currencies, so to avoid conflicts it would be best if the auction were conducted in a neutral currency. But finding such a currency may be non-trivial. There are only a handful of currencies in the world whose supply is sufficiently abundant to conduct an auction of this size, and most of the time those currencies will be backed by governments who favour one side in the dispute. If they use this favouritism to provide access to credit denominated in their currency at a discounted rate, that threatens the fairness of the auction. Hence\n\nObjection 9. The proposal assumes a given currency in which to conduct the auction, but in practice any choice of currency may favour one side.\n\nThe last three objections are, as mentioned, somewhat administrative. It is possible that in a particular situation they could be overcome, though I think that it is more likely that they would pose serious difficulties to a would-be auction-wielding pacifier. But that’s not the serious problem with the proposal. The real problem, as the first six objections show, is that it favours rich, secretive, belligerent states that are disposed to make spurious land claims over poor, democratic, pacifist states that only make genuine land claims.\n\n\n\nSteiner, Hillel, and Jonathan Wolff. 2003. “A General Framework for Resolving Disputed Land Claims.” Analysis 63 (3): 188–89. https://doi.org/10.1093/analys/63.3.188.\n\n\n\n\n",
    "preview": "posts/2021-01-04-nine-objections-to-steiner-and-wolff-on-land-disputes/shetland.jpg",
    "last_modified": "2021-02-04T22:02:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-review-of-real-conditionals/",
    "title": "Review of “Real Conditionals”",
    "description": "Review of William Lycan, “Real Conditionals”. Oxford: Clarendon Press, 2001.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-10-01",
    "categories": [
      "book review",
      "on books",
      "conditionals",
      "logic",
      "language"
    ],
    "contents": "\nOver the last two decades, William Lycan’s work on the semantics of conditionals has been distinguished by his careful attention to the connection between syntax and semantics, and more generally by his impeccible methodology. Lycan takes compositionality seriously, so in his theory the meaning of ‘even if’, for example, should be a combination of the meaning of ‘even’ and the meaning of ‘if’. After reading his work, it’s hard to take seriously work which does not share this methodology.\nLycan’s semantics for conditionals makes central use of what he calls ‘events’. An event is not a possible world, for it need not be complete or consistent. It is more like what Barwise and Perry call a ‘situation’. Conditionals are quantifiers over events, as follows:\nP if Q = P in any event in which Q\nP only if Q = P in no event other than one in which Q\nP even if Q = P in any event including any in which Q (17)\nThe quantifiers here are contextually restricted. Lycan includes in the semantic analysis a predicate of events R, whose role is to restrict the quantifiers over events. An event satisfies R only if it is ‘envisaged’, which is similar to saying it is a ‘real’ or ‘relevant’ possibility. The value of R changes frequently; sometimes it even changes mid-sentence. This fact is appealed to frequently in explaining some surprising behaviour of conditionals. For example, the invalidity of antecedent-strengthening: if p then r, so if p and q then r, is explained by saying the class of events relevant to the truth of the conclusion may be larger than the class of events relevant to the truth of the premise. In particular, at least one event in which p and q is relevant to the conclusion, but no such event need be relevant to the premise. A similar explanation is given for the failure of transitivity and contraposition.\nThe quantifier domain must include some non-actual events or conditionals will turn into material implications. Surprisingly, Lycan says that sometimes the quantifier includes only non-actual events. In these cases, it is possible that all (relevant) p-events can be q-events, even though p is true but not q. That is, in these cases modus ponens is invalid. Lycan argues persuasively that the case against modus ponens is at least as strong as the case against antecedent-strengthening, contraposition and modus tollens.\nThere is an extended discussion of ‘even’, which is necessary for providing a theory of ‘even if’. Lycan first suggests that Even Grannie was sober means Everyone, including Grannie, was sober. The quantifier domain includes everyone no less likely than Grannie to be sober. After discussing some counterexamples, Lycan suggests that instead it means Everyone plus Grannie was sober, where the quantifer ranges over everyone whom you would expect to be sober. Lycan is committed to ‘even’ being a quantifier because of its syntactic similarity to ‘only’, and because of the “initial plausibility of … universally quantified paraphrases” (121) of sentences involving ‘even’. The discussion here is fascinating, but not conclusive. It isn’t clear, for example, that ‘even’ and ‘only’ have the same syntactic role. Compare Even supposing Jack were here, he wouldn’t help with *Only supposing Jack were here, he wouldn’t help.\nLycan also includes a helpful discussion of how his theory handles Allan Gibbard’s ‘Riverboat Puzzle’ and related cases. It is troubling, for those who don’t analyse conditionals as material implications, that sometimes one speaker can say If p, q, another can say If p, not q, and both seem to be speaking truly. Lycan argues we should accept this troubling consequence, but explain it by making R sensitive to epistemic considerations.\nAs well as these points, Lycan raises some powerful objections to ‘No Truth Value’ theories of conditionals, and against the extensive use of probability theory in semantics. The book concludes with two appendicies on ‘non-conditional’ conditionals, such as If you’re hungry, there’s biscuits on the sideboard.\nThere’s a lot to like about this book, not least it’s witty, even charming, style. Lycan considers more examples, from more diverse sources, than most writers. The theory he presents is innovative and at least aims to be comprehensive. And of course there are some good arguments for it. Despite this there are, as always, occasional grounds for complaint.\nAlthough Lycan is very careful to get the syntax of ‘if’ right, and proves that unlike ‘and’ and ‘or’ it is not a co-ordinating conjunction, it is not so clear that the syntactic evidence provides distinctive support for his semantic theory. If it’s consistent with the syntax to say p if q means All relevant q-events are p-events, it’s consistent with the syntax to say that it means All nearby q-worlds are p-worlds. So the syntactic argument for preferring Lycan’s theory, to, say, Stalnaker’s, is not obviously overpowering. Lycan suggests that we can naturally paraphrase conditionals as quantifications over events, but since he is using event ‘in a slightly uncommon way’ (17) it is not obvious what support this gives for his theory.\nThere are few reasons to favour the use of events rather than worlds in the analysis. The fact that events can be incomplete seems to only cause complications for the theory. The fact that they can be inconsistent is used to rescue some intuitions about conditionals with impossible antecedents, but many would argue those intuitions should be discarded.\nBut the main worry is that Lycan needs to say more about some key notions, particularly about his R and about validity. In the discussion of the Riverboat Puzzle, Lycan says, “I do not have a good enough intuitive handle on my own notion of ‘relevance’ to provide a crushing answer [to a question about why certain events are not covered by R].” (173)  Lycan says that for an event to be R, “the utterer must have it at least tacitly in mind as a live prospect.” (19) All events in R are ‘envisaged’, to use the term he lands on. But “there is somethig slightly artificial or stylized about ‘envisaging’ … ‘Envisaging’ is not a purely de facto cognitive or other psychological state.” (30) The upshot is that the envisaged possibilities are some, but not always all, of those that are (possibly tacitly) regarded as live. Just which possibilities then? We are never given a specific account. Any account we do get is, as in the above quote, almost immediately qualified. Since R does so much work, the reader is probably owed a little more here. (This point is made at greater length in Ken Turner’s excellent review of Real Conditionals in the Journal of Pragmatics forthcoming.)\nWe are also never specifically given an account of validity. We are told that several argument forms, from antecedent-strengthening to modus ponens, are invalid. This seems to mean that one could assert their premises then reject, or a least decline to assert, their conclusion. It’s important that this process of assertion and rejection take place in real time, because the value of R needs to change for the arguments to be invalid. Lycan has some arguments that this conception of validity is the philosophically interesting one, but this deserves more treatment. The logical reforms it draws in go well beyond the logic of conditionals. On Lycan’s approach, All swans are white, so all Australian swans are white is, presumably, invalid, since the scope of the quantifier could change from premise to conclusion. And contraposition fails for valid arguments. Contraposed modus ponens: p, not q, so not if p, q is valid, but modus ponens is not.\nNone of this is to deny that Real Conditionals is a great contribution to the literature, and if it causes more theorists to pay serious attention to Lycan’s Event Theory, that would be an excellent consequence.\n \n \n\n\n\n",
    "preview": "posts/2021-01-04-review-of-real-conditionals/lycan.jpg",
    "last_modified": "2021-02-04T22:02:48-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-review-of-words-without-meaning/",
    "title": "Review of “Words Without Meaning”",
    "description": "Review of Christopher Gauker, “Words Without Meaning”. Cambridge: MIT Press, 2002.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-09-08",
    "categories": [
      "book review",
      "on books",
      "language"
    ],
    "contents": "\nIn philosophy it’s hard to find a view that hasn’t had an ism associated with it, but there are some. Some theories are too obscure or too fantastic to be named. And occasionally a theory is too deeply entrenched to even be conceptualised as a theory. For example, many of us hold without thinking about it the theory that “the central function of language is to enable a speaker to reveal his or her thoughts to a hearer,” (3) that in the case of declarative utterances the thoughts in question are beliefs whose content is some proposition or other, and that hearers figure out what the content of that belief is by virtue of an inference that turns on their beliefs about the meanings of the words we use. These claims might seem too trivial to even be called a theory. They have seemed too trivial to draw an ism. Christopher Gauker calls them ‘the received view’, and the purpose of his book Words Without Meanings (all page references to this book) is to argue against this received view and propose an alternative theory in its place. In Gauker’s theory the primary function of language is social coordination. If language ever functions as a conduit to the mind, this is a secondary effect.\n\nPublished in Notre Dame Philosophical Reviews.\nIt is useful to have an ism for everything, so let’s call ‘the received view’ Lockism, since Locke believed something similar. Of course, Locke probably didn’t have any detailed opinions about where the semantics/pragmatics distinction lies or what the role and importance Horn scales might be or how to build a compositional semantics for quantification, or indeed about many of the issues on which various contemporary Lockists have their most distinctive views, but there’s an intellectual legacy worth noting. Still, if Locke was a Lockist without having views on these matters, this starts to suggest how broad, and how divided, the Lockist church may be. Lockists need not agree on the semantic analyses of indicative conditionals or attitude reports. They need not even agree on whether there are such things as conventional implicatures or deep structures. An argument against Lockism will have to either focus on the few rather platitudinous points where Lockists agree, or try to respond to all the ways Lockists might develop their position. Gauker takes both options throughout his book. The central theses of the Lockist position that are attacked concern the nature and contents of beliefs, the nature of logical implication and the status of truth. Gauker’s attacks on Lockist theories of quantifier domain restriction and of presupposition rely more heavily on attacking all the variants of Lockism.\nBut the arguments against Lockism are not necessarily the most important parts of the book. Alongside the criticisms of Lockism, Gauker develops in great detail his own positive theory about the nature and role of linguistic communication. Gauker suggests “the primary function of assertions ... is to shape the manner in which interlocutors attempt to achieve their goals.” (52) Conversations do not take place in a vacuum. Conversants frequently talk because they want something. The world does not always make it easy for us to get what we want, but sometimes at least other people can tell us which ways work best.\nIt becomes crucial to Gauker’s theory here that certain actions are or are not in accord with certain sets of sentences. Given this idea the primary norm of conversation becomes: Say things such that others who act in accord with what you say, and with what else has been said, will achieve their goals. The concept of actions according with (sets of) sentences seems intuitive at first. If my goal is to download the new Matrix movie, then going to stealthatmovie.com is in accord with {‘The new Matrix movie is available at stealthatmovie.com’} while going to moviebootlegger.com, or anywhere else, is not. (These are, by the way, fake site names.) Given this idea of actions according with sets of sentences, we can then define the context, or set of relevant sentences, as the smallest set such that “all courses of action in accordance with it relative to the goal of the conversation are good ways of achieving the goal”. (56) We can then restate the primary norm as: Say things that are in the context. Those sentences will be useful to say, and their negations will be useful to deny. This idea of useful assertability becomes crucial to Gauker’s theory, often playing much the role that a Lockist has truth play. For example, validity gets defined in terms of assertability preservation in all contexts.\nClearly the concept of actions according with contexts given goals is quite crucial, but there’s less explication of it than we might hope. I have some idea what it might mean to say that my going to stealthatmovie.com accords with the proposition that the new Matrix movie is available at stealthatmovie.com, and I have some idea which facts in the world may make this true. But I don’t have as clear an idea about what it means to say this action accords with any sentence. Sentences are just marks on paper, or sound waves. We can be pretty sure that this accord between actions and marks on paper is not a primitive fact about the world. Lockists think that actions accord with sentences because sentences express propositions and some actions accord with propositions. But this isn’t Gauker’s account, and it isn’t clear what is. At one stage Gauker notes that the distinction between actions that accord with a context and those that do not will be primitive relative to the ‘fundamental norms of discourse’, which are the primary focus of Words Without Meanings. That sounds right, and I hope it’s a sign that we’ll see more details about the concept of accord in future work.\nThis issue though is important because there are a few reasons to worry about how the concept of accord will be explicated. First, whatever problems face Lockist theories of meaning, including some of the problems Gauker raises, may recur here. Second, some theories of accord will introduce entities that are functionally just like meanings, so if meaning is a functional concept, as seems plausible, those theories will not end up being theories of words without meaning. Third, as Gauker notes, there are serious epistemological questions about how we could ever learn which actions accord with which contexts relative to which goals. Those who are impressed by Fodor’s arguments for the systematicity of human linguistic competence will probably think these questions raise insuperable difficulty for anything like Gauker’s program. On the other hand, those that are impressed by Gauker’s program will probably find these Fodorian claims overstated.\nWords Without Meaning concludes with three chapters setting out a rather distinctive view of belief. Gauker argues that a complete account of the role of belief ascriptions should be sufficient for a theory of belief. This is not because of a general policy that explaining the talk about something is sufficient to explain the thing in general. Such a policy is not entirely antithetical to Gauker’s overall picture, but it would be hard to defend in all cases. Rather, Gauker argues, in practice we have little use for beliefs and desires other than in our ascriptions of them, so an account of their ascription is all the account we need. Many philosophers will baulk here, because they think folk psychology provides a crucial role for beliefs and desires. Since folk psychology is a crucial part of how we predict the actions of other people, and of how we explain their actions, there is an important aspect of the nature of beliefs and desires that a mere account of their ascriptions will not capture. These philosophers will not agree with Gauker that an “account of the attribution of beliefs and desires is already an account of [their] nature.” (271‑2)\nGauker’s response to these philosophers is to question the explanatory and predictive capacity of folk psychology. He argues first that the explanatory, and especially the predictive, power of folk psychology is much over-rather. And more importantly, he argues that when there do appear to be good folk psychological explanations or predictions, there are equally good explanations that do not appeal to beliefs and desires. The argument for this involves running through several cases with some care, but very roughly the common theme is that beliefs and desires (if they exist) are themselves capable of explanation, so at least most of the time we can replace an explanation in terms of beliefs and desires with one that appeals to the explanations of those very beliefs and desires. This gives us a fairly general strategy for dispensing with folk psychological concepts in explanation and prediction.\nThis does not mean that we adopt an error theory of belief or desire ascriptions. Gauker thinks these have a use, so they are properly assertable. Their role, in general, is to let us speak on behalf of other people. “The primary function of attributions of belief and desire is to extend the range of participation in conversation.” (226) When I say that Harry believes that tech stocks are good investments, I say on Harry’s behalf that tech stocks are good investments. Unfortunately, we never get a complete positive characterisation of when it is permissible to say something on Harry’s behalf. We are told that such assertions, like all assertions, must be relevant to the conversation, but beyond that not a lot. We are told that it can’t just be permissible to say this just in case Harry would be disposed to say it, were he here. Harry might have a habit of keeping his investment ideas to himself, but still believe that tech stocks are good investments. And we’re told that this can be permissible to say even if Harry has never made an ‘inner assertion’ that tech stocks are good investments. But this doesn’t amount to a positive characterisation. Further, it’s not clear how to extend this account to all attitude reports, especially reports of desire-like attitudes. One could truly say Brian wants to play for the Red Sox, but in doing so one is not making a command, or even a request, on my behalf.\nAs well as these intriguing positive proposals, there are several arguments against Lockism. Chapter 2, on mental representation, is an attack on the Lockist position that there are beliefs with propositional content. Gauker first notes that any attempt to provide an atomistic theory of mental content seems to run into insuperable counterexamples. The main focus is Fodor’s asymmetric dependence theory, but a few other atomist theories are raised and dismissed. Gauker suggests that holistic theories are a little more promising, but when we look at the details we see that these all fall to a version of Putnam’s model-theoretic argument. Gauker’s argument here differs from Putnam’s in two key respects. First, it concerns primarily mental content, rather than linguistic content. Second, it has fewer theoretical overheads. Gauker shows that the argument never really needed any complicated mathematics; the formalism in the standard semantics for first-order logic is quite sufficient. Despite those two differences, the argument is fairly familiar, and the moves that could be made in response are also, by now, fairly familiar. Gauker quickly surveys these moves, and notes why he thinks none of them work, but the survey will probably be too brief to convince many who are happy with their preferred reply to Putnam. Those who are not happy with any of the replies to Putnam, or who would be more impressed by a version of Putnam’s argument that did not drift into needless technicality, should enjoy Gauker’s argument.\nThe middle half of Words Without Meanings consists of six case studies designed to show that Gauker’s approach can solve problems that are intractable for Lockism. Three of these are described as being in pragmatics, the other three in semantics. Here Gauker more often has to revert to arguing against each of the different versions of Lockism in the literature, for there are few points of agreement among Lockists once we get to the details on how language works. This is particularly clear when we look at pragmatics. I guess most Lockists agree that there is a pragmatics/semantics distinction, and most of those who do agree think that there is such a thing as scalar implicature. Beyond that there are disagreements everywhere. So Gauker is more often required to argue against all the versions of Lockism in existence. Even if he succeeds against all of them, Lockism is a growing doctrine, and a smart Lockist could often take Gauker’s positive ideas and incorporate them into Lockism. So it’s not clear we’ll see any knock-down argument against Lockism here. But maybe there will be an interesting abductive argument develop, and in any case it is always worthwhile to see Gauker’s positive account. Space prevents a full discussion of many of the issues raised here, but I’ll provide a quick summary of the salient issues, and why Gauker thinks he has an advantage over his Lockist rivals.\nThe first case study concerns domains of discourse, which mostly means domains of quantification. To use Gauker’s example, imagine Tommy runs into Suzy’s room, where Suzy is playing with her marbles, and says “All the red ones are mine.” What determines the domain of Tommy’s quantifier? If it is what Tommy intends, then we might end up saying that his sentence is, surprisingly, true. For Tommy, it turns out, intends only to speak of the marbles in his room, which are as it turns out all his. But if we don’t take it to be what Tommy intends, and instead let the domain be set by what Suzy thinks the domain is, or what a reasonable hearer would think the domain is, then we undermine the Lockist picture that the role of language is for the speaker’s thoughts to be communicated. In these cases it is the thoughts of the hearer, or of a reasonable hearer, seem to determine the meaning of what is said. Gauker suggests it is better to say that the domain is the class of things that are relevant to the goal of the conversation that Tommy and Suzy are having.\nThe second case concerns presupposition. Allegedly, some sentences are such that they cannot be properly asserted or denied unless some condition, the presupposition, is met. Sentences containing factive attitude verbs are sometimes held to fall into this category. So I cannot affirm or deny I regret that you failed the test unless you failed the test. There are several Lockist theories of presupposition, but Gauker argues that none of them can satisfactorily explain how asserting such sentences can inform the hearer of the truth of the presupposition, in this case that the test was in fact failed. Gauker’s theory, which does not have a special category of truth conditions apart from assertability conditions, does not have this difficulty. For a similar reason, however, the Lockist theory that rejects the concept of presupposition also avoids any problem of informative presupposition.\nThe third case concerns Gricean implicature. Gauker notes, correctly, that we can well explain the effects of Gricean implicature without presuming that the hearer even contemplates what the speaker had in mind in speaking. But this kind of contemplation is essential to Grice’s official story. Gauker’s alternative suggestion is that we can explain non-literal communication by assuming the hearer draws inferences about the context from the assertability of what is actually said.\nThe next three case studies are classified as ‘Semantics’, so we might hope that here Lockists will present a more unified target. But two of the studies seem, from a Lockist perspective, to concern the semantics/pragmatics boundary, so again there will be several varieties of Lockism that need to be addressed.\nThe first semantics study concerns quantifiers. Gauker argues that in practice (1) is a bad argument form, (1a) for instance is invalid, while (2) is a good argument form.\nEverything is F. Therefore, a is F. 1a. Everything is made of wood. Therefore, Socrates is made of wood.\na is F. Therefore, something is F.\nGauker argues in some detail that various Lockist theories of quantifier domain restriction cannot explain the asymmetry here. On his theory, the asymmetry falls out quite naturally, since quantification is always over named objects, and once named an object is relevant. So (1) need not be valid, since a need not have been named, but (2) must be valid.\nThe last two case studies are the most interesting, and the most intricate. I can’t do justice in a small space to the details of Gauker’s theory, but I’ll say a little about the issues raised. Chapter 8 concerns conditionals. Gauker thinks he has a telling argument against a central Lockist claim. The primary intuition is that (3) and (4) are logically equivalent, i.e. each entails the other, (at least when p and q are not themselves conditionals), but they are not equivalent when embedded in longer sentences. In particular, (5) and (6) need not be equivalent.\nEither not p or q\nIf p then q\nEither not p or q, or r\nIf p then q, or r\nIf this is right, then what Gauker calls ‘the Equivalence Principle’, that substitution of logical equivalent constituents preserves truth-conditional content, is false. Gauker suggests this is a serious problem for Lockism. There are, however, a few Lockist theories in which Equivalence fails. For example, in classical supervaluationism, p or not p is a logical truth, and p is equivalent to p is true, but p is true or not p is not a logical truth. So some Lockists have learned to live without Equivalence. More importantly, the data that suggests that (3) and (4) are logically equivalent isn’t unequivocal. Some Lockists have provided arguments as to why (3) and (4) will usually have the same assertion conditions even though they have different truth conditions. (Gauker notes Robert Stalnaker’s 1975 paper ‘Indicative Conditionals’ that argues for this line.) If those arguments can be made to succeed, then we can keep Equivalence by denying that (3) really entails (4).\nMore interesting than the possible Lockist replies is Gauker’s own theory. He manages, quite impressively I think, to provide a recursive definition of truth conditions for the connectives without keeping Equivalence. The rough idea is that If p then q is true iff p strictly implies q relative to the context. Strict implication theories usually block the inference from (5) to (6), as Gauker’s does, but they also normally block the inference from (3) to (4). In Gauker’s theory, however, because entailment is defined in terms of assertability-preservation, and disjunctions can only be asserted if one or other disjunct is assertable in every possibility left open by the context, the inference from (3) to (4) is valid. Roughly, any contextually salient possibility either contains not p or q, so all the possibilities that contain p contain q, in which case If p then q is assertable. This theory still has some counter-intuitive features, since the paradoxes of material implication are still with us, but it’s a fascinating addition to the literature on conditionals.\nThe final case study concerns truth, and in particular the semantic paradoxes. Gauker argues that extant Lockist responses to the paradoxes are not capable of handling metalinguistic versions of the paradox. In particular, Lockist theories struggle with sentences like (7).\ndoes not express a true sentence in this context.\n\nGauker’s argument that his theory does better than the Lockist here has two parts. First, he has a detailed demonstration that it is impossible to infer a contradiction directly from (7) in his theory. Second, he argues that a Lockist explanation of what’s going on with (7) has to posit that uttering, or writing, ‘this context’ changes the context. This might be true, indeed Gauker endorses a similar claim in his response to the paradoxes. But on most Lockist accounts of what contexts are, we could replace the demonstrative with some other phrase that more directly picks out the context. For example if a context is just an ordered n-tuple, we could just replace ‘this context’ with a description of the n-tuple that is, actually, the context. Here it does look as if Gauker’s theory has more resources than the traditional Lockism. It remains to be seen whether Gauker’s theory is completely free from the paradoxes - it’s quite a bit harder to come up with a consistent theory of truth than it is to block the liar paradox - but again Gauker provides an interesting alternative to existing approaches, and one that experts in the area should pay close attention.\nOverall, what should we make of Words Without Meanings? I think the book has three major aims, and it succeeds in two of them. The first aim is to extend Gauker’s preferred theory of linguistic communication to show how it handles presupposition, quantification, conditionals, attitude reports and truth ascriptions. In this it succeeds quite well, especially in showing how the project holds together technically. The second aim is to raise a host of problems for the Lockist theory, problems that are deserving of serious consideration and response. And again, there is no doubt it succeeds. Even if one thinks that all the problems Gauker raises can be solved, having them set forth so sharply certainly advances the debate. The third aim, the big one, is to convince Lockists that their research program is moribund, and Gauker’s contextualist alternative is the way of the future. That aim, in short, is for a revolution in semantics. (And in any fields that presuppose Lockist semantics. Many of our best syntactic theories have to be revised if Gauker is correct.) Here I think the book is less successful, if only because the aim is so high. It’s not clear how any short book, and the MIT series Words Without Meaning is in is clearly a series for short books, could trigger such a revolution. Lockism may have its weaknesses, and Gauker shines a spotlight on a few, but it’s been a relatively productive program the last fifty years, so overthrowing it will not be easy. Such a revolution would need a longer book, or books, answering among other questions the metaphysical and epistemological questions about Gauker’s concept of actions according with sentences we noted above. Gauker’s work always leaves the impression that he has worked through the relevant material in much more detail than is apparent from a superficial reading of the text, so such books and papers may well be in the pipeline. If one is already on Gauker’s side in these disputes, one should heartily welcome the wealth of detail Words Without Meaning adds to his program. If one is more conservative, more orthodox, one should perhaps be worried about the anomalies rising, but not panicked. At least, not panicked yet.\n\n\n\n",
    "preview": "posts/2021-01-04-review-of-words-without-meaning/gauker.jpg",
    "last_modified": "2021-02-04T22:03:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-review-of-theories-of-vagueness/",
    "title": "Review of “Theories of Vagueness”",
    "description": "Review of Rosanna Keefe, “Theories of Vagueness”. Cambridge: Cambridge University Press, 2000.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-09-01",
    "categories": [
      "book review",
      "on books",
      "vagueness",
      "logic"
    ],
    "contents": "\nMany philosophers, I suspect, are partial to supervaluational theories of vagueness. And with good reason. Its rivals all seem to promise metaphysical mysteries concerning hitherto unnoticed, and perhaps unnoticeable, sharp boundaries around our concepts, or radical revision in our logical practices. And not only have philosophers been so tempted. The texts are a little unclear, but it seems several economists can be read as adopting supervaluational solutions to the difficulties raised by vagueness in economic concepts. Given its popularity, and plausibility, supervaluationism deserves a book-length defence. Yet this is the first such book in the philosophical canon.\n\nPublished in Philosophy and Phenomenological Research 67: 491-4.\nAnd it is a fine defence of supervaluationism, though I doubt it is entirely successful. I ended up, a little contrariwise, feeling less convinced in the hegemony of supervaluational approaches than I was when I started. In part this was because Keefe was so clear in setting out where some rival approaches, especially degree-based approaches, failed that I felt she had inadvertently pointed out where her opponents should move next. Keefe’s positive theory is fairly familiar, though she often marshalls new arguments for it, so this review will not dwell on exposition but concentrate on Keefe’s arguments.\nThe book effectively divides into three parts. The first two chapters involve some scene-setting, and a discussion of the various methodological principles adopted. The next four chapters are attacks on non-supervaluational theories. And the final two chapters defend Keefe’s preferred version of supervaluationism. Starting with methodology seemed to be a good idea, but the discussion didn’t break much new ground. It turns out, surprisingly enough, that reflective equilibrium is useful in theorising about vagueness too.\nThe first rival theory to be examined is epistemicism, and Keefe presents two main arguments. One of these is fairly familiar, epistemicists have no theory about how predicates get the particular precise extensions that they do. This is true, but then supervaluationists aren’t exactly flush with theories about how predicates get the particular imprecise extensions that they do either. The other criticism is more interesting. Epistemicism posits a sharp boundary between the tall and not-tall, but we don’t know where this boundary is. It is a mystery why we do not have this knowledge, one that epistemicists try to solve by showing we could not know where the boundary is. But there are other mysteries too. For some reason, we don’t try to find out where the boundary is, and we don’t have beliefs about where it is. That we couldn’t get this kind of knowledge doesn’t explain these omissions. Like Hobbes trying to square the circle, we all try impossible things sometimes. So epistemicism has more explaining to do than it has hitherto done.\nKeefe spends two chapters attacking theories based on degrees of truth. There are several related objections raised to these theories, but fundamentally they all boil down to the problem of false precision. If there is no fact of the matter as to whether Kylie is happy is true or not, then there is no fact of the matter as to whether it is true to degree 0.314. The most natural formulation of degree theories assumes there are facts of this latter form. More complicated formulations are either incoherent or incomplete. Keefe is good at working through the various moves that have been attempted to avoid this problem, and showing that none of them work. But at times she seemed too content to refute theories that had appeared in the literature, rather than anticipating future challengers. One particular challenge seemed particularly worthy of consideration. At one point Keefe introduces a new connective \\(≥_T\\) to mean true to the same or a greater degree. She notes that most extant degree theorists are committed to a connectedness principle for \\(≥_T\\): either \\(p ≥_T q\\) or \\(q ≥_T p\\). But this principle is implausible given their other commitments. At this point it seemed relevant to wonder how well a theory that dropped all talk of degrees of truth and just took this connective \\(≥_T\\) as primitive could avoid Keefe’s objections. Indeed, at the equivalent point in his discussion in Vagueness, Timothy Williamson considers some arguments against just this position, but his discussion is rather brief. One can’t reply to every possible response, but this one seemed so apposite, I would have liked to see Keefe’s response to it.\nKeefe holds that a sentence is true iff it is true on all admissible precisifications, i.e. that truth is supertruth. She says little about what makes a precisification acceptable, except that it must respect penumbral connections, and that admissibility is a vague matter. One consequence of this is that schema (T): S is true iff S is not always true. Keefe suggests that the standard arguments for (T) are circular, because they assume that there are no truth-value gaps. And, following van Fraassen, she notes something similar to (T) is true, and this is good enough.\nShe holds that an argument is valid iff it preserves truth, i.e. supertruth. Hence we have S \\(\\dashv \\vdash\\) S is true.This interderivability might explain why we, mistakenly, think (T) is true. There is a familiar problem with this move, one stressed by Williamson. On all precisifications, all theorems of classical logic are true, so these all end up being true. So to that extent supervaluationism preserves classical logic. But not all admissible inference rules of classical logic preserve supertruth. In particular, the deduction theorem is no longer admissible. We can’t infer (2) from (1):\nS \\(\\vdash\\) S is true.\n\\(\\vdash\\) S ⊃ S is true.\nKeefe responds by noting that something similar to the deduction theorem is true, and this might explain our mistaken attachment to it. Keefe assumes the language contains an operator D, read ‘definitely’, such that DA means A is supertrue. Then the following rule is admissible:\n⊃I*\nFrom A,B \\(\\vdash\\) C, infer B \\(\\vdash\\) DA ⊃ C\n\nWe think the standard ⊃ introduction rule, the deduction theorem, is acceptable because we mistake it for this one. Keefe notes we can set out similar kinds of rules for argument by cases (∨-elimination) and reductio ad absurdum (¬-introduction). These are intended to be small deviations from classical logic, but they strip the proof theory of much of its power. Keefe’s rules are insufficient to prove p ⊃ r \\(\\vdash\\) (p ∧ q) ⊃ r, or p ⊃ p. One might question the value of inference rules with such little power.\nThere is little on the specific problems associated with the problem of the many for supervaluationism. Stephen Schiffer1 recently proposed a variant on the problem, suggesting that the standard supervaluational solution misclassifies some speech reports. Keefe replies that Schiffer’s argument doesn’t seem to go through if we adopt Davidson’s paratactic theory of speech reports. Well, maybe it doesn’t, but if supervaluationism requires the paratactic theory of speech reports, that seems highly relevant to its ultimate success, but Keefe merely assigns it a footnote.\nThere is a little more on the most obvious difficulty for supervaluationism, that it verifies some strange existentials. Consider a Sorites series of objects arranged with respect to F-ness, each a little more F than its predecessor, with the extremes being clearly F and not-F respectively. Then the sentence There is a pair of adjacent objects such that one is F and the other is not is supertrue. Keefe notes that we can distinguish here between the truth of the existential and the truth of an instance. And she notes that pragmatic theories due to Fine and Tappenden might explain our unwillingness to assert the existential, even if it is true. Still, it is would have been nice to see some discussion on whether this is a particular problem when F is a phenomenal predicate, or when it is ‘ineradicably vague’, as Dummett and other think predicates like ‘sort of nearby’ are.\nThe major innovation in the book is its treatment of higher-order vagueness. Keefe notes the following argument raises a serious difficulty for supervaluationism here\n\nAccording to the theory, a sentence is true simpliciter iff it is true on all complete and admissible specifications [i.e. on all precisifications]. But for any sentence, either it is true on all complete and admissible specifications (hence true simpliciter) or not (hence borderline or false). So there is no scope for avoiding sharp boundaries to the borderline cases or for accommodating borderline borderline cases. (202)\n\nKeefe’s response is to claim that the argument assumes, falsely, that there is “a precise and unique set of complete and admissible specifications.” (202) Keefe denies this and instead develops a theory of higher-order vagueness based on supervaluating the concept of admissibility. But it is not clear where the argument does assume this. After all, the argument makes no mention of sets. And it is a little unclear just why this assumption should be false. Keefe argues that there is no such set because complete and admissible specification is vague, just as there is no precise and unique set of tall things because tall is vague. But even though complete and admissible specification is vague, on every precisification there is still a unique set of complete and admissible specifications, so it is true that there is such a set, so there is one. (Keefe accepts the S is true therefore S direction of (T).)\nIt is also unclear how the vagueness of the term complete and admissible specification is relevant. The supervaluational idea was that a sentence is true iff it is true on all specifications (precisifications) that are complete and admissible, not iff it is true on all specifications satisfying the term complete and admissible. It is a use/mention confusion to hold the latter view. But if the former is correct, the vagueness of any term, even ‘admissible’, is irrelevant to the above argument. So there still seems to be work to do on higher-order vagueness.\nI’ve focussed on the negatives here, but this shouldn’t overshadow how many good parts this book has. The coverage of the literature is peerless, the writing is always crisp and clear, and in many places Keefe’s arguments move the debate in interesting new directions. It would be a great book to teach from, and indeed I would already be doing so, if only it were available in paperback. I do hope the publishers correct this problem shortly.\n\n“Two Issues of Vagueness” Monist 81: 193-214.↩︎\n",
    "preview": "posts/2021-01-03-review-of-theories-of-vagueness/keefe.jpg",
    "last_modified": "2021-02-04T15:30:03-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-03-many-many-problems/",
    "title": "Many Many Problems",
    "description": "Recently four different papers have suggested that the supervaluational solution to the Problem of the Many is flawed. Stephen Schiffer has argued that the theory cannot account for reports of speech involving vague singular terms. Vann McGee and Brian McLaughlin say that theory cannot, yet, account for vague singular beliefs. Neil McKinnon has argued that we cannot provide a plausible theory of when precisifications are acceptable, which the supervaluational theory needs. And Roy Sorensen argues that supervaluationism is inconsistent with a directly referential theory of names. McGee and McLaughlin see the problem they raise as a cause for further research, but the other authors all take the problems they raise to provide sufficient reasons to jettison supervaluationism. I will argue that none of these problems provide such a reason, though the arguments are valuable critiques. In many cases, we must make some adjustments to the supervaluational theory to meet the posed challenges. The goal of this paper is to make those adjustments, and meet the challenges.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-08-29",
    "categories": [
      "language",
      "logic",
      "vagueness"
    ],
    "contents": "\n\nContents\nSchiffer’s Problem\nNatural Properties\nMcGee and McLaughlin’s Challenge\nVague Objects\nMcKinnon on Coins and Precisifications\nSorensen on Direct Reference\nConclusions and Confessions\n\nSchiffer’s Problem\nStephen Schiffer suggests the following argument refutes supervaluationism. The central point is that, allegedly, the supervaluational theory of vague singular terms says false things about singular terms in speech reports.\nPublished in Philosophical Quarterly 55: 481-501.\nPhoto by Yoni Lerner via Creative Commons.\n\nPointing in a certain direction, Alice says to Bob, ‘There is where Harold and I first danced the rumba.’ Later that day, while pointing in the same direction, Bob says to Carla, ‘There is where Alice said she and Harold first danced the rumba.’ Now consider the following argument:\nBob’s utterance was true.\nIf the supervaluational semantics were correct, Bob’s utterance wouldn’t be true.\n\\(\\therefore\\) The supervaluational semantics isn’t correct. (Schiffer 2000, 321)\n\nAssuming Bob did point in pretty much the same direction as Alice, it seems implausible to deny (1). The argument is valid. So the issue is whether (2) is correct. Schiffer has a quick argument for (2), which I will paraphrase here. On supervaluational semantics, a sentence is true iff each of its acceptable precisifications is true. In this case, this means that if Bob’s utterance is true then it must be true however we precisify ‘there.’ Each precisification of ‘there’ will be a (precise) place, and since ‘there’ is rather vague, many of these precisifications will be acceptable. For Bob’s utterance to be true, then, Alice must have said of every one of those places that it was the place where Harold and her first danced the rumba. But Alice couldn’t have said all those things, so (2) is true.\nSchiffer suggests that one way out of this problem would be to accept the existence of a vague object: the place where Harold and Alice first danced the rumba. I will note in section four several reasons for thinking the cost of this move is excessive. Fortunately, there is a cheaper way home.\nSchiffer underestimates the scope of supervaluationism. On Schiffer’s vision of the theory, a precisification assigns a precise content to a word, and hence to a sentence, then the world determines whether that content is satisfied, and hence whether the sentence is true on that precisification. This is hardly an unorthodox view of how supervaluationism works, it seems for instance to be exactly the view defended in Keefe (2000), but it is neither the only way, nor the best way, forward. We could say, rather, that a precisification assigns content to every linguistic token in the world, and the truth conditions of every one of these tokens is then determined relative to that global assignment of content. So if a precisification P assigns a place x to Bob’s word ‘there,’ Bob’s utterance is true according to that precisification iff P also assigns x to Alice’s utterance of ‘there.’ That is, Bob’s utterance is true according to P iff the precisification of his words by P just is what Alice said according to P.1\nIt is a dramatic widening of the scope of precisifications to claim that they assign content to every linguistic token in the world, rather than just words in the sentence under consideration, but it can be justified.2 Consider how we would react if later in the day, pointing in the crucial direction, Alice said, ‘Harold and I never danced the rumba there.’ We would think that Alice had contradicted herself – that between her two statements she must have said something false. A standard supervaluationist account, where sentences are precisified one at a time, cannot deliver this result. On such a view, it might be that each of Alice’s utterances are true on some precisifications, so they are both neither true nor false. On my theory, each precisification applies to both of Alice’s utterances (as well as every other utterance ever made) and since on each precisification one or other of the utterances is false, it turns out supertrue that Alice said something false, as desired. The current view allows for penumbral connections between sentences, as well as penumbral connections within sentences. Just as someone who says, “That is red and orange” says something false, my theory decrees that someone who says, “That is red. That is orange,” while pointing at the same thing says something false, even if the object is in the vague area ‘between’ red and orange.\nIt is crucial for this response to work that on every precisification, Alice and Bob’s demonstratives are co–referential. It does not seem like a particular expansion of supervaluational theory to posit this as a penumbral connection between the two words. At least, it seems plausible enough to do this if Alice and Bob really are pointing in a similar direction. If their demonstrations are only roughly co-directional, then on some precisifications they may well pick out different objects. This will definitely happen if some admissible precisification of Alice’s ‘there’ is not an admissible precisification of Bob’s ‘there.’ In such a case, the theory here predicts that Bob’s utterance will be indeterminate in truth value. But if Alice and Bob only vaguely pointed in the same direction this is the correct prediction.\nNatural Properties\nSchiffer’s problem seems to have been solved with a minimum of fuss, but there is still a little work to do. Above I posited a penumbral connection between Alice’s and Bob’s words without explaining how such a connection could arise. This connection can be explained by some general considerations about content, considerations closely tied to the view of vagueness as semantic indecision that provides the best motivation for supervaluationism. As a few writers have pointed out (Quine 1960; Putnam 1980; Kripke 1982), there is not enough in our dispositions to use words to fix a precise content all terms in our lexicon. This does not immediately imply a thorough-going content scepticism because, as a few writers have also pointed out (Putnam 1973; Kripke 1980; Lewis 1983, 1984), meanings ain’t (entirely) in the head. Sometimes our words refer to a particular property or object rather than another not because our dispositions make this so, but because of some particular feature of that property or object. David Lewis calls this extra feature ‘naturalness’: some properties and objects are more natural than others, and when our verbal dispositions do not discriminate between different possible contents, naturalness steps in to finish the job and the more natural property or object gets to be the content.\nWell, that’s what happens when things go well. Vagueness happens when things don’t go well. Sometimes our verbal dispositions are indiscriminate between several different contents, and no one of these is more natural than all the rest. In these cases there will be many unnatural contents not eliminated by our dispositions that naturalness does manage to eliminate, but there will be still be many contents left uneliminated. Consider, for example, all the possible properties we might denote by ‘tall woman.’ As far as our usage dispositions go, it might denote any one of the following properties: woman taller than 1680mm, woman taller than 1681mm, woman taller than 1680.719mm, etc. And it does not seem that any of these properties are more natural than any other. Hence there is no precise fact about what the phrase denotes. Hence it is vague. In sum, our dispositions are never enough to settle the content of a term. In some cases, such as ‘water,’ ‘rabbit,’ ‘plus,’ ‘brain’ and ‘vat,’ nature is kind enough to, more or less, finish the job. In others it is not, and vagueness is the result.\n(The above reasoning has a surprising consequence. Perhaps our verbal dispositions are consistent with the predicate Tall X denoting the property of being in the top quartile of Xs by height. Unlike each of the properties mentioned in the text, this is a more natural property than many of its competitors. So if this kind of approach to vagueness is right, there might not be quite as much vagueness as we expected.)\nIf this is how vagueness is created, then there is a natural way to understand how precisifications remove vagueness. Vagueness arises because more natural than is a partial order on putative contents, and hence there might be no most natural content consistent with our verbal dispositions. If this relation only defined a strict ordering, so whatever the candidate meanings were, one of them would be most natural, vagueness might be defeated. Well, that isn’t true in reality, but it is true on each precisification. Every precisification is a completion of the ‘naturalness’ partial order. That is, each precisification P defines a strict order, more natural-P than, on possible contents of terms such that o1 is more natural-P than o2 if (but not only if) o1 is more natural than o2. The particular contents of terms according to P is then defined by using the more natural-P than relation where the more natural than relation is used in the real theory of content.\nThis conjecture meshes nicely with my theory of the role of precisifications. First, it explains why precisifications apply to the whole of language. Since a precisification does not just remedy a defect in a particular word, but a defect in the content generation mechanism, precisifications are most naturally applied not just to a single word, but to every contentful entity. Secondly, it explains why we have the particular penumbral connections we actually have. Recall that it was left a little unexplained above why Alice’s and Bob’s use of ‘there’ denoted the same precise place. On the current conjecture, Alice’s term refers to a particular place x according to P because x is more natural–P than all the other places to which Alice might have referred. If this is so, then x will be more natural–P than all the other places to which Bob might have referred, so it will also be the referent according to P of Bob’s there. Hence according to every precisification, Bob’s utterance will be true, as Schiffer required.\nWe can also explain some other unexplained penumbral connections by appeal to naturalness. Consider the sentence David Chalmers is conscious. Unless this is supertrue, supervaluationism is in trouble. It is vague just which object is denoted by David Chalmers. On every precisification, there are other objects that massively overlap David Chalmers. Indeed, these very objects are denoted by ‘David Chalmers’ on other precisifications. These objects are not conscious, since if one did there would be two conscious objects where, intuitively, there is just one. But each of these rogue objects must be in the extension of ‘conscious’ on the precisifications where it is the denotation of ‘David Chalmers.’ So ‘conscious’ must be vague in slightly unexpected ways, and there must be a penumbral connection between it and ‘David Chalmers’: on every precisification, whatever object is denoted by that name is in the extension of ‘conscious,’ while no other potential denotata of ‘David Chalmers’ is in the extension. How is this penumbral connection to be explained? Not by appeal to the meanings of the terms! Even if ‘David Chalmers’ has descriptive content, it is highly implausible that this includes being conscious. (After all, unless medicine improves a bit in a thousand years Chalmers will not be conscious.) Rather, this penumbral connection is explained by the fact that the very same thing, naturalness, is used in resolving the vagueness in the terms ‘conscious’ and ‘David Chalmers.’ If the precisification makes one particular possible precisification of ‘David Chalmers,’ say d1, more natural than another, d2, then it will make properties satisfied by d1­ more natural than those satisfied by d2, so every precisification will make the denotation of ‘David Chalmers’ fall into the extension of ‘conscious.’\nWe can say the same thing about Alice’s original statement: That is where Harold and I first danced the rumba. Since one can’t first dance the rumba with Harold in two different places, it seems Alice’s statement can’t be true relative to more than one precisification of ‘That.’ But really the phrase after ‘is’ is also vague, and there is a penumbral connection (via naturalness) between it and the demonstrative. Hence we can say Alice’s statement is supertrue without appealing to any mysterious penumbral connections.\nMcGee and McLaughlin’s Challenge\nVann McGee and Brian McLaughlin (2000) raise a challenge for supervlauational approaches to the Problem of the Many that uses belief reports in much the way that Schiffer’s problem uses speech reports. They fear that without further development, the supervaluational theory cannot distinguish between the de re and de dicto readings of (4).\nRalph believes that there is a snow-capped mountain within sight of the equator.\nThey claim, correctly, that (4) should have both a de dicto reading and a de re reading, where in the latter case it is a belief about Kilimanjaro. The problem with the latter case is unclear how Ralph’s belief can be about Kilimanjaro itself. To press the point, they consider an atom at or around the base of Kilimanjaro, called Sparky, and define “Kilimanjaro(+) to be the body of land constituted … by the atoms that make up Kilimanjaro together with Sparky [and] Kilimanjaro(-) [to] be the body of land constituted … by the atoms that make up Kilimanjaro other than Sparky.” (129) The problem with taking (4) to be true on a de re reading is that “there isn’t anything, either in his mental state or in his neural state or in his causal relations with his environment that would make one of Kilimanjaro(+) and Kilimanjaro(-), rather than the other, the thing that Ralph’s belief is about.” (146) So if the truth of (4) on a de re reading requires that Ralph believes a singular, or object-dependent, proposition, about one of Kilimanjaro(+) and Kilimanjaro(-), then (4) cannot be true. Even worse, if the truth of (4) requires that Ralph both that Ralph believes a singular proposition about Kilimanjaro(+), that it is a snow-capped mountain within sight of the equator, and the same proposition about Kilimanjaro(-), then given some knowledge about mountains on Ralph’s part, (4) cannot be true, because that would require Ralph to mistakenly believe there are two mountains located roughly where Kilimanjaro is located.\nWe should not be so easily dissuaded. It is hard to identify exactly which features of Ralph’s “mental state or neural state or causal relations with his environment” that make it the case that he believes that two plus two equals four, but does not believe that two quus two equals four. (I assume Ralph is no philosopher, so lacks the concept QUUS.) I doubt, for example, that the concept PLUS has some causal influence over Ralph that the concept QUUS lacks. But Ralph does have the belief involving PLUS, and not the belief involving QUUS. He has this belief not merely in virtue of his mental or neural states, or his causal interactions with his environment, but in virtue of the fact that PLUS is a more natural concept than QUUS, and hence is more eligible to be a constituent of his belief.\nSo if Kilimanjaro(+) is more natural than Kilimanjaro(-), it will be a constituent of Ralph’s belief, despite the fact that there is no other reason to say his belief is about one rather than the other. Now, in reality Kilimanjaro(+) is no more natural than Kilimanjaro(-). But according to any precisification, one of them will be more natural than the other, for precisifications determine content by determining relative naturalness. Hence if Ralph has a belief with the right structure, in particular a belief with a place for an object (roughly, Kilimanjaro) and the property being within sight of the equator, then on every precisification he has a singular belief that a Kilimanjaro-like mountain is within sight of the equator. And notice that since naturalness determines both mental content and verbal content, on every precisification the constituent of that belief will be the referent of ‘Kilimanjaro.’ So even on a de re reading, (4) will be true.\nSchiffer’s problem showed that we should not take precisifications to be defined merely over single sentences. McGee and McLaughlin’s problem shows that we should take precisifications to set the content not just of sentences, but of mental states as well. Precisifications do not just assign precise content to every contentful linguistic token, but to every contentful entity in the world, including beliefs. This makes the issue of penumbral connections that we discussed in section two rather pressing. We already noted the need to establish penumbral connections between separate uses of demonstratives. Now we must establish penumbral connections between words and beliefs. The idea that precisifications determine content by determining relative naturalness establishes these connections.\nTo sum up, McGee and McLaughlin raise three related problems concerning de re belief. Two of these concern belief reports. First, how can we distinguish between de re and de dicto reports? If I am right, we can distinguish between these just the way Russell suggested, by specifying the scope of the quantifiers. McGee and McLaughlin suspect this will not work because in general we cannot argue from (5) to (6), given the vagueness of ‘Kilimanjaro.’\nKilimanjaro is such that Ralph believes it to be within sight of the equator.\nThere is a mountain such that Ralph believes it to be within sight of the equator.\nWhether or not we want to accept a semantics in which we must restrict existential generalisation in this way as a general rule, we can give an independent argument that (6) is true whenever (4) is true on a de re reading (i.e. whenever (5) is true). The argument is just that on every precisification, the subject of Ralph’s salient singular belief is a mountain, so (6) is true on every precisification. This argument assumes that there is a penumbral connection between the subject of this belief, as we might say the referent of ‘Kilimanjaro’ in his language of thought3, and the word ‘mountain.’ But since we have already established that there is such a connection between ‘Kilimanjaro’ in his language of thought and ‘Kilimanjaro’ in public language, and there is obviously a connection between ‘Kilimanjaro’ in public language and the word ‘mountain,’ as ‘Kilimanjaro is a mountain’ is supertrue, this assumption is safe. So the second puzzle McGee and McLaughlin raise, how it can be that the relevant de re reports can be true, has also been addressed.\nThere is a third puzzle McGee and McLaughlin raise that the reader might think I have not addressed. How can it be that Ralph can actually have a de re belief concerning Kilimanjaro? I have so far concentrated on belief reports, not merely on beliefs, and my theory has relied crucially on correlations between the vagueness in these reports and the vagueness in the underlying belief. It might be thought that I have excluded the most interesting case, the one where Ralph has a particular belief with Kilimanjaro itself as a constituent. While I will end up denying Ralph can have such a belief, I doubt this a problematic feature of my view. The theory outlined here denies that Ralph has object–dependent beliefs, but not that he has de re beliefs. I deny that Ralph has a belief that has Kilimanjaro(+) as a constituent, but it is hard to see how Ralph could have such a belief, since it very hard to see how he could have had a belief that has Kilimanjaro(+) rather than Kilimanjaro(-) as its subject. (This was McGee and McLaughlin’s fundamental point.) If we think that having a de re belief implies having a belief whose content is an object–dependent proposition, then we must deny that there are de re beliefs about Kilimanjaro. Since there is no object that is determinately a constituent of the proposition Ralph believes, it is a little hard to maintain that he believes an object–dependent proposition.4 But this is not the only way to make sense of de re beliefs.\nRobin Jeshion has argued that whether a belief is de re depends essentially on its role in cognition. “What distinguishes de re thought is its structural or organisational role in thought” (Jeshion 2002, 67)5 I won’t rehearse Jeshion’s arguments here, just their more interesting conclusions. We can have de re beliefs about an object iff we have a certain kind of mental file folder for the object. This folder need not be generated by acquaintance with the object, so acquiantanceless de re belief is possible. Indeed, the folder could have been created defectively, so there is no object that the information in the folder is about.6 In this case, the contents of the folder are subjectless de re beliefs. Jeshion doesn’t discuss this, but presumably the folder must not have been created purely to be the repository for information about the bearer of a certain property, whoever or whatever that is. We have to rule out this option if we follow Szabó (2000) in thinking the folder metaphor plays a crucial role in explaining our talk and thought involving descriptions. Provided the folder was created with the intent that it record information about some object, rather than merely information about whatever object has a particular property, its contents are de re beliefs. (To allow for distinct folders ‘about’ non-existent objects, we must allow that it is possible that such folders do have their reference fixed by their contents, but as long as this was not the intent in creation these folders can suffice for de re belief. This point lets us distinguish between my folder for Vulcan and my folder for The planet causing the perturbations of Mercury. Both are individuated by the fact that they contain the proposition This causes the perturbations of Mercury. It is this feature of the folder that fixes their reference, or in this case their non-reference. Only in the latter case, however, was this the intent in creating the folder, so its contents are de dicto beliefs, while the contents of the former are de re beliefs.)\nNow we have the resources to show how Ralph can have de re beliefs concerning Kilimanjaro. When Ralph hears about it, or sees it, he opens a file folder for Kilimanjaro. This is not intended to merely be a folder for the mountain he just heard about, or saw. It is intended to be a folder for that. (Imagine here that I am demonstrating the mountain in question.) The Kripkenstein point about referential indeterminacy applies to folders as much as to words. This point is closely related to Kripke’s insistence that his indeterminacy argument does not rely on behaviourism. So if Ralph’s folder is to have a reference, it must be fixed in part by the naturalness of various putative referents. But that is consistent with Ralph’s folder containing de re beliefs, since unless Ralph is a certain odd kind of philosopher, he will not have in his folder that Kilimanjaro is peculiarly eligible to be a referent. So the referent of the folder is not fixed by its contents (as the referent for a folder about The mountain over there, whatever it is, would be, or how the referent for a folder about The natural object over there, whatever it is, would be), and the contents of this folder are still de re beliefs Ralph has about Kilimanjaro. This was a bit roundabout, but we have seen that the Problem of the Many threatens neither the possibility that Ralph is the subject of true de re belief ascriptions, nor that he actually has de re beliefs.\nVague Objects\n\n“I think the principle that to be is to be determinate is a priori, and hence that it is a priori that there is no de re vagueness.” (Jackson 2001, 657–58)\n\nSo do I. I also think there are a few arguments for this claim, though some of them may seem question-begging to the determined defender of indeterminate objects. Most of these arguments I will just mention, since I assume the reader has little desire to see them detailed again. One argument is just that it is obvious that there is no de re vagueness. Such ‘arguments’ are not worthless. The best argument that there are no true contradictions is of just this form, as Priest (1998) shows. And it’s a good argument! Secondly, Russell’s point that most arguments for de re vagueness involve confusing what is represented with its representation still seems fair (Russell 1923). Thirdly, even though the literature on this is a rather large, it still looks like the Evans-Salmon argument against vague identities works, at least under the interpretation David Lewis gives it, and this makes it hard to see how there could be vague objects (Evans 1978; Salmon 1981; Lewis 1988). Fourthly, Mark Heller (1996) argues that we have to allow that referential terms are semantically vague. He says we have to do so to explain context dependence but there are a few other explanatory projects that would do just as well. Since semantic conceptions of vagueness can explain all the data that are commonly taken to support ontological vagueness, it seems theoretically unparsimonious to postulate ontological vagueness too. That’s probably enough, but let me add one more argument to the mix. Accepting that Kilimanjaro is be a vague material object distinct from both Kilimanjaro(+) and Kilimanjaro(-) has either metaphysical or logical costs. To prove this, I derive some rather unpleasant metaphysical conclusions from the assumption that Kilimanjaro is vague. The proofs will use some contentious principles of classical logic, but rejecting those, and hence rejecting classical logic, would be a substantial logical cost. The most contentious such principle used will be an instance of excluded middle: Sparky is or is not a part of Kilimanjaro. I also assume that if for all x other than Sparky that x is a part of y iff it is a part of z, then if Sparky is part of both y and z, or part of neither y nor z, then y and z coincide. If someone can contrive a mereological theory that rejects this principle, it will be immune to these arguments.\nIt is very plausible that material objects are individuated by the materials from which they are composed, so any coincident material objects are identical. Properly understood, that is a good account of what it is to be material. The problem is getting a proper understanding. Sider (1996) interprets it as saying that no two non-identical material objects coincide right now. His project ends up running aground over concerns about sentences involving counting, but his project, of finding a strong interpretation of the principle is intuitively compelling. David (Lewis 1986 Ch. 4) defends a slight weaker version: no two non-identical material objects coincide at all times. Call this the strong composition principle (scp). The scp is (classically) inconsistent with the hypothesis that Kilimanjaro is vague. If Sparky is part of Kilimanjaro, then Kilimanjaro and Kilimanjaro(+) always coincide. If Sparky is not part of Kilimanjaro then Kilimanjaro and Kilimanjaro(-) always coincide. Either way, two non-identical objects always coincide, which the scp does not allow.\nSome think the scp is refuted by Gibbard’s example of Lumpl and Goliath (Gibbard 1975). The most natural response to Gibbard’s example is to weaken our individuation principle again, this time to: no two non-identical material objects coincide in all worlds at all times. Call this the weak compositional principle (wcp). Since there are worlds in which Goliath is composed of bronze, but Lumpl is still a lump of clay in those worlds, Lumpl and Goliath do not refute the wcp. Some may think that even the wcp is too strong7, but most would agree that if vague objects violated the wcp, that would be a reason to believe they don’t exist.\nGiven a plausible metaphysical principle, which I call Crossover, vague objects will violate the wcp. As shown above, Kilimanjaro actually (always) coincides with Kilimanjaro(+) or Kilimanjaro(-), but is not identical with either. Crossover is the following principle:\nCrossover\nFor any actual material objects x and y there is an object z that coincides with x in the actual world and y in all other worlds.\n\nGiven that arbitrary fusions exist, Crossover is entailed by, but does not entail, the doctrine of arbitrary modal parts: that for any object o and world w, if o exists in w then o has a part that only exists in w. But Crossover does not have the most surprising consequence of the doctrine of arbitrary modal parts: that for any object o there is an object that has essentially all the properties o actually has.\nLet K1 be the object that coincides with Kilimanjaro in this world and Kilimanjaro(+) in all other worlds. Let K2 be the object that coincides with Kilimanjaro in this world and Kilimanjaro(-) in all other worlds. If Sparky is part of Kilimanjaro then K1 and Kilimanjaro(+) coincide in all worlds, but they are not identical, since it is determinate that Sparky is actually part of Kilimanjaro(+) and not determinate that it is part of K1. If Sparky is not part of Kilimanjaro then K2 and Kilimanjaro(-) coincide in all worlds, but they are not identical, since it is determinate that Sparky is not actually part of Kilimanjaro(-) and not determinate that it is not part of K2. Either way, we have a violation of the wcp. So the following three claims are (classically) inconsistent.\nCrossover.\nThe wcp.\nKilimanjaro is a vague object that indeterminately has Sparky as a part.\nI think the first two are highly plausible, so accepting (c) is costly. I already noted the plausibility of the wcp, so the focus should be on Crossover. On Lewis’s account of modality, it is clearly true, as is the stronger doctrine of arbitrary modal parts. On a fictionalist theory of modality based on Lewis’s account, it is still true, or at least true in the fiction that we must adopt to make sense of modal talk. So the principle is not without merits. And dialectically, opposing Crossover will be problematic for the believer in vague objects. Either an object’s modal profile is determined by its categorical properties or it isn’t. If it is, then the wcp will entail the scp, so by the above reasoning vague objects will be inconsistent with the wcp. If it is not, then it is hard to see why an object could not have a completely arbitrary modal profile, say the profile of some other ordinary material object. But that means Crossover is true, and again we cannot have both the wcp and vague objects. Probably the best way out for the believer in vague objects will be to short-circuit this reasoning by abandoning classical logic, presumably by declining to endorse the version of excluded middle with which I started. But that is undoubtedly a costly move, particularly for a supervaluationist.\nMcKinnon on Coins and Precisifications\nMost of our discussions of the Problem of the Many relate to the vagueness in a single singular term, and a single ordinary object. As McKinnon reminds us, however, there is not just one mountain in the world, there are many of them, and supervaluationists are obliged to say plausible things about statements that are about many mountains. Or, to focus on McKinnon’s example, we must not only have a plausible theory of coins, but of coin exhibitions. These do raise distinctive problems. Imagine we have an exhibition with, as we would ordinarily say, 2547 coins, each numbered in the catalogue. So to each number n there correspond millions of coin-like entities, coin*s in Sider’s helpful phrase (Sider 2001), and each precisification assigns a coin* to a number. In general, Sider holds that something is an F* iff it has all the properties necessary and sufficient for being an F except the property of not massively overlapping another F. There are some interesting questions about how independent these assignments can be. If one precisification assigns coin* c1 to n1, and another assigns coin* c2 to n2 (distinct from n1) then is there a guaranteed to be a precisification that assigns both c1 to n1 and c2 to n2? In other words, may the precisifications of each numeral (construed as a coin denotation) be independent of each other? The following example suggests not. Say Cj is the set of coin*s that are possible precisifications of j. This set may be vague because of higher–order vagueness, but set those difficulties aside. If every member of C1728 has a duplicate in C1729, then presumably only precisifications that assigned duplicates to ‘1728’ and ‘1729’ would be admissible. If the exhibition has two Edward I pennies on display to show the obverse and reverse, and miraculously these coins are duplicates, such a situation will arise.\nThis case is fanciful, so we don’t know whether in reality the precisifications of the numerals are independent. We probably can’t answer this question, but this is no major concern. McKinnon has found a question which the supervaluationist should feel a need to answer, but to which neither answer seems appropriate. Say that a precisification is principled iff there is some not-too-disjunctive property F such that for each numeral n, the precisification assigns to n the F-est coin* in Cn. If F does not come in degrees, then the precisification assigns to n the F in Cn. McKinnon’s question to the supervaluationist is: Are all precisifications principled? He aims to show either answering ‘yes’ or ‘no’ gets the supervaluationist in trouble. ‘Yes’ leads to there being too few precisifications; ‘No’ leads to there being too many. Let us look at these in order.\nI have little to say for now on the first horn of this dilemma. McKinnon’s survey of principled precisifications only considers cases where F is intrinsic, and I postpone for now investigation of extrinsic principles. Nevertheless, he does show that if F must be intrinsic, then there are not enough principled precisifications to generate all the indeterminacy our coin exhibit intuitively displays. The other horn is trickier.\nA precisification must not only assign a plausible coin* to each numeral, it must do so in such a way that respects penumbral connections. McKinnon thinks that unprincipled, or arbitrary precisifications, will violate (NAD) and (NAS).\nNon-Arbitrary Differences (NAD)\nFor any coin and non-coin, there is a principled difference between them which forms the basis for one being a coin and the other being a non-coin.\n\nNon-Arbitrary Similarities (NAS)\nFor any pair of coins, there is a principled similarity between them which forms the basis for their both being coins.\n\nMcKinnon holds these are true, so they should be true on all precisifications, but they are not true on unprincipled precisifications, so unprincipled precisifications are unacceptable. The motivation for (NAD) and (NAS) is clear. When we list the fundamental properties of the universe, we will not include being a coin. Coinness doesn’t go that deep. So if some things are coins, they must be so in virtue of their other properties. From this (NAD) and (NAS) follow.\nThe last step looks dubious. Consider any coin, for definiteness say the referent of ‘1728,’ and a coin* that massively overlaps it. The coin* is not a coin, so (a) one of these is a coin and the other is not, and (b) the minute differences between them cannot form the basis for a distinction between coins and non-coins. Hence (NAD) and (NAS) fail. At best, it seems, we can justify the following claims. If something is a coin* and something else is not, then there is a principled difference between them that makes one of them a coin* and the other not. Something is a coin iff it is a coin* that does not excessively overlap a coin. If this is the best we can do at defining ‘coin,’ then the prospects for a reductive physicalism about coins might look a little dim, though this is no threat to a physicalism about coins that stays neutral on the question of reduction. (I trust no reader is an anti-physicalist about coins, but it is worth noting how vexing questions of reduction can get even when questions of physicalism are settled.)\nSo I think this example refutes (NAD) and (NAS). Do I beg some questions here? Well, my counterexample turns crucially on the existence of kinds of objects, massively overlapping coin*s, that some people reject, and indeed that some find the most objectionable aspect of the supervaluationist solution. But this gets the burden of proof the wrong way around. I was not trying to refute (NAD) and (NAS). I just aimed to parry an argument based on those principles. I am allowed to appeal to aspects of my theory in doing so without begging questions. I do not want to rest too much weight on this point, however, for issues to do with who bears the burden of proof are rarely easily resolved, so let us move on.\nMy main response to McKinnon’s dilemma is another dilemma. If the principled similarities and differences in (NAD) and (NAS) must be intrinsic properties, then those principles are false, because there is no principled intrinsic difference between a coin and a token, or a coin and a medal. If the principled similarities and differences in (NAD) and (NAS) may be extrinsic properties, then those principles may be true, but then the argument that there are not enough principled precisifications fail, since now we must consider precisifications based on extrinsic principles. Let’s look at the two halves of that dilemma in detail, in order.\nA subway token is not a coin. Nor is a medal.8 But in their intrinsic respects, subway tokens often resemble certain coins more than some coins resemble other coins. Imagine we had a Boston subway token (which looks a bit like an American penny, but larger), an American penny, a British 20p piece (which is roughly heptagonal) and an early Australian holey dollar (which has a hole in it). There is no non-disjunctive classification of these by intrinsic properties that includes the penny, the 20p piece and the holey dollar in one group, and the subway token in the other. Any group that includes the penny and the other coins will include the token as well. So if we restrict attention to intrinsic similarities and differences, (NAD) and (NAS) are false.\nThere is a difference between these coins and the subway token. The coins were produced with the intent of being legal tender, the token was not. Perhaps we can find a difference between coins and non-coins based on the intent of their creator.9 This might make (NAD) and (NAS) true. But note that given the theory of precisifications developed in section 3, on every precisification, one and only one of the precisifications of ‘1728’ will be the subject of an intention on the part of its manufacturer. Just which of the objects is the subject of this intent will vary from precisification to precisification, but there is only one on every precisification. So we can say that on every precisification, the coin is the one where the intent of its creator was that it be used in a certain way. Indeed, on any precisification we may have antecedently thought to have existed, we can show that precisification to be principled by taking F to be the property being created with intent of being used in a coin-like way.10 So now we can say that restricting attention to the principled precisifications does not unduly delimit the class of precisifications.\nLet’s sum up. To argue against the possibility of unprincipled precisifications, McKinnon needed to justify (NAD) and (NAS). But these are only true when we allow ‘principled differences’ to include differences in creatorial intent. And if we do that we can see that every prima facie admissible precisification is principled, so we can give an affirmative answer to McKinnon’s question.\nIt might be objected that this move relies heavily on the fact that for many artefacts creative intent is constitutive of being the kind of thing that it is. But a Problem of the Many does not arise only for artefacts, so my solution does not generalise. This is little reason for concern since McKinnon’s problem does not generalise either. (NAD) and (NAS) are clearly false when we substitute ‘mountain’ for ‘coin.’ Consider a fairly typical case where it is indeterminate whether we have one mountain or two.11 In this case it might be not clear whether, for example, we have one mountain with a southern and a northern peak, or two mountains, one of them a little north of the other. Whether there is one mountain here or two, clearly the two peaks exist, and their fusion exists too. The real question is which of these three things is a mountain. However this question is resolved, a substitution instance of (NAD) with the two objects being the southern peak and the fusion of the two peaks will be false. So in this case a relatively unprincipled precisification will be acceptable. The point here is that mountain*s that are not mountains exist (either the peaks or their fusion will do as examples), and that suffices to refute McKinnon’s alleged penumbral connections and allow, in this case, a negative answer to his question.\nSorensen on Direct Reference\nAccording to orthodoxy, we can use descriptions to determine the reference of names without those descriptions becoming part of the meaning of the name. This, apparently, is what happened when Leverrier introduced ‘Neptune’ to name, not merely describe, the planet causing certain perturbations, and when someone introduced ‘Jack the Ripper’ to name, not merely describe, the person performing certain murders. So let us introduce ‘Acme’ as the name for the first tributary of the river Enigma. As Sorensen suggests, this can create certain problems.\n\nWhen [explorers] first travel up the river Enigma they finally reach the first pair of river branches. They name one branch ‘Sumo’ and the other ‘Wilt.’ Sumo is shorter but more voluminous than Wilt. This makes Sumo and Wilt borderline cases of ‘tributary’ … ‘Acme’ definitely refers to something, even though it is vague whether it refers to Sumo and vague whether it refers to Wilt. (Sorensen 2000, 180)\n\nIf ‘Acme,’ ‘Sumo’ and ‘Wilt’ are all vague names related in this way, Sorensen thinks the supervaluationist has a problem. The sentences ‘Acme is Sumo’ and ‘Acme is Wilt’ both express propositions of the form \\(\\langle x = y \\rangle\\). For exactly one of them, x is y. Since the proposition contains just the objects x and y (and the identity relation) but not their route into the proposition, there is no vagueness in the proposition. Hence there is no way to precisify either proposition. So a supervaluationist cannot explain how these propositions are vague.\nThis is no problem for supervaluationism, since supervaluationism says that sentences, not propositions, are vague. Indeed, most supervaluationists would say that no proposition is ever vague. Thinking they are vague is just another instance of the fallacy Russell identified: attributing properties of the representation to the entity, in this case a proposition, represented.\nBut maybe there is a problem in the area. One natural way of spelling out the idea that names directly refer to objects is to say that the meaning of a name is its referent. And one quite plausible principle about precisifications is that precisifications must not change the meaning of a term, they may merely provide a meaning where none exists. Now the supervaluationist has a problem. For it is true that one of Sorensen’s identity sentences is true in virtue of its meaning, since its meaning determines that it expresses a proposition of the form \\(\\langle x = x \\rangle\\). But each sentence is false on some precisifications, so some precisifications change the meaning of the terms involved.\nThe best way to respond to this objection is simply to bite the bullet. We can accept that some precisifications alter meanings provided we can provide some other criteria for acceptability of precisifications. I offered one such proposal in section 2. An acceptable precisification takes the partial order more natural than, turns it into a complete order without changing any of the relations that already exist, and uses this new relation to generate meanings. If we proceed in this way it is possible, for all we have hitherto said, that on every precisification the proposition expressed by ‘Acme is Sumo’ will be of the form \\(\\langle x = y \\rangle\\), so just the named object, rather than the method of naming, gets into the proposition. The central point is that since precisifications apply to the processes that turn semantic intentions into meanings, rather than to sentences with meanings, there is no guarantee they will preserve meanings. But if we like directly referential theories of names we should think this perfectly natural. If names are directly referential then Sorensen’s argument that there are vague sentences that are true in virtue of their meaning works. But this is consistent with supervaluationism.\nOne challenge remains. If precisifications change meanings, why should we care about them, or about what is true on all of them? This is not a new challenge; it is a central plank in Jerry Fodor and Ernest Lepore’s (1996) attack on supervaluationism. A simple response is just to say that we should care about precisifications because this method delivers the right results in all core cases, and an intuitively plausible set of results in contentious cases. This kind of instrumentalism about the foundations of a theory is not always satisfying.12 But if that’s the biggest problem supervaluationists have, they should be able to sleep a lot easier than the rest of us.\nConclusions and Confessions\nI have spent a fair bit of time arguing that supervaluationism is not vulnerable to a few challenges based on the Problem of the Many. Despite doing all this, I don’t believe supervaluationism to be quite true. So why spend this time? Because the true theory of vagueness will be a classical semantic theory, and everything I say about supervaluationism above applies mutatis mutandis to all classical semantic theories. I focussed on supervaluationism because it is more familiar and more popular, but I need not have.\nWhat is a classical semantic theory? That’s easy - it’s a theory that is both classical and semantic. What is a classical theory? It is one that incorporates vagueness while preserving classical logic. How much of classical logic must we preserve? That’s a hard question, though it is relevant to determining whether supervaluationism is (as it is often advertised) a classical theory. Williamson (1994) notes that supervaluationism does not preserve classical inference rules, and Hyde (1997) notes that it does not preserve some classically valid multiple–conclusion sequents. Keefe (2000) argues that neither of these constitutes an important deviation from classical logic. I’m inclined to disagree with Keefe on both points. Following Read (2000), I take it that the best response to the anti-classical arguments in Dummett (1991) takes the essential features of classical logic to be its inferential rules as formulated in a multiple–conclusion logic. But we need not adjudicate this dispute here. Why should we want a classical theory? The usual arguments for it are based on epistemic conservatism, and I think these arguments are fairly compelling. I also think that no non–classical theory will be able to provide a plausible account of quantification.13\nWhat is a semantic theory? It is one that makes vagueness a semantic phenomenon. It is not necessarily one that makes vagueness a linguistic phenomenon. That would be absurd in any case, since clearly some non–linguistic entities, maps, beliefs and pictures for example, are vague. But the more general idea that vagueness is a property only of representations is quite attractive. It links up well with the theory of content Lewis outlines in “Languages and Language” - all Languages (in his technical sense) are precise, vagueness in natural language is a result of indecision about which Language we are speaking.\nTrenton Merricks (2001) argues against this picture, claiming that all semantic vagueness (he says ‘linguistic,’ but ignore that) must arise because of metaphysical or epistemic vagueness. He claims that if (17) is vague, then so is (18), and (18)’s vagueness must be either metaphysical or semantic.\nHarry is bald.\n‘Bald’ describes Harry.\nOne might question the inference from (17)‘s vagueness to (18) - on some supervaluational theories if (17) is vague then (18) is false. But I will let that pass, for there is a simpler problem in the argument. Merricks claims that if (18) is vague, then it is vague whether ’Bald’ has the property describing Harry, and this is a kind of metaphysical vagueness. It is hard to see how this follows. If there is metaphysical vagueness, there is presumably some object o and some property F such that it is vague whether the object has the property. Presumably the object here is the word ‘bald’ and the property is describing Harry. But words alone do not have properties like describing Harry. At best, words in languages do so. So maybe the object can be the ordered pair \\(\\langle \\text{`Bald'}, l \\rangle\\), where l is a language. But which one? Not one of Lewis’s Languages, for then it is determinate whether <‘Bald,’ l> has the property describing Harry. So maybe a natural language, perhaps English! But it is doubly unclear that English is an object. First, it is unclear whether we should reify natural languages to such a degree that we accept that ‘English’ refers to anything at all. Secondly, if we say ‘English’ does refer, why not say that it refers to one of Lewis’s Languages, thought it is vague which one? That way we can say that the sentence ‘Bald’ in English describes Harry is vague without there being any object that vaguely instantiates a property. Now on a supervaluational theory this approach may have the unwanted consequence that “English is a precise language” is true, since it is true on all precisifications. It does not seem that this problem for the supervaluationist generalises to be a problem for all semantic theories of vagueness, so Merricks has raised no general problem for semantic theories of vagueness. (The problem for the supervaluationist here is not new. For some discussion see Lewis’s response, in “Many, but Almost One” to the objection, there attributed to Kripke, that the supervaluationist account makes it true that all words are precise.)\nIf we have a classical semantic theory that provides a concept of determinateness, then we can define acceptable precisifications as maximal consistent extensions of the set of determinate truths. Given that, it follows pretty quickly that determinate truth implies truth on all precisifications. And this is sufficient for the major objections canvassed above to get a foothold, and hence be worthy of response, though as we have seen none of them will ultimately succeed. Still, our theory may differ from supervaluationism in many ways. For one thing, it might explain determinateness in ways quite different from those in supervaluationism. For example, the theory in Field (2000) is a classical semantic theory14, but it clearly goes beyond supervaluational theory because it has an interesting, if ultimately flawed, explanation of determinateness in terms of Shafer functions. Other classical semantic theories may differ from supervaluationism by providing distinctive theories of higher order vagueness.\nThe most promising research programs in vagueness are within the classical semantic framework. Like all research programs, these programs need a defensive component, to fend off potential refutations and crisis. This avoids unwanted crises in the program, and as we have seen here we can learn a bit from seeing how to defend against certain attacks. There will undoubtedly be more challenges in the time ahead, but for now the moves in this paper brings the defensive side of the program up to date.\n\n\nDummett, Michael. 1991. The Logical Basis of Metaphysics.Cambridge, MA: Harvard.\n\n\nEvans, Gareth. 1978. “Can There Be Vague Objects?” Analysis 38 (4): 208. https://doi.org/10.1093/analys/38.4.208.\n\n\nField, Hartry. 2000. “Indeterminacy, Degree of Belief, and Excluded Middle.” Noûs 34 (1): 1–30. https://doi.org/10.1111/0029-4624.00200.\n\n\nFine, Kit. 1994. “Compounds and Aggregates.” Noûs 28 (2): 137–58. https://doi.org/10.2307/2216046.\n\n\nFodor, Jerry A., and Ernest Lepore. 1996. “What Cannot Be Valuated Cannot Be Valuated, and It Cannot Be Supervaluated Either.” Journal of Philosophy 93 (10): 516–35. https://doi.org/10.5840/jphil1996931013.\n\n\nFriedman, Milton. 1953. “The Methodology of Positive Economics.” In Essays in Positive Economics, 3–43. Chicago: University of Chicago Press.\n\n\nGibbard, Allan. 1975. “Contingent Identity.” Journal of Philosophical Logic 4 (2): 187–221. https://doi.org/10.1007/bf00693273.\n\n\nHausman, Daniel. 1992. “Why Look Under the Hood?” In Essays in Philosophy and Economic Methodology, 70–73. Cambridge: Cambridge University Press.\n\n\nHeller, Mark. 1996. “Against Metaphysical Vagueness.” Philosophical Perspectives 10: 177–85. https://doi.org/10.2307/2216242.\n\n\nHyde, Dominic. 1997. “From Heaps and Gaps to Heaps of Gluts.” Mind 106 (424): 641–60. https://doi.org/10.1093/mind/106.424.641.\n\n\nJackson, Frank. 2001. “Responses.” Philosophy and Phenomenological Research 62 (3): 653–64. https://doi.org/10.2307/2653545.\n\n\nJeshion, Robin. 2002. “Acquiantanceless de Re Belief’.” In Meaning and Truth: Investigations in Philosophical Semantics, edited by Joseph Keim Campbell, Michael O’Rourke, and David Shier, 53–74. New York: Seven Bridges Press.\n\n\nKeefe, Rosanna. 2000. Theories of Vagueness. Cambridge: Cambridge University Press.\n\n\nKripke, Saul. 1980. Naming and Necessity. Cambridge: Harvard University Press.\n\n\n———. 1982. Wittgenstein on Rules and Private Language. Oxford: Basil Blackwell.\n\n\nLeeds, Stephen. 2000. “A Disquotationalist Looks at Vagueness.” Philosophical Topics 28 (1): 107–28. https://doi.org/10.5840/philtopics200028119.\n\n\nLewis, David. 1983. “New Work for a Theory of Universals.” Australasian Journal of Philosophy 61 (4): 343–77. https://doi.org/10.1080/00048408312341131.\n\n\n———. 1984. “Devil’s Bargains and the Real World.” In The Security Gamble: Deterrence in the Nuclear Age, edited by Douglas Maclean, 141–54. Totowa, NJ: Rowman; Allenheld.\n\n\n———. 1986. On the Plurality of Worlds. Oxford: Blackwell Publishers.\n\n\n———. 1988. “Vague Identity: Evans Misunderstood.” Analysis 48 (3): 128–30. https://doi.org/10.1093/analys/48.3.128.\n\n\nMcGee, Vann, and Brian McLaughlin. 2000. “The Lessons of the Many.” Philosophical Topics 28 (1): 129–51. https://doi.org/10.5840/philtopics200028120.\n\n\nMerricks, Trenton. 2001. “Varieties of Vagueness.” Philosophy and Phenomenological Research 62 (1): 145–57. https://doi.org/10.2307/2653593.\n\n\nPriest, Graham. 1998. “What Is so Bad about Contradictions?” Journal of Philosophy 95 (8): 410–26. https://doi.org/10.2307/2564636.\n\n\nPutnam, Hilary. 1973. “Meaning and Reference.” Journal of Philosophy 70 (19): 699–711. https://doi.org/10.2307/2025079.\n\n\n———. 1980. “Models and Reality.” Journal of Symbolic Logic 45 (3): 464–82. https://doi.org/10.2307/2273415.\n\n\nQuine, W. V. O. 1960. Word and Object. Cambridge, MA.: MIT Press.\n\n\nRussell, Bertrand. 1923. “Vagueness.” Australasian Journal of Philosophy and Psychology 1 (2): 84–92. https://doi.org/10.1080/00048402308540623.\n\n\nSalmon, Nathan. 1981. Reference and Essence. Princeton: Princeton University Press.\n\n\nSchiffer, Stephen. 2000. “Replies.” Philosophical Issues 10 (1): 320–43. https://doi.org/10.1111/j.1758-2237.2000.tb00029.x.\n\n\nSider, Theodore. 1996. “All the World’s a Stage.” Australasian Journal of Philosophy 74 (3): 433–53. https://doi.org/10.1080/00048409612347421.\n\n\n———. 2001. “Maximality and Intrinsic Properties.” Philosophy and Phenomenological Research 63 (2): 357–64. https://doi.org/10.1111/j.1933-1592.2001.tb00109.x.\n\n\nSorensen, Roy. 2000. “Direct Reference and Vague Identity.” Philosophical Topics 28 (1): 177–94. https://doi.org/10.5840/philtopics200028123.\n\n\nSzabó, Zoltan Gendler. 2000. “Descriptions and Uniqueness.” Philosophical Studies 101 (1): 29–57. https://doi.org/10.1023/A:1026437211756.\n\n\nWeatherson, Brian. 2005. “True, Truer, Truest.” Philosophical Studies 123 (1-2): 47–70. https://doi.org/10.1007/s11098-004-5218-x.\n\n\nWilliamson, Timothy. 1994. Vagueness. Routledge.\n\n\nFollowing Schiffer, we ignore the vagueness in ‘is where Harold and I first danced the rumba.’ This phrase is vague, but its vagueness raises no extra issues of philosophical importance.↩︎\nThanks to John Hawthorne for the following argument.↩︎\nI do not mean here to commit myself to anything like the language of thought hypothesis. This is just being used as a convenient shorthand.↩︎\nThis is hard, but not perhaps impossible. One might say that on every precisification, Ralph believes a proposition that has a mountain as a constituent, and hence as an essential part.↩︎\nI don’t know if Jeshion would accept the corollary that if belief is too unstructured to allow for the possibility of such organisational roles, then there is no de re belief, but I do.↩︎\nWhich is not just to say that there is no object that has all the properties in the folder. This is neither necessary nor sufficient for the folder to be about the object, as Kripke’s discussion of ‘famous deeds’ descriptivism should make clear.↩︎\nKit Fine (1994) does exactly this.↩︎\nSome people I have asked think tokens are coins, but no one thinks medals are coins, so if you (mistakenly) think tokens are coins, imagine all my subsequent arguments are phrased using medals rather than tokens.↩︎\nNote that I say little here about what the intent of the creator must be. I don’t think that the intent must always be to create legal tender. A ceremonial coin that is created, for example, to be tossed before the start of a sporting match is still a coin, although it is not intended to be tender. But intent still matters. If someone had made a duplicate of that ceremonial coin with the intent of awarding it as a medal to the victorious captain, it would be a medal and not a coin.↩︎\nBecause of the problems raised in the previous footnote, I will not try and say just what this intention amounts to. There are complications when (a) the creator is a corporate entity rather than an individual and (b) the coins are mass–produced rather than produced individually. But since the story is essentially the same, I leave the gruesome details out here.↩︎\nThis case is rather important in the history of the problem, because its discussion in Quine (1960) is one of the earliest presentations in print of anything like the problem of the many.↩︎\nThe largest debate in the history of philosophy of economics concerned whether we could, or should, be instrumentalists about the ideally rational agents at the core of mainstream microeconomic theory. See Friedman (1953) for the classic statement of the instrumentalist position, and Hausman (1992) for the most amusing and enlightening of the countably many responses.↩︎\nSee the last section of Weatherson (2005) for a detailed defence of this claim.↩︎\nAt least, it strikes me as a classical semantic theory. Ryan Wasserman has tried to convince me that properly understood, it is really an epistemic theory. Space prevents a thorough account of why I think Field’s theory is flawed. Briefly, I think the point in Leeds (2000) that Field’s concept of a numerical degree of belief needs substantially more explanation than Field gives it can be developed into a conclusive refutation.\n\n↩︎\n",
    "preview": "posts/2021-02-03-many-many-problems/kilimanjaro.jpg",
    "last_modified": "2021-02-05T20:45:52-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-what-good-are-counterexamples/",
    "title": "What Good are Counterexamples?",
    "description": "Intuitively, Gettier cases are instances of justified true beliefs that are not cases of knowledge. Should we therefore conclude that knowledge is not justified true belief? Only if we have reason to trust intuition here. But intuitions are unreliable in a wide range of cases. And it can be argued that the Gettier intuitions have ag reater resemblance to unreliable intuitions than to reliable intuitions. What’s distinctive about the faulty intuitions, I argue, is that respecting them would mean abandoning a simple, systematic and largely successful theory in favour of a complicated, disjunctive and idiosyncratic theory. So maybe respecting the Gettier intuitions was the wrong reaction, we should instead have been explaining why we are all so easily misled by these kinds of cases.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-07-01",
    "categories": [
      "games and decisions",
      "epistemology",
      "methodology"
    ],
    "contents": "\n\nContents\nIntuitions\nCorrecting Mistakes\nNaturalness in a theory of meaning\nKeeping Conceptual Analysis\nAgainst the Psychologists\nIn defence of analysis\nNaturalness and the JTB theory\n\nThe following kind of scenario is familiar throughout analytic philosophy. A bold philosopher proposes that all Fs are Gs. Another philosopher proposes a particular case that is, intuitively, an F but not a G. If intuition is right, then the bold philosopher is mistaken. Alternatively, if the bold philosopher is right, then intuition is mistaken, and we have learned something from philosophy. Can this alternative ever be realised, and if so, is there a way to tell when it is? In this paper, I will argue that the answer to the first question is yes, and that recognising the right answer to the second question should lead to a change in some of our philosophical practices.\nThe problem is pressing because there is no agreement across the sub-disciplines of philosophy about what to do when theory and intuition clash. In epistemology, particularly in the theory of knowledge, and in parts of metaphysics, particularly in the theory of causation, it is almost universally assumed that intuition trumps theory. Shope’s The Analysis of Knowledge contains literally dozens of cases where an interesting account of knowledge was jettisoned because it clashed with intuition about a particular case. In the literature on knowledge and lotteries it is not as widely assumed that intuitions about cases are inevitably correct, but this still seems to be the working hypothesis.1 And recent work of causation by a variety of authors, with a wide variety of opinions, generally takes the same line: if a theory disagrees with intuition about a case, the theory is wrong.2 In this area exceptions to the rule are a little more frequent, particularly on the issues of whether causation is transitive and whether omissions can be causes, but in most cases the intuitions are taken to override the theories. Matters are quite different in ethics. It is certainly not a good thing for utilitarian theories that we very often feel that the action that maximises utility is not the right thing to do. But the existence of such cases is rarely taken to be obviously and immediately fatal for utilitarian theories in the way that, say, Gettier cases are taken to be obviously and immediately fatal for theories of knowledge that proclaim those cases to be cases of knowledge. Either there is some important difference here between the anti-utilitarian cases and the Gettier cases, a difference that justifies our differing reactions, or someone is making a mistake. I claim that it is (usually) the epistemologists and the metaphysicians who are wrong. In more cases than we usually imagine, a good philosophical theory can teach us that our intuitions are mistaken. Indeed, I think it is possible (although perhaps not likely) that the justified true belief (hereafter, JTB) theory of knowledge is so plausible that we should hold onto it in preference to keeping our intuition that Gettier cases are not cases of knowledge.\nMy main interests here are methodological, not epistemological. Until the last section I will be arguing for the JTB theory of knowledge, but my main interest is in showing that one particular argument against the JTB theory, the one that turns on the fact that it issues in some rather unintuitive pronouncements about Gettier cases, is not in itself decisive. Still, the epistemological issues are important, which is one reason I chose to focus on the JTB theory, and at the end I will discuss how the methodological conclusions drawn here may impact on them in an unexpected way.\nIntuitions\nLet us say that a counterexample to the theory that all Fs are Gs is a possible situation such that most people have an intuition that some particular thing in the story is an F but not a G. The kinds of intuition I have in mind are what George Bealer (1998) calls intellectual “seemings.” Bealer distinguishes intellectual seemings, such as the intuition that Hume’s Principle is true, or that punishing a person for a crime they did not commit is unjust, from physical seemings, such as the ‘intuition’ that objects fall if released, or perhaps that the sun rotates around the earth. We shall be primarily concerned here with intellectual seemings, and indeed I shall only call these intuitions in what follows.\nAs Bealer notes, whether something seems to be true can be independent of whether we believe it to be true. Bealer himself notes that Frege’s Axiom V seems to be true, though we know it is false. It does not seem to be the case, in the relevant sense, that 643 x 721 = 463603. Unless one is rather good at mental arithmetic, there is nothing that 643 x 721 seems to be; it is out of the reach of intuition. These are not the only ways that seemings and belief can come apart. One can judge that something seems to be the case while neither believing nor disbelieving it. This is a sensible attitude to take towards the view that one cannot know that a particular ticket will lose in a fair lottery. This is despite the fact that it certainly seems one cannot know this. If one’s intuitions are running rampant, one may even have an intuition about something that one believes to be strictly indeterminate. For example, some people may have the intuition that the continuum hypothesis is true, even though they believe on reflection that it is indeterminate whether it is true.\nThe distinction between intuitions and belief is important because it helps reduce the violence that revisionary philosophical views do to our pre-existing positions. When I say that Gettier cases may be cases of knowledge, I am not denying that there is a strong intuition that they are not cases of knowledge. I am not denying that a Gettier case does not seem to be a case of knowledge. The same thing occurs in ethics. Utilitarians rarely deny that it seems that punishing innocents is the wrong thing to do. They urge that in certain, rare, cases this might be one of those things that seems to be true despite being false. The case that knowledge is justified true belief is meant to be made in full awareness of the fact that certain cases of justified true beliefs seem to not be cases of knowledge.\nActually, although we will not make much of it here, this last claim is not true as a general statement about all people. Jonathan Weinberg, Stephen Stich and Shaun Nichols have reported Weinberg, Stich, and Nichols (2001) that the intuition that Gettier cases are not cases of knowledge is not universally shared. It is not entirely clear what the philosophical relevance of these discoveries is. It might show that we who have Gettier intuitions speak a different language from those who do not. It might show (though as Stich and Nichols point out it is rather hard to see how) that philosophers know a lot more about knowledge than other folk. I think it is rather unlikely that this is true, but we shall bracket such concerns for now, and continue on the assumption that all parties have the Gettier intuitions. Since I shall want to argue that knowledge may still be justified belief in any case, I am hardly tilting the playing field in my direction by making this assumption.\nGiven that intuitions are what Bealer calls intellectual seemings, and given that the example of Axiom V shows that seemings can be mistaken, what evidence have we that they are not mistaken in the cases we consider here? Arguably, we have very little indeed. Robert Cummins (1998) argues that in general intuition should not be trusted as an evidential source because it cannot be calibrated. We wouldn’t have trusted the evidence Galileo’s telescope gave us about the moon without an independent reason for thinking his telescope reliable. Fortunately, this can be done; we can point the telescope at far away terrestrial mountains, and compare its findings with the findings of examining the mountains up close and personal. There is no comparable way of calibrating intuitions. Clearly we should suspicious of any method that has been tested and found unreliable, but there are tricky questions about the appropriate level of trust in methods that have not been tested. Ernest Sosa (1998) argues in response to Cummins that this kind of reasoning leads to an untenable kind of scepticism. Sosa notes that one can make the same point about perception as Cummins makes about intuition: we have no independent way of calibrating perception as a whole. There is a distinction to be drawn here, since perception divides into natural kinds, visual perception, tactile perception, etc, and we can use each of these to calibrate the others. It is hard to see how intuitions can be so divided in ways that permit us to check some kinds of intuitions against the others. In any case, the situation is probably worse than Cummins suggests, since we know that several intuitions are just false. It is interesting to note the many ways in which intuition does, by broad agreement, go wrong.\nMany people are prone to many kinds of systematic logical mistakes. Most famously, the error rates on the Wason Selection Task are disturbingly large. Although this test directly measures beliefs rather than intuitions, it seems very likely that many of the false beliefs are generated by mistaken intuitions. As has been shown in a variety of experiments, the most famous of which were conducted by Kahneman and Tversky, most people are quite incompetent at probabilistic reasoning. In the worst cases, subjects held that a conjunction was more probable than one of its conjuncts. Again, this only directly implicates subjects’ beliefs, but it is very likely that the false beliefs are grounded in false intuitions. (The examples in this paragraph are discussed in detail in Stich (1988, 1992).)\nAs noted above, most philosophers would agree that many, if not most, people have mistaken moral intuitions. We need not agree with those consequentialists who think that vast swathes of our moral views are in error to think that (a) people make systematic moral mistakes and (b) some of these mistakes can be traced to mistaken intuitions. To take the most dramatic example, for thousands of years it seemed to many people that slavery was morally acceptable. On a more mundane level, many of us find that our intuitive judgements about a variety of cases cannot be all acceptable, for it is impossible to find a plausible theory that covers them all.3 Whenever we make a judgement inconsistent with such an intuition, we are agreeing that some of our original intuitions were mistaken.\nFrom a rather different direction, there are many mistaken conceptual intuitions, with the error traceable to the way Gricean considerations are internalised in the process of learning a language. Having learned that it would be improper to use t to describe a particular case, we can develop the intuition that this case is not an F, where F is the property denoted by t. For example, if one is careless, one can find oneself sharing the intuition expressed by Ryle in The Concept of Mind that morally neutral actions, like scratching one’s head, are neither voluntary nor involuntary (Ryle 1949). The source of this intuition is the simple fact that it would be odd to describe an action as voluntary or involuntary unless there was some reason to do so, with the most likely such reason being that the action was in some way morally suspect. The fact that the intuition has a natural explanation does not stop it being plainly false. We can get errors in conceptual intuitions from another source. At one stage it was thought that whales are fish, that the Mars is a star, the sun isn’t. These are beliefs, not intuitions, but there are clearly related intuitions. Anyone who had these beliefs would have had the intuition that in a situation like this (here demonstrating the world) the object in the Mars position was a star, and the objects in the whale position were fish. The empirical errors in the person’s belief will correlate to conceptual errors in their intuition. To note further that the kind of error being made here is conceptual not empirical, and hence the kind of error that occurs in intuition, note that we need not have learned anything new about whales, the sun or Mars to come to our modern beliefs. (In fact we did, but that’s a different matter.) Rather, we need only have learned something about the vast bulk of the objects that are fish, or stars, to realise that these objects had been wrongly categorised. The factor we had thought to be the most salient similarity to the cases grouped under the term, being a heavenly body visible in the night sky for ‘star,’ living in water for ‘fish,’ turned out not to be the most important similarity between most things grouped under that term. So there is an important sense in which saying whales are fish, or that the sun is not a star, may reveal a conceptual (rather than an empirical) error.\nThere seems to be a link between these two kinds of conceptual error. The reason we say that the Rylean intuitions, or more generally the intuitions of what (Grice 1989 Ch. 1) called the Type-A philosophers, are mistaken is that the rival, Gricean, theory attaches to each word a relatively natural property. There is no natural property that actions satisfy when, and only when, we ordinarily describe them as voluntary. There is a natural property that covers all these cases, and other more mundane actions like scratching one’s head, and that is the property we now think is denoted by ‘voluntary.’ This notion of naturalness, and the associated drive for systematicity in our philosophical and semantic theories, will play an important role in what follows.\nCorrecting Mistakes\nThe following would be a bad defence of the JTB theory against counterexamples. We can tell that all counterexamples to the JTB theory are based on mistaken intuitions, because the JTB theory is true, so all counterexamples to it are false. Unless we have some support for the crucial premise that the JTB theory is true, this argument is rather weak. And that support should be enough to not only make the theory prima facie plausible, but so convincing that we are prepared to trust it rather than our judgements about Gettier cases.\nIn short, the true theory of knowledge is the one that does best at (a) accounting for as many as possible of our intuitions about knowledge while (b) remaining systematic. A ‘theory’ that simply lists our intuitions is no theory at all, so condition (b) is vital. And it is condition (b), when fully expressed, that will do most of the work in justifying the preservation of the JTB theory in the face of the counterexamples.\nThe idea that our theory should be systematic is accepted across a wide range of philosophical disciplines. This idea seems to be behind the following plausible claims by Michael Smith: “Not only is it a platitude that rightness is a property that we can discover to be instantiated by engaging in rational argument, it is also a platitude that such arguments have a characteristic coherentist form.” (1994: 40) The second so-called platitude just points out that it is a standard way of arguing in ethics to say, you think we should do X in circumstances C1, circumstances C2 are just like C1, so we should do X in C1. The first points out that not only is this standard, it can yield surprising ethical knowledge. But this is only plausible if it is more important that final ethics is systematic than that first ethics, the ethical view delivered by intuition, is correct. In other words, it is only plausible if ethical intuitions are classified as mistaken to the extent that they conflict with the most systematic plausible theory. So, for example, it would be good news for utilitarianism if there was no plausible rival with any reasonable degree of systematicity.\nThis idea also seems to do important work in logic. If we just listed intuitions about entailment, we would have a theory on which disjunctive syllogism (A and ~A \\({\\vee}\\) B entail B) is valid, while ex falso quadlibet (A and ~A entail B) is not. Such a theory is unsystematic because no concept of entailment that satisfies these two intuitions will satisfy a generalised transitivity requirement: that if C and D entail E, and F entails D then C and F entail E. (This last step assumes that ~A entails ~A \\({\\vee}\\) B, but that is rarely denied.) Now one can claim that a theory of entailment that gives up this kind of transitivity can still be systematic enough, and Neil Tennant (1992) does exactly this, but it is clear that we have a serious cost of the theory here, and many people think avoiding this cost is more important than preserving all intuitions.\nIn more detail, there are four criteria by which we can judge a philosophical theory. First, counterexamples to a theory count against it. While a theory can be reformist, it cannot be revolutionary. A theory that disagreed with virtually all intuitions about possible cases is, for that reason, false. The theory: X knows that p iff X exists and p is true is systematic, but hardly plausible. As a corollary, while intuitions about any particular possible case can be mistaken, not too many of them could be. Counterexamples are problematic for a theory, the fewer reforms needed the better, it’s just not that they are not fatal. Importantly, not all counterexamples are as damaging to a theory as others. Intuitions come in various degrees of strength, and theories that violate weaker intuitions are not as badly off as those that violate stronger intuitions. Many people accept that the more obscure or fantastic a counterexample is, the less damaging it is to a theory. This seems to be behind the occasional claim that certain cases are “spoils to the victor” – the idea is that the case is so obscure or fantastic that we should let theory rather than intuition be our guide. Finally, if we can explain why we have the mistaken intuition, that counts for a lot in reducing the damage the counterexample does. Grice did not just assert that the theory on which an ordinary head scratch was voluntary was more systematic than the theory of voluntariness Ryle proposed, he provided an explanation of why it might seem that his theory was wrong in certain cases.\nSecondly, the analyses must not have too many theoretical consequences which are unacceptable. Consider Kahneman and Tversky’s account of how agents actually make decisions, prospect theory, as an analysis of ‘good decision.’ (Disclaimer: This is not how Kahneman and Tversky intend it.) So the analysis of ‘good decision’ is ‘decision authorised by prospect theory.’ It is a consequence of prospect theory that which decision is “best” depends on which outcome is considered to be the neutral point. In practice this is determined by contextual factors. Redescribing a story to make different points neutral, which can be done by changing the context, licences different decisions. I take it this would be unacceptable in an analysis of ‘good decision,’ even though it means the theory gives intuitively correct results in more possible cases than its Bayesian rivals4. In general, we want our normative theories to eliminate arbitrariness as much as possible, and this is usually taken to be more important than agreeing with our pre-theoretic intuitions about particular cases. Unger uses a similar argument in Living High and Letting Die to argue against the reliance on intuitions about particular cases in ethics. We have differing ethical intuitions towards particular cases that differ only in the conspicuousness of the suffering caused (or not prevented), we know that conspicuousness is not a morally salient difference, so we should stop trusting the particular intuitions. (Presumably this is part of the reason that we find Tennant’s theory of entailment so incredible, prima facie. It is not just that violating transitivity seems unsystematic, it is that we have a theoretical intuition that transitivity should be maintained.)\nThirdly, the concept so analysed should be theoretically significant, and should be analysed in other theoretically significant terms. This is why we now analyse ‘fish’ in such a way that whales aren’t fish, and ‘star’ in such a way that the sun is a star. This is not just an empirical fact about our language. Adopting such a constraint on categories is a precondition of building a serious classificatory scheme, so it is a constraint on languages, which are classificatory schemes par excellance. Even if I’m wrong about this, the fact that we do reform our language with the advance of science to make our predicates refer to theoretically more significant properties shows that we have a commitment to this restriction.\nFinally, the analysis must be simple. This is an important part of why we don’t accept Ryle’s analysis of ‘voluntary.’ His analysis can explain all the intuitive data, even without recourse to Gricean implicature, and arguably it doesn’t do much worse than the Gricean explanation on the second and third tests. But Grice’s theory can explain away the intuitions that it violates, and importantly it does so merely with the aid of theories of pragmatics that should be accepted for independent reasons, and it is simpler, so it trumps Ryle’s theory.\nMy main claim is that even once we have accepted that the JTB theory seems to say the wrong thing about Gettier cases, we should still keep an open mind to the question of whether it is true. The right theory of knowledge, the one that attributes the correct meaning to the word ‘knows,’ will do best on balance at these four tests. Granted that the JTB theory does badly on test one, it seems to do better than its rivals on tests two, three and four, and this may be enough to make it correct.\nNaturalness in a theory of meaning\nLet’s say I have convinced you that it would be better to use ‘knows’ in such a way that we all now assent to “She knows” whenever the subject of that pronoun truly, justifiably, believes. You may have been convinced that only by doing this will our term pick out a natural relation, and there is evident utility in having our words pick out relations that carve nature at something like its joints. Only in that way, you may concede, will our language be a decent classificatory scheme of the kind described above, and it is a very good thing to have one’s language be a decent classificatory scheme. I have implicitly claimed above that if you concede this you should agree that I will have thereby corrected a mistake in your usage. But, an objector may argue, it is much more plausible to say that in doing so I simply changed the meaning of ‘knows’ and its cognates in your idiolect. The meaning of your words is constituted by your responses to cases like Gettier cases, so when I convince you to change your response, I change the meaning of your words.\nThis objection relies on a faulty theory of meaning, one that equates meaning with use in a way which is quite implausible. If this objection were right, it would imply infallibilism about knowledge ascriptions. Still, the objection does point to a rather important point. There is an implicit folk theory of the meaning of ‘knows,’ one according to which it does not denote justified true belief. I claim this folk theory is mistaken. It is odd to say that we can all be mistaken about the meanings of our words; it is odd to say that we can’t make errors in word usage. I think the latter is the greater oddity, largely because I have a theory which explains how we can all make mistakes about meanings in our own language.\nHow can we make such mistakes? The short answer is that meanings ain’t in the head. The long answer turns on the kind of tests on analyses I discussed in section two. The meaning of a predicate is a property in the sense described by Lewis (1983)5: a set, or class, or plurality of possibilia. (That is, in general the meaning of a predicate is its intension.6) The interesting question is determining which property it is. In assigning a property to a predicate, there are two criteria we would like to follow. The first is that it validates as many as possible of our pre-theoretic beliefs. The second is that it is, in some sense, simple and theoretically important. How to make sense of this notion of simplicity is a rather complex matter. Lewis canvasses the idea that there is a primitive ‘naturalness’ of properties which measures simplicity and theoretical significance7, and I will adopt this idea. Space restrictions prevent me going into greater detail concerning ‘naturalness,’ but if something more definite is wanted, for the record I mean by it here just what Lewis means by it in the works previously cited.8\nSo, recapitulating what I said in section two, for any predicate t and property F, we want F meet two requirements before we say it is the meaning of t. We want this meaning assignment to validate many of our pre-theoretic intuitions (this is what we test for in tests one and two) and we want F to be reasonably natural (this is what we test for in tests three and four). In hard cases, these requirements pull in opposite directions; the meaning of t is the property which on balance does best. Saying ‘knows’ means ‘justifiably truly believes’ does not do particularly well on the first requirement. Gettier isolated a large class of cases where it goes wrong. But it does very well on the second, as it analyses knowledge in terms of a short list of simple and significant features. I claim that all its rivals don’t do considerably better on the first, and arguably do much worse on the second. (There are considerations pulling either way here, as I note in section seven, but it is prima facie plausible that it does very well on the second, which is all that we consider for now.) That the JTB theory is the best trade-off is still a live possibility, even considering Gettier cases.\nThis little argument will be perfectly useless this theory of meaning (owing in all its essential features to Lewis) is roughly right. There are several reasons for believing it. First, it can account for the possibility of mistaken intuitions, while still denying the possibility that intuitions about meaning can be systematically and radically mistaken. This alone is a nice consequence, and not one which is shared by every theory of meaning on the market. Secondly, as was shown in sections one and two, it seems to make the right kinds of predictions about when meaning will diverge from intuitions about meaning.\nThirdly, it can account for the fact that some, but not all, disagreements about the acceptability of assertions are disputes about matters of fact, not matters of meaning. This example is from Cummins: “If a child, asked to use ‘fair’ in a sentence, says,”It isn’t fair for girls to get as much as boys,\" we should suspect the child’s politics, not his language\" (1998, 120). This seems right; but if the child had said “It is fair that dreams are purple,” we would suspect his language. Perhaps by ‘fair’ he means ‘nonsensical’ or something similar. A theory of meaning needs to account for this divergence, and for the fact that it is a vague matter when we say the problem is with the child’s language, and when with his politics. In short, saying which disputes are disputes about facts (or values or whatever), and which about meanings, is a compulsory question for a theory of meaning.\nThe balance theory of meaning I am promoting can do this, as the following demonstration shows. This theory of meaning is determinedly individualistic. Every person has an idiolect determined by her dispositions to apply terms; a shared language is a collection of closely-enough overlapping idiolects. So the child’s idiolect might differ from ours, especially if he uses ‘fair’ to mean ‘nonsensical.’ But if the idiolect differs in just how a few sentences are used, it is likely that the meaning postulate which does best at capturing his dispositions to use according to our two criteria, is the same as the meaning postulate which does best at capturing our dispositions to use. The reason is that highly natural properties are pretty thin on the ground; one’s dispositions to use a term have to change quite a lot before they get into the orbit of a distinct natural property. So despite the fact that I allow for nothing more than overlapping idiolects, in practice the overlap is much closer to being exact than on most ‘overlapping idiolect’ theories.\nWith this, I can now distinguish which disputes are disputes about facts, and which are disputes about meaning. Given that there is a dispute, the parties must have different dispositions to use some important term. In some disputes, the same meaning postulate does best on balance at capturing the dispositions of each party. I say that here the parties mean the same thing by their words, and the dispute is a dispute about facts. In others, the difference will be so great that different meaning postulates do best at capturing the dispositions of the competing parties. In these cases, I say the dispute is a dispute about meaning.\nNow, I can explain the intuition that the JTB theorist means something different to the rest of us by ‘knows. That is, I can explain this intuition away. It seems a fair assumption that the reasonably natural properties will be evenly distributed throughout the space of possible linguistic dispositions. If this is right, then any change of usage beyond a certain magnitude will, on my theory, count as a change of meaning. And it is plausible to suppose the change I am urging to our usage, affirming rather than denying sentences like, “Smith knows Jones owns a Ford” is beyond that certain magnitude. But the assumption of even distribution of the reasonably natural properties is false. That, I claim, is what the failure of the ’analysis of knowledge’ merry-go-round to stop shows us. There are just no reasonably natural properties in the neighbourhood of our disposition to use ‘knows.’ If this is right, then even some quite significant changes to usage will not be changes in meaning, because they will not change which is the closest reasonably natural property to our usage pattern. The assumption that the reasonably natural properties are reasonably evenly distributed is plausible, but false. Hence the hunch that I am trying to change the meaning of ‘knows’ is plausible, but false.\nThe hypothesis that when we alter intuitions because of a theory we always change meanings, on the other hand, is not even plausible. When the ancients said “Whales are fish,” or “The sun is not a star,” they simply said false sentences. That is, they said that whales are fish, and believed that the sun is not a star. This seems platitudinous, but the ‘use-change implies meaning-change’ hypothesis would deny it.\nIt has sometimes been suggested to me that conceptual intuitions should be given greater privilege than other intuitions; that I am wrong to generalise from the massive fallibility of logical, ethical or semantic intuitions to the massive fallibility of conceptual intuitions. Since I am on much firmer ground when talking about these non-conceptual cases, if such an attack were justified it would severely weaken my argument. Given what has been said so far we should be able to see what is wrong with this suggestion. Consider a group of people who systematically assent to “If A then B implies if B then A.” On this view these people are expressing a mistaken logical intuition, but a correct conceptual intuition. So their concept of ‘implication’ doesn’t pick out implication, or at the very least doesn’t pick out our concept of ‘implication.’ Now if we are in that group, this summary becomes incoherent, so this position immediately implies that we can’t be mistaken about our logical intuitions. Further, we are no longer able to say that when these people say “If A then B implies if B then A,” they are saying something false, because given the reference of ‘implies’ in their idiolect, this sentence expresses a true proposition. This is odd, but odder is to come. Assuming again we are in this group, it turns out to be vitally important in debates concerning philosophical logic to decide whether we are engaging in logical analysis or conceptual analysis. It might turn out a correct piece of conceptual analysis of ‘implication’ picks out a different relation to the correct implication relation we derive from purely logical considerations. If logical intuitions are less reliable than conceptual intuitions, as proposed, and assent to sentences like “If A then B implies if B then A” reveals simultaneously a logical and a conceptual intuition, this untenable conclusion seems forced. I conclude that conceptual intuitions are continuous with other intuitions, and should be treated in a similar way.\nKeeping Conceptual Analysis\nThe following would be a bad way to respond to the worry that the JTB theory amounts to a change in the meaning of the word ‘knows.’ For the worry to have any bite, facts about the meaning of ‘knows’ will have to be explicable in terms of facts about the use of ’knows. But facts about use can only tell us about the beliefs of this community about knowledge, not what knowledge really is. Since different communities adopt different standards for knowledge, we should only trust ours over theirs if (a) we have special evidence that our is correct or (b) we are so xenophobic that we trust ours simply because it is ours. “Many of us care very much whether or cognitive processes lead to beliefs that are true, or give us power over nature, or lead to happiness. But only those with a deep and free-floating conservatism in matters epistemic will care whether their cognitive processes are sanctioned by the evaluative standards that happen to be woven into our language” (Stich 1988, 109). “The intuitions and tacit knowledge of the man or woman in the street are quite irrelevant. The theory seeks to say what knowledge really is, not what folk epistemology takes it to be” (Stich 1992, 252)9. Facts about use can only give us the latter, so they are not what are relevant to my inquiry.\nStich takes this to be a general reason for abandoning conceptual analysis. Now while I think, and have argued above, that conceptual analysis need not slavishly follow intuition, I do not think that we should abandon it altogether. Stich’s worry seems to be conceptual analysis can only tell us about our words, not about our world. But is this kind of worry coherent? Can we say what will be found when we get to this real knowledge about the world? Will we be saying, “This belief of Smith’s shouldn’t be called knowledge, but really it is?” We need to attend to facts about the meaning of ‘knows’ in order to define the target of our search. If not, we have no way to avoid incoherencies like this one.\nTo put the same point another way, when someone claims to find this deep truth about knowledge, why should anyone else care? She will say, “Smith really knows that Jones owns a Ford, but I don’t mean what everyone else means by ‘knows.’” Why is this any more interesting than saying, “Smith really is a grapefruit, but I don’t mean what everyone else means by ‘grapefruit’?” If she doesn’t use words in the way that we do, we can ignore what she says about our common word usage. Or at least we can ignore it until she (or one of her colleagues) provides us with a translation manual. But to produce a translation manual, or to use words the way we do, she needs to attend to facts about our meanings. Again, incoherence threatens if she doesn’t attend to these facts but claims nevertheless to be participating in a debate with us. These points are all to be found in Chapter 2 of Jackson (1998).\nAn underlying assumption of the first reply is that there is a hard division between facts about meaning and facts about the world at large; that a principle like: No ‘is’ from a ‘means’ holds. This principle is, however, mistaken. All instances of the following argument pattern, where t ranges over tokenings of referring terms, are valid.\nP1:\nt refers unequivocally to \\({\\alpha}\\).\n\nP2:\nt refers unequivocally to \\({\\beta}\\).\n\nC:\n\\({\\alpha}\\) = \\({\\beta}\\)\n\nFor example, from the premise that ‘POTUS’ refers unequivocally to the President of the United States, and the premise that ‘POTUS’ refers unequivocally to Bush, we can validly infer that Bush is President of the United States. Since P1 and P2 are facts about meaning, and C is a fact about the world, any principle like No ‘is’ from a ‘means’ must be mistaken. So this worry about how much we can learn from conceptual analysis, from considerations of meaning, is mistaken.\nI call this inference pattern the R-inference. That the R-inference is valid doesn’t just show Stich’s critique rests on the false assumption No ‘is’ from a ‘means’. It can be used to provide a direct response to his critique. The problem is meant to be that conceptual analysis, the method of counterexamples, can at best provide us with claims like: ‘knows’ refers to the relation justifiably truly believes. We want to know facts about knowledge, not about the term ‘knows,’ so the conceptual analyst seems to have been looking in the wrong place. But it is a platitude that ‘knows’ refers to the relation knows. I call such platitudes, that ‘t’ refers to t, instances of the R-schema10. We can use the R-schema together with the R-inference to get the kind of conclusion our opponents are looking for.\nP1:\n‘Knowledge’ refers unequivocally to the relation justifiably truly believes.\n\nP2:\n‘Knowledge’ refers unequivocally to the relation knows.\n\nC:\nThe relation knows is the relation justifiably truly believes.\n\nMore colloquially, the conclusion says that knowledge is justified true belief. Everyone agrees (I take it) that conceptual analysis could, in principle, give us knowledge of facts of the form of P1. So the opponents of conceptual analysis must either deny P2, or deny that C follows from P1 and P2. In other words, for any such argument they must deny that the R-schema is true, or that the R-inference is valid. I hope the reader will agree that neither option looks promising.\nAgainst the Psychologists\nSomeone excessively impressed by various results in the psychological study of concepts may make the following objection to the theory of meaning here proffered. “Why think that we should prefer short lists of necessary and sufficient conditions? This seems like another one of those cases where philosophers take their aesthetic preferences to be truth-indicative, much like the ‘taste for desert landscapes’ argument. Besides, haven’t psychologists like Eleanor Rosch shown that our concepts don’t have simple necessary and sufficient conditions? If that’s right, your argument falls down in several different places.”\nStrictly speaking, my preference is not just for short lists of necessary and sufficient conditions. But it is, for reasons set out more fully in the next section, for short theories that fit the meaning of some term into a network of other properties. And my argument would fall down if there was no reason to prefer such short theories. And, of course, short lists of necessary and sufficient conditions are paradigmatically short theories. One reason I prefer the JTB analysis to its modern rivals is its brevity. Some of the reasons for preferring short lists are brought out by considering the objections to this approach developed by psychologists. I’ll just focus on one of the experiments performed by Rosch and Mervis, the points I make can be generalised.\nRosch and Mervis (1975) claim that “subjects rate superordinate semantic categories as having few, if any, attributes common to all members.” (p. 20) (A superordinate semantic category is one, like ‘fruit,’ which has other categories, like ‘apple,’ ‘pear’ and ‘banana,’ as sub-categories.) Here’s the experiment they ran to show this. For each of six superordinate categories (‘furniture,’ ‘fruit,’ ‘weapon,’ ‘vegetable,’ ‘vehicle’ and ‘clothing’) they selected twenty category members. So for ‘fruit’ the members ranged from ‘orange’ and ‘apple’ to ‘tomato’ and ‘olive.’ They then asked a range of subjects to list the attributes they associated with some of these 120 category members. Each subject was presented with six members, one from each category, and for each member had a minute and a half to write down its salient attributes.\n\n[F]ew attributes were given that were true of all twenty members of the category – for four of the categories there was only one such item; for two of the categories, none. Furthermore, the single attribute that did apply to all members, in three cases was true of many items besides those within that superordinate (for example, “you eat it” for fruit). Rosch and Mervis (1975)\n\nThey go on to conclude that the superordinate is not defined by necessary and sufficient conditions, but by a ‘family resemblance’ between members. This particular experiment was taken to confirm that the number of attributes a member has with other members of the category is correlated with a previously defined measure of prototypicality.11 They claim that the intuition, commonly held amongst philosophers, that there must be some attribute in common to all the members, is explicable by the fact that the highly prototypical members of the category all do share quite a few attributes in common, ranging from 3 attributes in common to the highly prototypical vegetables, to 36 for the highly prototypical vehicles.\nOne occasionally hears people deride the assumption that there are necessary and sufficient conditions for the application of a term, as if this was the most preposterous piece of philosophy possible. Really, this assumption is no more than the assumption that dictionaries can be written, and without any reason to think otherwise, seems perfectly harmless. Perhaps, though, the Rosch and Mervis experiments provide a reason to think otherwise, a reason for thinking that the conditions of applicability for terms like ‘fruit,’ ‘weapon,’ and perhaps ‘knowledge’ are Wittgensteinian family resemblance conditions, rather than short lists of necessary and sufficient conditions, the kinds of conditions that fill traditional dictionaries.\nWhen we look closely, we see that the experiments do not show this at all. One could try and knock any such argument away by claiming the proposal is incoherent. The psychologists claim that there are no necessary and sufficient conditions for being a weapon, but something is a weapon iff it bears a suitable resemblance to paradigmatic weapons. In one sense, bearing a suitable resemblance to a paradigmatic weapon is a condition, so it looks like we just have a very short list of necessary and sufficient conditions, a list of length one. (Jackson 1998, 61) makes a similar point in response to Stich’s invocation of Rosch’s experiments. This feels like it’s cheating, so I’ll move onto other objections. I’ll explain below just why it feels like cheating.\nPhilosophers aren’t particularly interested in terms like ‘weapon,’ so these experiments only have philosophical interest if the results can be shown to generalise to terms philosophers care about. In other words, if can be shown that terms like ‘property,’ ‘justice,’ ‘cause’ and particularly ‘knows’ are cluster concepts, or family resemblance terms. But there is a good reason to think this is false. As William Ramsey (1998) notes, if F refers to a cluster concept, then for any proposed list of necessary and sufficient properties for F-hood, it should be easy to find an individual which is an F but which lacks some of these properties. To generate such an example, just find an individual which lacks one of the proposed properties, but which has several other properties from the cluster. It should be harder to find an individual which has the properties without being an F. If the proposed analysis is even close to being right, then having these conditions will entail having enough of the cluster of properties that are constitutive of F-hood to be an F. Note, for example, that all of the counterexamples Wittgenstein (1953) lists to purported analyses of ‘game’ are cases where something is, intuitively, a game but which does not satisfy the analysis. If game is really a cluster concept, this is how things should be. But it is not how things are with knowledge; virtually all counterexamples, from Gettier on, are cases which are intuitively not cases of knowledge, but which satisfy the proposed analysis. This is good evidence that even if some terms in English refer to cluster concepts, ‘knows’ is not one of them.\nSecondly, Rosch and Mervis’s conclusions about the nature of the superordinate categories makes some rather mundane facts quite inexplicable. In this experiment the subjects weren’t told which category each member was in, but for other categories they were. Imagine, as seems plausible, one of the subjects objected to putting the member in that category. Many people, even undergraduates, don’t regard olives and tomatoes as fruit. (“Fruit on pasta? How absurd!”) When the student asks why is this thing called a fruit, other speakers can provide a response. It is not a brute fact of language that tomatoes are fruit. It is not just by magic that we happened to come to a shared meaning for fruit that includes tomatoes, and that if faced with a new kind of object, we would generally agree about whether it is a fruit. It is because we know how to answer such questions. This answer to the Why is it called ‘fruit?’ question had better be a sufficient condition for fruitness. If not, the subject is entitled to ask why having that property makes it a fruit. And unless there are very many possible distinct answers to this question, which seems very improbable, there will be a short list of necessary and sufficient conditions for being a fruit. But for this example, at least, ‘fruit’ was relatively arbitrary, so there will be a short list of necessary and sufficient conditions for being an F, for pretty much any F.\nThirdly, returning to ‘fruit,’ we can see that Rosch and Mervis’s experiments could not possibly show that many superordinate predicates in English are cluster concepts. For they would, if successful, show that ‘fruit’ is a cluster concept, and it quite plainly is not. So by modus tollens, there is something wrong with their methodology. Some of the other categories they investigate, particularly ‘weapon’ and ‘furniture’ might be relatively cluster-ish, in a sense to be explained soon, but not ‘fruit.’ As the OED says, a fruit is “the edible product of a tree, shrub or other plant, consisting of the seed and its envelope.” If nothing like this is right, then we couldn’t explain to the sceptical why we call tomatoes, olives and so on fruit.\nSo the conclusion that philosophically significant terms are likely to be cluster concepts is mistaken. To close, I note one way the cluster concept view could at least be coherent. Many predicates do have necessary and sufficient conditions for their applicability, just as traditional conceptual analysis assumed. In other words, they have analyses. However, any analysis must be in words, and sometimes the words needed will refer to quite recherche properties. The properties in the analysans may, that is, be significantly less natural than the analysandum.\nIn some contexts, we only consider properties that are above a certain level of naturalness. If I claim two things say my carpet and the Battle of Agincourt, have nothing in common, I will not feel threatened by an objector who points out that they share some gruesome, gerrymandered property, like being elements of {my carpet, the Battle of Agincourt}. Say that the best analysis of F-hood requires us to use predicates denoting properties which are below the contextually defined border between the ‘natural enough’ and ‘too gruesome to use.’ Then there will be a sense in which there is no analysis of F into necessary and sufficient conditions; just the sense in which my carpet and the Battle of Avignon have nothing in common. Jackson’s argument feels like a cheat because he just shows that there will be necessary and sufficient conditions for any concept provided we are allowed to use gruesome properties, but he makes it sound like this proviso is unnecessary. If Rosch and Mervis’s experiments show anything at all, it is that this is true of some common terms in some everyday-ish contexts. In particular, if we restrict our attention to the predicates that might occur to us within ninety seconds (which plausibly correlates well with some level of naturalness), very few terms have analyses. Thus far, Rosch and Mervis are correct. They go wrong by projecting truths of a particular context to all contexts.\nIn defence of analysis\nIn the previous section I argued that various empirical arguments gave us no reason to doubt that ‘knows’ will have a short analysis. In this section we look at various philosophical arguments to this conclusion. One might easily imagine the following objection to what has been claimed so far. At best, the above reasoning shows that if ‘knows’ has a short analysis, then the JTB analysis is correct, notwithstanding the intuitions provoked by Gettier cases. But there is little reason to think English terms have analyses, as evidenced by the failure of philosophers to analyse even one interesting term, and particular reasons to think that ‘knows’ does not have an analysis. These reasons are set out by (Williamson 2000 Ch. 3), who argues, by appeal to intuitions about a particular kind of case, that there can be no analysis of ‘knows’ into independent clauses, one of which describes an internal state of the agent and the other of which describes an external state of the agent. This does not necessarily refute the JTB analysis, since the concepts of justification and belief in use may be neither internal nor external in Williamson’s sense. And if we are going to revise intuitions about the Gettier cases, we may wish to revise intuitions about Williamson’s cases as well, though here it is probably safest to not do this, because it is unclear just what philosophical benefit is derived from this revision. In response to these arguments I will make two moves: one defensive and one offensive. The defensive move is to distinguish the assumptions made here about the structure of the meaning of ‘knows,’ and show how these assumptions do not have some of the dreadful consequences suggested by various authors. The offensive move, with which we begin, is to point out the rather unattractive consequences of not making these assumptions about the structure of the meaning of ‘knows.’\nIn terms of the concept of naturalness used above, the relation denoted by ‘knows’ might fall into one of three broad camps:\nIt might be rather unnatural;\nIt might be fairly natural in virtue of its relation to other, more natural, properties; or\nIt might be a primitive natural property, one that does not derive its naturalness from anything else.\nMy preferred position is (b). I think that the word ‘knows,’ like every other denoting term in English, denotes something fairly natural. And I don’t think there are any primitively natural properties or relations in the vicinity of the denotation of this word, so it must derive its naturalness from its relation to other properties or relations. If this is so, we can recover some of the structure of its meaning by elucidating those relationships. If it is correct, that is exactly what I think the JTB theory does. This is not to say that justification, truth or belief are themselves primitively natural properties, but rather that we can make some progress towards recovering the source of the naturalness of knowledge via its decomposition into justification, truth and belief. But before investigating the costs of (b), let us look at the costs of (a) and (c).\nI think we can dispense with (c) rather quickly. It would be surprising, to say the least, if knowledge was a primitive relation. That X knows that p can hardly be one of the foundational facts that make up the universe. If X knows that p, this fact obtains in virtue of the obtaining of other facts. We may not be able to tell exactly what these facts are in general, but we have fairly strong opinions about whether they obtain or not in a particular case. This is why we are prepared to say whether or not a character knows something in a story, perhaps a philosophical story, without being told exactly that. We see the facts in virtue of which the character does, or does not, know this. This does not conclusively show that knowledge is not a primitively natural property. Electrical charge presumably is a primitively natural property, yet sometimes we can figure out the charge of an object by the behaviour of other objects. For example, if we know it is repulsed by several different negatively charged things, it is probably negatively charged. But in these cases it is clear our inference is from some facts to other facts that are inductively implied, not to facts that are constituted by the facts we know. (Only a rather unreformed positivist would say that charge is constituted by repulsive behaviour.) And it does not at all feel that in philosophical examples we are inductively (or abductively) inferring whether the character knows that p.\nThe more interesting question is whether (a) might be correct. This is, perhaps surprisingly, consistent with the theory of meaning advanced above. I held, following Lewis, that the meaning of a denoting term is the most natural object, property or relation that satisfies most of our usage dispositions. It is possible that the winner of this contest will itself be quite unnatural. This is what happens all the time with vague terms, and indeed it is what causes, or perhaps constitutes, their vagueness. None of the properties (or relations) that we may pick out by ‘blue’ is much more natural than several other properties (or relations) that would do roughly as well at capturing our usage dispositions, were they the denotation of ‘blue.’12 And indeed none of these properties (or relations) are particularly natural; they are all rather arbitrary divisions of the spectrum. The situation is possibly worse when we consider what Theodore Sider (2001) calls maximal properties. A property F is maximal iff things that massively overlap an F are not themselves an F. So being a coin is maximal, since large parts of a coin, or large parts of a coin fused with some nearby atoms outside the coin, are not themselves coins. Sider adopts the following useful notation: something is an F* iff it is suitable to be an F in every respect save that it may massively overlap an F. So a coin* is a piece of metal (or suitable substance) that is (roughly) coin-shaped and is (more or less) the deliberate outcome of a process designed to produce legal tender. Assuming that any collection of atoms has a fusion, in the vicinity of any coin there will be literally trillions of coin*s. At most one of these will be a coin, since coins do not, in general, overlap. That is, the property being a coin must pick out exactly one of these coin*s. Since the selection will be ultimately arbitrary, this property is not very natural. There are just no natural properties in the area, so the denotation of ‘coin’ is just not natural.\nThese kind of considerations show that option (a) is a live possibility. But they do not show that it actually obtains. And there are several contrasts between ‘knows,’ on the one hand, and ‘blue’ and ‘coin’ on the other, which suggest that it does not obtain. First, we do not take our word ‘knows’ to be as indeterminate as ‘blue’ or ‘coin,’ despite the existence of some rather strong grounds for indeterminacy in it. Secondly, we take apparent disputes between different users of the word ‘knows’ to be genuine disputes, ones in which at most one side is correct, which we do not necessarily do with ‘blue’ and ‘coin.’ Finally, we are prepared to use the relation denoted by ‘knows’ in inductive arguments in ways that seem a little suspect with genuinely unnatural relations, as arguably evidenced by our attitudes towards ‘coin’ and ‘blue.’ Let’s look at these in more detail.\nIf we insisted that the meaning of ‘knows’ must validate all of our dispositions to use the term, we would find that the word has no meaning. If we just look at intuitions, we will find that our intuitions about ‘knows’ are inconsistent with some simple known facts. (Beliefs, being regimented by reflection, might not be inconsistent, depending on how systematic the regimentation has been.) For example, the following all seem true to many people.\nKnowledge supervenes on evidence: if two people (not necessarily in the same possible world) have the same evidence, they know the same things.\nWe know many things about the external world.\nWe have the same evidence as some people who are the victims of massive deception, and who have few true beliefs about their external world.\nWhatever is known is true.\nThese are inconsistent, so they cannot all be true. We could take any three of these as an argument for the negation of the fourth, though probably the argument from (1) (2) and (3) to the negation of (4) is less persuasive than the other three such arguments. I don’t want to adjudicate here which such argument is sound. All I want to claim here is that there is a fact of the matter about which of these arguments is sound, and hence about which of these four claims is false. If two people are disagreeing about which of these is false, at most one of them is right, and the other is wrong. If ‘knows’ denoted a rather unnatural relation, there would be little reason to believe these things to be true. Perhaps by more carefully consulting intuitions we could determine that one of them is false by seeing that it had the weakest intuitive pull. If we couldn’t do this, it would follow that in general there was no fact of the matter about which is false, and if someone wanted to use ‘know’ in their idiolect so that one particular one of these is false, there would be no way we could argue that they were wrong. It is quite implausible that this is what should happen in such a situation. It is more plausible that the dispute should be decided by figuring out which group of three can be satisfied by a fairly natural relation. This, recall, is just how we resolve disputes in many other areas of philosophy, from logic to ethics. If there is no natural relation eligible to be the meaning of ‘knows,’ then probably this dispute has no resolution, just like the dispute about what ‘mass’ means in Newtonian mechanics.13\nThe above case generalises quite widely. If one speaker says that a Gettier case is a case of knowledge and another denies this (as Stich assures us actually happens if we cast our linguistic net wide enough) we normally assume that one of them is making a mistake. But if ‘knows’ denotes something quite unnatural, then probably each is saying something true in her own idiolect. Each party may make other mistaken claims, that for example what they say is also true in the language of all their compatriots, but in just making these claims about knowledge they would not be making a mistake. Perhaps there really is no fact of the matter here about who is right, but thinking so would be a major change to our common way of viewing matters, and hence would be a rather costly consequence of accepting option (a). Note here the contrast with ‘blue’ and ‘coin.’ If one person adopts an idiosyncratic usage of ‘blue’ and ‘coin,’ one on which there are determinate facts about matters where, we say, there are none, the most natural thing to say is that they are using the terms differently to us. If they insist that it is part of their intention in using the terms to speak the same way as their fellows we may (but only may) revise this judgement. But in general there is much more inclination to say that a dispute over whether, say, a patch is blue is merely verbal than to say this about a dispute over whether X knows that p.\nFinally, if knowledge was a completely unnatural relation, we would no more expect it to play a role in inductive or analogical arguments than does grue, but it seems it can play such a role. One might worry here that blueness also plays a role in inductive arguments, as in: The sky has been blue the last n days, so probably it will be blue tomorrow. If blueness is not natural, this might show that unnatural properties can play a role in inductive arguments. But what is really happening here is that there is, implicitly, an inductive argument based on a much narrow colour spectrum, and hence a much more natural property. To see this, note that we would be just as surprised tomorrow if the sky was navy blue, or perhaps of the dominant blue in Picasso’s blue period paintings, as if it were not blue at all.\nSo there are substantial costs to (a) and (c). Are there similar costs to (b)? If we take (b) to mean that there is a decomposition of the meaning of ‘knows’ into conditions, expressible in English, which we can tell a priori are individually necessary and jointly sufficient for knowledge, and such that it is also a priori that they represent natural properties, then (b) would be wildly implausible. To take just one part of this, Williamson (2000) notes it is clear that there are some languages in which such conditions cannot be expressed, so perhaps English is such a language too. And if this argument for ‘knows’ works it presumably works for other terms, like ‘pain,’ but it is hard to find such an a priori decomposition of ‘pain’ into more natural properties. Really, all (b) requires is that there be some connection, perhaps only discoverable a posteriori, perhaps not even humanly comprehensible, between knowledge and other more primitively natural properties. These properties need not be denoted by any terms of English, or any other known language.\nMost importantly, this connection need not be a decomposition. If knowledge is the most general factive mental state, as Williamson proposes, and being factive and being a mental state are natural properties, then condition (b) will be thereby satisfied. If knowledge is the norm of assertion, as Williamson also proposes, then that could do as the means by which knowledge is linked into the network of natural properties. This last assumes that being an assertion is a natural property, and more dangerously that norms as natural, but these are relatively plausible assumptions in general. In neither case do we have a factorisation, in any sense, of knowledge into constituent properties, but we do have, as (b) requires, a means by which knowledge is linked into the network of natural properties. It is quite plausible that for every term which, unlike ‘blue’ and ‘coin’ are not excessively vague and do not denote maximal properties, something like (b) is correct. Given the clarifications made here to (b), this is consistent with most positions normally taken to be anti-reductionist about those terms, or their denotata.\nNaturalness and the JTB theory\nI have argued here that the following argument against the JTB theory is unsound.\nP1.\nThe JTB theory says that Gettier cases are cases of knowledge.\n\nP2.\nIntuition says that Gettier cases are not cases of knowledge.\n\nP3.\nIntuition is trustworthy in these cases.\n\nC.\nThe JTB theory is false.\n\nThe objection has been that P3 is false in those cases where following intuition slavishly would mean concluding that some common term denoted a rather unnatural property while accepting deviations from intuition would allow us to hold that it denoted a rather natural property. Peter Klein (in conversation) has suggested that there is a more sophisticated argument against the JTB theory that we can draw out of the Gettier cases. Since this argument is a good illustration of the way counterexamples should be used in philosophy, I’ll close with it.\nKlein’s idea, in effect, is that we can use Gettier cases to argue that being a justified true belief is not a natural property, and hence that P3 is after all true. Remember that P3 only fails when following intuition too closely would lead too far away from naturalness. If being a justified true belief is not a natural property to start with, there is no great danger of this happening. What the Gettier cases show us, goes the argument, is that there are two ways to be a justified true belief. The first way is where the belief is justified in some sense because it is true. The second way is where it is quite coincidental that the belief is both justified and true. These two ways of being a justified true belief may be natural enough, but the property being a justified true belief is just the disjunction of these two not especially related properties.\nI think this is, at least, a prima facie compelling argument. There are, at least, three important points to note about it. First, this kind of reasoning does not obviously generalise. Few of the examples described in Shope (1983) could be used to show that some target theory in fact made knowledge into a disjunctive kind. The second point is that accepting this argument is perfectly consistent with accepting everything I said above against the (widespread) uncritical use of appeal to intuition. Indeed, if what I said above is broadly correct then this is just the kind of reasoning we should be attempting to use when looking at fascinating counterexamples. Thirdly, if the argument works it shows something much more interesting than just that the JTB theory is false. It shows that naturalness is not always transferred to a conjunctive property by its conjuncts.\nI assume here that being a justified belief and being a true belief are themselves natural properties, and being a justified true belief is the conjunction of these. The only point here that seems possibly contentious is that being a true belief is not natural. On some forms of minimalism about truth this may be false, but those forms seem quite implausibly strong. Remember that saying being a true belief is natural does not imply that has an analysis – truth might be a primitively natural component of this property. And remember also that naturalness is intensional rather than hyperintensional. If all true beliefs correspond with reality in a suitable way, and corresponding with reality in that way is a natural property, then so is being a true belief, even if truth of belief cannot be explained in terms of correspondence.\nThis is a surprising result, because the way naturalness was originally set up by Lewis suggested that it would be transferred to a conjunctive property by its conjuncts. Lewis gave three accounts of naturalness. The first is that properties are perfectly natural in virtue of being co-intensive with a genuine universal. The third is that properties are natural in virtue of the mutual resemblance of their members, where resemblance is taken to be a primitive. On either account, it seems that whenever being F is natural, and so is being G, then being F and G will be natural.14 The second account, if it can be called that, is that naturalness is just primitive. If the Gettier cases really do show that being a justified true belief is not natural, then they will have shown that we have to fall back on just this account of naturalness.\n\n\n\nArmstrong, D. M. 1978. Universals and Scientific Realism. Cambridge: Cambridge University Press.\n\n\nBealer, George. 1998. “Intuition and the Autonomy of Philosophy.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 201–40. Lanham: Rowman & Littlefield.\n\n\nCummins, Robert. 1998. “Reflection on Reflective Equilibrium.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 113–28. Lanham: Rowman & Littlefield.\n\n\nDeRose, Keith. 1996. “Knowledge, Assertion and Lotteries.” Australasian Journal of Philosophy 74 (4): 568–79. https://doi.org/10.1080/00048409612347531.\n\n\nField, Hartry. 1973. “Theory Change and the Indeterminacy of Reference.” Journal of Philosophy 70 (14): 462–81. https://doi.org/10.2307/2025110.\n\n\nGrice, H. Paul. 1989. Studies in the Way of Words. Cambridge, MA.: Harvard University Press.\n\n\nHorowitz, Tamara. 1998. “Philosophical Intuitions and Psychological Theory.” Ethics 108 (2): 367–85. https://doi.org/10.1086/233809.\n\n\nHorwich, Paul. 1999. Meaning. Oxford: Oxford University Press.\n\n\nJackson, Frank. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nLangton, Rae, and David Lewis. 1998. “Defining ‘Intrinsic’.” Philosophy and Phenomenological Research 58 (2): 333–45. https://doi.org/10.2307/2653512.\n\n\n———. 2001. “Marshall and Parsons on ‘Intrinsic’.” Philosophy and Phenomenological Research 63 (2): 353–55. https://doi.org/10.2307/3071068.\n\n\nLewis, David. 1983. “New Work for a Theory of Universals.” Australasian Journal of Philosophy 61 (4): 343–77. https://doi.org/10.1080/00048408312341131.\n\n\n———. 1984. “Putnam’s Paradox.” Australasian Journal of Philosophy 62 (3): 221–36. https://doi.org/10.1080/00048408412340013.\n\n\n———. 1992. “Meaning Without Use: Reply to Hawthorne.” Australasian Journal of Philosophy 70 (1): 106–10. https://doi.org/10.1080/00048408112340093.\n\n\n———. 2001. “Redefining ’Intrinsic’.” Philosophy and Phenomenological Research 63 (2): 381–98. https://doi.org/10.2307/3071071.\n\n\nMenzies, Peter. 1996. “Probabilistic Causation and the Pre-Emption Problem.” Mind 105 (417): 85–117. https://doi.org/10.1093/mind/105.417.85.\n\n\nNelkin, Dana. 2000. “The Lottery Paradox, Knowledge, and Rationality.” Philosophical Review 109 (3): 373–409. https://doi.org/10.2307/2693695.\n\n\nRamsey, William. 1998. “Prototypes and Conceptual Analysis.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 161–77. Lanham: Rowman & Littlefield.\n\n\nRosch, Eleanor, and Carolyn Mervis. 1975. “Family Resemblances: Studies in the Internal Structure of Categories.” Cognitive Science 7 (4): 573–605. https://doi.org/10.1016/0010-0285(75)90024-9.\n\n\nRyle, Gilbert. 1949. The Concept of Mind. New York: Barnes; Noble.\n\n\nShope, Robert. 1983. The Analysis of Knowledge. Princeton: Princeton University Press.\n\n\nSider, Theodore. 2001. “Maximality and Intrinsic Properties.” Philosophy and Phenomenological Research 63 (2): 357–64. https://doi.org/10.1111/j.1933-1592.2001.tb00109.x.\n\n\nSosa, Ernest. 1998. “Minimal Intuition.” In Rethinking Intuition, edited by Michael DePaul and William Ramsey, 257–69. Lanham: Rowman & Littlefield.\n\n\nStich, Stephen. 1988. “Reflective Equilibrium, Analytic Epistemology and the Problem of Cognitive Diversity.” Synthese 74 (3): 391–413. https://doi.org/10.1007/bf00869637.\n\n\n———. 1992. “What Is a Theory of Mental Representation?” Mind 101 (402): 243–63. https://doi.org/10.1093/mind/101.402.243.\n\n\nTennant, Neil. 1992. Autologic. Edinburgh: Edinburgh University Press.\n\n\nUnger, Peter. 1996. Living High and Letting Die. Oxford: Oxford University Press.\n\n\nWeinberg, Jonathan, Stephen Stich, and Shaun Nichols. 2001. “Normativity and Epistemic Intuitions.” Philosophical Topics 29 (1): 429–60. https://doi.org/10.5840/philtopics2001291/217.\n\n\nWilliamson, Timothy. 2000. Knowledge and its Limits. Oxford University Press.\n\n\nWittgenstein, Ludwig. 1953. Philosophical Investigations. London: Macmillan.\n\n\nSee, for example, DeRose (1996) and Nelkin (2000)↩︎\nSee, for example, Menzies (1996), or any of the papers in the special Journal of Philosophy issue on causation, April 2000.↩︎\nThe myriad examples in Unger (1996) are rather useful for reminding us just how unreliable our moral intuitions are, and how necessary it is to employ reflection and considered judgement in regimenting such intuitions.↩︎\nA point very similar to this is made in Horowitz (1998).↩︎\nThe theory of meaning outlined here is deeply indebted to Lewis (1983, 1984, 1992).↩︎\nThere are tricky questions concerning cointensional predicates, but these have fairly familiar solutions, which I accept. For ease of expression here I will ignore the distinction between properties and relations – presumably ‘knows’ denotes a relation, that is a set of ordered pairs.↩︎\n‘Measures’ may be inappropriate here. Plausibly a property is simple because it is natural.↩︎\nFor more recent applications of naturalness in Lewis’s work, see Langton and Lewis (1998, 2001)and Lewis (2001).↩︎\nThe paper from which this quote is drawn is about the content of mental states, so originally it had ‘mental representation’ for ‘knowledge’ and ‘psychology’ for ‘epistemology.’ But I take it that (a) this isn’t an unfair representation of Stich’s views and (b) even if it is, it is an admirably clear statement of the way many people feel about the use of intuitions about possible cases, and worth considering for that reason alone.↩︎\n(Horwich 1999, 115–30) discusses similar schema, noting that instances involving words in foreign languages, or indexical expressions, will not be platitudinous. He also notes a way to remove the presumption that there is such a thing as knowledge, by stating the schema as \\({\\forall}\\)x (‘knowledge’ refers to x iff knowledge = x). For ease of expression I will stick with the simpler formulation in the text.↩︎\nIn previous work they had done some nice experiments aimed at getting a grip on our intuition that apples are more prototypical exemplars of fruit than olives are.↩︎\nI include the parenthetical comments here so as not to prejudge the question of whether colours are properties or relations. It seems unlikely to me that colours are relations, either the viewers or environments, but it is not worth quibbling over this here.↩︎\nNote that in that dispute the rivals are quite natural properties, but seem to be matched in their naturalness. In the dispute envisaged here, the rivals are quite unnatural, but still seem to be matched. For more on ‘mass,’ see Field (1973).↩︎\nI follow Armstrong (1978) here in assuming that there are conjunctive universals.↩︎\n",
    "preview": "posts/2021-01-04-what-good-are-counterexamples/paradox_store.jpg",
    "last_modified": "2021-02-04T22:05:17-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-07-are-you-a-sim/",
    "title": "Are You a Sim?",
    "description": "Nick Bostrom argues that if we accept some plausible assumptions about how the future will unfold, we should believe we are probably not humans. The argument appeals crucially to an indifference principle whose content is unclear. I set out four possible interpretations of the principle, none of which can be used to support Bostrom's argument. On the first two interpretations the principle is false; on the third it does not entail the conclusion; and on the fourth it only entails the conclusion given an auxiliary hypothesis which we have no reason to believe.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-07-01",
    "categories": [
      "epistemology",
      "notes"
    ],
    "contents": "\n\nContents\nFirst Interpretation\nSecond Interpretation\nThird Interpretation\nFourth Interpretation\nConclusion\n\nIn Will Wright’s delightful game The Sims, the player controls a neighbourhood full of people, affectionately called sims. The game has no scoring system, or winning conditions. It just allows players to create, and to some extent participate in, an interesting mini-world. Right now the sims have fairly primitive psychologies, but we can imagine this will be improved as the game evolves. The game is very popular now, and it seems plausible that it, and the inevitable imitators, will become even more popular as its psychological engine becomes more realistic. Since each human player creates a neighbourhood with many, many sims in it, in time the number of sims in the world will vastly outstrip the number of humans.\nPublished in Philosophical Quarterly 53: 425-431.\nPicture by Elven*Nicky via Creative Commons\nLet’s assume that as the sims become more and more complex, they will eventually acquire conscious states much like yours or mine. I do not want to argue for or against this assumption, but it seems plausible enough for discussion purposes. I’ll reserve the term Sim, with a capital S, for a sim that is conscious. By similar reasoning to the above, it seems in time the number of Sims in the world will far outstrip the number of humans, unless humanity either (a) stops existing, or (b) runs into unexpected barriers to computing power or (c) loses interest in these kinds of simulators. I think none of these is likely, so I think that over time the ratio of Sims to humans will far exceed 1:1.\nNick Bostrom (2003) argues that given all that, we should believe that we are probably Sims. Roughly, the argument is that we know that most agents with conscious states somewhat like ours are Sims. And we don’t have any specific evidence that tells on whether we are a Sim or a human. So the credence we each assign to I’m a Sim should equal our best guess as to the percentage of human-like agents that are Sims, which is far above \\(\\frac{1}{2}\\). As Glenn Reynolds put it, “Is it live, or is it Memorex? Statistically, it’s probably Memorex. Er, and so are you, actually.”1 (Is it worrying that we used the assumption that we are human to generate this statistical argument? Not necessarily; if we are Sims then the Sims:humans ratio is probably even higher, so what we know is a lower bound on the proportion of human-like agents that are Sims.) Less roughly, the argument appeals crucially to the following principle:\n(#)\nCr(Sim  fSim = x) = x\n\nHere Cr is a rational credence function. I will adopt David Lewis’s theory of de se belief, and assume that the credence function is defined over properties, rather than propositions Lewis (1979). Whenever I use a term that normally stands for a proposition inside the scope of Cr, it stands for the property of being in a world where that proposition is true. So fSim = x stands for the property of being in a world where 100x% of the human-like agents are Sims.\nAs Bostrom notes, the main reason for believing (#) is that it is an instance of a plausible general principle, which I’ll call (##).\n(##)\n\\({\\forall}{\\Phi}\\): Cr(\\({\\Phi}\\)  f\\({\\Phi}\\) = x) = x\n\nBostrom does not formulate this more general principle, but it is clear that he intends something like it to be behind his argument, for many of the defences of (#) involve substituting some other property in place of Sim in statements like (#). So I will focus here on whether anything like (##) is plausibly true, and whether it supports (#). There are many ways we could interpret (##), depending on whether we take Cr to be a rational agent’s current credences, or in some sense the prior credences before they are affected by some particular evidence, and on whether we take the quantifier to be restricted or unrestricted. Five particular interpretations stand out as being worth considering. None of these, however, provides much reason to believe (#), at least on the reading Bostrom wants to give it. In that reading (#) the credence function represents the current credences of an agent much like you or me. If (#) isn’t interpreted that way, it can’t play the dialectical role Bostrom wants it to play. On two of the interpretations, (##) is false, on two others it may be true but clearly does not entail (#), and on the fifth it only entails (#) if we make an auxiliary assumption which is far from obviously true.\nFor ease of exposition, I will assume that Cr describes in some way the credences at some time of a particular rational human-like agent, Rat, who is much like you or me, except that she is perfectly rational.\nFirst Interpretation\nCr in (##) measures Rat’s current credences, and the quantifier in (##) is unrestricted. On this interpretation, (##) is clearly false, as Bostrom notes. Rat may well know that the proportion of human-like agents that are like spaghetti westerns is rather low, while rationally being quite confident that she likes spaghetti westerns. For any property \\({\\Phi}\\) where Rat has some particular information about whether he is one of the \\({\\Phi}\\)s or not, that information, and not general facts about the proportion of human-like agents that are \\({\\Phi}\\), can (indeed should) guide Rat’s credences. So those substitution instances of (##) are false.\nSecond Interpretation\nJust like the first interpretation, except that we restrict the quantifier range so that it only ranges over properties such that Rat does not know whether she possesses them. This interpretation seems to be hinted at by Bostrom when he says, “the bland indifference principle expressed by (#) prescribes indifference only between hypotheses about which observer you are, when you have no information about which of these observers you are.” Even given this restriction, (##) is still false, as the following example shows.\nAssume that Rat knows that fSim > 0.9, which Bostrom clearly takes to be consistent with rationality. And assume also that Rat, being a normal human-like agent, knows some fairly specific, and fairly distinctive facts about her conscious life. If Rat is anything like you or me, she will have experiences that he can be fairly sure are unique to her. Last night, for instance, while Rat was listening to Go-Betweens bootlegs, watching baseball, drinking beer, rocking in his rocking chair and thinking about Bostrom’s simulation argument, she stubbed her toe in a moderately, but not excessively, painful way. Few people will have done all these things at once, and none in quite that way. Let C be the property of ever having had an experience almost just like that. Rat knows he is a C. She is very confident, though not certain, that she is the only human-like C. Let a suman be the property of being C and human, or not-C and a Sim. For much of the paper we’re going to be concerned with the following two properties.\n\n\\(x\\) is a suman =df\\(x\\) is a human \\(C\\) or a Sim who is not a \\(C\\).\n\\(x\\) is a him =df\\(x\\) is a Sim \\(C\\) or a human who is not a \\(C\\).\n\nWe are following Bostrom in assuming that Rat does not know whether she is a Sim so she does not know whether she is a suman. But given that almost no one is C, it follows that fsuman \\({\\approx}\\) fSim. Hence fsuman > 0.85, for if it is less than fSim, it is not much less. But if Cr(a suman) > 0.85, and Cr(Sim) > 0.9, and Rat is coherent, it follows that Cr(C) < 0.25. But we assumed that Rat knew that she was a C, and however knowledge and credence are to be connected, it is inconceivable that one could know something while one’s credence in it is less than \\(\\frac{1}{4}\\). Hence it must be false that Cr(C) < \\(\\frac{1}{4}\\), but we inferred that from given facts about the story and (##), as interpreted here. Hence (##), as interpreted here, is false.\nThird Interpretation\nOne natural response ot the previous objection is that there shoul dbe some way of restricting (##) so that it does not apply to properties like being a suman. Intuitively, the response is that even though Rat doesn’t know whether she is a suman, she knows something that is relevant to whether she is a suman, namely that she is a \\(C\\). The problem with this response is that any formal restriction on (##) that implements this intuition ends up giving us a version so weak that it doesn’t entail (#).\nThe idea is that what went wrong in the previous case is that even though Rat does not know whether she is a suman, she knows something relevant to this. In particular, she knows that if she is a suman, she is one of the sumans that is human, rather than one of the ones that is a Sim. Our third interpretation avoids the difficulties this raises by restricting the quantifier in (##) even further. Say that a property \\({\\Phi}\\) is in the domain of the quantifier iff (a) Rat does not know whether she is \\({\\Phi}\\), and (b) there is no more specific property \\({\\Phi}\\)\\(^\\prime\\) such that Rat knows that if she is \\({\\Phi}\\), then she is \\({\\Phi}\\)\\(^\\prime\\).2 This will rule out the applicability of (##) to properties like a suman. Unfortunately, it will also rule out the applicability of (##) to properties like being a Sim. For Rat knows that if she is a Sim, then she is a Sim that is also a C. So now (##) doesn’t entail (#).\nThis kind of problem will arise for any attempt to put a purely formal restriction on (##). The problem is that, as Goodman noted in a quite different context (Goodman 1955), there is no formal distinction between the ‘normal’ properties, being a human and being a sim, and the ‘deviant’ properties, being a suman and being a him. The following four biconditionals are all conceptual truths, and hence must all receive credence 1.\nIf the obvious truth of (1a) implies that Rat cannot apply (##) to the property o being a suman once she knows that she is a \\(C\\), for (1a) makes that evidence look clrarly relevant to the issue of whether she is suman, then similar reasoning suggests that the obvious truth of (2a) implies that Rat cannot apply (##) to the properties of being a human once she knows that she is a \\(C\\), for (2a) makes that evidence look clearly relevant to the issue of whether she is human. The point is that a restriction on (##) that is to deliver (#) must fine some epistemologically salient distinction between the property of being human and the property of being suman if it is to rule out one application of (##) without ruling out the other, and if we only consider formal constraints, we won’t find such a restriction. Our final attempt to justify (#) from something like (##) attempts to avoid this problem by appealing directly to the nature of Rat’s evidence.\nFourth Interpretation\nThe problems with the three interpretations of (##) so far have been that they applied after Rat found out something distinctive about herself, that she was a C. Perhaps (##) is really a constraint on prior credence functions. A priori, Rat’s credences should be governed by an unrestricted version of (##). We then have the following argument for (#). (As noted above, (#) is a constraint on current credences, so it is not immediately entailed by a constraint on prior credences such as (##) under its current interpretation.)\nP1\nA priori, Rat’s conditional credence in her being a Sim given that fSim is x is x.\n\nP2\nAll of Rat’s evidence is probabilistically independent of the property of being a Sim.\n\nC\nRat’s current conditional credence in her being a Sim given that fSim is x is x.\n\nThis interpretation may be reasonably faithful to what Bostrom had in mind. The argument just sketched looks similar enough to what he hints at in the following quote: “More generally, if we knew that a fraction x of all observers with human-type experiences live in simulations, and we don’t have any information that indicate that our own particular experiences are any more or less likely than other human-type experiences to have been implemented in vivo rather than in machina, then our credence that we are in a simulation should equal x.” So it’s not unreasonable to conclude that he is committed to P2, and intends it to be used in the argument that you should give high credence to being a Sim.3 Further, this version of (##), where it is restricted to prior credences, does not look unreasonable. So if P2 is true, an argument for (#) might just succeed. So the issue now is just whether P2 is true.\nWhy might we reject P2? Any of the following three reasons might do. First, Rat’s evidence might be constituted by more than her conscious phenomenal states. This reply has an externalist and an internalist version. On the externalist version, Rat’s perceptual evidence is constituted in part by the objects she is perceiving. Just as seeing a dagger and hallucinating a dagger provide different evidence, so does seeing a dagger and sim-seeing a sim-dagger. For reasns Williamson notes, a Sim may not know that she has different evidence to someone seeing a dagger when she sim-sees a sim-dagger, but that does not imply that she does not have different evidence unless one also assumes, implausibly, that agents know exactly what their evidence is Williamson (2000). On the internalist version, our evidence is constituted by our sensory irritations, just as Quine said it is (Quine 1973). If Rat’s evidence includes the fact that her eyes are being irritated thus-and-so, his credence conditional on that that she is human should be 1, for if she were a Sim she could not have this evidence because she would not have eyes. She may, depending on the kind of Sim she is, have sim-eyes, but sim-eyes are not eyes. So Bostrom needs an argument that evidence supervenes on conscious experiences, and he doesn’t clearly have one. This is not to say that no such argument could exist. For example, Laurence BonJour provides some intriguing grounds for thinking that our fundamental evidence does consist in certain kinds of conscious states, namely occurrent beliefs (BonJour 1999), but we’re a long way from knowing that the supervenience claims holds. And if the supervenience claim does not hold, then even if Sims and humans have the same kind of experiences, they may not have the same kind of evidence. And if that is true, it is open to us to hold that Rat’s non-experiential evidence entails that she is not a Sim (as both Williamson and Quine suggest), so her evidence will not be independent of the question of whether she is a Sim.\nSecondly, even if every one of Rat’s experiences is probabilistically independent of the hypothesis that she is a Sim, that doesn’t give us a sufficient reason to believe that her total evidence is so independent. Just because e1 and e2 are both probabilistically independent of H, the conjunction e1 \\({\\wedge}\\) e2 might not be independent of H. So possibly our reasons for accepting P2 involve a tacit scope confusion.4\nFinally, we might wonder just why we’d even think that Rat’s evidence is probabilistically independent of the hypothesis that she is human. To be sure, her evidence does not entail that she is human. But that cannot be enough to show that it is probabilistically independent. For the evidence also does not entail that she is suman. And if P2 is true, then the evidence must have quite a bit of bearing on whether she is suman. For Rat’s prior credence in being suman is above 0.9 but apparently her posterior credence in it should be below 0.15. So the mere fact that the evidence does not entail that she is human cannot show that it is probabilistically independent of her being human, for the same reasoning would show it is probabilistically independent of his being suman.\nMore generally, we still need a distinction here between the property of being human and the property of being suman that shows why ordinary evidence should be independent of the first property but not the second. One might think the distinction can reside in the fact that being human is a natural property, while being suman is gruesome. The lesson of Goodman’s riddle of induction is that we have to give a privileged position in our epistemic framework to natural properties like being human, and this explains the distinction. This response gets the status of privileged and gruesome properties back-to-front. The real lesson of Goodman’s riddle is that credences in hypotheses involving natural properties should be distinctively sensitive to new evidence. Our evidence should make us quite confident that all emeralds are green, while giving us little reason to think that all emeralds are grue. What P2 says is that a rather natural hypothesis, that Rat is human, is insensitive to all the evidence Rat has, while a rather gruesome hypothesis, that Rat is suman, is sensitive to this evidence. The riddle of induction gives us no reason to believe that should happen.\nIt seems, though this is a little speculative, that the only reason for accepting P2 involves a simple fallacy. It is true that we have no reason to think that some evidence, say C, is more or less likely given that Rat is human rather than a Sim. But from this we should not conclude that we have a reason to think it is not more or less likely given that Rat is human rather than a Sim, which is what P2 requires. Indeed, drawing this kind of conclusion will quickly lead to a contradiction, for we can use the same ‘reasoning’ to conclude that we have a reason to think her evidence is not more or less likely given that Rat is a suman rather than a him.\nConclusion\nNothing I have said here implies that Rat should have a high credence in her being human. But it does make one argument that she should not have a high credence in this look rather tenuous. Further, it is quite plausible that if there is no good reason not to give high credence to a hypothesis, then it is rationally permissible to give it such a high credence. It may not be rationally mandatory to give it such a high credence, but it is permissible. If Rat is very confident that she is human, even while knowing that most human-like beings are Sims, she has not violated any norms of reasoning, and hence is not thereby irrational. In that respect she is a bit like you and me.\n\n\nBonJour, Laurence. 1999. “Foundationalism and the External World.” Philosophical Perspectives 13: 229–49. https://doi.org/10.1111/0029-4624.33.s13.11.\n\n\nBostrom, Nick. 2003. “Are You Living in a Computer Simulation?” The Philosophical Quarterly 53 (211): 243–55. https://doi.org/10.1111/1467-9213.00309.\n\n\nElga, Adam. 2000. “Self-Locating Belief and the Sleeping Beauty Problem.” Analysis 60 (2): 143–47. https://doi.org/10.1093/analys/60.2.143.\n\n\nGoodman, Nelson. 1955. Fact, Fiction and Forecast. Cambridge: Harvard University Press.\n\n\nLewis, David. 1979. “Attitudes de Dicto and de Se.” Philosophical Review 88 (4): 513–43. https://doi.org/10.2307/2184646.\n\n\nQuine, W. V. O. 1973. The Roots of Reference. La Salle: Open Court.\n\n\nWilliamson, Timothy. 2000. “Scepticism and Evidence.” Philosophy and Phenomenological Research 60 (3): 613–28. https://doi.org/10.2307/2653819.\n\n\nLink. Reynolds’s comment wasn’t directly about Bostrom, but it bore the ancestral of the relation refers to Bostrom’s paper.↩︎\nI think it is this interpretation of (##) that Adam Elga implicitly appeals to in his solution to the Sleeping Beauty problem Elga (2000).↩︎\nJamie Dreier pointed out to me that what Bostrom says here is slightly more complicated than what I, hopefully charitably, attribute to him. A literal reading of Bostrom’s passage suggests he intends the following principle.\n  \\({\\forall}\\)e: Cr(e  Human) - Cr(e  Sim) = Cr(e  Human) - Cr(e  Sim)      (B)\nThe quantifier here ranges over possible experiences e, e is the actual experience Rat has, and Cr is the credence function at the ‘time’ when Rat merely knows that he is human-like and fSim is greater than 0.9. I suggested a simpler assumption:\n  Cr(Human  e) = Cr(Sim  e)            (I)\nBostrom needs something a little stronger than (I) to get his desired conclusion, for he needs this to hold not just for Rat’s experience e, but for your experience and mine as well. But we will not press that point. Given that point, though, (I) is all he needs. And presumably the reason he adopts (B) is because it looks like it entails (I). And indeed it does entail (I) given some fairly innocuous background assumptions.↩︎\nThanks to Jamie Dreier for reminding me of this point.\n\n↩︎\n",
    "preview": "posts/2021-01-07-are-you-a-sim/sims.jpg",
    "last_modified": "2021-02-05T20:41:25-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-review-of-vagueness-and-contradiction/",
    "title": "Review of “Vagueness and Contradiction”",
    "description": "Review of Roy Sorensen, “Vagueness and Contradiction”. Cambridge: Cambridge University Press, 2000.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-04-01",
    "categories": [
      "book review",
      "on books",
      "vagueness",
      "logic"
    ],
    "contents": "\nLike all epistemicists, Roy Sorensen holds that vagueness poses no threat to classical logic, and that appearances to the contrary are the result of mistakenly assigning semantic force to certain barriers to inquiry. We may not be able to know whether 932 seconds after noon is still noonish, but there is a fact of the matter about whether it is. Hence the sentence 932 seconds after noon is noonish is either true or false, just as adherents of classical logic presuppose, and the threat from vagueness to classical logic dissolves.\n\nPublished in Australasian Journal of Philosophy 81: 290-292.\nBut Sorensen is not an orthodox epistemicist. He does not hold that these barriers to inquiry rise because of our limited powers of discrimination. That would imply that a more discerning observer, say God, could know where the boundary is. Sorensen holds that vagueness is an absolute barrier to inquiry. No one can know whether 932 seconds after noon is noonish, even God. This is because competently using the vague predicate ‘noonish’ requires believing a particular analytic falsehood involving it, and having this belief prevents knowing the truth about a borderline case. Much of this book is dedicated to defending these surprising claims. The first half of the book argues that this is the right thing to say about vague cases, and the second half provides more general arguments that we can and should believe analytic falsehoods.\nIt’s illuminating to compare Sorensen’s epistemicism with that of Timothy Williamson. A core feature of Williamson’s position is neatly summarised in this quote, which Sorensen cites: “for the epistemicist, definiteness is truth under all sharp interpretations of the language indiscriminable from the right one.” (“On the Structure of Higher Order Vagueness” Mind 108 (1999): 127‑43.)  Sorensen disagrees with the Williamson’s position in four ways. Two of these disagreements are immediate, and two are with deeper presuppositions of Williamson’s.\nFirst, Sorensen thinks that Williamson here ignores the need for ‘completeness’. Williamson holds that p is definite iff, roughly, it is true on all interpretations we cannot know to be incorrect. Call these the admissible interpretations. Sorensen claims that is not enough for p to be knowable, and hence definitely true. It must also be knowable that these are all the admissible interpretations.\nSecondly, indiscriminability is always indiscriminability by something, so on Williamson’s account definiteness is only defined relative to a discriminator. Sorensen wants there to be absolute borderline cases, and absolute indefiniteness, so he cannot rest with this definition. Sorensen thinks that unless we accept absolute borderline cases, we do not properly respect the sense in which vagueness is an absolute barrier to inquiry. The two other innovations in Sorensen’s theory guarantee that his theory has place for absolute indefiniteness.\nConsider a normal Sorites conditional, say If 932 seconds after noon is noonish, so is 933 seconds. Sorensen holds that being a competent user of ‘noonish’ requires that one believe every such conditional involving ‘noonish’. Someone who failed to believe it would not be competent in the language. Although Sorensen always puts this in terms of linguistic competence, he also says that one who didn’t believe this couldn’t have beliefs about the extension of our predicates. The most natural conclusion to draw is that from Sorensen’s perspective, one who doesn’t believe the Sorites conditional lacks the concept NOONISH. Sorensen talks about predicates rather than concepts, so he doesn’t put it quite this way, but it succinctly summarises the picture he sketches. Moreover, beliefs in such Sorites conditionals are a priori, despite the fact that one of them is analytically false. These are distinctive views, and they need good arguments.\nA bad argument would be, “It is always irrational to deny a Sorites conditional, so it is always rational to believe it.” This ignores the possibility that agnosticism about the conditional is always possible, and sometimes desirable. Sorensen does not endorse this argument, though he does note it shows that Sorites conditionals satisfy a ‘negative conception’ of the a priori: no empirical evidence can make us believe they are false.\nSorensen’s argument seems to be that believing every Sorites conditional gives us many true beliefs at the cost of only one false belief. He thinks that cost is worth the benefit. But this is at best a reason why we should believe Sorites conditionals, not why God should. And it doesn’t imply much about why God needs to believe this falsehood if He is to have the concept NOONISH. If we think subjectivism about language implies that God can’t know more about our language than we do, that may draw God into our dilemma. But it should seem very implausible, especially to an epistemicist, that God can’t know more about our language than we do. So even if it is good advice to believe every Sorites conditional, it does not follow that those who spurn this advice lack any concepts, or lack linguistic competence.\nIn any case, there are other costs to adopting Sorensen’s advice and believing a bunch of Sorites conditionals that we know includes a falsehood. For example, we can no longer safely believe the logical consequences of some things we believe. Sorensen happily accepts that consequence. He holds p and q can both be a priori even though their conjunction is not a priori. In chapter 6 Sorensen replies to several arguments against this, including a purported proof that a priority agglomerates across conjunction. The proof assumes that all logical truths are a priori. Sorensen says this is false because there are logical truths that are too complex for us to believe, a priori or otherwise. Since the only logical truth needed in the proof was p → (q → (p & q)), this is not obviously a sound response.\nSorensen has a more speculative reason for thinking there is absolute vagueness. (This is the final way in which Sorensen’s position differs from Williamson.) Consider a card that has The sentence on the other side of this card is false written on each side. If the sentences have truth values, then one is true and the other false. Whichever is true is a truth without a truthmaker, for any truthmaker would do just as well at making the other true. So Sorensen concludes that here we have a truth without a truthmaker, and the truthmaker principle is false. If there are some truths without truthmakers, there could be several. Sorensen holds that a is F is such a truth whenever a is an F which is a borderline F. Assume further that only truths with truthmakers are knowable, because knowability goes via knowing truthmakers, and we conclude that no one could know of a borderline F whether it is F. This is quite an interesting line of thought, and it deserves further attention. (Sorensen is quite upfront about how speculative it is.) Two immediate issues spring to mind. First, it is not clear how this is still a version of epistemicism, for vagueness is now at base a metaphysical phenomenon. There are epistemic consequences, but vagueness is constituted by the fact that there are truths without truthmakes, not by the unknowability of these. Secondly, and relatedly, it is no longer clear how higher order vagueness will be incorporated into the model. The most obvious thought is that there will be no truthmaker for the claim that some particular truth has a truthmaker. But whether some object is a truthmaker for some truth is not contingent, and it is notoriously difficult to apply truthmaker theory to necessary truths. Since every proposition entails any necessary truth, it is plausible that any object is a truthmaker for a necessary truth.\nSorensen argues that we should believe all Sorites conditionals, including ones that are analytically false. He notes this requires an argument that we can, and should, believe some analytic falsehoods. (He calls these impossibilities ‘contradictions’, a term some may think should be reserved for sentences of the form p & ¬p.) His argument that we can is fairly quick. Assume, for reductio, the philosophical thesis that we cannot believe analytic falsehoods. As a philosophical thesis, this is analytically true if true at all. But Sorensen believes its negation. So it is possible for someone to believe an analytic falsehood. The weakest premise here is that if we cannot believe analytic falsehoods, then it is analytic that we cannot. If it turns out that only creatures with a language of thought can believe analytic falsehoods, and it is a contingent feature of us that we lack a language of thought, Sorensen’s premise is false.\nThe argument that we should believe some analytic falsehoods uses a version of the preface paradox. If we can believe analytic falsehoods, we should take apparent occurrences of this (as when we make arithmetic errors) at face value. That is, reason demands we believe that we believe an analytic falsehood. But this implies it is provable that our beliefs are not collectively true. So if we follow the dictates of reason, it is provable we believe something provably false. This argument is obviously useful for removing a particular barrier to accepting Sorensen’s account of vagueness, that it seems absurd that reason could require we believe an analytic falsehood. But even if we reject Sorensen’s theory of vagueness, they are independently interesting contributions to the theory of belief.\nSorensen makes two distinctive contributions to the theory of vagueness here. First, he argues that linguistic competence demands that we believe Sorites conditionals. Secondly, he links the existence of vagueness to the failure of the truthmaker principle. As those familiar with Sorensen’s work will suspect, he makes these contributions in a lively and entertaining way. Anyone working on vagueness, and especially anyone interested in investigating the range of theories of vagueness that preserve classical logic, should pay it close attention.\n\n\n\n",
    "preview": "posts/2021-01-03-review-of-vagueness-and-contradiction/book_cover.jpg",
    "last_modified": "2021-02-04T15:30:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-07-epistemicism-parasites-and-vague-names/",
    "title": "Epistemicism, Parasites, and Vague Names",
    "description": "John Burgess has recently argued that Timothy Williamson's attempts to avoid the objection that his theory of vagueness is based on an untenable metaphysics of content are unsuccessful. Burgess's arguments are important, and largely correct, but there is a mistake in the discussion of one of the key examples. In this note I provide some alternative examples and use them to repair the mistaken section of the argument.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2003-04-01",
    "categories": [
      "logic",
      "language",
      "vagueness",
      "notes"
    ],
    "contents": "\nWhy is it so implausible that there is a sharp boundary between the rich and the non-rich? Perhaps we find it implausible merely because we (implicitly) believe that if there were such a boundary we would be able to discover where it is. If this is so we should revise our judgements. As Timothy Williamson (1994, 2000) has shown, if there were such a boundary we would not know where it is. Still, this is not the only reason for being sceptical about the existence of such a boundary. In “Vagueness, Epistemicism and Response-Dependence” John Burgess (2001) outlines an impressive objection to the existence of such boundaries, and in particular to epistemicist theories that posit their existence. Burgess’s objection is based not on principles about the epistemology of content, as the bad objection just stated is, but rather on principles about the metaphysics of content.\nPublished in Australasian Journal of Philosophy 81: 276-279.\nPicture by Pacific Klaus via Creative Commons.\nIf a word t has content c, this must be the case in virtue of some more primitive fact obtaining. Facts about content, such as this, are not among the fundamental constituents of reality. Roughly, facts about linguistic content must obtain in virtue of facts about use. But there are simply not enough facts about use to determine a precise meaning for paradigmatically vague terms like ‘rich.’ Any theory that holds that ‘rich’ does have a precise meaning must meet this objection. As Burgess argues, Williamson’s attempts to do this have not been entirely successful. Burgess argues, persuasively, that epistemicists owe us a theory of how terms like ‘rich’ get to have the precise meaning they apparently have given that the facts about use do not seem to generate a precise meaning. He also argues, less persuasively, that Williamson’s ‘parasitic’ strategy for meeting this obligation is unsuccessful. Indeed, the argument here rests at one point on a premiss that is clearly false. I will suggest a way to patch the argument and reinstate the objection to epistemicism.\nThe obligation to provide a theory that generates content in terms of use does not just fall on the epistemicists. We indeterminists about content must also discharge it. Assume that we have done so, and we have a theory of content that divides sentences into (at least) the true, the false and the indeterminate. (Williamson 1994, 207–8) argues that the only reason we believe that any sentences fall into this third category is that we are respecting a mythical symmetry between truth and falsity. We are falling into the trap of thinking that if a sentence is not somehow made false, it is not false. The true story is that if an assertoric sentence has content, and it is not made true, it isfalse. This provides the basis for Williamson’s ‘parasitic’ strategy: wait for the indeterminist to offer a theory of when sentences are true, accept that part of the indeterminist theory, and say all other sentences that express propositions are false. If the strategy works, then there is no way the indeterminist can meet the obligation to provide a theory of content without the epistemicist also being able to do so, so there is no argument for indeterminism here. (There are complications, to put it mildly, with this strategy when the indeterminist allows the border between the true and the indeterminate to be vague. Burgess lets these potential problems slide, and so shall I.)\nThe strategy rests on the purported asymmetry between truth and falsity. Burgess claims that positing such an asymmetry makes epistemicism inconsistent. Consider a colour patch that is around the border between red and orange. Burgess claims, correctly, that an indeterminist theory of content may say that (1) and (2) are indeterminate, and hence Williamson might be committed to the position that (1) and (2) are false, and hence so is (3).\nThat patch is red.\nThat patch is orange.\nThat patch is either red or orange.\nBut this is hopeless because “on the epistemicist view, there is a sharp boundary in the series between red and orange; every patch is either one or the other.” (Burgess 2001, 519) This last claim is false. According to epistemicism, there is a sharp boundary between red and not red, so the patch is either red or not red. But the epistemicist need not hold that if the patch is not red, then it is orange. It is consistent with epistemicism that there are colours strictly between red and orange, just as it is consistent with epistemicism that there are colours strictly between red and yellow, and just as it is consistent with epistemicism that there are colours strictly between red and blue. Hence it is possible that the colour of this patch is strictly between red and orange, and thus is neither red nor orange. So this line of reasoning does not work. Perhaps the argument can be easily fixed. According to the indeterminist, both (1) and (4) are indeterminate. Hence according to Williamson’s ‘parasitic’ theory of content, both (1) and (4) are false, so (5) is false.\nThat patch is red.\nThat patch is not red.\nThat patch is either red or not red.\nThis is more like a problem, because Williamson certainly is committed to the truth of (5). However, it is easy to see how Williamson should respond. The theory of content sketched above (or more precisely, the strategy for converting indeterminist theories to determinist ones) was only meant to apply to simple sentences. A simple sentence is true iff the indeterminist says it is true. The truth value of compound sentences, like (4) and (5), is given by a standard Davidsonian theory of truth. Hence (1) is false and (4) is true, as required.\nThe best way to resurrect Burgess’s argument is to shift our attention from vague predicates to vague names. Consider any mountain, say Kilimanjaro. It is vague just where the mountain starts, so it will be vague just which atoms constitute Kilimanjaro. Kilimanjaro is some fusion of atoms or other, but it is indeterminate just which one it is. Some of these fusions have different masses, and some have different shapes, so no sentence of the form of (6) or of (7) will be true according to the indeterminist.\nKilimanjaro has shape s.\nKilimanjaro has mass m.\nHence according to the Williamson’s asymmetric theory of truth, any sentence of either of these forms is false. Note that this holds even if we restrict the application of his theory to simple sentences. Now let K be a set of fusions of atoms {f1, f2, …, fn} such that it is determinate that Kilimanjaro is one of these fusions. (Because of higher-order vagueness it may be impossible to find such a set that does not contain any fusion that is determinately not Kilimanjaro. That will not matter; all that we require is that Kilimanjaro is one of these fusions.) Let si be the shape of fi and mi its mass. Then for all i, (6.i) and (7.i) are false, as we just argued.\nKilimanjaro has shape si.\n\nKilimanjaro has mass mi.\n\nHence both (8) and (9) are false.\nKilimanjaro has shape s1 or Kilimanjaro has shape s2 or … or Kilimanjaro has shape sn.\nKilimanjaro has mass m1 or Kilimanjaro has mass m2 or … or Kilimanjaro has mass mn.\nAnd the epistemicist is committed to (8) and (9) being true. We may not be able to discover which disjunct is true, but that is no reason to think that the disjunction is not true. Burgess’s argument was that if we adopt Williamson’s advice for constructing a theory of content, we will misclassify sentences that express penumbral connections. He was basically right, but we need to use a different example to prove it.\nI assumed above that Kilimanjaro is a fusion of atoms. Some may object to this on the grounds that Kilimanjaro has different temporal and modal properties to any fusion of atoms. I doubt such objections ultimately work, but for present purposes the important thing to note is that the argument can go through without such an assumption. Even if Kilimanjaro is not identical to any fusion in K, it is clear that Kilimanjaro (actually, now) exactly overlaps some member of K. And since Kilimanjaro has the same (actual, present) shape and mass as any fusion of atoms it exactly overlaps, it still follows that (8) and (9) are true.\nIf we do assume that Kilimanjaro is one of the fusions, then we can generate another case where Williamson’s theory generates false predictions. Since at most one of the fusions is a mountain, it follows that (10.i) is indeterminate for all i on an indeterminist theory of content, and hence false according to Williamson.\nfi is a mountain.\n\nHence his theory mistakenly predicts that (11) is false, when it is by hypothesis true.\nf1 is a mountain or f2 is a mountain or … or fn is a mountain.\nThis argument does rest on a contentious bit of metaphysics, but it still seems basically sound.\nI did not assume at any point that Kilimanjaro is a vague object. I did assume that ‘Kilimanjaro’ is a vague name, but it is consistent with the argument I have presented that there are no vague objects, and the vagueness in ‘Kilimanjaro’ consists in it being indeterminate which precise object it denotes.\nAs Burgess demonstrates, it is fair to require that the epistemicist provide a theory of how terms get the precise content they do. Williamson attempted to show he was in just as good a position to discharge this obligation as the indeterminist by providing an algorithm for converting any indeterminist theory of content into one acceptable to the epistemicist. Burgess argued that the algorithm produced unacceptable results when we applied it to vague sentences such as (1) and (2). This particular argument is no good; the algorithm does not seem to produce implausible results in that case. We can make this form of argument work, however, especially if we focus on vague names. Applying the algorithm to any plausible indeterminist theory produces the result that every disjunct in (8) and (9) are false, and hence that these disjunctions are false. Since the epistemicist is (correctly) committed to these sentences being true, Burgess was correct to conclude that “this particular attempt to implement the parasite strategy is doomed to failure.”\n\n\n\nBurgess, John. 2001. “Vagueness, Epistemicism and Response-Dependence.” Australasian Journal of Philosophy 79 (4): 507–24. https://doi.org/10.1080/713659306.\n\n\nWilliamson, Timothy. 1994. Vagueness. Routledge.\n\n\n———. 2000. Knowledge and its Limits. Oxford University Press.\n\n\n\n\n",
    "preview": "posts/2021-01-07-epistemicism-parasites-and-vague-names/parasites.jpg",
    "last_modified": "2021-02-05T15:29:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-misleading-indexicals/",
    "title": "Misleading Indexicals",
    "description": "I argue against well informed observer theories about the referent of indexicals.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2002-10-01",
    "categories": [
      "notes",
      "language"
    ],
    "contents": "\nIn ‘Now the French are invading England’ Komarine Romdenh-Romluc (2002) offers a new theory of the relationship between recorded indexicals and their content. Romdenh-Romluc’s proposes that Kaplan’s basic idea, that reference is determined by applying a rule to a context, is correct, but we have to be careful about what the context is, since it is not always the context of utterance. A few well known examples illustrate this. The ‘here’ and ‘now’ in ‘I am not here now’ on an answering machine do not refer to the time and place of the original utterance, but to the time the message is played back, and the place its attached telephone is located. Any occurrence of ‘today’ in a newspaper or magazine refers not to the day the story in which it appears was written, nor to the day the newspaper or magazine was printed, but to the cover date of that publication.\nPublished in Analysis, 62: 308-310.\nPicture by Bernard Spragg via Creative Common.\nStill, it is plausible that for each (token of an) indexical there is a salient context, and that ‘today’ refers to the day of its context, ‘here’ to the place of its context, and soon. Romdenh-Romluc takes this to be true, and then makes a proposal about what the salient context is. It is ‘the context that Ac would identify on the basis of cues that she would reasonably take U to be exploiting.’ (2002, 39) Ac is the relevant audience, ‘the individual who it is reasonable to take the speaker to be addressing,’ and who is assumed to be linguistically competent and attentive. (So Ac might not be the person U intends to address. This will not matter for what follows.) The proposal seems to suggest that it is impossible to trick a reasonably attentive hearer about what the referent of a particular indexical is. Since such trickery does seem possible, Romdenh-Romluc’s theory needs (at least) supplementation. Here are two examples of such tricks.\n\nThanks to Europa Malynicz, Adam Sennet and Ted Sider for helpful comments.\n\nExample One\nImagine that at my university, the email servers are down, so all communication from the office staff is by written notes left in our mailboxes. I notice that one of my colleagues, Bruce, has a rather full mailbox, and hence must not have been checking his messages for the last day or two. I also know that Bruce is a forgetful type, and if someone told him that he’d forgotten about a faculty meeting yesterday, he’d probably believe them. In fact he hasn’t forgotten; the meeting is for later today. So I decide to play a little trick on him. I write an official looking note saying ‘There is a faculty meeting today,’ leave it undated, and put it in Bruce’s mailbox underneath several other messages, so it looks like it has been there for a day or two. When Bruce sees it he is appropriately tricked, and for an instant panics about the meeting that he has missed.\n\nIt seems to me that what I wrote on the note was true. It was horribly misleading, to be sure, but still true. And as a few people have pointed out over the years, most prominently Bill Clinton I guess, it is possible to mislead people with the truth. But on Romdemh-Romluc’s proposal, what I said was false, since my audience (Bruce) reasonably took the context to be a day earlier in the week.\n\nExample Two\nThis example is closely based on a recent TV commercial. Jack leaves the following message on Jill’s answering machine late one Saturday night. ‘Hi Jill, it’s Jack. I’m at Rick’s. This place is wild. There’s lots of cute girls here, but I’m just thinking about you.’ In the background loud music is playing, as if Jack were at a nightclub, indeed as if Jack were at Rick’s, so Jill reasonably concludes that Jack was at Rick’s when he sent the message, and hence that ‘here’ refers to Rick’s. In fact Jack was home alone, but wanted to hide this fact, so he turned the stereo up to full volume while leaving the message. Despite the fact that a reasonable and attentive member of the target audience inferred on the basis of contextual clues left by Jack that the context was Rick’s, it was not. The context was Jack’s house, and ‘here’ in Jack’s message referred to his house. Jack’s trick may be less morally reprehensible than mine, but at least I managed to avoid lying, something Jack failed to do.\n\nIn Example One I said something true even though what the hearer took me to say was false. In Example Two Jack says something false, though what the hearer takes him to say may well be true, assuming that there are a lot of cute girls at Rick’s. Romdenh-Romluc’s theory predicts that neither of these things is possible, so it does not work as it stands. This, of course, is not to say that anyone else (myself included) has a better theory readily available, so it is unclear whether the right lesson to draw from these examples is that Romdenh-Romluc’s theory needs to have some epicycles added, or that we need to try a rather different approach. One simple epicycle makes the theory extensionally adequate, but philosophically uninteresting. Consider modifying the theory to require Ac to be not just reasonable and attentive, but informed of U’s circumstances. Then the context identified by Ac will be the salient context for determining the referent of U’s indexicals. But saying this is not to offer a theory of content for recorded indexicals, it is merely to say that ideally placed observers have access to all the relevant semantic facts. Even this might be wrong if epistemicism about vagueness is correct, but if that is true then Romdenh-Romluc’s theory is probably radically mistaken, for then there are facts about content that cannot be reasonably believed, even by an attentive and informed observer. We still seem to be a fair distance from having an acceptable theory.\n\n\n\nRomdenh-Romluc, Komarine. 2002. “Now the French Are Invading England.” Analysis 62 (1): 34–41. https://doi.org/10.1093/analys/62.1.34.\n\n\n\n\n",
    "preview": "posts/2021-01-04-misleading-indexicals/kabukicho.jpg",
    "last_modified": "2021-02-04T22:01:13-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-review-of-sameness-and-substance-renewed/",
    "title": "Review of “Sameness and Substance Renewed”",
    "description": "Review of David Wiggins, “Sameness and Substance Renewed”. Cambridge: Cambridge University Press, 2001.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2002-09-06",
    "categories": [
      "book review",
      "on books",
      "metaphysics"
    ],
    "contents": "\nSameness and Substance Renewed (hereafter, 2001) is, in effect, a second edition of Wiggins’s 1980 book Sameness and Substance (hereafter, 1980), which in turn expanded and corrected some ideas in his 1967 Identity and Spatio-Temporal Continuity (hereafter, 1967). All three books have similar aims. The first is to argue, primarily against Geach, that identity is absolute not relative. The second is to argue that, despite this, whenever an identity claim a = b is true, there is a sortal f such that a is the same f as b. The biggest difference between 1967 and the two later books is that the later books contain much more detail on what a sortal must be if this claim, called D, is to be both correct and philosophically interesting. The third aim is to apply the first two conclusions to the topic of personal identity.\n\nThis review first published in Notre Dame Philosophical Reviews.\nFor the bulk of 2001, most of the changes to 1980 are confined to footnotes, the bulk of these consisting of a citation of, and occasionally a brief comment upon, a post 1980 publication that bears on Wiggins’s approach to the topic. This changes in the last two chapters. The penultimate chapter is a mostly new discussion of the determinacy of identity. In the final chapter, on personal identity, Wiggins retracts many of the claims made in the matching chapter of 1980, and raises some interesting objections to Parfit’s account of personal identity. Apart from those, the major changes to 1980 are stylistic. No longer is the material considered more peripheral printed in smaller type, though to make up for this there are frequent exhortations to skip these sections. And the longer notes of the 1980 edition are now mostly incorporated into the text, several prefixed with advice that they not be read.\nIn the preface Wiggins says that it is a matter of no concern whether 1980 and 2001 are the same book. But it is of concern to me, twice over. First, it makes a large difference to what kind of review should be written whether this is an old book reissued or a new book. Secondly, the difficulty in answering this question draws out some problems Wiggins’s theory faces when we try to apply it outside the realms of physics and biology. On the first problem, the reader who hopes here to find a comprehensive discussion of the literature on identity post-1980 as it strikes David Wiggins will be disappointed. Three examples should help to illustrate this.\nIn 1967 Wiggins held, quite sensibly, that a statue is not identical to the bronze that it is made of, but rather is constituted by that bronze. This was an important move in his response to Geach. If identity is absolute, and the statue is identical with the bronze, then we can’t say that when the statue is remoulded into a vase, we have the same lump of bronze but a different artwork. In 2001 he says much the same thing. This still seems like good common sense, but the problem is that in the intervening 34 years there has been a mass of work on constitution, most of it concluding that the constitution relation is much more problematic than we had originally thought. In a genuinely new work, or even perhaps in a revised old work, this material should have been addressed.\nIn 1980 Wiggins makes it quite clear he dislikes perdurantist theories of persistence that hold that an object persists from t­1 to t2 by having instantaneous temporal parts at every time in [t1, t2]. Just why he dislikes perdurantism is unclear, since all his arguments are directed against the conjunction of this view with the striking, and not especially popular, view that our ordinary names refer to these instantaneous objects. In 2001 it is still clear he dislikes perdurantism. But we find little on the barrage of arguments perdurantists have offered in the last 21 years in support of their position. All we find is a footnote expressing agreement with Mark Johnston’s response to Lewis’s ‘problem of temporary intrinsics’ argument, and a citation of a paper expanding upon said agreement. Given that this book largely reprints previously available material, including more detail here would not have been absurd, and saying something about other arguments quite appropriate.\nThe third example is a little more serious. In the book’s new chapter, he outlines approvingly Evans’s proof that identity is always determinate identity, and cites (without outlining) a proof by Williamson that distinctness is always determinate. From these proofs he quite naturally concludes that the prospects for indeterminate identities are pretty grim. But he doesn’t engage with those who maintain that, despite all this, there really are indeterminate identities. It would have been worthwhile, for instance, to see a more direct engagement between his views and those of, say, Terrence Parsons, who over the last 15 years has developed a rather detailed theory on which indeterminate identity is possible. It will probably turn out that Wiggins’s position is entirely correct, and Parsons’s position basically mistaken, but that’s no reason to not take Parsons more seriously.\nApart from this oversight, there is one rather odd feature in the discussion of determinacy. Wiggins says that “it can be perfectly determinate which mountain x is without x’s extent being determinate.” (166) The idea is that it can be determinate that x is, say, this mountain, while it is indeterminate whether, say, that foothill is part of x. Such a position always feels strained to me, but it is certainly not unfamiliar. But it is very hard to see how it is meant to fit in with Wiggins’s picture of the role of sortal concepts, such as mountain. On page 70 he says a sortal concept is such that grasp of it determines “what changes x tolerates without there ceasing to exist such a thing as x.” (He actually says ‘substance-concept’, not ‘sortal concept’ there, but these phrases seem to be used synonymously.) The trouble should be apparent. For it to be determinate what x is, presumably just is for it to be determinate that x is this mountain. That is, it is determinate that x falls under the sortal mountain, and that sortal must determine persistence conditions, else it fails to be a sortal. But that means x’s persistence conditions are determined. So there is a determinate fact, perhaps unknown and perhaps even unknowable, about how far in the future one can go without leaving x behind. (I assume here that if x’s temporal extent is determined by which sortal it falls under, then x’s temporal extent is determinate. I imagine some will deny this claim, but it looks like a platitude to me.) On the other hand, it seems that it can be determinate what x is even though the conditions of x’s spatial persistence, conditions that determine how far westward one can go without leaving x behind, are not determinate. Just how this asymmetry is to be tolerated is not explained.\nMuch of the interest in this edition will focus on the new material on personal identity, and I shall say more about this below. But it is worth going over the central claims of the earlier part of the book. The crucial principle is called D(ii). The derivation of it appeals crucially to D(i). (Both definitions, and the commentary, from page 64.)\nD(i) (x)(t) [(x exists at t) → (∃g) (g(x) at t)].\nD(ii) (x)(∃g)(t) [(x exists at t) → (g(x) at t)].\n‘x’ ranges over three-dimensional continuants, ‘t’ over times and ‘g’ over sortals. The argument for D(i) is that for any object at any time there is an answer to the Aristotelian question What is it? This answer is a sortal so, as just noted, it must determine principles of persistence. It must also determine “a prnciple of activity, a principle of functioning or a principle of operation”. (72) If the last claim looks disjunctive, that’s because it is. Sortals for living objects determine principles of activity, sortals for artifacts determine principles of functioning. Just what philosophically interesting features these principles share is never satisfactorily explained. So there’s a suspicion that there is no decent concept of sortal that covers the kinds of things living creatures are and the kinds of things artifacts are. In slogan form sortal isn’t a sortal. Two other considerations reinforce that suspicion.\nFirst, there are objects that don’t naturally fall under any known sortal. Just looking at the computer I’m now using, there is the latch that holds the lid down when it’s closed, the button that opens the CD tray, the brightness control, and the stick that plays some (but not all) the functional roles of a mouse. It’s far from obvious that any of these falls under a sortal, at least if a sortal must determine persistence conditions and a principle of functioning.\nSecondly, there’s a tension between the kinds of sortals Wiggins thinks appropriate for artifacts and what he says about persistence. Artifact sortals are, he says, functional kinds. These sortals are meant to determine persistence conditions. Whether an object has persisted is not meant to depend on extrinsic, or external, factors. This is the upshot of his Only a and b rule (96), which is important in ruling out ‘best deserver’ theories of persistence. That rule says that we don’t need to consider objects other than a or b to determine whether a is b. So whether a, the boy genius is b, the Nobel Prize winning author, cannot depend on the existence or otherwise of a person more closely continuous with a than b happens to be. Hence Nozick’s theory of personal identity, which rejects this, cannot be true. But to determine whether a´, the brightness control on my computer at t1 is b´, the volume control on Jack’s computer at t2, we need to see which sortals a´ and b´ fall under, to see whether a´ has persisted. That will depend, in part, on whether a´ still falls under that sortal, whatever it is. Such a sortal will sort by functional role, and whether that functional role is fulfilled at all times between t1 and t2 will be determined by things other than a´ and b´. The point generalises: in most cases whether an object continues to fill a functional role often turns on the existence, and behaviour, of other objects. The natural conclusion to be drawn here is that D(i) might be false for artifacts.\nClearly D(i) doesn’t entail D(ii). But, bracketing our concerns about its truth, it might still be usable in an abductive argument for D(ii). Wiggins does just this. The argument, and this is I think the only argument for D(ii), is that it best explains our widespread agreement over whether an object has survived. Wiggins says that, “Our capacity for massive agreement about this is much more remarkable than our occasional disagreement,” (66) and given D(i), D(ii) is the best explanation of this. Two replies. First, the agreement is not all that widespread, even in actual cases. Think, for example, about the range of disagreement over whether a corpse is a thing that once lived, and hence the disagreement over whether Auntie is buried behind the back shed, or Auntie no longer exists. Secondly, there is a better explanation – the perdurantist explanation given by Lewis. This not only explains why there is agreement just where there is agreement, but why there is disagreement where there is disagreement. The story is familiar. All sorts of continuants (fusions of temporal parts) exist, but we choose which ones to refer to and quantify over because of our particular interests, and those choices are codified in our linguistic practices. When dealing with historically familiar situations, membership of linguistic, and more broadly cultural, communities commits us to common answers. Since most situations are historically familiar, we have agreement in most cases. When we deal with new cases, either generated by new technology or new imaginativeness, and our interests do not pick out a clearly preferable continuant, we do not have agreement. If this is right, we will agree about the familiar and disagree about the unfamiliar. Happily, this is exactly what we find – we all know what the persistence conditions for cows, pigs and chicken are, we do not know the persistence conditions for corporate entities or pieces of software or even relocated football teams in perfectly everyday cases.\nIn 1980, the chapter on personal identity had two main aims. The first was to argue that Lockean considerations about continuity of memory could be used in an account of personal identity, even if they would have to be used as reference fixers rather than as constituents of a reductive analysis. The opponents here were those followers of Bishop Butler, most prominently Anthony Flew, who held that any such consideration would be hopelessly circular. The second aim was to argue that it is a conceptual truth that persons are animals. The targets here were (unnamed) philosophers who wanted to provide a complete functional analysis of a person. The objections were mainly political but since it is unclear whether the philosophy of personal identity should be part of metaphysics or ethics, that’s perfectly acceptable.\nIn 2001, the aims have changed. Wiggins retracts everything he said against Butler and Flew, and then spends most of the chapter in a lengthy discussion of Parfit’s theory of personal identity. Butler’s original complaint against Locke was that the concept of memory needs a pre-existing concept of personal identity to be applied so it cannot be used in a non-circular account of identity. In the bluntest version of this complaint, it is held that ‘A remembers X-ing’ is properly represented as ‘A remembers A X-ing’, which requires an identity between the referents of the two occurences of ‘A’. Wiggins’s complaint, in 1980, was that this position costs us some vital distinctions. We want to distinguish, for example, ‘A imagines being an elephant’ from ‘A imagines A being an elephant’. In the first case only, A imagines something possible. And what holds for imagining, he thought, holds for remembering. Wiggins now rejects the last step here. Remembering, unlike imagining, has a tie to the truth. A can only remember X-ing if A in fact X-ed. This last claim might not be part of the logical form of ‘A remembers X-ing’, but there is a close relation between the two. Wiggins calls the relationship presupposition, but the name here doesn’t matter much. So Butler, and Flew, were right – appeals to memory in a theory of personal identity are hopelessly circular, because they presuppose that debates about identity through time are resolved.\nThe rest of the chapter outlines concerns with Parfit’s theory of personal identity, based on his concept of quasi-memory, and with the intuitions behind some cases that support Parfit’s theory. Quasi-memory, unlike memory, need not be factive, but what is quasi-remembered must have happened somewhere, to someone. Wiggins launches a barrage of attacks on this idea, of which the following three seem most telling. First, quasi-memory could not be defined (and is not defined) as memory minus factiveness, because conceptual subtraction is undefined in the absence of conceptual analysis, and we don’t have a conceptual analysis here. Secondly, the concept of quasi-memory may seem to make sense for quite general, de dicto memories, but it runs into trouble with de re, or even with more specific memories. If I am to quasi-remember climbing Big Ben on my sixteenth birthday, must someone have climbed Big Ben on my sixteenth birthday? Or perhaps on their sixteenth birthday? Thirdly, even if Parfit can define the concept of accurate quasi-memory, that won’t get us the general concept of quasi-memory, because again conceptual subtraction doesn’t make sense.\nThere are some good points here for followers of Parfit to consider. They are followed by some interesting considerations about why we might rethink our intuitions about personal identity in cases involving brain swaps. Those interested in personal identity debates, and particularly Wiggins’s and Parfit’s contributions, should pay close attention here. These will not be the only people to whom this new volume has interest. Sameness and Substance was an important statement of a rather commonsensical solution to some of the hardest questions in metaphysics, a solution with which everyone working in the field should be well acquainted. And those who have not previously read it closely will find that the stylistic changes (having a uniform font size, incorporating the longer notes into the text) make the renewed version of Sameness and Substance much more accessible than the original.\n\n\n\n",
    "preview": "posts/2021-01-03-review-of-sameness-and-substance-renewed/wiggins.jpg",
    "last_modified": "2021-02-04T15:29:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-review-of-moral-uncertainty-and-its-consequences/",
    "title": "Review of “Moral Uncertainty and Its Consequences”",
    "description": "Review of Ted Lockhart, “Moral Uncertainty and Its Consequences”. Oxford: Oxford University Press, 2000.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2002-07-01",
    "categories": [
      "book review",
      "on books",
      "ethics",
      "moral uncertainty"
    ],
    "contents": "\nFor many years now, Peter Singer has been arguing that we should not eat meat, and that we should give more money to famine relief. Many have been convinced, but many more remain sceptical. However, on one point most of us would agree: the actions that Singer recommends here are certainly morally permissible. One rarely feels a twang of moral doubt when eating tofu curry or writing cheques to Oxfam. Even if we do not find Singer totally convincing, we may still feel this moral doubt when eating sirloin, or spending frivolously rather than charitably. If we accept the main principle in Ted Lockhart’s book Moral Uncertainty and Its Consequences, these twangs of moral doubt should be sufficient to make us amend our behaviour.\n\nPublished in Mind 111: 693-696.\nThe main principle Lockhart endorses is that we should perform actions that we are maximally confident are morally permissible. We might be quite confident that having the sirloin is morally permissible, but if we are not certain, and we are certain the tofu is permissible, we should stick to tofu. Similarly, if we are certain that large donations to famine relief are permissible, and not certain that not making these donations is permissible, the chequebook should come out. The principle is not just for left-wingers. As Lockhart notes, approvingly, it can also be used in anti-abortion arguments. In most cases, not having an abortion is almost certainly permissible. Perhaps there is an exception for cases of extreme fetal deformity, but not in everyday cases. So if the woman considering an abortion wants to do the action that is most probably morally permissible, and has any doubts about the permissibility of the procedure, she should decline the abortion.\nThe bulk of Lockhart’s book is devoted to case studies where this principle is deployed, and amendments to the principle generated by considerations of these cases are adopted. The cases include abortion, patient confidentiality, Roe v Wade and, briefly, charitable giving. The theme behind the studies is that even if people cannot come to agreement on what is morally right, they can come to agreement on what should be done according to the principle, at least as variously amended, and this should be sufficient to provide recommendations for action. Lockhart stresses that if this line of reasoning is correct, then applied ethicists can provide good advice on practical action without conclusively resolving apparently intractable ethical problems.\nThere are three main amendments Lockhart suggests to the principle. First, he suggests that if moral rightness comes in degrees, we should maximise the expected moral rightness of our actions, rather than the probability that we are doing the right thing. Secondly, in situations where we cannot work out which action maximises expected rightness, because perhaps we do not have perfect access to the relevant subjective probabilities, we should choose the action which most probably maximises expected rightness, or more generally has the highest expected expected degree of moral rightness. And thirdly, he says that we should maximise the expected rightness of courses of action, rather than of individual actions. One might quibble with these amendments, particularly I think with the second, but they do not seem to affect the core philosophical issues.\nThe principle has some rather striking consequences, so striking we might fear for its refutation by a quick modus tollens. Lockhart, of course, does not think this is so. He does not discuss the vegetarianism issue, and endorses the anti-abortion implications, but argues that the principle need not have such striking implications concerning charitable giving. He notes that for some people, those who think it probable enough that substantial charitable giving is a very bad thing to do, because we have such strong obligations to ourselves and those nearest and dearest, his principle does not recommend such giving (109).\nThere is a more direct reason for thinking the principle stands in need of some further clarification and defence. It is rather unclear what kind of norm the principle is stating, and hence what force the should in it is has. Lockhart says it is a norm of rational action, but it seems in practice to be neither that, nor a moral norm. To see this, consider the following case where someone clearly does not follow the principle. While on her way to visit a sick friend in hospital, Jane is convinced by a fellow subway rider that morality requires an impersonal concern for the whole world. She is convinced that morality requires that she not visit her friend, but instead find the patient most in need of a visitor, and see them. But when she gets to the hospital, her new moral belief is not strong enough to overcome her desire to visit her friend in need, which, feeling a little guilty, she does.\nAssuming that Jane’s newfound moral beliefs are wrong, and that in fact she did the right thing, what criticisms can we make of her action? Not that it was immoral, because she did the right thing, visiting her sick friend, and she acted for the right reason, acting out of care for her friend. Nor, it seems, that it was prudentially irrational, for she did what she believed would best satisfy her desires. Perhaps the fact that her new moral beliefs were not sufficiently motivating indicates a lack of resolve, or even a weakness of will, but alternatively one might think that Jane displayed commendable, and virtuous, common sense in not abandoning her friend precipitously. In any case, I doubt Jane’s action cannot be criticised, even if her resolve can be. Since Jane clearly violated Lockhart’s principle, she did not act in the way she thought most likely to be morally permissible, but her action seems immune from criticism, that suggests the principle should not be an action guiding norm.\nOne might argue that Jane has a moral responsibility to desire to do the right thing, and if she had this desire, she would have been rationally required to not visit her friend. If one believes in such a responsibility, then one will think that Jane acted against a desire she should have, that she was, at best, lucky that she did the right thing, and hence she was irrational. Lockhart compares such agents, who do the right thing against their better judgement, to gamblers who bet their life savings on unlikely, but ultimately successful, outcomes. (34)\nThis line of reasoning, however, ultimately does not provide grounds for criticising Jane. A moral agent may well have a moral responsibility to desire to do the things that happen to be the right things to do. For example, she may well have a responsibility to want to visit her sick friends, and to help those in need, and not cause harm to others. But she does not have a responsibility to want to do the right thing, whatever it turns out to be. Indeed, she would be a worse moral agent if many of her actions were motivated by such a desire. She should want to visit her friend because she cares about her friend, not because it is, in the abstract, the right thing to do. Michael Smith has described the desire to do the right thing, whatever it turns out to be, as a moral fetish, and this often seems appropriate. (The Moral Problem, Oxford: Blackwell, 1994, p. 76)\nIt is no discredit to Jane that she lacks this general desire, and in some cases it may be a virtue. If Jane has the general desire, if in Smith’s terminology she is a moral fetishist, then she may be prudentially required to follow Lockhart’s principle, but not otherwise, and she is not required, by any normative standard, to be a moral fetishist. If Jane (virtuously) does not have that general desire to do the right thing, whatever it turns out to be, then she is importantly dissimilar to the gambler, who does (and should) desire to bet on the successful outcome, whatever it turns out to be.\nWhatever the merits of Lockhart’s main principle, his approach raises several fascinating theoretical questions. For example, there is a substantial literature on what the motivational effects of coming to hold a new moral view are, and what they should be. But what is, and what should be, the motivational effects of coming to hold, say, that it is more probable than not that meat eating is permissible? From a different angle, if moral attitudes are more like desires than like beliefs, as some expressivists suggest, then can we even have the attitude that it is more probable than not that meat eating is permissible? Although in general Lockhart says little directly on these theoretical questions, it is a great service to show how they arise.\nIf Lockhart’s main principle is correct, it has rather radical implications for how applied ethics is practiced. Even if it is not, consideration of the issues Lockhart raises may provide a novel and valuable outlook on some familiar theoretical questions.\n\n\n\n",
    "preview": "posts/2021-01-03-review-of-moral-uncertainty-and-its-consequences/lockhart.jpg",
    "last_modified": "2021-02-04T15:28:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-03-review-of-rethinking-intuition/",
    "title": "Review of “Rethinking Intuition”",
    "description": "Review of Michael DePaul and William Ramsey, eds. “Rethinking Intuition: The Psychology of Intuition and Its Role in Philosophical Inquiry.” Lanham, Md.: Rowman & Littlefield, 1998.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2002-01-01",
    "categories": [
      "book review",
      "on books",
      "methodology"
    ],
    "contents": "\nThis collection arose out of a conference on intuitions at the University of Notre Dame in April 1996. The papers in it mainly address two related questions: (a) How much evidential weight should be assigned to intuitions? and (b) Are concepts governed by necessary and sufficient conditions, or are they governed by ‘family resemblance’ conditions, as Wittgenstein suggested? The book includes four papers by psychologists relating and analyzing some empirical findings concerning intuitions and eleven papers by philosophers endorsing various answers to these questions.\n\nPublished in Ethics 112: 361-364.\nThe first section consists of the papers by psychologists. In these papers, the main target is the traditional philosopher who holds, inter alia, that the answer to a is “quite a lot” and the answer to b is the former, that there are necessary and sufficient conditions for most philosophically interesting concepts. If you like these answers, then you might spend your time Chisholming away at concepts like ‘justice,’ ‘knowledge,’ and ‘causation’—proposing snappy analyses and testing them against intuitions about possible cases. But if you don’t like these answers, you might prefer to make pointed criticisms of the presuppositions of such a methodology and suggest some more empirically defensible ways of coming to understand concepts. Indeed, this is just what the psychologists writing here do.\nThe papers by the philosophers are, very roughly, divided up according to their answers to these questions. The second section, titled “Rethinking Intuition and Philosophical Method,” consists of papers disagreeing with traditional philosophy about a or b. (This section includes papers by Stephen Stich, Robert Cummins, Hilary Kornblith, Tamara Horowitz, William Ramsey, and Alvin Goldman and Joel Pust.) The really radical position, expressed most clearly by Stich, is that traditional philosophy is wrong on both counts. We need to bring much more empirical research to bear on explicating crucial concepts in ethics, epis- temology, and so forth, and the explications we will end up with will not be short lists of necessary and sufficient conditions. The third section, titled “Defending the Philosophical Tradition,” contains, mostly, defenses of one of the traditional views. (This section includes papers by George Bealer, Richard Foley, Ernest Sosa, George Graham and Terry Horgan, and Michael DePaul.) The main aim here is to defend the value of intuitions as evidence; there is no explicit defense of the traditional view of concepts. Despite this neat rationale, the editors’ classification breaks down in a few cases. For example, in Kornblith’s paper he indicates substantial agreement with the paper by Graham and Horgan. So it is a little unclear why these papers are in these opposing sections. There is one other philosophical paper: Gary Gutting’s historical introduction is printed in a special ‘Introduction’ section.\nThree of the papers have the phrase “Reflective Equilibrium” in their title, so it might be expected that there would be some cutting-edge discussions about how to balance competing desiderata in achieving equilibrium. We don’t get such a discussion, and perhaps with good reason. With a nod in the direction of Goodman, Rawls, and Daniels, the writers mostly agree that if the aim of ethical or epistemological theory is, primarily, to systematize our intuitions, then reflective equilibrium (RE) is the way to do it. The papers here are, quite self- consciously, interested in the more basic question of whether that is what we want ethics or epistemology to do. I’ll conclude by saying a bit more about the papers which most clearly address this question. For the radicals, Cummins argues that “philosophical intuition is epistemologically useless” (p. 125). For the traditionals, on the other hand, Michael DePaul argues that RE provides “close to a correct answer” to the question, “How should we conduct philo- sophical inquiry?” (p. 294).\nCummins compares evidence from intuitions to evidence from other sources, like telescopes. He notes two related features of telescopes which, he thinks, makes them more trustworthy sources of evidence than intuitions. First, telescopes can be calibrated. We can apply telescopes to cases about which we have reliable independent evidence and see whether they deliver appropriate answers. For example, we can point a telescope at a distant mountain and see whether it looks the same through the telescope as it does up close and personal. If so, we can trust what it shows about places we have never before seen, such as heavenly bodies. If not, we not only learn that the telescope is untrustworthy but also may learn a little about the way in which it fails. Unlike telescopes, intuitions cannot be independently checked. They can only be checked against other intuitions. Hence, argues Cummins, they are untrustworthy. As Sosa notes, the comparison here may be unfair. Even though we can calibrate telescopes, we cannot calibrate observation as a whole. We can only calibrate particular kinds of observations against other kinds of observations and particular kinds of intuitions against other kinds of intuitions. Intuition, in this respect, is just like observation, and since we trust observations, we should trust intuitions.\nCummins’s other critique is that what evidence we do have about intuitions suggests that they are artifacts of the process by which they are produced rather than reliable guides to their subject matters. The idea is that the presence of a certain intuition concerning fairness tells us more about the source of the intuition (usually the person who has the intuition) than about fairness. If this is right, then intuitions are obviously not evidential. Cummins’s argument is that there are only five possible sources of intuitions, and examination of each suggests that intuitions are artifacts of the process by which they are produced. To prove this, Cummins works through each of the five possible sources and argues for each that an intuition derived from that source has no evidential value. Argument by cases in this way, when there are five possible cases to cover, is never going to be satisfactory. For example, one of the cases Cummins considers is that intuitions are evidential because they arise from possession of concepts. Something like this view is endorsed in the papers by Bealer and by Goldman and Pust. Cummins thinks this does not work because our concepts are just sets of beliefs. One’s concept of an elevator is just everything one believes about elevators. If anything like this is right, then the fact that we intuit that p just means that we believe p and that could not be evidence that p. But the theory of concepts he has in mind cannot be right. As Fodor has pointed out, it seems people can share concepts while having different beliefs involving those concepts. Indeed, something like this must be right if genuine disagreement is possible. If possessing a concept just meant having certain beliefs, then it would be impossible for people with radically different beliefs about a subject to share concepts relating to that subject. Since such sharing is possible, concept possession does not reduce to having certain beliefs. The main point is not that there is an insurmountable problem for Cummins here—maybe a more detailed discussion could show that his account of concepts is right and Fodor’s is wrong—but rather that with such a wide terrain to cover, a short argument is not going to win many converts.\nMichael DePaul is much more content with intuitions playing a central role in philosophy. Indeed, he seems happy to let them do all the work. His paper imagines a dialogue between himself and a friendly barfly who wants to be told all about how philosophy works. At some point in the conversation, DePaul’s character decides to present the new friend with an extended summary of how RE works. The friend is bemused that philosophers seem to only sit around and compare intuitive judgments. It does seem, notes the friend, a trifle self-indulgent. DePaul’s response attempts to defend RE by an argument that any alternative method would be irrational. Any alternative, argues DePaul, would have to (a) abandon reflection, (b) reflect incompletely, by leaving out certain beliefs, principles, or whatever enters into reflection, or (c) not allow results of reflection to influence final theory. As DePaul notes, it would be irrational to accept any of these options. DePaul acknowledges two possible criticisms here, criticisms which he admits he is not sure how to answer. The first is that it is not clear what is wrong with being irrational, at least in the sense DePaul has in mind. The second is that even if we have a reason not to be irrational, it is not clear how strong a reason this is and, hence, whether irrationality might be justifiable on occasion because it fulfills some greater purpose.\nThere is a third criticism that more closely reflects the problem raised by DePaul’s interlocutor. When someone says that philosophy should be about more than systematizing intuitions, they are not advocating alternatives to RE but, rather, supplements to it. The point of the criticism was that there must be other sources of evidence for moral or conceptual claims, other than just intuition. (This, apparently, is intuitively obvious!) DePaul provides a good response to someone who wants to say that intuitions have no evidential value at all. But he does not answer the critic who denies that intuitions provide the only evidence that might bear on philosophical problems.\nThis is a very useful collection to have published. A study of the role of intuition should be at the heart of any investigation into philosophical methodology. And such an investigation will have to take into account both the empirical findings about how intuition works and the philosophical considerations about how much importance should be attached to intuitions. The papers here do not look like the last word on any of these questions, but they are a helpful, and perhaps overdue, first word.\n\n\n\n",
    "preview": "posts/2021-01-03-review-of-rethinking-intuition/intuition.jpg",
    "last_modified": "2021-02-04T15:27:54-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-from-classical-to-intuitionistic-probability/",
    "title": "From Classical to Intuitionistic Probability",
    "description": "We generalize the Kolmogorov axioms for probability calculus to obtain conditions defining, for any given logic, a class of probability functions relative to that logic, coinciding with the standard probability functions in the special case of classical logic but allowing consideration of other classes of \"essentially Kolmogorovian\" probability functions relative to other logics. We take a broad view of the Bayesian approach as dictating *inter alia* that from the perspective of a given logic, rational degrees of belief are those representable by probability functions from the class appropriate to that logic. Classical Bayesianism, which fixes the logic as classical logic, is only one version of this general approach. Another, which we call Intuitionistic Bayesianism, selects intuitionistic logic as the preferred logic and the associated class of probability functions as the right class of candidate representions of epistemic states (rational allocations of degrees of belief). Various objections to classical Bayesianism are, we argue, best met by passing to intuitionistic Bayesianism -- in which the probability functions are taken relative to intuitionistic logic -- rather than by adopting a radically non-Kolmogorovian, e.g. non-additive, conception of (or substitute for) probability functions, in spite of the popularity of the latter response amongst those who have raised these objections. The interest of intuitionistic Bayesianism is further enhanced by the availability of a Dutch Book argument justifying the selection of intuitionistic probability functions as guides to rational betting behaviour when due consideration is paid to the fact that bets are settled only when/if the outcome betted on becomes known.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2001-09-01",
    "categories": [
      "logic",
      "games and decisions"
    ],
    "contents": "\n\nContents\nIntroduction\nMotivating Intuitionistic Bayesianism\nMore on Intuitionistic Probability Functions\nBets and Intuitionistic Probability Functions\nAppendix: The Morgan–Leblanc–Mares Calculus\n\nIntroduction\nIt is a standard claim of modern Bayesian epistemology that reasonable epistemic states should be representable by probability functions. There have been a number of authors who have opposed this claim. For example, it has been claimed that epistemic states should be representable by Zadeh’s fuzzy sets, Dempster and Shafer’s evidence functions, Shackle’s potential surprise functions, Cohen’s inductive probabilities or Schmeidler’s non-additive probabilities.1 A major motivation of these theorists has been that in cases where we have little or no evidence for or against \\(p\\), it should be reasonable to have low degrees of belief in each of \\(p\\) and \\({\\lnot}\\)\\(p\\), something apparently incompatible with the Bayesian approach. There are two broad types of response to this situation, the second of which shows the incompatibility just mentioned is more apparent than real. The first of these – much in evidence in the work of the writers just cited – is to replace or radically reconstrue the notion of probability taken by that approach to represent degrees of belief. The second – to be defended here – seeks to maintain the core of standard probability theory but to generalize the notion of a probability function to accommodate variation in the background logic of the account; this allows us to respond to such issues as the low degree of belief in a proposition and its negation by simply weakening the background logic from classical to intuitionistic logic. Thus if Bayesianism is construed as in our opening sentence, one way to respond to the objections of the heterodox writers listed above is to trade in classical Bayesianism for intuitionistic Bayesianism. Since for many theorists at least the motivation for their opposition to Bayesianism is grounded in either verificationism or anti-realism, a move to a intuitionistic theory of probability seems appropriate. Indeed, as Harman (1983) notes, the standard analysis of degrees of belief as dispositions to bet leads naturally to a intuitionistic theory of probability. We give a Dutch Book argument in defence of constructive Bayesianism in Section 4 below.\nPublished in Notre Dame Journal of Philosophical Logic 44: 111-123.\nThanks to Alan Hájek, Graham Oppy and, especially, Lloyd Humberstone for comments and suggestions on various drafts of this paper.\nThe appropriate generalization of the notion of a probability function makes explicit allowance for a sensitivity to the background logic. The latter we identify with a consequence relation, such as, in particular, the consequence relation \\(\\vdash_{CL}\\) associated with classical logic or the consequence relation \\(\\vdash_{IL}\\) associated with intuitionistic logic. To keep things general, we assume only that the languages under discussion have two binary connectives: \\({\\vee}\\) and \\({\\wedge}\\). No assumptions are made about how a consequence relation on such a language treats compounds formed using these connectives, though of course in the cases in which we are especially interested, \\(\\vdash_{CL}\\) and \\(\\vdash_{IL}\\), such compounds have the expected logical properties. We take the language of these two consequences relations to be the same, assuming in particular that negation (\\({\\lnot}\\)) is present for both. Finally, if \\(A\\) belongs to the language of a consequence relation \\(\\vdash\\), then we say that \\(A\\) is a \\(\\vdash\\)-thesis of \\(\\vdash\\) \\(A\\) and that \\(A\\) is a \\(\\vdash\\)-antithesis if for all \\(B\\) in that language \\(A\\) \\(\\vdash\\) \\(B\\). (Thus the \\(\\vdash\\)-theses and antitheses represent the logical truths and logical falsehoods as seen from the perspective of \\(\\vdash\\).) We are now in a position to give the key definition.\nIf \\(\\vdash\\) is a consequence relation, then a function Pr mapping the language of \\(\\vdash\\) to the real interval [0,1] is a \\(\\vdash\\)-probability function if and only if the following conditions are satisfied:\n(P0)\nPr(\\(A\\)) = 0 if \\(A\\) is a \\(\\vdash\\)-antithesis.\n\n(P1)\nPr(\\(A\\)) = 1 if \\(A\\) is a \\(\\vdash\\)-thesis\n\n(P2)\nIf \\(A\\) \\(\\vdash\\) \\(B\\) then Pr(\\(A\\)) \\({\\leq}\\) Pr(\\(B\\))\n\n(P3)\nPr(\\(A\\)) + Pr(\\(B\\)) = Pr(\\(A\\) \\({\\vee}\\) \\(B\\)) + Pr(\\(A\\) \\({\\wedge}\\) \\(B\\))\n\nIf \\(\\vdash\\) is \\(\\vdash_{CL}\\), then we call a \\(\\vdash\\)-probability function a classical probability function; if \\(\\vdash\\) is \\(\\vdash_{IL}\\) we call a \\(\\vdash\\)-probability function an intuitionistic probability function. The position described above as constructive Bayesianism would replace classical probability functions by intuitionistic probability functions as candidate representations of reasonable epistemic states. Note that classical probability functions in this sense are exactly those obeying the standard probability calculus axioms. In paricular, the familiar negation axiom dictating that Pr(\\({\\lnot}\\)\\(A\\)) = 1 – Pr(\\(A\\)) emerges as a by-product of the interaction between the general (i.e., logic-independent) condition (P3) and, via (P0) and (P1), the logic-specific facts that \\(A\\) \\({\\wedge}\\) \\({\\lnot}\\)\\(A\\) is a \\(\\vdash_{CL}\\)-antithesis and \\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\) is a \\(\\vdash_{CL}\\)-thesis for any \\(A\\).\nAlthough it is these two kinds – intuitionistic and classical – of probability functions we shall be dealing with specifically in what follows, we emphasize the generality of the above definition of a \\(\\vdash\\)-probability function, and invite the reader to consider what effect further varying the choice of \\(\\vdash\\) has on the behaviour of such functions. Our attention will be on the comparative merits of \\(\\vdash_{CL}\\) and \\(\\vdash_{IL}\\) in this regard. (It may have occurred to the reader in connection with (P3) above that we might naturally have considered a generalized version of (P3) for ‘countable additivity.’ Whether such a condition ought be adopted will turn on some rather difficult questions concerning the use of infinities in constructive reasoning; let us leave it as a question for further research. We have stated (P3) in its finitary form so as not to require that intuitionistic probability functions satisfy the more contentious general condition.)\nIn the following section we shall review some of the motivations for intuitionistic Bayesianism. The arguments are rather piecemeal; they are designed to show that given the philosophical commitments various writers in the field have expressed they would be better off taking this route, i.e., focussing on the class of intuitionistic probability functions, than – as many of them have suggested –abandoning Bayesianism in our broad sense. In particular, we shall urge that moves in the latter direction which involve abandoning (what we shall call) the Principle of Addition are seriously undermotivated.\nOne aspect of the Bayesian perspective which we have not considered concerns the dynamics rather than the statics of epistemic states: in particular the idea that changes in such states are governed for rational agents by the principle of conditionalizing on new information. This requires that we have a dyadic functor available for expressing conditional probabilities. Accordingly, where Pr is for some consequence relation \\(\\vdash\\) a \\(\\vdash\\)-probability function, we favour the standard account and take the associated conditional \\(\\vdash\\)-probability function Pr( , ) to be given by Pr(\\(A\\),\\(B\\)) = Pr(\\(A\\) \\({\\wedge}\\) \\(B\\))/Pr(\\(B\\)) when Pr(\\(B\\)) \\({\\neq}\\) 0, with Pr(\\(A\\),\\(B\\)) undefined when Pr(\\(B\\)) = 0. The intention, of course, is that Pr(\\(A\\),\\(B\\)) represents the conditional probability of \\(A\\) given \\(B\\). We defer further consideration of conditional probability until the Appendix.\nMotivating Intuitionistic Bayesianism\nThere are four main reasons for grounding preferring intuitionistic over classical probability functions as representing the range of reasonable epistemic states. These are: (1) a commitment to verificationism, (2) a commitment to anti-realism, (3) preservation of the principle of Addition, and (4) avoidance of direct arguments for the orthodox approach. Now some of these will be viewed by some people as bad reasons for adopting the given position, a reaction with which it is not hard to sympathise. In particular, the verificationist and anti-realist elements of the theory might well be viewed as negatives. These arguments are principally directed at showing that by their own lights, various opponents of classical Bayesianism would do better to adopt the intuitionistic Bayesian position than some still more heterodox non-Bayesian account.\n2.1 A standard objection to classical Bayesianism is that it has no way of representing complete uncertainty. Because of the failures of Laplace’s principle of indifference, it can’t be said that uncertainty about \\(p\\) is best represented by assigning credence 1/2 to \\(p\\). Heterodox approaches usually allow the assignment of credence 0 to both \\(p\\) and \\({\\lnot}\\)\\(p\\) when an agent has no evidence at all as to whether or not \\(p\\) is true. Because these approaches generally require an agent to assign credence 1 to classical tautologies, including \\(p\\) \\({\\vee}\\) \\({\\lnot}\\)\\(p\\), these theories must give up the following Principle of Addition.\nAddition\nFor incompatible \\(A\\), \\(B\\): Bel(\\(A\\) \\({\\vee}\\) \\(B\\)) = Bel(\\(A\\)) + Bel(\\(B\\)).\n\n“Bel(\\(A\\))” is here used to mean the degree of belief the agent has in \\(A\\), and “incompatible” to apply to \\(A\\) and \\(B\\) in which for some favoured consequence relation \\(\\vdash\\), the conjunction of \\(A\\) with \\(B\\) is a \\(\\vdash\\)-antithesis. Such conditions as Addition are of course taken not as descriptive theories about all agents, since irrational agents would serve as counterexamples. Rather, they are proposed coherence constraints on all rational agents.\nThe Principle of Addition is stated in terms of degrees of belief, or credences. Where no ambiguity results we also use the same term to refer to the corresponding principle applied to \\(\\vdash\\)-probability functions, with incompatibility understood in terms of \\(\\vdash\\) (as just explained). Now in some writings (particularly Shafer’s) the reason suggested for giving up Addition is openly verificationist. Shafer says that when an agent has no evidence for \\(p\\), they should assign degree of belief 0 to \\(p\\). Degrees of belief, under this approach, must be proportional to evidence.2 In recent philosophical literature, this kind of verificationism is often accompanied by an insistence that validity of arguments be judged by the lights of \\(\\vdash_{IL}\\) rather than \\(\\vdash_{CL}\\).\nA similar line of thought is to be found in Harman (1983). He notes that when we don’t distinguish between the truth conditions for a sentence and its assertibility conditions, the appropriate logic is intuitionistic. And when we’re considering gambles, something like this is correct. When betting on \\(p\\) we don’t, in general, care if \\(p\\) is true as opposed to whether it will be discovered that \\(p\\) is true. A \\(p\\)-bet, where \\(p\\) asserts the occurrence of some event for instance, becomes a winning bet, not when that event occurs, but when \\(p\\) becomes assertible. So perhaps not just verificationists like Shafer, but all those who analyse degrees of belief as propensity to bet should adopt constructivist approaches to probability.\nTo see the point Harman is making, consider this example. We are invited to quote for \\(p\\)-bets and \\({\\lnot}\\)\\(p\\)-bets, where \\(p\\) is O. J. Simpson murdered his wife. If we are to take the Californian legal system literally, the probability of that given the evidence is strictly between one-half and one. To avoid one objection, these bets don’t just pay $1 if the bettor guesses correctly. Rather they pay $1 invested at market rates of interest at the time the bet is placed. The idea is that if we pay x cents for the bet now, when it is discovered that we have bet correctly we will receive a sum of money that is worth exactly as much as $1 now. Still, we claim, it might be worthwhile to quote less than 50 cents for each of the bets. Even if we will receive $1 worth of reward if we wager correctly, there is every possibility that we’ll never find out. So it might be that placing a bet would be a losing play either way. To allow for this, the sum of our quotes for the \\(p\\)-bet and the \\({\\lnot}\\)\\(p\\)-bet may be less than $1. As Harman points out, to reply by wielding a Dutch Book argument purporting to show that this betting practice is incoherent would be blatantly question-begging. That argument simply assumes that \\(p\\) \\({\\vee}\\) \\({\\lnot}\\)\\(p\\) is a logical truth, which is presumably part of what’s at issue. (In our terminology, this disjunction has the status of a \\(\\vdash_{CL}\\)-thesis which is not a \\(\\vdash_{IL}\\)-thesis.)\nHarman’s point is not to argue for a intuitionistic approach to probability. Rather, he is arguing against using probabilistic semantics for propositional logic. Such an approach he claims would be bound to lead to intuitionistic logic for the reasons given above. He thinks that, since this would be an error, the move to probabilistic semantics is simply misguided. Whatever we think of this conclusion, we can press into service his arguments for intuitionistic Bayesianism.\n2.2 The second argument for this approach turns on the anti-realism of some heterodox theorists. So George Shackle, for example, argues that if we are anti-realists about the future, we will assign positive probability to no future-directed proposition. The following summary is from a sympathetic interpreter of Shackle’s writing.\n\n[T]here is every reason to refuse additivity: [it] implies that the certainty that would be assigned to the set of possibilities should be ‘distributed’ between different events. Now this set of events is undetermined as the future – that exists only in imagination – is. (Ponsonnet 1996, 171)\n\nShackle’s anti-realism is motivated by what most theorists would regard as a philosophical howler: he regards realism about the future as incompatible with human freedom, and holds that human beings are free. The second premise here seems harmless enough, but the first is notoriously difficult to motivate. Nevertheless, there are some better arguments than this for anti-realism about the future. If we adopt these, it isn’t clear why we should ‘assign certainty’ to the set of possibilities.\nShackle is here assuming that for any proposition \\(p\\), even a proposition about the future, \\(p\\) \\({\\vee}\\) \\({\\lnot}\\)\\(p\\) is now true, although neither disjunct is true. Given his interests it seems better to follow Dummett here and say that if we are anti-realists about a subject then for propositions \\(p\\) about that subject, \\(p\\) \\({\\vee}\\) \\({\\lnot}\\)\\(p\\) fails to be true. Hence we have no need to ‘assign certainty to the set of possibilities.’ Or perhaps more accurately, assigning certainty to the set of possibilities does not mean assigning probability 1 to \\(p\\) \\({\\vee}\\) \\({\\lnot}\\)\\(p\\); in particular, condition (P1) on \\(\\vdash\\)-probability functions does not require this when we choose \\(\\vdash\\) as \\(\\vdash_{IL}\\).\n2.3 The third motivation for adopting an intuitionistic approach to probability is that it allows us to retain the Kolmogorov axioms for probability, in particular the Principle of Addition. This principle has, to my mind at least, some intuitive motivation. And the counterexamples levelled against it by heterodox theorists seem rather weak from the intuitionistic Bayesian perspective. For they all are cases where we might feel it appropriate to assign a low probability to a proposition and its negation3. Hence if we are committed to saying Pr(\\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\)) = 1 for all \\(A\\), we must give up the Principle of Addition. But the intuitionistic Bayesian simply denies that in these cases Pr(\\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\)) = 1, so no counterexample to Addition arises. This denial is compatible with condition (P1) on Pr’s being a \\(\\vdash_{IL}\\)-probability function since, as already noted, \\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\) is not in general a \\(\\vdash_{IL}\\)-thesis.\n2.4 The final argument for taking an intuitionistic approach is that it provides a justification for rejecting the positive arguments for classical Bayesianism. These provide a justification for requiring coherent degrees of belief to be representable by the classical probability calculus. There are a dizzying variety of such arguments which link probabilistic epistemology to decision theory, including: the traditional Dutch Book arguments found in Ramsey (1926), Teller (1973) and Lewis (1999); de-pragmatized Dutch Book arguments which rely on consistency of valuations, rather than avoiding actual losses, as in Howson and Urbach (1989), Christensen (1996) and Hellman (1997); and arguments from the plausibility of decision theoretic constraints to constraints on partial beliefs, as in Savage (1954), Maher (1993) and Kaplan (1996). As well as these, there are arguments for classical Bayesianism which do not rely on decision theory in any way, but which flow either directly from the definitions of degrees of belief, or from broader epistemological considerations. A summary of traditional arguments of this kind is in Paris (1994). Joyce (1998) provides an interesting modern variation on this theme.\nAll such arguments assume classical – rather than, say, intuitionistic – reasoning is appropriate. The intuitionist has a simple and principled reason for rejecting those arguments. The theorist who endorses \\(\\vdash_{CL}\\) when considering questions of inference, presumably lacks any such simple reason. And they need one, unless they think it appropriate to endorse one position knowing there is an unrefuted argument for an incompatible viewpoint.\nWe are not insisting that non-Bayesians will be unable to refute these arguments while holding on to \\(\\vdash_{CL}\\). We are merely suggesting that the task will be Herculean. A start on this project is made by Shafer (1981), which suggests some reasons for breaking the link between probabilistic epistemology and decision theory. Even if these responses are successful, such a response is completely ineffective against arguments which do not exploit such a link. As we think these are the strongest arguments for classical Bayesianism, non-Baeyesians have much work left to do. And it is possible that this task cannot be completed. That is, it is possible that the only questionable step in some of these arguments for classical Bayesianism is their use of non-constructive reasoning. If this is so only theorists who give up \\(\\vdash_{CL}\\) can respond to such arguments.\nIn sum, non-Bayesians need to be able to respond to the wide variety of arguments for Bayesianism. Non-Bayesians who hold on to \\(\\vdash_{CL}\\) must do so without questioning the implicit logical assumptions of such arguments. Given this restriction, producing these responses will be a slow, time-consuming task, the responses will in all likelihood be piecemeal, providing little sense of the underlying flaw of the arguments, and for some arguments it is possible that no effective response can be made. Intuitionistic Bayesians have a quick, systematic and, we think, effective response to all these arguments.\nMore on Intuitionistic Probability Functions\nHaving explained the motivation for intuitionistic Bayesianism, let us turn our attention in greater detail to its main source of novelty: the intuitionistic probability functions. We concentrate on logical matters here, in the following section justifying the singling out of this class of probability functions by showing that an epistemic state represented by Bel is invulnerable to a kind of Dutch Book if and only if Bel is an intuitionistic probability function.\nFor the case of specifically classical probability functions, the conditions (P0)–(P4) of Section 1 involve substantial redundancy. For example, we could replace (P2) and (P3) by – what would in isolation be weaker conditions – (P2\\(^\\prime\\)) and (P3\\(^\\prime\\)).\n(P2\\(^\\prime\\))\nIf \\(A\\) \\(\\dashv\\) \\(\\vdash\\) \\(B\\) then Pr(\\(A\\)) = Pr(\\(B\\))\n\n(P3\\(^\\prime\\))\nIf \\(\\vdash\\) \\({\\lnot}\\)(A \\({\\wedge}\\) B) then Pr(\\(A\\) \\({\\vee}\\) \\(B\\)) = Pr(\\(A\\)) + Pr(\\(B\\))\n\nHowever, in the general case of arbitrary \\(\\vdash\\)-probability functions (or rather: those for which \\({\\lnot}\\) is amongst the connectives of the language of \\(\\vdash\\)), such a replacement would result in a genuine weakening, as we may see from a consideration of the class of \\(\\vdash_{IL}\\)-probability functions. While both (P2\\(^\\prime\\)) and (P3\\(^\\prime\\)) are satisfied for \\(\\vdash\\) as \\(\\vdash_{IL}\\), the class of functions Pr satisfying (P0), (P1), (P2\\(^\\prime\\)) and (P3\\(^\\prime\\)) is broader (for this choice of \\(\\vdash\\)) than the class of intuitionistic probability functions. To see this, first note that the function P, defined immediately below, satisfies (P0), (P1), (P2) and (P3\\(^\\prime\\)), but not (P3).\n\\[P(A) = \n\\begin{cases}\n1 \\text{ if } p \\vee q~ \\vdash_{IL}~ A \\\\\n0 \\text{ otherwise}\n\\end{cases}\\]\n(Here \\(p\\) and q are a pair of atomic sentences.) To see that (P3\\(^\\prime\\)) is satisfied, assume P(\\(A\\) \\({\\vee}\\) \\(B\\)) = 1 and \\(\\vdash_{IL}\\) \\({\\lnot}\\)(A \\({\\wedge}\\) \\(B\\)). Then \\(p\\) \\({\\vee}\\) q \\(\\vdash_{IL}\\) \\(A\\) \\({\\vee}\\) \\(B\\), and \\(B\\) \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\). Hence \\(p\\) \\({\\vee}\\) q \\(\\vdash_{IL}\\) \\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\), but this only holds if either (1) \\(p\\) \\({\\vee}\\) q \\(\\vdash_{IL}\\) \\(A\\) or (2) \\(p\\) \\({\\vee}\\) q \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\). (For if \\(p\\) \\({\\vee}\\) q \\(\\vdash_{IL}\\) \\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\), then \\(p\\) \\(\\vdash_{IL}\\) \\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\) and q \\(\\vdash_{IL}\\) \\(A\\) \\({\\vee}\\) \\({\\lnot}\\)\\(A\\), whence by a generalization, due to Harrop, of the Disjunction Property for intuitionistic logic, either \\(p\\) \\(\\vdash_{IL}\\) \\(A\\) or \\(p\\) \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\) and similarly either q \\(\\vdash_{IL}\\) \\(A\\) or q \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\). Thus one of the following four combinations obtains: (a) \\(p\\) \\(\\vdash_{IL}\\) A and q \\(\\vdash_{IL}\\) \\(A\\), (b) \\(p\\) \\(\\vdash_{IL}\\) \\(A\\) and q \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\), (c) \\(p\\) \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\) and q \\(\\vdash_{IL}\\) \\(A\\), (d) \\(p\\) \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\) and q \\(\\vdash_{IL}\\) \\({\\lnot}\\)\\(A\\). But cases (b) and (c) can be ruled out since they would make \\(p\\) and q \\(\\vdash_{IL}\\)-incompatible, contradicting their status as atomic sentences, and from (a) and (d), (1) and (2) follow respectively.) If (1) first holds then P(\\(A\\)) = 1, as required. If (2) holds then \\(p\\) \\({\\vee}\\) q \\(\\vdash_{IL}\\) (\\(A\\) \\({\\vee}\\) \\(B\\)) \\({\\wedge}\\) \\({\\lnot}\\)\\(A\\) and (\\(A\\) \\({\\vee}\\) \\(B\\)) \\({\\wedge}\\) \\({\\lnot}\\)\\(A\\) \\(\\vdash_{IL}\\) \\(B\\), so P(\\(B\\)) = 1. The other cases are trivial to verify and are left to the reader.\nTo see (P2) is needed (for the current choice of \\(\\vdash\\)), as opposed to just (P2\\(^\\prime\\)), consider the following Kripke tree.\nWe introduce a “weighting” function w by setting w(1) = 0.2, w(2) = 0.3, w(3) = -0.1 and w(4) = 0.6. For any \\(A\\), let P(\\(A\\)) = \\({\\Sigma}\\)w(i), where the summation is across all points i that force \\(A\\). So P(\\(p\\)) = 0.6 and P(\\({\\lnot}{\\lnot}\\)\\(p\\)) = 0.5, contradicting (P2). But (P0), (P1), (P2\\(^\\prime\\)) and (P3) are all satisfied, showing that (P2) is in the general case not derivable from these three conditions.\nBets and Intuitionistic Probability Functions\nSay that an \\(A\\)-bet is a bet that pays $1 if \\(A\\) and nothing otherwise. These will sometimes be called bets on \\(A\\). In this theory, as in real life, it is possible that neither \\(A\\)-bets nor \\({\\lnot}\\)\\(A\\)-bets will ever be collected, so holding an \\(A\\)-bet and a \\({\\lnot}\\)\\(A\\)-bet is not necessarily as good as holding $1. An \\(A\\)-bet becomes a winning bet, i.e. worth $1, just when it becomes known that \\(A\\). We will assume that bookmakers and punters are both logically proficient and honest, so that when a \\(B\\)-bet becomes a winning bet and \\(B\\) \\(\\vdash_{IL}\\) \\(A\\), then an \\(A\\)-bet is a winning bet. The picture underlying this story is the Kripke tree semantics for intuitionistic logic. Bettors are thought of as being at some node of a Kripke tree, an \\(A\\)-bet wins at that stage iff \\(A\\) is forced by that node. Bettors do not know that any future nodes will be reached, so they cannot be confident that all bets on classical tautologies (\\(\\vdash_{CL}\\)-theses) will be winning. And more importantly, we take it that an (\\(A\\) \\({\\vee}\\) \\(B\\))-bet wins if and only if an \\(A\\)-bet wins or a \\(B\\)-bet wins. Again this mirrors the fact that \\(A\\) \\({\\vee}\\) \\(B\\) is forced at a node iff \\(A\\) is forced or \\(B\\) is forced.\nFinally, to get the Dutch Book style argument going, assume that for any sequence of bets on \\(A\\)1, \\(A\\)2, ..., \\(A\\)k, the bettor values the sequence at $(Bel(\\(A\\)1) + Bel(\\(A\\)2) + ... + Bel(\\(A\\)k)). This is obviously unrealistic and economically suspect4, but is perhaps a useful analogy. Then Bel leads to coherent valuations in all circumstances iff Bel is a intuitionistic probability function. That is, if Bel is not an intuitionistic probability function (henceforth: IPF) then there will be two finite sequences of bets S1 and S2 such that S1 is guaranteed to pay at least as much as S2 in all circumstances, but S2 is given higher value by the agent. For simplicity Bel will be called incoherent if this happens, and coherent otherwise. If Bel is an IPF there are no two such sequences, so it is coherent.\nIf Bel is not an IPF then we just need to look at which axiom is breached in order to construct the sequences. For example, if (P3) is breached then let the sequences be \\(\\langle\\)\\(A\\), \\(B\\)\\(\\rangle\\) and \\(\\langle\\)\\(A\\) \\({\\vee}\\) \\(B\\), A \\({\\wedge}\\) \\(B\\)\\(\\rangle\\). The same number of propositions from each sequence are forced at every node of every Kripke tree, so the coherence requirement is that the two sequences receive the same value. But ex hypothesi they do not, so Bel is incoherent. Similar proofs suffice for the remaining axioms (the remaining conditions on \\(\\vdash\\)-probability functions, that is, as they apply in the special case of \\(\\vdash\\) = \\(\\vdash_{IL}\\)).\nTo show that if Bel is an IPF it is coherent, we need some more notation. Let \\(\\langle\\)\\(A\\)1, ..., \\(A\\)k\\(\\rangle\\) be a sequence of propositions. Then say cn,k is the proposition true iff at least n of these are true. So c2,3 is the proposition (\\(A\\)1 \\({\\wedge}\\) \\(A\\)2) \\({\\vee}\\) (\\(A\\)1 \\({\\wedge}\\) \\(A\\)3) \\({\\vee}\\) (\\(A\\)2 \\({\\wedge}\\) \\(A\\)3). Assuming Bel is a IPF, we prove the following lemma holds for all k:\nThe proof is by induction on k. For k=1 and k=2, the proof is given by the axioms. So it remains only to complete the inductive step. For ease of reading in the proof we write \\(A\\) for Bel(\\(A\\)) where no ambiguity would result.\nBy the inductive hypothesis we have:\nSince \\(c_{i,k} \\vee A_{k+1}\\) \\(\\dashv\\) \\(\\vdash\\) \\(c_{i,k+1} \\vee A_{k+1}\\) and \\(c_{i,k} \\wedge A_{k+1}\\) \\(\\dashv\\) \\(\\vdash\\) \\(c_{i+1,k+1} \\wedge A_{k+1}\\) we have:\nNow, \\(c_{1,k+1} \\vee~ A_{k+1}\\) \\(\\dashv\\) \\(\\vdash\\) \\(c_{i,k+1}\\) and \\(c_{k+1,k+1} ~\\wedge~ A_{k+1}\\) \\(\\dashv\\) \\(\\vdash\\) \\(c_{k+1,k+1}\\) from the definitions of \\(c\\). So substituting in these equivalences and slightly renumbering, we get:\nRegrouping the last two summations and applying (P3),\nAnd cancelling out the second term on each side gives us the result we want. From this it follows immediately that Bel is coherent. Let S1 and S2 be any two sequences such that S1 is guaranteed to pay as much as S2. That is, that S2 pays $n entails S1 pays at least $n for all n. Now the lemma shows that for each sequence of bets, their value equals the sum of the probability that they’ll pay at least n for all values of n, up to the length of the sequence. So by as many appeals to (P2) as there are bets in S1, we have that the value of S2 is less than or equal to the value of S1, as required.\nGiven the well-known problems with Dutch Book arguments5, it might be wondered if we can give a different justification for the axioms. Indeed it may be considered helpful to have a semantics for the logic which does not refer to betting practices. One possibility is to say that IPFs are normalised measures on Kripke trees. The idea is that the probability of a proposition is the measure of the set of points at which the proposition is forced. It is straightforward to give a non-constructive proof that the axioms are sound with respect to these semantics, but making this proof constructive and providing any proof that the axioms are complete is a harder task. So for now this Dutch Book justification for the axioms is the best available.\nAppendix: The Morgan–Leblanc–Mares Calculus\nIn a series of papers (Morgan and LeBlanc (1983a, 1983b), Morgan and Mares (1995)) an approach to probability grounded in intuitionistic logic has been developed. The motivation is as follows. A machine contains an unknown set of propositions S, which need not be consistent. Pr(\\(A\\), \\(B\\)) is the maximal price we’d pay for a bet that S and \\(B\\) intuitionistically entail A (S, A \\(\\vdash_{IL}\\) B, that is). By standard Dutch Book arguments, we obtain axioms for a probability calculus which has some claim to being constructivist. The point of this section is to register the shortcomings of this approach as a theory of uncertain reasoning from evidence – to point out, that is, the implausibility of interpreting the axioms they derive as normative constraints on degrees of belief. (It should be noted from the start that this was not the advertised purpose of their theory, and at least one of the authors (Mares) has said (p.c.) that the primary purpose of constructing these theories was to generalise of the triviality results proved in Lewis (1976). So the purpose of this appendix may be to argue for something that isn’t in dispute: that these theories can’t be pushed into double duty as theories of reasoning under uncertainty.)\nThe axiomatisations given in the Morgan and Leblanc papers differs a little from that given in the Morgan and Mares paper, but the criticisms levelled here apply to their common elements. In particular, the following four axioms are in both sets.\n(C1)\n0 \\({\\leq}\\) Pr(\\(A\\), \\(B\\)) \\({\\leq}\\) 1\n\n(C2)\nPr(\\(A\\), \\(A\\) \\({\\wedge}\\) \\(B\\)) = 1\n\n(C3)\nPr(\\(A\\), \\(B\\) \\({\\wedge}\\) C)  Pr(\\(B\\), C) = Pr(\\(B\\), \\(A\\) \\({\\wedge}\\) C)  Pr(\\(A\\), C)\n\n(C4)\nPr(\\(A\\) \\({\\supset}\\) \\(B\\), C) = Pr(\\(B\\), \\(A\\) \\({\\wedge}\\) C)\n\nThese four are enough to get both the unwanted consequences. In particular, from these we get the ‘no negative evidence’ rule: Pr(\\(A\\), \\(B\\) \\({\\wedge}\\) C) \\({\\geq}\\) Pr(\\(A\\), \\(B\\)). The proof is in Morgan and Mares (1995) Now given the semantic interpretation they have adopted, this is perhaps not so bad. After all, if we can prove \\(A\\) from \\(B\\) and S, we can certainly prove it from \\(B\\) \\({\\wedge}\\) C and S, but the converse does not hold. However from our perspective this feature seems a little implausible. In particular, if C is \\({\\lnot}\\)\\(A\\), it seems we should have Pr(\\(A\\), \\(B\\) \\({\\wedge}\\) \\({\\lnot}\\)\\(A\\)) = 0 unless \\(B\\) \\(\\vdash_{IL}\\) \\(A\\), in which case Pr(\\(A\\), \\(B\\) \\({\\wedge}\\) \\({\\lnot}\\)\\(A\\)) is undefined.\nIt shouldn’t be that surprising that we get odd results given (C4). Lewis (1976) shows that adopting it for a (primitive or defined) connective ‘\\({\\rightarrow}\\)’ within the classical probability calculus leads to triviality. And neither the arguments he uses there nor the arguments for some stronger conclusions in Lewis (1999) rely heavily on classical principles. The papers by Morgan and Leblanc don’t discuss this threat, but it is taken discussed in detail in Morgan and Mares (1995). Morgan and Mares note that it’s possible to build a theory based on (C1) to (C4) that isn’t trivial in the sense Lewis described. But these theories still have enough surprising features that they aren’t suitable for use as a theory of reasoning under uncertainty.\nIn intuitionistic logic we often take the falsum \\({\\perp}\\) as a primitive connective, functioning as a \\(\\vdash_{IL}\\)-antithesis. Hence a set S is intuitionistically consistent iff we do not have S \\(\\vdash_{IL}\\) \\({\\perp}\\). Now the following seems a plausible condition:\n(C\\({\\perp}\\))\nFor consistent \\(B\\), Pr(\\({\\perp}\\), \\(B\\)) = 0.\n\nGiven consistent evidence, we have no evidence at all that the falsum is true. Hence we should set the probability of the falsum to 0 (as required by our condition (P0) from Section 1). Given Morgan and Leblanc’s original semantic interpretation there is less motivation for adopting (C\\({\\perp}\\)), since S might be inconsistent. The restriction to consistent \\(B\\) in (C\\({\\perp}\\)) is imposed because we take Pr(\\(A\\), \\(B\\)) to be undefined for inconsistent \\(B\\), as explained at the end of Section 1. (In more detail: if \\(B\\) is a \\(\\vdash_{IL}\\)-antithesis then Pr(\\(B\\)) = 0 for any intuitionistic probability function Pr, whence the undefinedness of Pr(\\(A\\), \\(B\\)) by the remarks at the end of that section.) Morgan, Leblanc and Mares take it to be set at 1. The choice here is a little arbitrary, the only decisive factor being apparently the easier statement of certain results. Now if we take the falsum as a primitive the next move is usually to introduce \\({\\lnot}\\) as a defined connective, as follows.\n\\({\\lnot}\\)\\(A\\) =df\\(A\\) \\({\\supset}\\) \\({\\perp}\\)\nAssuming \\(A\\) \\({\\wedge}\\) B is consistent, it follows from (C4) and (C\\({\\perp}\\)) that Pr(\\({\\lnot}\\)\\(A\\), \\(B\\)) = 0. Again, from our perspective this is an implausible result. The main purpose of this appendix has been to show that the Morgan–Leblanc–Mares probability calculus cannot do the work Bayesians want a probability calculus to do. That is, it is implausible to regard their Pr(\\(A\\), \\(B\\)) as the reasonable degree of belief in \\(A\\) given \\(B\\). Hence the account of conditional probability these authors offer diverges from the intuitionistic Bayesianism that we have been urging heterodox theorists to endorse.\n\n\nChristensen, David. 1996. “Dutch-Book Arguments de-Pragmatized: Epistemic Consistency for Partial Believers.” Journal of Philosophy 93 (9): 450–79. https://doi.org/10.2307/2940893.\n\n\nCohen, L. Jonathan. 1977. The Probable and the Provable. Oxford: Clarendon Press.\n\n\nDempster, Arthur. 1967. “Upper and Lower Probabilities Induced by a Multi-Valued Mapping.” Annals of Mathematical Statistics 38: 325–39. https://doi.org/10.1214/aoms/1177698950.\n\n\nHarman, Gilbert. 1983. “Problems with Probabilistic Semantics.” In Developments in Semantics, edited by Alex Orenstein and Rafael Stern, 243–37. New York: Haven.\n\n\nHellman, Geoffery. 1997. “Bayes and Beyond.” Philosophy of Science 64 (2): 191–221. https://doi.org/10.1086/392548.\n\n\nHowson, Colin, and Peter Urbach. 1989. Scientific Reasoning. La Salle: Open Court.\n\n\nJoyce, James M. 1998. “A Non-Pragmatic Vindication of Probabilism.” Philosophy of Science 65 (4): 575–603. https://doi.org/10.1086/392661.\n\n\nKaplan, Mark. 1996. Decision Theory as Philosophy. Cambridge: Cambridge University Press.\n\n\nLewis, David. 1976. “Probabilities of Conditionals and Conditional Probabilities.” Philosophical Review 85 (3): 297–315. https://doi.org/10.2307/2184045.\n\n\n———. 1999. “Why Conditionalize?” In Papers in Metaphysics and Epistemology, 403–7. Cambridge University Press.\n\n\nMaher, Patrick. 1993. Betting on Theories. Cambridge: Cambridge University Press.\n\n\nMorgan, Charles, and Hughes LeBlanc. 1983a. “Probabilistic Semantics for Intuitionistic Logic.” Notre Dame Journal of Formal Logic 24 (2): 161–80. https://doi.org/10.1305/ndjfl/1093870307.\n\n\n———. 1983b. “Probability Theory, Intuitionism, Semantics and the Dutch Book Argument.” Notre Dame Journal of Formal Logic 24 (3): 289–304. https://doi.org/10.1305/ndjfl/1093870372.\n\n\nMorgan, Charles, and Edward Mares. 1995. “Conditionals, Probability and Non-Triviality.” Journal of Philosophical Logic 24 (5): 455–67. https://doi.org/10.1007/bf01052599.\n\n\nParis, J. B. 1994. The Uncertain Reasoner’s Companion: A Mathematical Perspective. Cambridge: Cambridge University Press.\n\n\nPonsonnet, Jean-Marc. 1996. “The Best and the Worst in g. L. S. Shackle’s Decision Theory.” In Uncertainty in Economic Thought, edited by Christian Schmidt, 169–96. Cheltham: Edward Elgar.\n\n\nRamsey, Frank. 1926. “Truth and Probability.” In Philosophical Papers, edited by D. H. Mellor, 52–94. Cambridge: Cambridge University Press.\n\n\nSavage, Leonard. 1954. The Foundations of Statistics. New York: John Wiley.\n\n\nSchmeidler, David. 1989. “Subjective Probability and Expected Utility Without Additivity.” Econometrica 57 (3): 571–89. https://doi.org/10.2307/1911053.\n\n\nShackle, George. 1949. Expectation in Economics. Cambridge: Cambridge University Press.\n\n\nShafer, Glenn. 1976. A Mathematical Theory of Evidence. Princeton: Princeton University Press.\n\n\n———. 1981. “Constructive Probability.” Synthese 48 (1): 1–60. https://doi.org/10.1007/bf01064627.\n\n\nTeller, Paul. 1973. “Conditionalization and Observation.” Synthese 26 (2): 218–58. https://doi.org/10.1007/bf00873264.\n\n\nZadeh, Lofti A. 1978. “Fuzzy Sets as a Basis for a Theory of Probability.” Fuzzy Sets and Systems 1 (1): 3–28. https://doi.org/10.1016/0165-0114(78)90029-5.\n\n\nFor more details, see Zadeh (1978), Dempster (1967), Shafer (1976), Shackle (1949), Cohen (1977), Schmeidler (1989).↩︎\nThis assumption was shared by many of the participants in the symposium on probability in legal reasoning, reported in the Boston University Law Review 66 (1986).↩︎\nAgain the discussion in (Shafer 1976 ch. 2) is the most obvious example of this, but similar examples abound in the literature.↩︎\nIt is economically suspect because, in simplified terms, Bel(\\(A\\)) gives at best the use-value of an \\(A\\)-bet, but this is distinct from the exchange-value the agent places on the bet. And it is the exchange-value that determines her patterns of buying and selling.↩︎\nSee Maher (1993) for criticisms of the most recent attempts at successful Dutch Book arguments and references to criticisms of earlier attempts.\n\n↩︎\n",
    "preview": "posts/2021-01-04-from-classical-to-intuitionistic-probability/heyting.jpg",
    "last_modified": "2021-02-04T21:59:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-04-intrinsic-properties-and-combinatorial-principles/",
    "title": "Intrinsic Properties and Combinatorial Principles",
    "description": "Three objections have recently been levelled at the analysis of intrinsicness offered by Rae Langton and David Lewis. While these objections do seem telling against the particular theory Langton and Lewis offer, they do not threaten the broader strategy Langton and Lewis adopt: defining intrinsicness in terms of combinatorial features of properties. I show how to amend their theory to overcome the objections without abandoning the strategy.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2001-09-01",
    "categories": [
      "metaphysics",
      "intrinsic properties"
    ],
    "contents": "\n\nContents\nLangton and Lewis’s Theory\nThree Objections\nThe Set of Intrinsic Properties\nResponding to Counterexamples\nProblem Cases and Disjunctive Properties\nBack to Basics?\nConclusion\n\nThree objections have recently been levelled at the analysis of intrinsicness in Rae Langton and David Lewis’s “Defining ‘Intrinsic’.” Yablo (1999) has objected that the theory rests on “controversial and (apparently) irrelevant” judgements about the relative naturalness of various properties. Dan Marshall and Josh Parsons Marshall and Parsons (2001) have argued that quantification properties, such as being accompanied by an cube, are counterexamples to Langton and Lewis’s theory. And Theodore Sider Sider (2001) has argued that maximal properties, like being a rock, provide counterexamples to the theory. In this paper I suggest a number of amendments to Langton and Lewis’s theory to overcome these counterexamples. The suggestions are meant to be friendly in that the basic theory with which we are left shares a structure with the theory proposed by Langton and Lewis. However, the suggestions are not meant to be ad hoc stipulations designed solely to avoid theoretical punctures, but developments of principles that follow naturally from the considerations adduced by Langton and Lewis.\nPublished in Philosophy and Phenomenological Research 63: 365-380.\nThanks to David Lewis, Europa Malynicz, Dan Marshall, Daniel Nolan, Josh Parsons and Ted Sider for helpful discussions.\nPicture by Bernard Spragg via Creative Commons.\nLangton and Lewis’s Theory\nLangton and Lewis base their theory on a combinatorial principle about intrinsicness. If a property F is intrinsic, then whether a particular object is F is independent whether there are other things in the world. This is just a specific instance of the general principle that if F is intrinsic then whether some particular is F is independent of the way the rest of the world is. So if F is intrinsic, then the following four conditions are met:\nSome lonely object is F;\nSome lonely object is not-F;\nSome accompanied object is F; and\nSome accompanied object is not-F.\nThe quantifiers in the conditions range across objects in all possible worlds, and indeed this will be the quantifier domain in everything that follows (except where indicated). An object is ‘lonely’ if there are no wholly distinct contingent things in its world. The effect of including ‘distinct’ in this definition is that an object can be lonely even if it has proper parts; an object is not identical with its parts, but nor is it distinct from them. Following Langton and Lewis, I will say that any property that meets the four conditions is ‘independent of accompaniment.’\nAll intrinsic properties are independent of accompaniment, but so are some extrinsic properties. For example, the property being the only round thing is extrinsic, but independent of accompaniment. So Langton and Lewis do not say that independence of accompaniment is sufficient for intrinsicness. However, within a certain class of properties, what we might call the basic properties, they do say that any property independent of accompaniment is intrinsic. A property is basic if it is neither disjunctive nor the negation of a disjunctive property. Langton and Lewis define the disjunctive properties as follows:\n\n[L]et us define the disjunctive properties as those properties that can be expressed by a disjunction of (conjunctions of) natural properties; but that are not themselves natural properties. (Or, if naturalness admits of degrees, they are much less natural than the disjuncts in terms of which they can be expressed.) Langton and Lewis (2001)\n\nLangton and Lewis assume here that there is some theory of naturalness that can be plugged in here, but they are explicitly ecumenical about what the theory may be. They mention three possibilities: naturalness might be primitive; it might be defined in terms of which universals and tropes exist, if you admit such into your ontology; or it might be defined in terms of which properties play a special role in our theory. Call the first the primitivist conception, the second the ontological conception, and the third the pragmatic conception. (One can generate different versions of the pragmatic theory by altering what one takes to be ‘our theory.’ In Taylor (1993), which Langton and Lewis credit as the canonical statement of the pragmatic conception, naturalness is relativised to a theory, and the theories he focuses on are ‘regimented common sense’ and ‘unified science.’) Langton and Lewis’s intention is to be neutral as to the correct interpretation of naturalness whenever they appeal to it, and I will follow their policy.\nWith these concepts, we can now define intriniscness. A property is basic intrinsic iff it is basic and independent of accompaniment. Two objects are duplicates iff they have the same basic intrinsic properties. And a property is intrinsic iff there are no two duplicates that differ with respect to it.\nLangton and Lewis make one qualification to this definition: it is only meant to apply to pure, or qualitative, properties, as opposed to impure, or haeccceitistic, properties. One reason for this restriction is that if there are any impure intrinsic properties, such as being John Malkovich, they will not have the combinatorial features distinctive of pure intrinsic properties. If F is a pure intrinsic property then there can be two wholly distinct things in a world that are F. This fact will be crucial to the revised definition of intrinsicness offered below. However, it is impossible to have wholly distinct things in the same world such that each is John Malkovich. So for now I will follow Langton and Lewis and just say what it takes for a pure property to be intrinsic. As Langton and Lewis note, it would be nice to complete the definition by giving conditions under which impure properties are intrinsic, but the little task of working out the conditions under which pure properties are intrinsic will be hard enough for now.\nThree Objections\nStephen Yablo (Yablo 1999) criticises the judgements of naturalness on which this theory rests. Consider again the property being the only round thing, which is extrinsic despite being independent of accompaniment. If Langton and Lewis are right, this must not be a basic property. Indeed, Langton and Lewis explicitly say that it is the negation of a disjunctive property, since its negation can be expressed as: being round and accompanied by a round thing or being not-round. Yablo’s criticism is that it is far from obvious that the existence of this expansion shows that being the only round thing is disjunctive. For simplicity, let us name all the salient properties: \\[\\begin{aligned}\n\\textit{R}&=\\textsubscript{df}~\\textit{being the only round thing} \\\\\n\\textit{S}&=\\textsubscript{df}~\\textit{being not the only round thing}\\\\\n\\textit{T}&=\\textsubscript{df}~\\textit{being round and accompanied by a round thing}\\\\\n\\textit{U}&=\\textsubscript{df}~\\textit{being not\\nobreakdash-round}\\end{aligned}\\]\n(Something is accompanied by an F iff one of its distinct worldmates is F.) Langton and Lewis claim that since S = T \\({\\vee}\\) U, and S is much less natural than T and than U, S is disjunctive, so R is not basic. Yablo notes that we can also express S as being round if accompanied by a round thing, so it differs from T only in that it has an if where T has an and. Given this expansion, we should be dubious of the claim that S is much less natural than T. But without that claim, R already provides a counterexample to Langton and Lewis’s theory, unless there is some other expression of R or S that shows they are disjunctive.1\nDan Marshall and Josh Parsons Marshall and Parsons (2001) argue that the same kind of difficulties arise when we consider certain kinds of quantificational properties. For example, let E be the property being such that a cube exists. This is independent of accompaniment, since a lonely cube is E, a lonely sphere is not E, each of us is accompanied and E, and each of Max Black’s two spheres is accompanied and not E. So it is a counterexample to Langton and Lewis if it is basic. Marshall and Parsons note that, like all properties, it does have disjunctive expressions. For example x is E iff x is a cube or x is accompanied by a cube. And E is a less natural property than being a cube. But it is not at all intuitive that E is much less natural than the property being accompanied by a cube. This does not just show that Langton and Lewis have to cease being ecumenical about naturalness, because on some conceptions of naturalness it is not clear that E is much less natural than being accompanied by a cube. Rather, this example shows that there is no conception of naturalness that could play the role that Langton and Lewis want. The properties E and being accompanied by a cube seem just as natural as each other on the ontological conception of naturalness, on the pragmatic conception of naturalness, and, as far as anyone can tell, on the primitivist conception. This is not because E is particularly natural on any of these conceptions. It certainly does not, for example, correspond to a universal, and it does not play a special role in our thinking or in ideal science. But since there is no universal for being accompanied by a cube, and that property does not play a special role in our thinking or in ideal science, it seems likely that each property is as natural as the other.\nTheodore Sider (2001) notes that similar problems arise for maximal properties, like being a rock. A property F is maximal iff large parts of Fs are typically not Fs. For example, being a house is maximal; a very large part of a house, say a house minus one window ledge, is not a house, it is just a large part of a house. Purported proof: call the house minus one window ledge house-. If Katie buys the house she undoubtedly buys house-, but she does not thereby buy two houses, so house- is not a house. As Sider notes, this is not an entirely conclusive proof, but it surely has some persuasive force. Maximal properties could easily raise a problem for Langton and Lewis’s definition. All maximal properties are extrinsic; whether a is a house depends not just on how a is, but on what surrounds a. Compare: House- would be a house if the extra window ledge did not exist; in that case it would be the house that Katie buys. But some maximal properties are independent of accompaniment. Being a rock is presumably maximal: large parts of rocks are not rocks. If they were then presumably tossing one rock up into the air and catching it would constitute juggling seventeen rocks, making an apparently tricky feat somewhat trivial. But there can be lonely rocks. A rock from our planet would still be a rock if it were lonely. Indeed, some large rock parts that are not rocks would be rocks if they were lonely. And it is clear there are be lonely non-rocks (like our universe), accompanied rocks (like Uluru) and accompanied non-rocks (like me).\nSince being a rock is independent of accompaniment and extrinsic, it is a counterexample if it is basic. Still, one might think it is not basic. Perhaps being a rock is not natural on the primitivist conception. (Who is to say it is?) And perhaps it does not correspond to a genuine universal, or to a collection of tropes, so it is a disjunctive property on the ontological conception of naturalness. Sider notes, however, that on at least one pragmatic conception, where natural properties are those that play a special role in regimented common sense, it does seem particularly natural. Certainly it is hard to find properties such that being a rock can be expressed as a disjunction of properties that are more central to our thinking than being a rock. So this really does seem to be a counterexample to Langton and Lewis’s theory.\nThe Set of Intrinsic Properties\nIt is a platitude that a property F is intrinsic iff whether an object is F does not depend on the way the rest of the world is. Ideally this platitude could be morphed into a definition. One obstacle is that it is hard to define the way the rest of the world is without appeal to intrinsic properties. For example, even if F is intrinsic, whether a is F is not independent of whether other objects have the property not being accompanied by an F, which I will call G. To the extent that having G is a feature of the way the rest of the world is, properties like G constitute counterexamples to the platitude. Since platitudes are meant to be interpreted to be immune from counterexamples, it is wrong to interpret the platitude so that G is a feature of the way the rest of the world is. The correct interpretation is that F is intrinsic iff whether an object is F does not depend on which intrinsic properties are instantiated elsewhere in the world.\nIf what I call the independence platitude is to be platitudinous, we must not treat independence in exactly the same way as Langton and Lewis do. On one definition, whether a is F is independent of whether the rest of the world is H iff it is possible that a is F and the rest of the world H, possible that a is not-F and the rest of the world H, possible that a is F and the rest of the world not-H, and possible that a is not-F and the rest of the world not-H. On another, whether a if F is independent of whether the rest of the world is H iff whether a is F is entirely determined by the way a itself, and nothing else, is, and whether the rest of the world is H is determined by how it, and not a, is. This latter definition is very informal; hence the need for the formal theory that follows. But it does clearly differ from the earlier definition in a couple of cases. The two definitions may come apart if F and H are excessively disjunctive. More importantly, for present purposes, they come apart if F is the necessary property (that everything has), or the impossible property (that nothing has). In these cases, whether a is F is entirely settled by the way a, and nothing else is, so in the latter sense it is independent of whether the rest of the world is H. But it is not the case that all four possibilities in the former definition are possible, so it is not independent of whether the rest of the world is H in that sense. Since there is some possibility of confusion here, it is worthwhile being clear about terminology. When I talk about independence here, I will always mean the latter, informal, definition, and I will refer to principles about which combinations of intrinsic properties are possible, principles such as Langton and Lewis’s principle that basic intrinsic properties are independent of accompaniment, as combinatorial principles. So, in the terminology I am using, the combinatorial principles are attempts to formally capture the true, but elusive, independence platitude with which I opened this section.\nSince the platitude is a biconditional with intrinsic on either side, it will be a little tricky to morph it into a definition. But we can make progress by noting that the platitude tells us about relations that hold between some intrinsic properties, and hence about what the set of intrinsic properties, which I will call SI, must look like.\nFor example, from the platitude it follows that SI is closed under Boolean operations. Say that F and G are intrinsic. This means that whether some individual a is F is independent of how the world outside a happens to be. And it means that whether a is G is independent of the way the world outside a happens to be. This implies that whether a is F and G is independent of the way the world outside a happens to be, because whether a is F and G is a function of whether a is F and whether a is G. And that means that F and G is intrinsic. Similar reasoning shows that F or G, and not F are also intrinsic. Call this condition Boolean closure.\nAnother implication of the independence platitude is that SI must be closed under various mereological operations. If F is intrinsic then whether a is F is independent of the outside world. If some part of a is F, that means, however the world outside that part happens to be, that part will be F. So that means that however the world outside a is, a will have a part that is F. Conversely, if a does not have a part that is F, that means all of a’s parts are not F. As we saw above, if F is intrinsic, so is not F. Hence it is independent of the world outside a that all of its parts are not F. That is, it is independent of the world outside a that a does not have a part that is F. In sum, whether a has a part that is F is independent of how the world outside a turns out to be. And that means having a part that is F is intrinsic. By similar reasoning, the property Having n parts that are F will be intrinsic if F is for any value of n. Finally, the same reasoning shows that the property, being entirely composed of n things that are each F is intrinsic if F is intrinsic. The only assumption used here is that it is independent of everything outside b that b is entirely composed of the particular things that it is composed of, but again this seems to be a reasonable assumption. So, formally, if F \\({\\in}\\) SI, then Having n parts that are F \\({\\in}\\) SI, and Being entirely composed of n things that are F \\({\\in}\\) SI. Call this condition mereological closure.\nFinally, and most importantly, various combinatorial principles follow from the independence platitude. One of these, that all intrinsic properties are independent of accompaniment, forms the centrepiece of Langton and Lewis’s theory. The counterexamples provided by Marshall and Parsons, and by Sider, suggest that we need to draw two more combinatorial principles from the platitude. The first is that if F and G are intrinsic properties, then whether some particular object a is F should be independent of how many other things in the world are G. More carefully, if F and G are intrinsic properties that are somewhere instantiated then, for any n such that there is a world with n+1 things, there is a world constituted by exactly n+1 pairwise distinct things, one of which is F, and the other n of which are all G. When I say the world is constituted by exactly n+1 things, I do not mean that there are only n+1 things in the world; some of the n+1 things that constitute the world might have proper parts. What I mean more precisely is that every contingent thing in the world is a fusion of parts of some of these n+1 things. Informally, every intrinsic property is not only independent of accompaniment, it is independent of accompaniment by every intrinsic property. As we will see, this combinatorial principle, combined with the Boolean closure principle, suffices to show that Marshall and Parsons’s example, being such that a cube exists, is extrinsic.\nSometimes the fact that a property F is extrinsic is revealed by the fact that nothing that is F can be worldmates with things of a certain type. So the property being lonely is extrinsic because nothing that is lonely can be worldmates with anything at all. But some extrinsic properties are perfectly liberal about which other properties can be instantiated in their world; they are extrinsic because their satisfaction excludes (or entails) the satisfaction of other properties in their immediate neighbourhood. Sider’s maximal properties are like this. That a is a rock tells us nothing at all about what other properties are instantiated in a’s world. However, that a is a rock does tell us something about what happens around a. In particular, it tells us that there is no rock enveloping a. If there were a rock enveloping a, then a would not be a rock, but rather a part of a rock. If being a rock were intrinsic, then we would expect there could be two rocks such that the first envelops the second.2 The reason that being a rock is extrinsic is that it violates this combinatorial principle. (As a corollary to this, a theory which ruled out being a rock from the class of the intrinsic just because it is somehow unnatural would be getting the right result for the wrong reason. Being a rock is not a property like being a lonely electron or an accompanied non-electron that satisfies the independence platitude in the wrong way; rather, it fails to satisfy the independence platitude, and our theory should reflect this.)\nSo we need a second combinatorial principle that rules out properties like being a rock. The following principle does the job, although at some cost in complexity. Assume there is some world w1, which has some kind of spacetimelike structure.3 Let d1 and d2 be shapes of two disjoint spacetimelike regions in w1 that stand in relation A. Further, suppose F and G are intrinsic properties such that in some world there is an F that wholly occupies a region with shape d1, and in some world, perhaps not the same one, there is a G that wholly occupies a region with shape d2. By ‘wholly occupies’ I mean that the F takes up all the ‘space’ in d1, and does not take up any other ‘space.’ (There is an assumption here that we can identify shapes of spacetimelike regions across possible worlds, and while this assumption seems a little contentious, I hope it is acceptable in this context.) If F, G, d1, d2 and A are set up in this way, then there is a world where d1 and d2 stand in A, and an F wholly occupies a region of shape d1 in that world, and a G wholly occupies a region of shape d2 in that world. In short, if you could have an F in d1, and you could have a G in d2, and d1 and d2 could stand in A, then all three of those things could happen in one world. This kind of combinatorial principle has been endorsed by many writers on modality (for example Lewis 1986 and Armstrong 1989), and it seems something we should endorse in a theory on intrinsic properties.\nIn sum, the set of intrinsic properties, SI, has the following four properties:\n\\(B\\)\nIf F \\({\\in}\\) SI and G \\({\\in}\\) SI then F and G \\({\\in}\\) SI and F or G \\({\\in}\\) SI and not F \\({\\in}\\) SI\n\n\\(M\\)\nIf F \\({\\in}\\) SI then Having n parts that are F \\({\\in}\\) SI and Being entirely composed of exactly n things that are F \\({\\in}\\) SI\n\n\\(T\\)\nIf F \\({\\in}\\) SI and G \\({\\in}\\) SI and there is a possible world with n+1 pairwise distinct things, and something in some world is F and something in some world is G, then there is a world with exactly n+1 pairwise distinct things such that one is F and the other n are G.\n\n\\(S\\)\nIf F \\({\\in}\\) SI and G \\({\\in}\\) SI and it is possible that regions with shapes d1 and d2 stand in relation A, and it is possible that an F wholly occupy a region with shape d1 and a G wholly occupy a region with shape d2, then there is a world where regions with shapes d1 and d2 stand in A, and an F wholly occupies the region with shape d1 and a G wholly occupies the region with shape d2.\n\nMany other sets than SI satisfy (B), (M), (T) and (S). That is, there are many sets Ik such that each condition would still be true if we were to substitute Ik for SI wherever it appears. Say that any such set is an I-set. Then F is intrinsic only if F is an element of some I-set. Is every element of every I-set intrinsic? As we will see, sadly the answer is no. However, most of the counterexamples proposed to Langton and Lewis’s theory are not elements of any I-set, so we already have the resources to show they are extrinsic.\nResponding to Counterexamples\nMarshall and Parsons noted that E, the property being such that a cube exists, is independent of accompaniment. However, it is not part of any I-set. To see this, assume it is in Ik, which is an I-set. By (B), not E is also in Ik. So by (T), there is a world where something is E, and there are two things, one of which is E and the other of which is not E. But clearly this cannot be the case: if something in a world is E, so is everything else in the world. Hence Ik cannot be an I-set, contrary to our assumption. Intuitively, E is extrinsic because whether it is satisfied by an individual is not independent of whether other individuals satisfy it.\nSome other quantificational properties, such as being one of at most seventeen cubes, require a different argument to show that they are not in any I-set. Call that property E17. (Note, by the way, that E17 is independent of accompaniment, and not obviously disjunctive.) If E17 is in an I-set, then by (T) there is a world containing exactly 18 things, each of which is E17. But this is clearly impossible, since everything that is E17 is a cube, and everything that is E17 is in a world containing at most seventeen cubes. So E17 is not in any I-set, and hence is extrinsic. Similarly, being the only round thing cannot be in an I-set, because if it were by (T) there would be a world in which two things are the only round thing, which is impossible. So a definition of intrinsicness in terms of I-sets need not make the odd postulations about naturalness that Yablo found objectionable.\nAssume, for reductio, that being a rock is in an I-set. There is a rock that is roughly spherical, and there is a rock that has a roughly spherical hollow in its interior. (Actually, there are many rocks of each type, but we only need one of each.) Let d1 be the region the first rock takes up, and assume that the shape of the hollow in the second is also d1. If it is not, we could always find another rock with a hollow this shape, so the assumption is harmless. Let d2 be the region the second rock, the one with this nicely shaped hollow, takes up. If being a rock is an I-set, then by (S) there is a world where d2 exactly surrounds d1, there is a rock wholly occupying d1 and a rock wholly occupying d2. But this is impossible; if there were rock-like things in both d1 and d2, they would both be parts of a single large rock, that extends outside both d1 and d2 and if there were not a rock-like thing in one or the other region, then there would not be a rock in that region. So no set satisfying (S) contains being a rock, so that property is not in any I-set, and hence is extrinsic.\nThe first extrinsic property independent of accompaniment that Langton and Lewis consider is CS: being spherical and lonely or cubical and accompanied. This too is not in any I-set. Again, assume for reductio that it is. In the actual world, there are (accompanied) cubes that are entirely composed of eight smaller cubes. Both the large cube and the eight smaller cubes are accompanied, so they are both CS. Hence there is a CS that is entirely composed of eight things that are CS. By (M), being entirely composed of exactly eight things that are CS is in the I-set. By (B), being CS and entirely composed of exactly eight things that are CS is in the I-set. So by (T), there is a world in which something has that property, and there is nothing else. (To see that (T) entails this, let G be any element of the I-set, and let n be zero.) That is, there is a lonely CS that is composed of eight things that are CS. But this is impossible. A lonely CS is a sphere, but its eight parts are not lonely, and are CS, so they must be cubes. And no sphere is entirely composed of exactly eight cubes. So CS cannot be in an I-set, and hence is extrinsic.\nProblem Cases and Disjunctive Properties\nThose five successes might make us think that only intrinsic properties are ever in I-sets. However there are still some extrinsic properties that can slip into I-sets. For an example, consider the property LCS, defined as follows:\n\nx is LCS \\({\\leftrightarrow}\\) (x is cubical and not both lonely and simple) or (x is lonely, simple and spherical)\n\nThe smallest set containing LCS and satisfying (B) and (M) is an I-set. There is an important reason for this. Define a simple world as a world containing just one mereological simple, and a compound world as a world that is not a simple world. Whether a property satisfies (T) and (S) (or, more precisely, whether a set containing that property can satisfy (T) and (S)) depends on just how the property interacts with other properties in compound worlds and whether it is ever instantiated in simple worlds. Since the same things are LCS as are cubical in compound worlds, these two properties, LCS and being cubical, interact with other properties in compound worlds in the same way. And each property is instantiated in simple worlds, although they are instantiated in different simple worlds. In sum, the properties are similar enough to be indistinguishable by (T) and (S), and that means we will not be able to show that LCS is extrinsic using just those considerations.\nAny property that agrees with an intrinsic property, like being cubical, in the compound worlds, and is somehow extended so it is instantiated in simple worlds, will be in an I-set. This is not just because we have not put enough restrictions on what makes an I-set. There are just no combinatorial principles we could deduce from the independence platitude that LCS violates. This is because any such principle would, like (T) and (S), be satisfied or not depending just on how the property interacts with other properties in worlds where there are things to interact with, i.e. the compound worlds, and whether it is instantiated in the simple worlds. It is to the good that our deductions from the independence platitude did not show that LCS is extrinsic, because in an important sense LCS, like all properties that agree with some intrinsic property in all compound worlds, satisfies the platitude.\nSo at this point appeal to disjunctive and non-disjunctive properties is needed. Intuitively, intrinsic properties are not only capable of being instantiated in all possible combinations with other intrinsic properties, they are capable of being so instantiated in the same way in all these possible combinations. We need to distinguish between the disjunctive and the non-disjunctive properties in order to say which properties are instantiated the same way in all these different combinations.\nIt might be thought at this stage that we could just adopt Langton and Lewis’s definition of the disjunctive properties. If that definition worked, we could say the basic intrinsic properties are the non-disjunctive properties that are in I-sets, then define duplication and intrinsicness as they do in terms of basic intrinsics. The definition does not, it seems, work as it stands because it does not show that LCS is disjunctive. This will be easier to follow if we name all the components of LCS, as follows: \\[\\begin{aligned}\n\\textit{C}~&=~\\textit{being cubical} \\\\\n\\textit{L}~&=~\\textit{being lonely} \\\\\n\\textit{M}~&=~\\textit{being simple} \\\\\n\\textit{H}~&=~\\textit{being spherical} \\\\\n\\textit{LCS}~&=~(\\textit{C}~\\&~{\\lnot}(\\textit{L} \\& \\textit{M}))~{\\vee}~(\\textit{L} \\&~\\textit{M}~\\&~\\textit{H})\\end{aligned}\\]\nLet us agree that LCS is not a natural property, if naturalness is an on/off state, or is very unnatural, if naturalness comes in degrees. On Langton and Lewis’s first definition, it is disjunctive if it is a disjunction of conjunctions of natural properties. This seems unlikely: \\({\\lnot}\\)(L & M) is not a natural property. This is the property of being in a compound world, hardly a natural property. Similarly, C & \\({\\lnot}\\)(L & M), being a cube in a compound world, is hardly natural either. We could insist that these properties are natural, but at this point Yablo’s complaint, that clear facts like the extrinsicness of LCS are being made to rest on rather obscure facts, like the putative naturalness of being in a compound world, returns to haunt us. (I assume, for the sake of the argument, that L & M & H is a natural property, though this assumption could be easily questioned.) On the second definition, LCS is disjunctive if it is much less natural than \\({\\lnot}\\)(L & M), or than C & \\({\\lnot}\\)(L & M). Again, it seems unlikely that this is the case. These properties seem rather unnatural. I have defined enough terms that we can state in the lexicon of this paper just what \\({\\lnot}\\)(L & M) amounts to, i.e. being in a compound world, but the apparent simplicity of this definition should not make us think that the properties are natural. It is true in natural languages that predicates that are easy to express are often natural, but this fact does not extend across to the technical language that is employed here.\nThe way out is to change the definition of disjunctive properties. A property is disjunctive, intuitively, if it can be instantiated in two quite different ways. Most properties of the form: (N1 & U1) \\({\\vee}\\) (N2 & U2), where N1 and N2 pick out distinct (relatively) natural properties, and U1 and U2 pick out distinct (relatively) unnatural properties that are independent of N1 and N2, will be like this. If we name this predicate F, there will be two quite different types of Fs: those that are N1 and those that are N2. Note that this will be true no matter how unnatural U1 and U2 are; provided some Fs are N1, and some are N2, there will be these two ways to be F. So I suggest we amend Langton and Lewis’s definition of disjunctiveness as follows:\n\nA property F is disjunctive iff it can be expressed as a disjunction of conjunctions, i.e.: (A11 & … & A1n) \\({\\vee}\\) … \\({\\vee}\\) (Ak1 & … & Akm) and in each disjunct, at least one of the conjuncts is much more natural than F.\n\nOn this definition it is clear that LCS is disjunctive, since it is much less natural than being cubical and than being spherical, and in its expression above, being cubical is one of the conjuncts in the first disjunct, and being spherical is one of the conjuncts in the second disjunct. These kinds of comparisons of naturalness do not seem contentious, or any less obvious than the conclusions about extrinsicness we use them to generate. Further, the new definition of disjunctiveness is not meant to be an ad hoc fix. Rather this requirement that only one conjunct in each disjunct need be much more natural than F seems to follow directly from the reason we introduced the concept of disjunctiveness to begin with. For each F that satisfies the combinatorial principle (either independence of accompaniment in Langton and Lewis’s theory, or being in an I-set in my theory), we wanted to know whether it only does this because there are two or more ways to be an F. If F satisfies the definition of disjunctiveness I offer here, it seems there are two or more ways to be an F, so the fact that it can be in an I-set should not lead us to believe it is intrinsic.\nUsing this definition of disjunctiveness, we can say that the basic intrinsic properties are those that are neither disjunctive nor the negation of a disjunctive property, and are in at least one I-set, then say duplicates are things that share all basic intrinsic properties, and finally that intrinsic properties are properties shared by all duplicates. There are two reasons for thinking that this definition might well work. First, as we have seen it handles a wide range of hard cases. More importantly, the way that the hard cases were falling gave us reason to suspect that the only extrinsic properties that will be in I-sets are properties like LCS: properties that agree with some intrinsic property in all compound worlds. It is reasonably clear that these properties will be disjunctive according to the above definition. To see this, let F be the extrinsic property in an I-set, and let G be the intrinsic property it agrees with in all compound worlds. Then for some J, F can be expressed as (G & \\({\\lnot}\\)(L & M)) \\({\\vee}\\) (L & M & J), and it will presumably be much less natural than G, probably much less natural than J, and almost certainly much less natural than being simple, our L. So if these are the only kind of extrinsic properties in I-sets, our definition is correct.\nIndeed, if these are the only kinds of extrinsic properties in I-sets, we may not even need to worry about which properties should count as disjunctive. Say that a property F blocks another property G iff both F and G are in I-sets, but there is no I-set containing both F and G. If F and G were both intrinsic, then there would be an I-set they are both in, such as say SI, so the fact that there is no such I-set shows that one of them is extrinsic. Note that LCS blocks being cubical. To prove this, assume LCS and being cubical are in an I-set, say Ik. By two applications of (B), LCS and not cubical is in Ik. This property is instantiated in some possible worlds: it is instantiated by all lonely spheres. So by (T) there should be a world containing two things that satisfy LCS and not cubical. But only lonely, simple spheres satisfy this property, so there is no world where two things satisfy it, contradicting our assumption that LCS and being cubical can be in the same I-set. The proof here seems perfectly general: if G is intrinsic and F differs from G only in which things in simple worlds satisfy it, and G is in an I-set, then F will block G. Blocking, as defined, is symmetric, so the fact that F blocks G is no evidence that F is extrinsic, as opposed to G. Still, if G is much more natural than F, then in all probability the reason F blocks G is that they agree about all cases in compound worlds, and disagree just about the simple worlds. In that case, it seems that F is extrinsic, and G is intrinsic. So I think the following conjecture has merit: F is intrinsic iff it is in an I-set and does not block any property much more natural than itself. If the conjecture works, the only kind of naturalness comparisons we need to make will be between properties like LCS and properties like being cubical. Again, I think these kinds of comparisons should be fairly uncontentious.\nBack to Basics?\nMost of the work in my theory is done by the concept of I-sets. It might be wondered whether we can do without them. In particular, it might be thought that the new definition of disjunctivenes I offer in ?5 will be enough to rescue Langton and Lewis’s theory from the objections I have been fretting about. Indeed, the new definition of disjunctiveness does suffice for responding to Yablo’s objection. However, it will not do on its own, and I think it will end up being essential to define intrinsicness in terms of I-sets.\nYablo notes that a property like being the only red thing is independent of accompaniment, and that the way Langton and Lewis suggest showing it is disjunctive is by expressing its negation as being red and accompanied by a red thing, or not being red. Yablo criticises the claim that the first of these disjuncts really is a natural property. Above I agreed that this was a good objection. However, on the new definition of disjunctiveness, it is beside the point.\nTo show that not being the only red thing is disjunctive, we need only express it as a disjunction of conjunctions such that at least one conjunct in each disjunct is much more natural than it is. We have the disjunctive expansion of not being the only red thing, and the first disjunct is being red and accompanied by a red thing. Now this disjunct as a whole may not be particularly natural, but the first conjunct, being red, is much more natural than not being the only red thing. So all we need to show is that one of the conjuncts in the second disjunct is much more natural than the whole disjunction. Since the second disjunct has only one conjunct, this means we have to show not being red is much more natural than not being the only red thing. However, there seems to be no simple way to show this. It is just entirely unclear how natural properties like not being red should seem to be. My guess (for what it is worth) is that like most properties that can be expressed by negations of English predicates, it is very unnatural. Certainly it is very unnatural if we suppose, as seems fair in this context, that F is only a natural property if all the things that are F resemble each other in some important way. The class of things that are not red is as heterogeneous a class as you can hope to find; blue berries, green leaves, silver Beetles, colourless gases and immaterial souls all find their way in. It is true that in New Work for a Theory of Universals, David Lewis provides two importantly distinct criteria for naturalness. One is the resemblance criterion just mentioned. The other is that F is only perfectly natural if it is fundamental. It might be thought that when we look at this criterion, it does turn out that not being red is much more fundamental than being the only red thing. Even if this is the case, it is not clear that it does help, or more importantly, that it should help. The problem Langton and Lewis were trying to handle is that not being the only red thing satisfies a particular combinatorial principle (independence of accompaniment), but only, they say, because there are two different ways of instantiating that property: not being red and being accompanied by a red thing. The problem is that not being red is not a way to instantiate a property, because it is not a way that something could be. It seems very intuitive that ‘ways things could be,’ in this sense, are resemblance properties: they are properties that make for resemblance amongst their instantiators. And even if we can defend the claim that not being red is a fundamental property, the fact that it is not a resemblance property seems to undercut Langton and Lewis’s case here.\nThe new definition of disjunctiveness does not provide a defender of Langton and Lewis’s theory with a response to Yablo’s criticism. On the new definition of disjunctiveness, we do not have to show that being red and accompanied by a red thing is more natural than not being the only red thing in order to show that the latter is disjunctive. However, in order to show that not being the only red thing is disjunctive, we still need to show that not being red is a moderately natural property, and this does not seem to be true.\nConclusion\nThere are four major differences between the analysis of intrinsic properties provided here and the one provided by Langton and Lewis. Three of these are reflected in the difference between the combinatorial principle they use, independence of accompaniment, and the combinatorial principle I use, membership in an I-set. All properties that are in I-sets are independent of accompaniment, but they also have a few other nice features. First, membership in an I-set guarantees not just independence of whether there are other things, but independence of what other types of things there are. This is the independence principle encoded in condition (T) on I-sets. Secondly, membership in an I-set guarantees independence of where the other things are. This is the principle encoded in condition (S). Third, the mereological principle (M) has no parallel in Langton and Lewis’s theory.\nThe effect of these extra three restrictions is that I have to make many fewer appeals to naturalness than do Langton and Lewis. The fourth difference between their theory and mine is in the role naturalness considerations play in determining which properties are intrinsic. In section 5 I offer two ways of finishing the analysis using naturalness. The first is in the new definition of disjunctiveness; with this definition in hand we can finish the story just as Langton and Lewis suggest. The second is in terms of blocking: F is intrinsic iff it is in an I-set and does not block any property that it is much less natural than. Both ways are designed to deal with a quite specific problem: properties that differ only in which things instantiate them in simple worlds have the same combinatorial features, so a definition of intrinsicness in terms of combinatorial features (as is Langton and Lewis’s, and as is mine) will not be able to distinguish them. Still, both solutions seem likely to provide the same answer in all the hard cases: the right answer.\n\n\nLangton, Rae, and David Lewis. 2001. “Marshall and Parsons on ‘Intrinsic’.” Philosophy and Phenomenological Research 63 (2): 353–55. https://doi.org/10.2307/3071068.\n\n\nMarshall, Dan, and Josh Parsons. 2001. “Langton and Lewis on ‘Intrinsic’.” Philosophy and Phenomenological Research 63 (2): 347–51. https://doi.org/10.2307/3071067.\n\n\nSider, Theodore. 2001. “Maximality and Intrinsic Properties.” Philosophy and Phenomenological Research 63 (2): 357–64. https://doi.org/10.1111/j.1933-1592.2001.tb00109.x.\n\n\nTaylor, Barry. 1993. “On Natural Properties in Metaphysics.” Mind 102 (405): 81–100. https://doi.org/10.1093/mind/102.405.81.\n\n\nYablo, Stephen. 1999. “Intrinsicness.” Philosophical Topics 26 (1): 479–505. https://doi.org/10.5840/philtopics1999261/234.\n\n\nIt would be no good to say that Langton and Lewis should be more liberal with their definition of disjunctiveness, and say instead that a property is disjunctive iff it can be expressed as a disjunction. Any property F can be expressed as the disjunction F and G or F and not G, or for that matter, F or F, so this would make every property disjunctive.\nI do not want to dismiss out of hand the possibility that there is another expression of S that shows it is disjunctive. Josh Parsons suggested that if we define T\\(^\\prime\\) to be being accompanied by a round thing, then S is T\\(^\\prime\\) \\({\\vee}\\) U, and there is some chance that T\\(^\\prime\\) is more natural than S on some conceptions of naturalness. So we cannot derive a decisive counterexample from Yablo’s discussion. Still, Langton and Lewis need it to be the case that on any account of naturalness, there is an expression that shows S or R to be disjunctive, and unless T\\(^\\prime\\) is much more natural than S on all conceptions of naturalness, this task is still far from complete.↩︎\nI assume here that there are rocks with rock-shaped holes in their interior. This seems like a reasonable assumption, though without much knowledge of geology I do not want to be too bold here.↩︎\nPerhaps all worlds have some kind of spacetimelike structure, in which case this qualification is unnecessary, but at this stage it is best not to take a stand on such a contentious issue.\n\n↩︎\n",
    "preview": "posts/2021-01-04-intrinsic-properties-and-combinatorial-principles/rock.jpg",
    "last_modified": "2021-02-04T22:00:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-07-begging-the-question-and-bayesians/",
    "title": "Begging the Question and Bayesians",
    "description": "In a recent article Patrick Maher shows that the `depragmatised' form of Dutch Book arguments for Bayesianism tend to beg the question against their most interesting anti-Bayesian opponents. I argue that the same criticism can be levelled at Maher's own argument for Bayesianism.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2001-04-01",
    "categories": [
      "epistemology",
      "games and decisions",
      "notes"
    ],
    "contents": "\n\nContents\nBayesianism\nDutch Book Arguments\nRepresentation Theorems\nConclusion\n\nThe arguments for Bayesianism in the literature fall into three broad categories. There are Dutch Book arguments, both of the traditional pragmatic variety and the modern ‘depragmatised’ form. And there are arguments from the so-called ‘representation theorems.’ The arguments have many similarities, for example they have a common conclusion, and they all derive epistemic constraints from considerations about coherent preferences, but they have enough differences to produce hostilities between their proponents. In a recent paper, Maher (1997) has argued that the pragmatised Dutch Book arguments are unsound and the depragmatised Dutch Book arguments question begging. He urges we instead use the representation theorem argument as in Maher (1993). In this paper I argue that Maher’s own argument is question-begging, though in a more subtle and interesting way than his Dutch Book wielding opponents.\nPublished in Studies in History and Philosophy of Science Part A 30: 687-697.\nPicture by ankakay via Creative Commons.\nBayesianism\nWhat’s a Bayesian? The term these days covers so many different positions that the only safe course is to strictly define what one means by the term. The alternative, as the discussion in Walley (1996) shows, is to have one of the least interesting semantic debates ever held. I define a Bayesian to be one who is committed to two theses, which I’ll call (B1) and (B2).\n(B1)\nBelief comes by degrees.\n\n(B2)\nIt is a requirement of consistency that these degrees of belief, or credences, be consistent with the probability calculus.\n\nI should explain (B2) a little. Historically, Bayesians held that credences were, or at least ought be, reals in [0, 1], and the function Bel which takes any proposition into the agent’s credence in that proposition should be a probability function. Modern Bayesians, following Levi (1980) and Jeffrey (1983), allow that credences can be imprecise. In this case the consistency requirement is that there be some precisification of their credences which is a probability function. (B2) is deliberately ambiguous between the traditional and modern Bayesian positions, largely because nothing in this debate turns on this question1.\nThere are many other properties that could have been used to define Bayesians. For example, it could be suggested that it is requirement of being a Bayesian that one think rules like (B2) are derived by analysing credences as dispositions to bet. This is suggested by Kaplan (1993, 320) to be the “fundamental Bayesian insight.” Or it could be argued that being a Bayesian requires some an extra rule to the effect that credences are updated by conditionalisation. I haven’t included that for two reasons. First, I’m mostly interested in static constraints on credences, and secondly, some paradigm Bayesians like Levi (1980) and Fraassen (1989) reject this rule in its fully general form. Finally it might be suggested that Bayesians aren’t those that believe certain principles like (B1) and (B2), but only those who think these principles provide the foundation for all philosophy of science. So perhaps my definition is a bit liberal.\nNow it is well known that not everyone’s a Bayesian. One group of non-Bayesians who have received too little attention from their Bayesian rivals are those who accept (B1) but not (B2). That is, theorists who agree there are such things as credences, and even that credences are important for philosophy of science, but not that they ought be constrained by the probability calculus. The most interesting example is the theory of evidence developed by Dempster (1967, 1968) and Shafer (1976)\nDempster and Shafer, like many other theorists, think that when we have no evidence either for or against p, we should have low credences in both p and \\({\\lnot}\\)p. In the limit case it is acceptable to have our credence in both p and in \\({\\lnot}\\)p set at zero. Now this is in conflict with (B2), for it is a theorem of the probability calculus that Pr(p) + Pr(\\({\\lnot}\\)p) = 1, and so by (B2) it is a requirement of rationality that Bel(p) + Bel(\\({\\lnot}\\)p) = 0.\nThis intuition about cases where the evidence is low is formalised in a rather neat theory. For simplicity I’ll say how the theory works when we are interested in finitely many propositions; Shafer shows the infinite case can be dealt with but it doesn’t raise any philosophically interesting differences. We are interested in n propositions, so the possibility space contains 2n ‘worlds.’ A proposition A can be identified in the usual ways with the set of worlds at which it is true. The Bayesian has us place a normalised measure on this possibility space, with our credence in A being the measure of the set A. In Dempster and Shafer’s theory we place a normalised measure, which they call a ‘mass function’ on the power set of the worlds, excluding the null set. Our credence in A is calculated as the measure of the set of sets which are subsets of A. So in the simplest case, where we are just interested in one proposition p, the mass function is defined on {{p}, {\\({\\lnot}\\)p}, {p, \\({\\lnot}\\)p}}. Complete ignorance is represented by giving {p, \\({\\lnot}\\)p} mass one, and the other sets mass zero. Hence both Bel(p) and Bel(\\({\\lnot}\\)p) are zero. On the other hand, Bel(p \\({\\vee}\\) \\({\\lnot}\\)p) will be one, as is Bel(C) for any classical tautology C. As a consequence of this we will not have the addition rule.\nAddition:\nFor disjoint A, B, Bel(A \\({\\vee}\\) B) = Bel(A) + Bel(B)\n\nSince Bayesians believe in Addition and some opponents do not, arguments for Bayesianism should be inter alia arguments for Addition. I don’t want to argue that Dempster and Shafer’s theory is right. It has some internal problems, particularly with updating, which make it look not too promising as a general theory of evidence. The recent collection edited by Yager, Fedrizzi, and Kacprzyk (1994) has papers dealing with many of these issues for the interested reader. My interest in this theory is merely to show the kind of theorist the Bayesian must argue against. This is particularly important when the alleged problem with arguments for Bayesianism is that they are question-begging.\nAs a last point about the Dempster-Shafer theory it might be noted that not only does Addition fail for credences, the equivalent rule for valuing bets also fails. Put formally, let an A-bet be a bet which pays £1 if A and nothing otherwise. It is consistent with the Dempster-Shafer theory to say the value of an A-bet is always £Bel(A). So on this theory it is not the case that for disjoint A, B it is true that the value of an (A \\({\\vee}\\) B)-bet always equals the value of an A-bet plus the value of a B-bet.\nSince it is sometimes thought there is an argument showing this to be incoherent, it is worthwhile giving a partial defence of its consistency. An argument like the following appears, as we’ll see, to be endorsed by Maher. There exists a voucher which is an (A \\({\\vee}\\) B)-bet, a ticket which is an A-bet and a coupon which is a B-bet. Now anyone holding the ticket and the coupon will receive exactly the same payout in all circumstances as anyone holding the voucher, hence they must have the same value. Hence the value of the voucher is the value of the ticket plus the value of the coupon. The problem with this argument is that it assumes the ticket and the coupon are not what economists call complementary goods. Some goods, like say compact discs, have more value to a consumer if they hold certain other goods, like compact disc players. On the Dempster-Shafer theory, the ticket and the coupon may well be complementary goods. To anyone holding the ticket, the value of the coupon is the difference between value of the voucher and the value of the ticket, that is, Bel(A \\({\\vee}\\) B) - Bel(A). This will in general be greater than its ‘intrinsic’ value Bel(B). But this goes no way to showing that its value to someone without the ticket must be greater than Bel(B). This, in rough outline, is the objection Schick (1986) makes to Dutch Book arguments. So arguments for Addition which assume that A-bets and B-bets are not complements will beg the question against proponents of the Dempster-Shafer theory, who have a principled reason to reject that assumption.\nDutch Book Arguments\nAs I mentioned above, there are three broad categories of arguments for Bayesianism, two breeds of Dutch Book arguments and ‘representation theorem’ style arguments. In this section I’ll briefly deal with the Dutch Book arguments before looking at Maher’s version of the representation theorem argument in section 3.\nThe classic, pragmatic, Dutch Book argument, assumes that appropriate circumstances exist whereby the amount an agent would be prepared to pay for any A-bet is £Bel(A). Indeed, they assume this not only is true, but that it would remain true while the agent starts trading in bets. Given these assumptions, if the agent’s credences are not a probability function, a clever bookie who knows just their credences can sell them a ‘Dutch Book’ which is guaranteed to lose in all circumstances.\nEveryone’s got their favourite objection to this argument, so I won’t spend much time on it here. Maher (1993, 98) argues that the declining marginal utility of money means that this won’t work for pounds, and since there is no currency with a constant marginal utility this flaw can’t be resolved. This is a rather odd objection since Savage (1954) argued long ago that we could get around this problem by using bets denominated in lottery tickets. As mentioned above, Schick (1986) takes the possibility of complementary bets to be a crushing blow to the argument. My favourite objection turns on the distinction that Adam Smith famously drew attention to between the usefulness of a good and its market value. Bel(A) can determine, at most, the usefulness of an A-bet, so at disequilibrium a coherent agent should probably not trade A-bets for £Bel(A). And since there’s Dutch Books to be sold we must be at disequilibrium, so the initial assumption about ‘appropriate circumstances’ must be false. In any case, there are enough different objections to this argument that we can safely move on.\nThe depragmatised Dutch Book arguments, as in Howson and Urbach (1989), Christensen (1996) and Hellman (1997), do away with the prospect of a bookie actually milking the poor incoherent agent. Rather they use similar reasoning to show that there is something wrong with an agent whose credences are not probability functions. I will concentrate on Christensen’s argument, but similar comments apply to the other two arguments.\nChristensen does not believe that an agent who’s credence in A is x should be prepared to buy a A-bet for £A. However he does say that the agent should “evaluate such [trades] as fair” (Christensen 1996, 456). So credences may ‘sanction’ (his term) certain odds even if the agent does not desire to accept these sanctioned bets. This may come about because of the declining marginal utility of the currency in which the bets are denominated, or because of a dislike of gambling, or possibly because of the discrepancy I mentioned between use-value and exchange-value. Now making the safe enough assumption that credences that sanction trades which lead to sure loss are defective he concludes that credences which do not satisfy the probability calculus are defective.\nHowever, Maher (1997, 301–3) points out, the argument so far doesn’t get the conclusion Christensen wants. Indeed for some simple Shafer functions which are not probability functions no sure-loss trades will be sanctioned. To get Christensen’s conclusion, we need the extra premise that if two trades are sanctioned their sum is sanctioned. Equivalently, we need the premise that what bets are sanctioned is independent of what bets are already held. But given the definition of ‘sanction’ this just is the premise that credences must satisfy Addition. So the argument is question-begging against the writer who denies Addition. Maher shows that similar problems beset the arguments in Howson and Urbach (1989) and Hellman (1997).\nRepresentation Theorems\nThe alternative Maher supports is based around ‘representation theorems.’ A similar approach is taken by Kaplan (1996), but I’ll focus on Maher. In any case, the issues that arise are exactly the same. The basic idea is that it is a requirement on the coherence of an agent’s preferences that there exist a probability function and a set of utility functions equivalent up to affine transformation such that the agent prefers gamble f to g iff the expected utility of f given the probability function and any of the utility functions is greater than that of g. The probability function will give us the agent’s credences in all propositions. Preferences are decreed to be coherent if they satisfy a number of axioms that Maher defends. For example, it is required that preferences be transitive, that an agent not prefer f to g and prefer g to f, and so on. The claim here is that the defence of these axioms begs the question against a supported of the Dempster-Shafer approach.\nStrictly Maher does not quite believe that all coherent credence functions are probability functions. The argument to that conclusion requires that preferences be complete, and Maher does not think this is plausible. If we drop that assumption we get the conclusion that the agent’s credences should be represented by sets of probability functions as in Levi and Jeffrey, not a single probability function. However for convenience he assumes first that completeness holds, and I’ll follow this lead. Nothing pertaining to the success or otherwise of the argument turns on this point.\nThere are a few immediate problems with this approach. Maher needs to assume that if an agent has a higher credence in p than in q they will prefer a p-bet to a q-bet. The problem is that when it is unlikely that we will ever see p or \\({\\lnot}\\)p confirmed we may well prefer a q-bet to a p-bet even if we have a higher degree of belief in p. I would prefer a bet on the Yankees winning the next World Series to a bet on Oswald being Kennedy’s assassin, even though I have a higher degree of belief in Oswald’s guilt than the Yankees’s success, because betting on the Yankees gives me some chance of getting a payout.\nMaher is aware of this point, but his attempt to dispose of it is disastrous. His example is comparing a bet on the truth of the theory of evolution, construed as the claim that all life on earth is descended from a few species, with betting on its negation Maher (1993, 89). Taking scientists as his expert function he asks some biologists which of these bets they would prefer, on the assumption that there are extraterrestrials who have been observing earth from its formation and will adjudicate on the bet. He is rather happy that they all plump for betting on Darwin. But this is a perfectly useless result. The objection was that we can have degrees of belief on unverifiable propositions, but our attitudes to bets on these propositions will be quite different to our attitude towards bets on verifiable propositions. He has attempted to counter this by simply making the problematic proposition verifiable. When we drop the assumption that there are extraterrestrials, so the theory of evolution would become unverifiable, presumably most people would (strictly) prefer a bet on a fair coin landing heads to either a bet on the theory of evolution or its negation. Preferences in a situation when the theory is verifiable are completely irrelevant to the problem unverifiable theories pose for Bayesian philosophies of science. So we have a problem, although I’d be prepared to accept for the sake of the argument it can be finessed.\nThe major problem for Maher is that his argument is just as question-begging as the Dutch Book arguments he criticises, though in a more subtle and interesting way. For Maher’s argument to work we have to accept some constraints on preferences, such as transitivity. His argument is only as strong as the argument for these constraints. He has nine axioms which must be justified in some way2. The most interesting is Independence, which he construes as follows. D is a set of gambles and X a set of propositions, f \\({\\leq}\\) g means the agent either prefers g to f or is indifferent between them, f \\({\\equiv}\\) g means that f and g are exactly the same gamble, they have the same payouts in all possible worlds, and f \\({\\equiv}\\) g on A means that on all possible worlds in which A, f and g have the same payouts.\nIndependence\nFor all f, f\\(^\\prime\\), g, g\\(^\\prime\\) \\({\\in}\\) D and A \\({\\in}\\) X, if f \\({\\equiv}\\) f \\(^\\prime\\) on A, g \\({\\equiv}\\) g\\(^\\prime\\) on A, f \\({\\equiv}\\) g on \\({\\lnot}\\)A, f \\(^\\prime\\) \\({\\equiv}\\) g\\(^\\prime\\) on \\({\\lnot}\\)A and f \\({\\leq}\\) g then f \\(^\\prime\\) \\({\\leq}\\) g\\(^\\prime\\) (Maher 1993, 190)\n\nThe idea is that if f and g have the same payouts given some proposition, say A, our preference between f and g should be independent of its value. All that matters, according to this idea, is the comparative fact that if A turns out true f and g have identical payouts, so whichever of the bets is preferred given \\({\\lnot}\\)A should be preferred overall. The most famous examples where intuition says this may be violated are the Allais and Ellsburg ‘paradoxes.’ Since uncertainty plays a larger role in it, I’ll briefly sketch the Ellsburg paradox. An urn contains 90 balls. Thirty of these are yellow, and remainder are either black or red in unknown proportion. The payouts for the four gambles in question are given in this table.\n\n \nRed\nBlack\nYellow\nf\n£1\n0\n0\ng\n0\n0\n£1\nf\\(^\\prime\\)\n£1\n£1\n0\ng\\(^\\prime\\)\n0\n£1\n£1\n\nMany subjects prefer g to f, since they know the chance of a yellow ball being drawn but not that of a red ball, but prefer f\\(^\\prime\\) to g\\(^\\prime\\) since they know the chance of a red or black ball being drawn but not that of a black or yellow ball. This is easily justifiable under the Dempster-Shafer approach. Let B, R and Y be the propositions that a black, red and yellow ball respectively is drawn. Given the evidence about the composition of the urn, it seems plausible to set Bel(B) = Bel(R)=0, Bel(Y)=Bel(Y\\({\\vee}\\)B) = Bel(Y\\({\\vee}\\)R) = \\(\\frac{1}{3}\\) and Bel(B\\({\\vee}\\)R) = \\(\\frac{2}{3}\\). Bayesians say this is coherent, but it is perfectly acceptable under a Dempster-Shafer theory. Given these credences, the value of f is £0, and the value of g is £\\(\\frac{1}{3}\\). However the value of f\\(^\\prime\\) is £\\(\\frac{2}{3}\\) while the value of g\\(^\\prime\\) is just £\\(\\frac{1}{3}\\). Hence it is not only acceptable, but arguably a requirement of rationality that an agent prefer g to f but f\\(^\\prime\\) to g\\(^\\prime\\).\nWith these preferences, and setting A to ‘A black ball is not drawn’ we can see this violates Maher’s independence axiom. No objection yet, many people are just irrational. The real problem arises with Maher’s argument that people who choose in this way are irrational. The following two choice trees set out two ‘tree form’ versions of the choices facing these subjects.\n\nThe left-hand tree represents the choice between f and g. The subject is told that if a black ball is drawn they will receive nothing, but if it is not drawn they will have a choice between betting on red and betting on yellow. So far we have a standard enough dynamic choice problem. Maher proposes to make it synchronic by requiring that subjects specify in advance what they would do if they reached the square, that is if a black ball is not drawn. This, he claims, makes the situation exactly as if the agent was choosing between f and g. Now the right-hand tree is the same as the left-hand tree in all respects but one. If a black ball is drawn the agent receives £1, not nothing. But the only choice the agent has to make is exactly the same as in the left-hand tree, so they ought make the same choice. We can concede to Maher here that it would be irrational to specify, in advance, a preference for g over f in the left-hand tree and for f\\(^\\prime\\) over g in the right-hand tree. This is, however, insufficient for his conclusion.\nThe problem lies in his assumption that “it seems uncontroversial that the consequences a person values are not changed by representing the options in a tabular or tree form” Maher (1993, 71). As Seidenfeld (1994) makes clear, this is exactly what is controversial in these circumstances. Indeed this premise, call it Reduction, is expressly denied by a number of heterodox decision theorists, and by writers who deny Addition on the occasions they talk about decision theory. There is a good reason for this. As noted above, on the Dempster-Shafer theory, Bel(B\\({\\vee}\\)R)may be greater than Bel(B)+Bel(R). When evaluating the worth of choosing f\\(^\\prime\\) in the original, tabular, it seems plausible that it is Bel(B\\({\\vee}\\)R) that matters, not Bel(B)+Bel(R). However in the tree form problem all that matters to f\\(^\\prime\\) is Bel(B), for the possibility that we won’t need to choose, and Bel(R), for the possibility that we do.\nThe point is that Maher has to either assume agents only consider Bel(B) and Bel(R) when assessing f\\(^\\prime\\), not Bel(B\\({\\vee}\\)R), or that Bel(B\\({\\vee}\\)R) is some function of Bel(B) and Bel(R) so that we can ignore that complication, in his ‘uncontroversial’ assumption. The first option is implausible, surely when comparing f\\(^\\prime\\) and g\\(^\\prime\\) we just compare Bel(B\\({\\vee}\\)R) with Bel(B\\({\\vee}\\)Y). More interestingly, I claim that the second is question-begging. Given that virtually everyone agrees that in some cases, for example lotteries, degrees of belief should be probability functions, in some cases the function which gives us Bel(B\\({\\vee}\\)R) from Bel(B) and Bel(R) must be addition. Hence he must assume that Bel(B\\({\\vee}\\)R) = Bel(B)+Bel(R) for the move from tabular to tree form to be plausible. But this is just what he was trying to prove, so the argument is question-begging.\nConclusion\nMaher rightly objects to depragmatised Dutch Book arguments on the ground that they are question-begging. That is, they use their conclusion as an implicit premise. It is argued here that the same objection applies to Maher’s argument for Bayesianism. He relies on the reducibility of tree form decisions to table form decisions, but the only justification for this could be a reliance on Addition. But Addition was what he was trying to prove all along, so he isn’t allowed to take Reduction as a premise.\nThere are three moves that Maher could make here. First, he could say that Reduction is so obvious that it should be acceptable as a premise without justification. The resulting argument may be effective at convincing some agnostics about Bayesianism that their implicit assumptions all along were Bayesian, but it would be completely ineffective against the sceptics about Bayesianism I have been discussing. Secondly, he could come up with a new argument for Reduction that I haven’t considered here and isn’t vulnerable to this objection. Given the conclusions of the last section I doubt this is possible, but the ingenuity of philosophers shouldn’t be underestimated. Thirdly, and most interestingly, he could look for justifications of Bayesianism that do not rely on construing credences as dispositions to bet. Since the arguments from considerations about preferences to constraints on credences have so far all failed, the time might be right to look at the problem from a different direction.\n\n\nChristensen, David. 1996. “Dutch-Book Arguments de-Pragmatized: Epistemic Consistency for Partial Believers.” Journal of Philosophy 93 (9): 450–79. https://doi.org/10.2307/2940893.\n\n\nDempster, Arthur. 1967. “Upper and Lower Probabilities Induced by a Multi-Valued Mapping.” Annals of Mathematical Statistics 38: 325–39. https://doi.org/10.1214/aoms/1177698950.\n\n\n———. 1968. “A Generalisation of Bayesian Inference.” Journal of the Royal Statistical Society Series B 30: 205–47.\n\n\nFraassen, Bas Fraassenvan. 1989. Laws and Symmetry. Oxford: Clarendon Press.\n\n\nHart, A. G. 1942. “Risk, Uncertainty and the Unprofitability of Compounding Probabilities.” In Studies in Mathematical Economics and Econometrics, edited by F. McIntyre O. Lange and T. O. Yntema., 110–18. Chicago: University of Chicago Press.\n\n\nHellman, Geoffery. 1997. “Bayes and Beyond.” Philosophy of Science 64 (2): 191–221. https://doi.org/10.1086/392548.\n\n\nHowson, Colin, and Peter Urbach. 1989. Scientific Reasoning. La Salle: Open Court.\n\n\nJeffrey, Richard. 1983. “Bayesianism with a Human Face.” In Testing Scientific Theories, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.\n\n\nKaplan, Mark. 1993. “Confessions of a Modest Bayesian.” Canadian Journal of Philosophy 23 (sup1): 315–37. https://doi.org/10.1080/00455091.1993.10717353.\n\n\n———. 1996. Decision Theory as Philosophy. Cambridge: Cambridge University Press.\n\n\nKeynes, John Maynard. 1937. “The General Theory of Employment.” Quarterly Journal of Economics 51 (2): 209–23. https://doi.org/10.2307/1882087.\n\n\nKnight, Frank. 1921. Risk, Uncertainty and Profit. Chicago: University of Chicago Press.\n\n\nLevi, Isaac. 1980. The Enterprise of Knowledge. Cambridge, MA.: MIT Press.\n\n\nMaher, Patrick. 1993. Betting on Theories. Cambridge: Cambridge University Press.\n\n\n———. 1997. “Depragmatised Dutch Book Arguments.” Philosophy of Science 64 (2): 291–305. https://doi.org/10.1086/392552.\n\n\nSavage, Leonard. 1954. The Foundations of Statistics. New York: John Wiley.\n\n\nSchick, Frederick. 1986. “Dutch Bookies and Money Pumps.” Journal of Philosophy 83 (2): 112–19. https://doi.org/10.2307/2026054.\n\n\nSeidenfeld, Teddy. 1994. “When Normal and Extensive Form Decisions Differ.” In Logic, Methodology and Philosophy of Science, edited by Brian Skyrms Dag Prawitz and Dag Westerståhl, 451–63. Amsterdam: Elsevier.\n\n\nShafer, Glenn. 1976. A Mathematical Theory of Evidence. Princeton: Princeton University Press.\n\n\nTintner, Gerhard. 1941. “The Theory of Choice Under Subjective Risk and Uncertainty.” Econometrica 9 (3/4): 298–304. https://doi.org/10.2307/1907198.\n\n\nWalley, Peter. 1996. “Inferences from Multinomal Data: Learning about a Bag of Marbles (with Discussion).” Journal of the Royal Statistical Society Series B 58: 3–57.\n\n\nYager, R., M. Fedrizzi, and J. Kacprzyk, eds. 1994. Advances in the Dempster- Shafer Theory of Evidence. New York: John Wiley.\n\n\nHistorically the impetus for allowing imprecise credences was the economic distinction between insurable and uninsurable risks, as discussed in Knight (1921), Keynes (1937), Tintner (1941) and Hart (1942).↩︎\nThere is a further axiom designed to guarantee countable additivity, but that raises independent problems and won’t be discussed here.\n\n↩︎\n",
    "preview": "posts/2021-01-07-begging-the-question-and-bayesians/begging.jpg",
    "last_modified": "2021-02-05T15:30:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-07-indicative-and-subjunctive-conditionals/",
    "title": "Indicative and Subjunctive Conditionals",
    "description": "In any plausible semantics for conditionals, the semantics for indicatives and subjunctives will resemble each other closely. This means that if we are to keep the possible‐worlds semantics for subjunctives suggested by Lewis, we need to find a possible‐worlds semantics for indicatives. One reason for thinking that this will be impossible is the behaviour of rigid designators in indicatives. An indicative like ‘If the stuff in the rivers, lakes and oceans really is H~3~O, then water is H~3~O’ is non‐vacuously true, even though its consequent is true in no possible worlds, and hence not in the nearest possible world where the antecedent is true. I solve this difficulty by providing a semantics for conditionals within the framework of two‐dimensional modal logic. In doing so, I show that we can have a reasonably unified semantics for indicative and subjunctive conditionals.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2001-04-01",
    "categories": [
      "logic",
      "language",
      "conditionals"
    ],
    "contents": "\n\nContents\nA Grand Unified Theory?\nThe New Theory\nActually\nThe Analysis of Indicatives\nMotivations\n\nThe Details\nNearness\nNo Nearest Possible World\nThe General Theory\nClassifying Conditionals\n\nConclusion\n\nThis paper presents a new theory of the truth conditions for indicative conditionals. The theory allows us to give a fairly unified account of the semantics for indicative and subjunctive conditionals, though there remains a distinction between the two classes. Put simply, the idea behind the theory is that the distinction between the indicative and the subjunctive parallels the distinction between the necessary and the a priori. Since that distinction is best understood formally using the resources of two-dimensional modal logic, those resources will be brought to bear on the logic of conditionals.\nPublished in Philosophical Quarterly 51: 200-216.\nPicture by Daniel Palmer via Creative Commons.\nA Grand Unified Theory?\nOur primary focus is the indicative conditional ‘If \\(A\\), \\(B\\),’ written as \\(A \\rightarrow B\\). Most theorists fail to distinguish between this conditional and ‘If \\(A\\), then \\(B\\),’ and for the most part I will follow this tradition. The most notable philosophical exception is Grice, who suggested that only the latter says that \\(B\\) follows from \\(A\\) in some relevant way (1989: 63). Theorists do distinguish between this conditional and the subjunctive ‘If it were the case that \\(A\\), it would be the case that \\(B\\),’ written as \\(A \\,\\square\\!\\mathord\\to B\\). There is some debate about precisely where to draw the line between these two classes, which I’ll discuss in section three, but for now I’ll focus on cases far from the borderline. One important tradition in work on conditionals holds that the semantics of indicatives differs radically from the semantics of subjunctives. According to David Lewis (1973, 1976) and Frank Jackson (1987) for example, indicatives are truth-functional, but subjunctives are not. This makes a mystery of some of the data. For example, as Jackson himself writes:\n\nBefore the last presidential election commentators said ‘If Reagan loses, the opinion polls will be totally discredited,’ afterwards they said ‘If Reagan had lost, the opinion polls would have been totally discredited,’ and this switch from indicative to subjunctive counterfactual did not count as a change of mind (Jackson 1987, 66).\n\nThe point can be pushed further. To communicate the commentators’ pre-election opinions using indirect speech we would say something like (1).\nCommentators have said that if Reagan were to lose the opinion polls would be totally discredited.\nYet it is possible on Jackson’s view that what the commentators said was true, since Reagan won, yet the words after ‘that’ in (1) form a false sentence. So we can accurately report someone speaking truly by using a false sentence. Jackson’s response plays on the connections between \\(A \\rightarrow B\\) and the disjunction ‘Not-\\(A\\) or \\(B\\).’ That disjunction has undeniably different truth conditions to \\(A \\,\\square\\!\\mathord\\to\\) B. Pushing the truth conditions of \\(A \\rightarrow B\\) closer to those of \\(A \\,\\square\\!\\mathord\\to\\) B will move them away from ‘Not- \\(A\\) or \\(B\\).’ One gain in similarity and theoretical simplicity is bought at the cost of another. Jackson’s account, by making \\(A \\rightarrow B\\) have similar truth conditions to ‘Not - \\(A\\) or \\(B\\)’ but similar assertibility conditions to \\(A \\,\\square\\!\\mathord\\to B\\), tries to have the best of both worlds. How great the similarity between indicative conditionals and disjunctions really is, and hence how great the cost of linking indicatives and subjunctives, might well be questioned. After all, we don’t report an utterance of an indicative using a disjunction.\nTwo types of cases seem to threaten the success of a unified theory. First, rigidifying expressions like ‘actually’ behave differently in indicatives and subjunctives. Secondly, some conditionals differ in intuitive truth value when we transpose them from the indicative to the subjunctive. The most famous examples of this phenomenon involve various presidential assassinations. The effects of rigidity on conditionals are less explored, so we will first look at that. Consider the following example, from page 55 of Naming and Necessity.\nIf heat had been applied to this stick \\(S\\) at \\(t_0\\), then at \\(t_0\\) stick \\(S\\) would not have been one meter long.\nThe background is that we have stipulated that a metre is the length of stick \\(S\\) at time \\(t_0\\). (2) contrasts with (3), which seems false.\nIf heat was applied to this stick \\(S\\) at \\(t_0\\), then at \\(t_0\\) stick \\(S\\) was not one meter long.\nIf we have stipulated that to be a meter long is to be the length of \\(S\\) at \\(t_0\\), then whatever conditions \\(S\\) was under at \\(t_0\\), it was one meter long. As Jackson points out, we can get the same effect with explicit rigidifiers like ‘actually.’ We could, somewhat wistfully, say (4). It may even be true. But (5) seems barely coherent, and certainly not something we could ever say.\nIf Hillary Clinton were to become the next U.S. President, things would be different from the way they actually will be.\nIf Hillary Clinton becomes the next U.S. President, things will be different from the way they actually will be.\nIt looks like any theory of conditionals will have to account for a difference between the behaviour of rigid designators in indicatives and subjunctives. We may avoid the conclusion by showing that the difference only appears in certain types of conditionals, and we already have an explanation for those cases. For example, it is well known that usually one cannot say \\(A \\rightarrow B\\) if it is known that not-\\(A\\). As Dudman (1994) points out, (6) is clearly infelicitous on its most obvious reading.\n*Granny won, but if she lost she was furious.\nTo complete the diagnosis, note that the most striking examples of the different behaviour of rigid designators in different types of conditionals comes up in cases where the antecedent is almost certainly false. The effect is that the subjunctive can be asserted, but not the indicative. So this phenomenon may be explainable by some other part of the theory of conditionals.1 These are the most striking exemplars of the difference I am highlighting, but not the only examples. Hence, this point cannot explain all the data, though it may explain why pairs like (2)/(3) and (4)/(5) are striking. For instance, in the following pairs, the indicative seems appropriate and intuitively true, and the subjunctive seems inappropriate and intuitively false.\nIf C-fibres firing is what causes pain sensations, then C-fibres firing is what actually causes pain sensations.\nIf C-fibres firing were what caused pain sensations, then C-fibres firing would be what actually causes pain sensations.\nIf the stuff that plays the gold role has atomic number 42, then gold has atomic number 42.\nIf the stuff that played the gold role had atomic number 42, gold would have atomic number 42.\nIn (9) and (10) I assume that to play the gold role one must play it throughout a large part of the world, and not just on a small stage. Something may play the gold role in a small part of the world without being gold. Since there are pairs of conditionals like these where the indicative is appropriate, but the subjunctive is not, the explanation of the behaviour of rigid terms cannot rely on the fact that the antecedents of indicatives must be not known to be false. We will also need a more traditional example of the differences between indicatives and subjunctives, as in (11) and (12).\nIf Hinckley didn’t shoot Reagan, someone else did.\nIf Hinckley hadn’t shot Reagan, someone else would have.\nI have concentrated on the examples involving rigidity because they seem to pose a deeper problem for unifying the theory of conditionals than the presidential examples. As Jackson (1987, 75) points out, one can presumably explain (11) and (12) on a possible worlds account by varying the similarity metric between indicatives and subjunctives, or on a probabilistic account by varying the background evidence. It is unclear, however, how this will help with the rigidity examples. Assume, for example, that C-fibres firing is not what causes pain sensations. Still, (7) seems true, but its consequent is false in all possible worlds. Therefore, the nearest world in which its antecedent is true is a world in which its consequent is false, and on a simple possible worlds theory it should turn out false. On a simple probabilistic account, the probability that C-fibres firing actually cause pain sensations given that they do is 1, whatever the background evidence, so (8) should turn out true, contrary to our intuitions. So while the details deal with the presidential examples, the structure of the theory must deal with the rigidity examples.\nI will follow that strategy here. In section two I set out the framework of a unified possible worlds account of indicatives and subjunctives. In section three I present my preferred way of filling out the details of that framework. The framework deals with the differing behaviour of rigid designators in indicatives and subjunctives; the details deal with examples like (11) and (12). One reason for dividing the presentation in this way is to highlight the option of accepting the framework and filling in the details in different ways.\nThe New Theory\nActually\nAs Kripke (1980) showed, the reference for some terms is fixed by what plays a particular role in the actual world. Even if it were the case that XYZ fills the ocean, falls from the sky, is drinkable and transparent and so on, for short is watery, it would still be the case that water is H2O, not XYZ. For it would still be that H2O actually is watery. Whatever were the case, this world would be actual.\nYet, we want to have a way to talk about what would have happened had some other world been actual. In particular, had the actual world been one in which XYZ is watery, it would be true, indeed necessarily true, that water is XYZ. Throughout the 1970s a number of methods for doing this were produced. The following presentation is indebted to Davies and Humberstone (1980), but other approaches might have been used. The notation \\(\\vDash_y^x A\\) is interpreted as ‘\\(A\\) is true in world \\(y\\) from the perspective of world \\(x\\) as actual.’ So, letting @ be the actual world and \\(w\\) be a world in which only XYZ is watery, we can represent what was said informally above as follows.\n\n\\(\\vDash_@^@\\) H2O is watery and H2O is water.\n\\(\\vDash_w^@\\) XYZ is watery and H2O is water.\n\\(\\vDash_@^w\\) H2O is watery and XYZ is water.\n\\(\\vDash_w^w\\)XYZ is watery and XYZ is water.\n\nNow as Kripke noted, it is necessary but a posteriori that water is H2O. Conversely, it is a priori but contingent that water is watery. This is a priori because we knew before we determined what water really is that it would be whatever plays the watery role in this world, the actual world. In general \\(A\\) is necessary iff, given this is the actual world, it is true in all worlds. And \\(A\\) is a priori iff, whatever the actual world turns out to be like, it makes \\(A\\) true. So we get the following definitions.\n\n\\(A\\) is a priori iff for all worlds \\(w\\), \\(\\vDash_w^w\\) \\(A\\).\n\\(A\\) is necessary iff for all worlds \\(w\\), \\(\\vDash_w^@\\) \\(A\\).\n\nThe connection between actuality and the a priori is important. It is a priori that we are in the actual world. Something is a priori iff it is true whenever the two indices are the same. If we regard possible worlds as sets of sentences, we can think of the sets {\\(A\\): \\(\\vDash_x^x\\) \\(A\\)} for each possible world \\(x\\) as the epistemically possible worlds. Note that I don’t make the set of epistemically possible worlds relative to an evidence set, as others commonly do. Rather they are just the sets of sentences consistent with what we know a priori. More accurately, identify a world pair \\(\\langle x\\), \\(y \\rangle\\) with the set of {\\(A\\): \\(\\vDash_y^x\\) \\(A\\)}. Then \\(\\langle x\\), \\(y \\rangle\\) is an epistemically possible world pair iff \\(x\\) = \\(y\\).\nTo finish this formal excursion, we note the definition of ‘Actually \\(A\\).’ Given what has been said so far, this needs no explanation.\n\n\\(\\vDash_y^x\\)Actually \\(A\\) iff \\(\\vDash_x^x\\) \\(A\\).\n\nThe Analysis of Indicatives\nNow we have the resources for my theory of the truth conditions for indicatives. I also give the parallel truth condition for subjunctives to show the similarities.\n\n\\(\\vDash_@^@\\)\\(A \\rightarrow B\\) iff the nearest possible world \\(x\\) that \\(\\vDash_x^x\\) \\(A\\) is such that \\(\\vDash_x^x\\) \\(B\\).\n\\(\\vDash_@^@\\) \\(A \\,\\square\\!\\mathord\\to B\\) iff the nearest possible world \\(x\\) that \\(\\vDash_x^@\\) \\(A\\) is such that \\(\\vDash_x^@\\)\\(B\\).\n\nThese only cover the special case of what is true here from the perspective of this world as actual. We can partially generalise the analysis of indicatives in one dimension as follows.\n\n\\(\\vDash_w^w\\) \\(A \\rightarrow B\\) iff the nearest possible world \\(x\\) to \\(w\\) such that \\(\\vDash_x^x\\) \\(A\\) is such that \\(\\vDash_x^x\\) \\(B\\).\n\nI will make some comments below about how we might fully generalise the analysis, but for now, I want to focus on these simpler cases. Note that straight away this makes \\(A \\rightarrow\\) Actually \\(A\\) come out true, by the definition of ‘Actually.’ If we allow ourselves quantification over propositions, we can give an analysis of ‘things are different from the way they actually are,’ as follows:\n\n(\\(\\vDash_y^x\\) Things are different from the way they actually are) iff\n(\\(\\exists\\)\\(p\\): \\(\\vDash_y^x\\) \\(p\\) and not \\(\\vDash_x^x\\) \\(p\\))\n\nSince nothing both is and is not the case in \\(x\\) from the perspective of \\(x\\) as actual, this can never be true when \\(y\\) is \\(x\\). This explains why it can never serve as the consequent of an indicative conditional.\nMotivations\nThe theory outlined here is reasonably unified, and accounts for the rigidity phenomena, but without any further justification, the resort to two-dimensional modal logic is ad hoc. This subsection responds to that problem with some independent motivations for the theory. In particular I argue that this theory best captures the well-known epistemic feel of the indicative conditional.\nEver since Ramsey (1929/1990) most theorists have held that there is an epistemic element to indicatives. Here is Ramsey’s sketch of an analysis of indicatives.\n\nIf two people are arguing ‘If \\(p\\) will q?’ and are both in doubt as to \\(p\\), they are adding \\(p\\) hypothetically to their stock of knowledge and arguing on that basis about q; so that in a sense ‘If \\(p\\), q’ and ‘If \\(p\\), \\(\\neg\\)q’ are contradictories (Ramsey 1929/1990, 247n).\n\nNothing of the sort could be true about subjunctives. What is in our ‘stock of knowledge,’ or the contextually relevant knowledge, makes at most an indirect contribution to the truth- value of a subjunctive. It makes an indirect contribution because the common knowledge might affect the context, which in turn determines the similarity measure. But given a context, a subjunctive makes a broadly metaphysical claim, an indicative a broadly epistemic claim. Hence, the relationship between the indicative and subjunctive should parallel the relationship between the necessary and the a priori. As should be clear, this is exactly what happens on this theory.\nThe close similarity between the indicative/subjunctive distinction and the a priori/necessary distinction can be demonstrated in other ways. For example, corresponding to the contingent a priori (13) the indicative (14) is true, but the subjunctive (15) is false. And corresponding to the necessary a posteriori (16) the subjunctive (17) is true but the indicative (18) is false. (I am assuming that it is part of the definitions of the water role and the fire role that nothing can play both roles.)\nWater is what plays the water role.\nIf XYZ plays the water role, XYZ is water.\nIf XYZ played the water role, it would be water.\nWater is H2O.\nIf all H2O played the fire role, all water would be fire.\nIf all H2O plays the fire role, all water is fire.\nThis suggests the analysis sketched here is not ad hoc at all, but follows naturally from considerations about the necessary and a priori. These sketchy considerations might not provide much positive support for my theory. The main evidence for the theory, however, is the way it manages the hard cases, particularly cases involving rigid designation. What these considerations show is that the correct theory of indicatives may invoke the resources of two-dimensional modal logic without automatically renouncing any claim to systematicity.\nThe Details\nIn this section, I want to look at four questions. First, what can we say about the similarity measure at the core of this account? Secondly, how should we generalise the theory to cover cases where the definite description in the analysis appears to denote nothing? Thirdly, how should we generalise the theory to cover cases where the two indices differ? Finally, how should we draw the line between indicatives and subjunctives? If what I said in the previous section is correct, there should be something to say about each of these questions, and what is said should be motivated. While it is not important that what I say here is precisely true, I do hope that it is.\nNearness\nIdeally, we could use exactly the same similarity metric for both indicatives and subjunctives. The existence of pairs like (11) and (12) suggests this is impossible. So we must come up with a pair of measures on the worlds satisfying three constraints. First, the measure for subjunctives must deliver plausible verdicts for most subjunctive conditionals. Secondly, the measure for indicatives must deliver plausible verdicts for most indicative conditionals. Thirdly, the measures must be similar enough that we can explain the close relationship between indicatives and subjunctives set out in section one. The theory of section two requires that these objectives be jointly satisfiable. I will attempt to demonstrate that they are by outlining a pair of measures satisfying all three.\nLewis (1979a) provides the measure for subjunctives. He suggests the following four rules for locating the nearest possible world in which A is true.\nIt is of the first importance to avoid big, widespread, diverse violations of law.\nIt is of the second importance to maximise the spatio-temporal region throughout which perfect match of particular fact prevails.\nIt is of the third importance to avoid even small, localized, simple violations of law.\nIt is of little or no importance to secure approximate similarity of particular fact, even in matters that concern us greatly. (Lewis 1979a, 47–48)\nThe right measure for indicatives is somewhat simpler. Notice that whenever we know that \\(A \\supset B\\) and don’t know whether \\(A\\), \\(A \\rightarrow B\\) seems true. More generally, if I know some sentence \\(S\\) such that \\(A\\) and \\(S\\) together entail \\(B\\), and I would continue to know \\(S\\) even were I to come to doubt \\(B\\), then \\(A \\rightarrow B\\) will seem true to me. No matter how good a card cheat I know Sly Pete to be, if I know that he has the worse hand, and that whenever someone with the worse hand calls they lose, it will seem true to me that If Sly Pete calls, he will lose. Further, if someone else knows these background facts and tells me that If Sly Pete calls, he will lose, she speaks truthfully.\nThis data suggests that whenever there is a true \\(S\\) such that \\(A\\) and \\(S\\) entail \\(B\\), \\(A \\rightarrow B\\) is true. But this would mean \\(A \\rightarrow B\\) is true whenever \\(A \\supset B\\) is true, which seems incredible. On this theory it is true that If there is a nuclear war tomorrow, life will go on as normal. There are some very subtle attempts to make this palatable. The ‘Supplemented Equivalence Theory’ in Jackson (1987) may even be successful. But two problems remain for all theories saying \\(A \\rightarrow B\\) has the truth value of \\(A \\supset B\\). First, they make some apparently true negated conditionals turn out false, such as It is not true that if there is a nuclear war tomorrow, life will go on as normal. It is hard to see how an appeal to Gricean pragmatics will avoid this problem. Secondly, such theories fail the third task we set ourselves at the start of the section: explaining the close connections between indicatives and subjunctives.\nSo we might be tempted to try a different path. Let’s take the data at face value and say that \\(A \\rightarrow B\\) is true in a context if there is some \\(S\\) such that some person in that context knows \\(S\\), and \\(A\\) and \\(S\\) together entail \\(B\\). We can formalise this claim as follows. Let \\(d\\)(\\(x\\), \\(y\\)) be the ‘distance’ from \\(x\\) to \\(y\\). This function will satisfy few of the formal properties of a distance relationship, so remember this is just an analogy. Let K be the set of all propositions \\(S\\) known by someone in the context, \\(W\\) the set of all possible worlds, and \\(i\\) the impossible world, where everything is true. Then \\(d\\): \\(W \\times W \\cup \\{i\\} \\rightarrow \\Re\\) is as follows:\n\nIf \\(y = x\\) then \\(d\\)(\\(x\\), \\(y\\)) = 0\nIf \\(y \\in W, y \\neq x\\) and \\(\\forall S\\): \\(S \\in\\) K \\(\\supset \\vDash_y^y\\) \\(S\\), then \\(d\\)(\\(x\\), \\(y\\)) = 1\nIf \\(y\\) = \\(i\\) then \\(d\\)(\\(x\\), \\(y\\)) = 2\nOtherwise, \\(d\\)(\\(x\\), \\(y\\)) = 3\n\nLess formally, the nearest world to a world is itself. The next closest worlds are any compatible with everything known in the context, then the impossible world, then the possible worlds incompatible with something known in the context. It may seem odd to have the impossible world closer than some possible worlds, but there are two reasons for doing this. First, in the impossible world everything known to any conversational participant is true. Secondly, putting the impossible world at this position accounts for some examples. This is a variant on a well known case; see for example Gibbard (1981) and Barker (1997).\nJack and Jill are trying to find out how their local representative Kim, a Democrat from Texas, voted on a resolution at a particular committee meeting. So far, they have not even found out whether Kim was at the meeting. Jack finds out that all Democrats at the meeting voted against the resolution; Jill finds out that all Texans at the meeting voted for it. When they return to compare notes, Jack can truly say If Kim was at the meeting, she voted against the resolution, and Jill can truly say If Kim was at the meeting, she voted for the resolution. If \\(i\\) is further from the actual world than some possible world where Kim attended the meeting, these statements cannot both be true.\nIt may be thought the distance function needs to be more fine-grained to account for the following phenomena2. It seems possible that in each of the following pairs, the first sentence is true and the second false.\nIf Anne goes to the party, so will Billy.\nIf Anne goes to the party, Billy will not go.\n\nIf Anne and Carly go to the party, Billy will not go.\nIf Anne and Carly go to the party, so will Billy.\n\nIf Anne, Carly and Donna go to the party, so will Billy.\nIf Anne, Carly and Donna go to the party, Billy will not.\n\nAssume, as seems plausible, it is necessary and sufficient for \\(A \\rightarrow B\\) to be true that the nearest \\(A \\wedge B\\) world is closer than the nearest \\(A\\wedge \\neg B\\) world. (This does not immediately follow from the analysis in section 2, but is obviously compatible with it.) Given this, there is no context in which the first conditional in each pair is true, and the second false. McCawley (1996) points out a way to accommodate these intuitions. Every time a conditional is uttered, or considered in a private context, the context shifts so as to accommodate the possibility that its antecedent is true. So at first we don’t consider worlds where Carly or Donna turn up, and agree that (19a) is true and (19b) false because in those worlds Billy loyally follows Anne to the party. When (20a) or (20b) is uttered, or considered, we have to allow some worlds where Carly goes to the party into the context set. In some of these worlds Anne goes to the party and Billy doesn’t, the worlds where Carly goes to party. A similar story explains how (21a) can be true despite (20b) being false.3\nThis move does seem to save the theory from potentially troubling data, but without further support it may seems rather desperate. There are two independent motivations for it. First, it explains the inappropriateness of (6).\n*Grannie won, but if she lost she was furious.\nIf assertion narrows the contextually relevant worlds to those where the assertion is true, as Stalnaker (1978) suggests, and uttering a conditional requires expanding the context to include worlds where the antecedent is true, it follows that utterances like (6) will be defective. The speech acts performed by uttering each clause give the hearer opposite instructions regarding how to amend the context set. Secondly, McCawley’s assumption explains why we generally have little use for indicative conditionals whose antecedents we know are false. To interpret an indicative we first have to expand the context set to include a world where the antecedent is true, but if we know the antecedent is false we usually have little reason to want to do that. If there is a dispute over the size of the context set, we may want to expand it so as to avoid miscommunication, which explains why we will sometimes assert conditionals with antecedents we know to be false when trying to convince someone else that the antecedent really is false.\nSo we have a pair of measures that give plausible answers on a wide range of cases. Such a pair should also validate the close connection between indicatives and subjunctives we saw earlier. The data set out in section one suggests that this connection may be close to synonymy, as in (1), but in some cases, as in (11) and (12), the connection is much looser. The differing behaviour of rigid designators in indicatives and subjunctives reveals a further difference, but the two-dimensional nature of the analysis, not the particulars of the similarity metric, accounts for that. I propose to explain the data by looking at which facts we hold fixed when trying to determine the nearest possible world. The facts we hold fixed in evaluating indicatives and subjunctives, according to the two metrics outlined above, are the same in just the cases we feel that the indicatives and subjunctives say the same thing.\nWhen evaluating an indicative we hold fixed all the facts known by any member of the conversation. When evaluating a subjunctive we hold fixed (a) all facts about the world up to some salient time t and (b) the holding of the laws of nature at all times after t. The time t is the latest time such that some worlds fitting this description make \\(A\\) true and contain no large miracles. The two sets of facts held fixed match when we know all the salient facts about times before t, and know no particular facts about what happens after t.\nIn the opinion poll case, when evaluating the original indicative our knowledge at the earlier time was held fixed. We knew that the polls predicted a Reagan landslide, that when one makes spectacularly false predictions one is discredited, and so on. When we turn to evaluating the subjunctive, we hold fixed the facts about the world before the election (presumably the relevant time t) and some laws. Therefore, we hold fixed the polls predictions, and the law that when one makes spectacularly false predictions one is discredited. So the same facts are held fixed. And in general, this will happen whenever all we know is all the specific facts up to the relevant time, and some laws that allow us to extrapolate from those facts.\nIn the case where indicatives and subjunctives come apart, as in (11) and (12), the relevant knowledge differs from the first case. By hypothesis, we do not know who pulled the trigger, but we do know that a trigger was pulled. Our knowledge of the relevant facts does not consist in knowledge of all the details up to a salient time, and knowledge that the world will continue in a law-governed way after this. Therefore, we would predict that the indicatives and subjunctives would come apart, because what is held fixed when evaluating the two conditionals differs. We find exactly that. So the pair of measures can explain the close connection between indicatives and subjunctives when it exists, and explain why the two come apart when they do come apart.\nNo Nearest Possible World\nGenerally, there are three kinds of problems under this heading. First, there may be no \\(A\\)-worlds, and so no nearest \\(A\\)-world. Secondly, there may be an infinite sequence of ever-nearer \\(A\\)-worlds without a nearest \\(A\\)-world. Thirdly, there may be several worlds in a tie for nearest \\(A\\)-world. If the measure suggested in the previous section is correct, the first two problems do not arise here. The third problem, however, arises almost all the time, so we need to say something about it.\nThe approach I favour is set out in Stalnaker (1981). The comparative similarity measure is a partial order on the possible worlds. Stalnaker recommends we assess conditionals using supervaluations, taking the precisifications to be the complete extensions of this partial order. In particular, if several possible worlds tie for being the closest \\(A\\)-worlds4, then \\(A \\rightarrow B\\) will be true if they are all \\(B\\)-worlds, false if they are all \\(\\neg B\\)-worlds, and not truth-valued otherwise. For consistent \\(A\\), this makes \\(\\neg\\)(\\(A \\rightarrow B\\)) equivalent to \\(A \\rightarrow \\neg B\\). Since we generally deny \\(A \\rightarrow B\\) just when we would be prepared to assert \\(A \\rightarrow \\neg B\\), this seems like a good outcome.5 Further, this account makes \\(A \\rightarrow B\\) generally come out gappy when A is false. Many theorists hold that indicative conditionals, especially those with false antecedents, lack truth values.6 This can’t be right in general, since it is a platitude that \\(A \\rightarrow A\\) is true for every \\(A\\), but the position has some attraction. Happily, our theory respects the motivations behind such positions without violating the platitude.\nIn any case, these details are not important to the overall analysis. If someone favours a resolution of ties along the lines Lewis suggested this could easily be appended onto the basic theory.\nThe General Theory\nSo far, I have just defined what it is for \\(A \\rightarrow B\\) to be true in this world from the perspective of this world as actual. To have a fully general theory I need to say when \\(A \\rightarrow B\\) is true in an arbitrary world from the perspective of another (possibly different) world as actual. And that general theory must yield the theory above as a special case when applied to our world. As with the special theory above, the general theory will mostly be derived from Twin Earth considerations.\nIn general, \\(\\vDash_y^x\\) \\(A \\rightarrow B\\) iff the nearest world pair \\(\\langle z, v \\rangle\\) such that \\(\\vDash_v^z\\) \\(A\\) is such that \\(\\vDash_v^z\\) \\(B\\). Nearness is again defined epistemically, but what we know about \\(x\\) and \\(y\\) matters. In particular if \\(\\vDash_v^z\\) \\(C\\) for all sentences \\(C\\) such that someone in the context knows that \\(\\vDash_y^x\\)\\(C\\) , but not \\(\\vDash_w^u\\) \\(C\\) for some such \\(C\\) , then \\(\\langle z, v \\rangle\\) is closer to \\(\\langle x, y \\rangle\\) than is \\(\\langle u, w \\rangle\\). As should be clear from this, nearness is context-dependent, and the context it depends on is the actual speaker’s context. For conditionals as for quantified sentences, the same words will express different propositions in different contexts.\nLet’s draw out some consequences of this definition. First, for any \\(x\\) we know that \\(\\vDash_x^x\\) \\(C\\) for all a priori propositions \\(C\\). In particular, we know that \\(\\vDash_x^x\\)\\(D \\equiv\\) (Actually \\(D\\)) for any proposition \\(D\\), where ‘\\(\\equiv\\)’ represents the material biconditional. So the nearest world pair \\(\\langle z, v \\rangle\\) to \\(\\langle x, x \\rangle\\) must be one in which \\(z = v\\), even if that means \\(z\\) is the impossible world \\(i\\). Hence the general theory of indicatives reduces to the special theory set out above when applied to epistemically possible worlds: when assessing the truth value of an indicative in an epistemically possible world pair we need only look at other epistemically possible world pairs.\nSecondly, when evaluating conditionals with respect to epistemically impossible world pairs \\(\\langle x, y \\rangle\\), we need to use other epistemically impossible world pairs. For example, imagine some explorers are wandering around Twin Australia, a dry continent to the south of Twin Earth. As explorers of such lands are wont to do, they are dying of thirst, so they are seeking some watery stuff to save themselves. Without knowing whether they succeed, we know (22) is false.\nIf the explorers find some watery stuff, they will find some water.\nThis theory can explain the falsity of (22). We know, from the way Twin Earth is stipulated, that all the watery stuff of the explorers’ acquaintance is not water. So we know any watery stuff they find will not be water. And we know that water is scarce on Twin Earth, even scarcer than watery stuff in Twin Australia, so it is unlikely they will find some watery stuff and simultaneously stumble across some water.\nThis theory also explains occurrences of indicatives embedded in subjunctives. These are very odd, as should be expected if indicatives are about epistemic connections and subjunctives about metaphysical connections, but we can just make sense of them some of the time. For example, it seems possible to make sense of (23) and that it is true.\nIf the bullet that actually killed JFK had instead killed Jackie Kennedy, then it would be true that if Oswald didn’t kill Jackie Kennedy, someone else did.\nOn our theory, to evaluate this we first find the nearest world pair \\(\\langle @, w \\rangle\\) such that \\(\\vDash_w^@\\) The bullet that actually killed JFK instead killed Jackie Kennedy, and then evaluate the indicative relative to it. Now one thing we know about this world pair is that in it, someone killed Jackie Kennedy. So this must hold in all nearby world pairs. Hence in any such world pair that Oswald did not kill Jackie Kennedy, someone else did, so (23) turns out true.\nIt might be thought that such embeddings do not make particularly good sense. I have some sympathy for such a view. If one adopts the ‘special theory’ developed in the previous section, and rejects the general theory developed in this subsection, one may have an explanation for the impossibility of such embeddings. However, even if we cannot make sense of such embeddings, we still need to account for the truth conditions of indicatives relative to epistemically impossible world pairs to make sense of claims such as Necessarily (\\(A \\rightarrow A\\)).7\nClassifying Conditionals\nIn recent years, there has been extensive debate over where the line between indicatives and subjunctives falls. This debate focuses on whether ‘future indicatives’ like (24) are properly classified with indicatives or subjunctives.\nIf Booth doesn’t shoot Lincoln, someone else will.\nJackson (1990) and Bennett (1995) argue that this should go with ordinary indicatives. Dudman (1994) and Bennett (1988) argue that it should go with ordinary subjunctives, though this is not how Dudman would put it. This theory of indicatives appears to favour Jackson and (the later) Bennett, because of the apparent triviality of conditionals like (25).\nIf it will rain then it will actually rain.\nConclusion\nDespite its lack of attention in the literature, data about the role of rigid designators in indicatives deserve close attention. Any plausible theory of indicatives must be able to deal with it, and it isn’t clear how existing possible worlds theories could do so. The easiest way to build a semantics for indicatives is to say that “If \\(A\\) then \\(C\\)” is true just in case the nearest world in which \\(A\\) is true is a world where \\(C\\) is true. Even before the hard questions about the meaning of ‘nearest’ here start to be asked, we know a theory of this form is wrong because it makes mistaken predictions about the role of rigid designators. A conditional like “If the stuff in the rivers, lakes and oceans really is XYZ, then water is XYZ” is true, even though the consequent is true in no possible worlds. The simplest way to solve this difficulty is to revisit the idea of ‘true in a world.’ Rather than looking for a nearby world in which \\(A\\) is true, and asking whether \\(C\\) is true in it, we look for a nearby world \\(w\\) such that \\(A\\) is true under the supposition that \\(w\\) is actual, and ask whether \\(C\\) is true under the supposition that \\(w\\) is actual. In the terminology of Jackson (1998), we look at worlds considered as actual, rather than worlds considered as counterfactual. This simple change makes an important difference to the way rigid designators behave. There is no world in which water is XYZ. However, under the supposition that the stuff in the rivers, lakes and oceans really is XYZ, and the H2O theory is just a giant mistake, that is, under the supposition that we are in the world known as Twin Earth, water is XYZ. In short, “water is XYZ” is true in Twin Earth considered as actual, even though it is false in Twin Earth considered as counterfactual. So the data about behaviour of rigid designators in indicatives, data like the truth of “If the stuff in the rivers, lakes and oceans really is XYZ, then water is XYZ,” does not refute the hypothesis that “If \\(A\\) then \\(C\\)” is true iff the nearest world such that \\(A\\) is true in that world considered as actual is a world where \\(C\\) is true in that world considered as actual.\nIn section two we looked at how the formal structure of a theory built around that hypothesis might look. In section three we looked at how some of the details may be filled in. The most pressing task is to provide a similarity metric so we can have some idea about which worlds will count as being nearby. The theory I defended has three important features. First, it is epistemic. Which worlds are nearby depends on what is known by conversational participants. Secondly, it is contextualist in two respects. The first respect is that it is the knowledge of the audience that matters, not just the knowledge of the speaker and the intended audience. The second respect is that it allows that what is known by the audience may be affected by the utterance of the conditional. In particular, if the utterance of “If \\(A\\), \\(B\\)” causes the audience to consider \\(A\\) to be possible, and hence cease to know that \\(\\neg A\\), then \\(A\\) is not part of what is known for purposes of determining which worlds are nearby. (I assume here a broadly contextualist account of knowledge, as in Lewis (1996), but this is inessential. If you do not like Lewis’s theory, replace all references to knowledge here, and in section 3.1, with references to epistemic certainty. I presume that what is epistemically certain really is contextually variable in the way Lewis suggests.) Thirdly, it is coarse- grained: whether a world is nearby depends only on whether it is consistent with what is known, not ‘how much’ it agrees with what is known. The resultant theory seems to capture all the data, to explain the generally close connection between indicatives and subjunctives, and to explain the few differences which do arise between indicatives and subjunctives.\nThe other detail to be filled in concerns embeddings of indicatives inside subjunctives. The formalism here requires that we use the full resources of two- dimensional modal logic, but the basic idea is very simple. Consider a sentence of the form “If it were the case that \\(A\\), it would be the case that if \\(B\\), \\(C\\) .” Roughly, this will be true iff the metaphysically nearest world in which \\(A\\) is true, call it \\(w_A\\), is a world where \\(B \\rightarrow C\\) is true. And that will be true iff the epistemically nearest world to \\(w_A\\) is which \\(B\\) is true is a world where \\(C\\) is true. Less roughly, we have to quantify not over worlds, but over pairs of worlds, where the first element of the pair determines the reference for rigid designators, and the second element determines the truth of sentences given those references. But this only adds to the formal complexity; the underlying idea is still the same. The important philosophical point to note is that when we are trying to find the epistemically nearest world to \\(w_A\\) (or, more strictly, the nearest world pair to \\(\\langle @, w_A \\rangle\\)) the facts that have to be held fixed are the facts that we know about \\(w_A\\), not what our counterparts in \\(w_A\\), or indeed what any inhabitant of \\(w_A\\) knows about their world. These embeddings may be rare in everyday speech, but since they are our best guide to the truth values of indicatives in other possible worlds, they are theoretically very important.\n\n\nBarker, Stephen. 1997. “Material Implication and General Indicative Conditionals.” The Philosophical Quarterly 47 (187): 195–211. https://doi.org/10.1111/1467-9213.00055.\n\n\nBennett, Jonathan. 1988. “Farewell to the Phlogiston Theory of Conditionals.” Mind 97 (388): 509–27. https://doi.org/10.1093/mind/xcvii.388.509.\n\n\n———. 1995. “Classifying Conditionals: The Traditional Way Is Right.” Mind 104 (414): 331–54. https://doi.org/10.1093/mind/104.414.331.\n\n\nDavies, Martin, and I. L. Humberstone. 1980. “Two Notions of Necessity.” Philosophical Studies 38 (1): 1–30. https://doi.org/10.1007/bf00354523.\n\n\nDudman, V. H. 1994. “Against the Indicative.” Australasian Journal of Philosophy 72 (1): 17–26. https://doi.org/10.1080/00048409412345851.\n\n\nEdgington, Dorothy. 1995. “On Conditionals.” Mind 104 (414): 235–327. https://doi.org/10.1093/mind/104.414.235.\n\n\n———. 1996. “Lowe on Conditional Probability.” Mind 105 (420): 617–30. https://doi.org/10.1093/mind/105.420.617.\n\n\nGibbard, Allan. 1981. “Two Recent Theories of Conditionals.” In Ifs, edited by William Harper, Robert C. Stalnaker, and Glenn Pearce, 211–47. Dordrecht: Reidel.\n\n\nJackson, Frank. 1987. Conditionals. Blackwell: Oxford.\n\n\n———. 1990. “Classifying Conditionals.” Analysis 50 (2): 134–47. https://doi.org/10.1093/analys/50.2.134.\n\n\n———. 1998. From Metaphysics to Ethics: A Defence of Conceptual Analysis. Clarendon Press: Oxford.\n\n\nKripke, Saul. 1980. Naming and Necessity. Cambridge: Harvard University Press.\n\n\nLewis, David. 1973. Counterfactuals. Oxford: Blackwell Publishers.\n\n\n———. 1976. “Probabilities of Conditionals and Conditional Probabilities.” Philosophical Review 85 (3): 297–315. https://doi.org/10.2307/2184045.\n\n\n———. 1979a. “Counterfactual Dependence and Time’s Arrow.” Noûs 13 (4): 455–76. https://doi.org/10.2307/2215339.\n\n\n———. 1979b. “Scorekeeping in a Language Game.” Journal of Philosophical Logic 8 (1): 339–59. https://doi.org/10.1007/bf00258436.\n\n\n———. 1996. “Elusive Knowledge.” Australasian Journal of Philosophy 74 (4): 549–67. https://doi.org/10.1080/00048409612347521.\n\n\nMcCawley, James. 1996. “Conversational Scorekeeping and the Interpretation of Conditional Sentences.” In Grammatical Constructions, edited by Masayoshi Shibatani and Sandra Thompson, 77–101. Oxford: Clarendon Press.\n\n\nRamsey, Frank. 1929/1990. “Probability and Partial Belief.” In Philosophical Papers, edited by D. H. Mellor, 95–96. Cambridge University Press.\n\n\nStalnaker, Robert. 1978. “Assertion.” Syntax and Semantics 9: 315–32.\n\n\n———. 1981. “A Defence of Conditional Excluded Middle.” In Ifs, edited by William Harper, Robert C. Stalnaker, and Glenn Pearce, 87–104. Dordrecht: Reidel.\n\n\nAn anonymous reviewer for Philosophical Quarterly suggested this point.↩︎\nLewis (1973) makes this objection to a similar proposal for subjunctives; the objection has just as much force here as it does in the original case.↩︎\nThere is an obvious similarity between this argument and some of the uses of contextual dependence in Lewis’s theory of knowledge (Lewis 1996). Indeed, McCawley credits Lewis (1979b) as an inspiration for his ideas.↩︎\nOf course in this context \\(x\\) is an \\(A\\)- world iff \\(\\vDash_x^x\\) \\(A\\).↩︎\nEdgington (1996) furnishes some nice examples against the view that \\(A \\,\\square\\!\\mathord\\to B\\) should be false when there are several equally close \\(A\\)-worlds in a tie for closest and some are \\(B\\)-worlds but some are \\(\\neg B\\)-worlds.↩︎\nSee Edgington (1995) for an endorsement of this position and discussion of others who have held it.↩︎\nI am indebted to Lloyd Humberstone for pointing this out to me.\n\n↩︎\n",
    "preview": "posts/2021-01-07-indicative-and-subjunctive-conditionals/water.jpg",
    "last_modified": "2021-02-05T15:27:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-07-keynes-uncertainty-and-interest-rates/",
    "title": "Keynes, Uncertainty and Interest Rates",
    "description": "Uncertainty plays an important role in *The General Theory*, particularly in the theory of interest rates. Keynes did not provide a theory of uncertainty, but he did make some enlightening remarks about the direction he thought such a theory should take. I argue that some modern innovations in the theory of probability allow us to build a theory which captures these Keynesian insights. If this is the right theory, however, uncertainty cannot carry its weight in Keynes's arguments. This does not mean that the conclusions of these arguments are necessarily mistaken; in their best formulation they may succeed with merely an appeal to risk.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2001-04-01",
    "categories": [
      "epistemology",
      "games and decisions",
      "philosophy of economics"
    ],
    "contents": "\n\nContents\nImprecise Probabilities\nKeynes and Imprecise Probabilities\nThe Economic Consequences of Uncertainty\nUncertainty and Money\nUncertainty and Liquidity Preference\nUncertainty and Indecision\nDisquietude\nSummary\n\nKeynes (1936) clearly saw an important role for uncertainty in his General Theory. However, few contemporaries agreed with him, and subsequent ‘Keynesians’ generally obliterated the distinction between risk and uncertainty. In part this was caused by Keynes’s informal presentation of his views on uncertainty in The General Theory. This paper has two aims. The first is to sketch a formal theory of uncertainty which captures Keynes’s insights about the risk/uncertainty distinction. I argue that the theory of imprecise probabilities developed in recent years best captures Keynes’s intuitions about uncertainty. In particular this theory provides a formal distinction between risk and uncertainty, and allows for an analysis of Keynes’s ‘weight’ of arguments. However, the second aim is to show that if this is right then Keynes was wrong to draw the economic consequences of uncertainty that he did. In broad terms, I argue that uncertainty is economically impotent. It only has effects in conjunction with some other feature of models or the world, such as missing markets or agent irrationality. But these features plus the existence of risk are sufficient to get the conclusions Keynes wants. These conclusions of Keynes might be right, but if so they can be justified without reference to Keynesian uncertainty. At the end of the day, uncertainty is not as economically interesting as it appears.\nPublished in Cambridge Journal of Economics 26: 47-62.\nPicture by Extra Medium via Creative Commons.\nImprecise Probabilities\nIn the classical, or Bayesian1, model of rationality all rational agents have precise degrees of belief, or credences, in each proposition. There is a probability function \\(Bel\\) such that for any proposition \\(A\\), there is a number \\(Bel(A)\\). So if an agent believes \\(p\\) to degree \\(x\\) she believes \\(p\\) to degree \\(1-x\\). This is appropriate for some propositions. For example, if \\(p\\) is a proposition about the decay of an atom with known half-life, or about any event with a known objective chance and hence subject to risk and not uncertainty, the agent’s credences should reflect the chances. Since chances are precise and form a probability function, the credences will also have these properties. The Bayesian theory assumes that all situations can be treated by analogy with these.\nAs Keynes pointed out in the famous QJE article (Keynes 1937b), this analogy is clearly mistaken. When \\(p\\) is about the price of copper in thirty years, we do not know the chance that \\(p\\) will be true. And we do not have enough information to form a precise credence. As Keynes had argued in his Treatise on Probability sixteen years earlier, attempts to avoid this problem by appeal to a Principle of Indifference lead to contradiction. In The General Theory he noted that he still approved of this little argument (Keynes 1936, 152). Hence Bayesians have no way of representing our ignorance in uncertain situations. They say that all rational agents have a precise epistemic attitude towards each proposition, believing it to some precise degree, whereas ignorance consists in not having such a precise attitude.\nThe theory of imprecise probabilities avoids all of these difficulties. The theory is quite old, dating back to work by Gerhard Tintner (1941) and A. G. Hart (1942), but has only received extensive consideration recently. The best modern summaries are by the philosopher Isaac Levi (1980) and the statistician Peter Walley (1991). There are minor differences, but the theory I shall give captures all the common ingredients. According to Bayesians, states of rational agents are represented by a single probability function \\(Pr\\); in the imprecise theory they are represented by a set of probability functions \\(S\\). The agent’s credence in \\(p\\) is vague over the set of values that \\(Pr(p)\\) takes for \\(Pr \\in S\\). In the extreme case, for every \\(x \\in [0, 1]\\) there will be a \\(Pr \\in S\\) such that \\(Pr(p) = x\\). This represents almost total ignorance about \\(p\\). The set \\(S\\) is called the ‘representor’ of the agent whose credences it represents.\nIt is important to stress what \\(S\\) represents, because there has been some confusion over this2. The \\(Pr\\) do not represent the agent’s hypotheses about the correct distribution of objective chances. I use the phrase ‘objective chance,’ or just ‘chance,’ to refer to a property that plays a certain role in fundamental physics, the property which makes it the case that the whirrings of atoms in the void is indeterminate. Modern physics, or at least the most popular versions of it, teaches that chance infects all fundamental physical events. These chances fulfill all the properties that anyone has ever wanted in probabilities. They reflect long-run frequencies of repeated events, they put restrictions on reasonable degrees of belief, they can be properly applied to single cases, and so on. If all fundamental physical events are chance events, then arbitrary Boolean combinations of fundamental physical events should also, presumably, be chance events. But any event whatsoever is some combination of fundamental physical events, though for many it may not be clear which combination. So baseball games, romantic affairs and stock market movements are all chance events in this sense, even though they are not, for instance, repeatable events. Of course, trying to predict these using the laws of physics will be even less productive than trying to predict them using the methods we currently have available. Saying where all the atoms, or quarks, currently are is humanly impossible, and perhaps theoretically impossible as well. Even allowing for this, computing where they will move before they get there is beyond the capacity of any possible machine.\nI distinguish between a situation where the agent does not know the objective chance of some proposition, and a situation where the agent has no precise credence in that proposition. An agent can have a precise credence in \\(p\\) without knowing its objective chance. If the agent believes that a certain number of chance distributions are possible, and gives each of them a precise credence, this entails she has a precise credence in each event. (Imagine we see a fair coin be tossed, and land, but do not see how it falls. The objective chance that it shows heads is either one, if it does, or zero, otherwise. But the appropriate credence in the proposition, the coin has landed heads, is one half.) Rather the \\(Pr\\) represent the precise credence distributions that are consistent with real imprecise distribution. For example, for some rational agent, and some proposition \\(p\\), the agent’s epistemic state will determine that she believes \\(p\\) to a greater degree than 0.2, and a lesser degree than 0.4, but there will be no more facts about the matter. (In this case \\(S\\) will include a function \\(Pr\\) such that \\(Pr(p) = x\\) for each \\(x \\in [0.2,~0.4]\\).) If we ask her whether she thinks \\(p\\) is more likely than some proposition, call it \\(q\\), which she believes to degree 0.3, she will not be able to say one way or the other. And this is not just because she lacks rationality or powers of introspective observation. It is no requirement of rationality that she believe \\(p\\) is more likely, less likely or equally likely than \\(q\\) As Levi and Walley have pointed out, the Bayesian arguments purporting to show this is a constraint on rationality have been hopelessly circular.\nThe reasons for wanting to be able to represent uncertainty were stressed by Keynes, and are generally well known. Before showing why this theory captures Keynes’s intuitions about uncertainty, I will briefly mention two nice formal features of the theory of imprecise probabilities. On many theories of uncertainty, particularly those that represent uncertain agents as having interval valued degrees of belief, it is hard to explain comparative statements, like “\\(p\\) seems more likely to me than \\(q\\).” These comparatives are crucial to our everyday practices of probabilistic reasoning. We say \\(p\\) is more probable than \\(q\\) according to \\(S\\) iff for all \\(Pr \\in S, Pr(p) > Pr(q)\\). This lets us say, as seems right, that \\(A\\) is more probable than \\(A \\wedge B\\) for almost all propositions \\(A, B\\).\nThe second formal advantage is that we now have a simple way to update epistemic states on receiving new evidence. Let \\(S\\) be the agent’s current representor, and the new evidence be \\(e\\). Then the updated representor, \\(S_e\\) is given as follows: \\[S_e = \\{Pr(\\bullet | e): Pr \\in S\\}\\]\nThat is, we just conditionalise every probability function in \\(S\\). Again, updating has proven problematic for some approaches to uncertainty. The theory of evidence functions, developed by Dempster (1967) and Shafer (1976) allows that an agent can know that if either \\(e\\) or \\(\\neg e\\) comes in as evidence, their credence in \\(p\\) will rise. This seems absurd; we can know before an experiment that whatever happens we’ll be more confident in \\(p\\) than we are now.\nTo take a famous example, three prisoners \\(X\\), \\(Y\\) and \\(Z\\) are about to be exiled to Elba. The governor decides on a whim that he will pardon one, and casts a fair die to choose which. He tells the guards who is pardoned, but instructs them not to tell the prisoners yet. \\(X\\) pleads futilely with his guard, and finally asks, “Can you tell me the name of one of the others who won’t be pardoned.” The guard, realising this will not reveal \\(X\\)’s fate, agrees to answer. \\(X\\) thinks that if \\(Y\\) is pardoned, the guard will say \\(Z\\), so there is at least a one-third probability of that. And if \\(Z\\) is pardoned, the guard will say \\(Y\\), so there is also at least a one-third probability of that. But if he is pardoned, what the guard will have to decide what to say, and we can’t make probability judgements about free human decisions. On the Dempster-Shafer theory, the probability of \\(X\\) being freed is one-third, but the probability of \\(X\\) being freed and the guard saying \\(Y\\) goes to Elba is zero, and the probability of \\(X\\) being freed and the guard saying \\(Z\\) goes to Elba is zero. This is just a standard failure of additivity, and not at all objectionable. The problem is that when the guard says that \\(Y\\) will go to Elba, or that \\(Z\\) will go to Elba, the probability of \\(X\\) being freed rises to one-half. (I will not go through the mathematics here, because it can be found in any book on the Dempster-Shafer theory. See, for example, Walley (1991) or Yager, Fedrizzi, and Kacprzyk (1994).) Since \\(X\\) did not learn about his chances of freedom, this seems like a rather odd result. The theory of imprecise probabilities avoids this problem. It can be easily shown that on this theory for any evidence \\(e\\) if the probability of \\(p\\) given \\(e\\) is greater than the probability of \\(p\\), then the probability of \\(p\\) given \\(\\neg e\\) is less than the probability of \\(p\\). (Again Walley (1991) contains the proof.)\nKeynes and Imprecise Probabilities\nObviously enough, this is not the theory that Keynes formally endorses, either in his Treatise on Probability (Keynes 1921) or his economic writings. Nevertheless, I think it is an important theory for understanding Keynes’s use of uncertainty. This is because it, and it alone, captures all of the underlying motivations of Keynes’s theory of uncertainty. Hence any economic consequences of uncertainty Keynes wants to draw will have to be derivable from this theory.\nI have so far spoken blithely of ‘Keynes’s theory of uncertainty,’ implicitly assuming there is such a unique theory. In recent years a number of authors (e.g. Runde (1994a; Davis 1994; Coates 1996; Bateman 1996) have questioned this assumption, saying that Keynes changed his theory between the writing of the Treatise on Probability and The General Theory. I will not deal directly with such criticisms here for a number of reasons. First, the main dispute is over whether probabilities are given by logic or are ‘merely subjective,’ and that debate is independent of the debate about the effects of allowing imprecise probabilities. Secondly, there are obvious space constraints. Many of these alternative interpretations were put forward in book length arguments, and a fair response to them would not be short. Thirdly, and perhaps most importantly, I take it that the methodological game here is inference to the best explanation. Whatever criticisms I make of others’ interpretations would be rather weak unless I showed that some other overall story was more persuasive. And if I come up with a more persuasive story here criticisms of their accounts will be slightly redundant. So I hope the reader at least permits the indulgence of setting out a theory of Keynes’s ideas predicated on this rather controversial assumption.\nIn the Treatise on Probability (Keynes (1921), hereafter TP) Keynes says that probability is essentially a property of ordered pairs of propositions, or what he calls arguments. He writes \\(p / q = \\alpha\\), for the probability of hypothesis \\(p\\) on evidence \\(q\\) is \\(\\alpha\\). Now this value \\(\\alpha\\) is rather unusual. It sometimes is a number, but sometimes not; it sometimes can be compared to all numbers, but sometimes not; it sometimes can be compared to other probability values such as \\(\\beta\\), but sometimes not and it can enter into arithmetic operations. As a consequence probabilities are subject to all the usual rules of the classical probability calculus. For example, whenever \\(p\\) and \\(r\\) are inconsistent, then \\((p \\vee r) / q = p / q + r / q\\) always holds, even when none of these values is numerical.\nThese five properties are rather perplexing. Indeed, Keynes’s failure to explain or justify them fully is one of the main criticisms that Ramsey (Ramsey 1926, 161–66) launches at Keynes’s theory. But on this theory they all fall out as consequences of our definitions. If \\(p/q = \\alpha\\) then \\(\\alpha\\) will be numerical iff there is some \\(x\\) such that for all \\(Pr \\in S, Pr(p | q) = x\\). Similarly \\(\\alpha > y\\), for real valued \\(y\\), iff \\(Pr(p | q) > y\\) for all \\(Pr \\in S\\). A similar definition holds for \\(\\alpha < y\\) and \\(\\alpha = y\\), from which it can be seen that it is possible that \\(\\alpha\\) is neither greater than, less than, nor equal to \\(y\\). If none of these hold we say that \\(\\alpha\\) and \\(y\\) are incomparable. If \\(p / q = \\alpha\\) and \\(r / s = \\beta\\) then \\(\\alpha > \\beta\\) iff for all \\(Pr \\in S, Pr(p | q) > Pr(r | s)\\). Again similar definitions of less than and equal to apply, and the consequence of all these is that sometimes \\(\\alpha\\) and \\(\\beta\\) will be comparable, sometimes not.\nRamsey is right to question the intelligibility of Keynes’s use of addition and multiplication. We know what it means to add and multiply numbers, but we have no idea what it is to add or multiply non-numerical entities. However, on this theory addition and multiplication are perfectly natural. Since we represent \\(\\alpha\\) and \\(\\beta\\) by sets, generally intervals, then \\(\\alpha + \\beta\\) and \\(\\alpha \\dot \\beta\\) will be sets. They are defined as follows. Again let \\(p / q\\ = \\alpha\\) and \\(r / s = \\beta\\).\n\\[\\begin{aligned}\n\\alpha + \\beta &= \\{x: \\exists Pr \\in S (Pr(p | q) + Pr(r | s) = x)\\} \\\\\n\\alpha \\dot \\beta &= \\{x: \\exists Pr \\in S (Pr(p | q) \\dot Pr(r | s) = x)\\} \\end{aligned}\\]\nThese definitions are natural in the sense that we are entitled to say that the ‘+’ in \\(\\alpha + \\beta\\)means the same as the ‘+’ in 2 + 3. And the definitions show why Keynes’s \\(\\alpha\\)’s and \\(\\beta\\)’s will obey the axioms of the probability calculus. Even if \\(p / q\\) and \\(\\neg p / q\\) are non-numerical, \\(p / q + \\neg p / q\\) will equal {1}, or effectively 1. So we have something like the additivity axiom, without its normal counterintuitive baggage. The main problem with additivity is that sometimes we may have very little confidence in either \\(p\\) or \\(\\neg p\\), but we are certain that \\(p \\vee \\neg p\\). If we measure confidence by the lower bound on these probability intervals, this is all possible on our theory. Our technical apparatus removes much of the mystery behind Keynes’s theory, and fends off an important objection of Ramsey’s.\nThe most famous of Keynes’s conceptual innovations in the TP is his introduction of ‘weight.’ He does this in the following, relatively opaque, paragraph.\n\nAs the relevant evidence at our disposal increases, the magnitude of the probability of the argument may either decrease or increase, according as the new knowledge strengthens the unfavourable or the favourable evidence; but something seems to have increased in either case, – we have a more substantial basis upon which to rest our conclusion. I express this by saying that an accession of new evidence increases the weight of an argument. New evidence will sometimes decrease the probability of an argument, but it will always increase its ‘weight’ (Keynes 1921, 77, italics in original).\n\nThe idea is that \\(p / q\\) measures how the evidence in \\(q\\) is balanced between supporting \\(p\\) and supporting \\(\\neg p\\). The concept of weight is needed if we want to also know how much evidence there is. Note that weight only increases when relevant evidence comes in, not when any evidence comes in. The weight of the argument from my evidence to “Oswald killed JFK” is not increased when I discover the Red Sox won last night.\nThe simplest definition of relevance is that new evidence \\(e\\) is irrelevant to \\(p\\) given old evidence \\(q\\) iff \\(p / q \\wedge e)= p / q\\), and relevant otherwise. Now there is a problem. Two pieces of evidence \\(e_1\\) and \\(e_2\\) can be irrelevant taken together, but relevant taken separately. For a general example, let \\(e_1\\) be \\(p \\vee r\\) and \\(e_2\\) be \\(\\neg p \\vee r\\), for almost any proposition \\(r\\). If I receive \\(e_1\\) and \\(e_2\\) sequentially, the weight of the argument from my evidence to \\(p\\) will have increased twice as I receive these new pieces of evidence. So it must be higher than it was when I started. But if I just received the two pieces of evidence at once, as one piece of evidence, I would have properly regarded it as irrelevant. Hence the weight in question would be unchanged. So it looks as if weight depends implausibly not on what the evidence is, but on the order in which it was obtained.\nKeynes avoids this implausibility by tightening up the definition of irrelevance. He says that \\(e\\) is irrelevant to \\(p / q\\) iff there are no propositions \\(e_1\\) and \\(e_2\\) such that \\(e\\) is logically equivalent to \\(e_1\\ \\wedge e_2\\) and either \\(e_1\\) or \\(e_2\\) is relevant to \\(p / q\\). Unfortunately, as I noted in the previous paragraph for virtually any such evidence proposition there will be such propositions \\(e_1\\) and \\(e_2\\). This was first noticed by Carnap (1950). Keynes, had he noticed this, would have had three options. He could conceded that everything is relevant to everything, including last night’s baseball results to the identity of Kennedy’s assassin; he could have conceded that the order in which evidence appears does matter, or he could have given up the claim that new relevant evidence always increases the weight of arguments.\nThe last option is plausible. Runde (1990) defends it, but for quite different reasons. He thinks weight measures the ratio of evidence we have to total evidence we believe is available. Since new evidence might lead us to believe there is much more evidence available than we had previously suspected, the weight might go down. I believe it holds for a quite different reason, one borne out by Keynes’s use of uncertainty in his economics. In The General Theory (Keynes (1936), hereafter GT) Keynes stresses the connection between uncertainty and ‘low weight’ (GT: 148n). If we regard \\(p\\) as merely risky the weight of the argument from our evidence to \\(p\\) is high, if we regard \\(p\\) as uncertain the weight is low. In the Quarterly Journal of Economics article he argues that gambling devices are, or can be thought to be, free of uncertainty, whereas human actions are subject to uncertainty. So the intervention of humans can take a situation from being risky to being uncertain, and hence decrease the weight in question.\nFor example, imagine we are playing a rather simple form of poker, where each player is dealt five cards and then bets on who has the best hand. Before the bets start, I can work out the chance that some other player, say Monica, has a straight. So my credence in the proposition Monica has a straight will be precise. But as soon as the betting starts, my credence in this will vary, and will probably become imprecise. Do those facial ticks mean that she is happy with the cards or disappointed? Is she betting high because she has a strong hand or because she is bluffing? Before the betting starts we have risk, but no uncertainty, because the relevant probabilities are all known. After betting starts, uncertainty is rife.\nThe poker example supports my analysis of weight. If weight of argument rises with reduction of uncertainty, then in some rare circumstances weight of arguments decreases with new evidence. Let \\([x_1, x_2]\\) be the set given by \\(\\{x: Pr(p | q) = x\\}\\) for some \\(Pr \\in S\\}\\), where \\(S\\) is the agent’s representor. Then the weight of the argument from \\(p\\) to \\(q\\), for this agent, is \\(1-(x_2 - x_1)\\). That is, the weight is one when the agent has a precise degree of belief in \\(p\\), zero when she is totally uncertain, and increasing the narrower the interval \\([x_1, x_2]\\) gets. Now in most cases new relevant evidence will increase the weight, but in some cases, like when we are watching Monica, this will not happen. I follow Lawson (1985) in saying that \\(p\\) is uncertain for an agent with evidence \\(q\\) iff \\(p / q\\) is non-numerical, i.e. iff the weight of the argument from \\(q\\) to \\(p\\) is less than one. Hence we get the connection between uncertainty and weight Keynes wanted. I also claim that the bigger \\(x_2 - x_1\\) is, the more \\(p / q\\) is unlike a real number, the more uncertain \\(p\\) is. Keynes clearly intended uncertainty to admit of degrees Keynes (1937b), so this move is faithful to his intent.\nKeynes’s theory of probability is based around some non-numerical values whose nature and behaviour is left largely unexplained, and a concept of weight which is subject to a telling and simple objection. Nevertheless, his core ideas, that probabilities can but need not be precise, and that we need a concept like weight as well as just probability, both seem right for more general reasons. Hence the theory here, which captures the Keynesian intuitions while explaining away his mysterious non-numerical values and making the concept of weight more rigorous, looks to be as good as it gets for a Keynesian theory of uncertainty.\nOne particularly attractive feature of the account is how conservative it is at the technical level. We do not need to change our logic, change which things we think are logical truths, or which things follow from which other things, in order to support our account of uncertainty. This is in marked contrast to accounts based on fuzzy logic or on logics of vagueness. Not only are such changes in the logic unmotivated, they appear to lead to mistakes. No matter how uncertain we are about how the stock will move over the day, we know it will either close higher or not close higher; and we know it will not both close higher and not close higher. The classical laws of excluded middle and non-contradiction seem to hold even in cases of massive uncertainty. This seems to pose a serious problem for theories of uncertainty based on alternative logics. The best approach is one, like the theory here, which is innovative in how it accounts for uncertainty, and conservative in the logic it presupposes.\nSo as a theory of uncertainty I think this account has a lot to be said for it. However, it cannot support the economic arguments Keynes rests on it.\nThe Economic Consequences of Uncertainty\nUncertainty can impact on the demand for an investment in two related ways. First, it can affect the value of that particular investment; secondly, it can affect the value of other things which compete with that investment for capital. The same story is true for investment as a whole. First, uncertainty may reduce demand for investment directly by making a person who would otherwise be tempted to invest more cautious and hence reluctant to invest. Secondly, if this direct impact is widespread enough, it will increase the demand for money, and hence its price. But the price of money is just the market rate of interest. And the return that an investment must be expected to make before anyone, even an investor not encumbered by uncertainty, will make it is the rate of interest.\nWhen uncertainty reduces investment by increasing interest rates, I will say it has an indirect impact on investment. Keynes has an argument for the existence of this indirect impact. First, he takes the amount of consumption as a given (GT: 245). Or more precisely, for any period he takes the amount of available resources that will not be allocated to consumption as a given. There are three possible uses for these resources: they can be invested, they can be saved as bonds or loans, or they can be hoarded as money. There are many different types of investment, but Keynes assumes that any agent will already have made their judgement as to which is the best of these, so we need only consider that one. There will also be many different length bonds which the agent can hold. So as to simplify the discussion, Keynes proposes just treating these two at a time, with the shorter length bond called ‘money’ and the longer length loan called ‘debts’ (GT: 167n). Hence the rate of interest is the difference between the expected return of the shorter bond over the life of the longer bond and the return of the longer bond. So the rate of interest that we are interested in need not be positive, and when the two bond lengths are short will usually be zero. However, it is generally presumed in discussions that the rate is positive. Now, Keynes assumes that an agent will only allocate resources to investment if investment looks to be at least as worthwhile as holding money, and at least as worthwhile as holding debts. In other words, he makes the standard reduction of \\(n\\)-way choice to a set of 2-way choices3. Usually if someone is of a mind to invest they will not favour holding money over holding debts. The only motivation for holding money, given positive interest rates, could be a desire to have accessible command over purchasing power, and investment foregoes that command. So in practice we only need look at two of the three possible pairwise choices here. Hence I will ignore the choice between investing and holding money, and only look at the money-debt choice and the debt-investment trade-off.\nHolding a debt provides a relatively secure return in terms of money. Relatively secure because there is the possibility of default. In practice, this means that there is not a sharp distinction between debts and investments, rather a continuum with say government bonds at one extreme and long-term derivatives at the other. Some activities that have the formal structure of ‘debts,’ like say provision of venture capital, will be closer to the investment end of the continuum. Unlike debts then, investments as a rule do not have a secure return in terms of money. In most cases they do not even have a precise expected return (GT: 149; Keynes (1937b, 113)). Keynes does not presume that this means that people never invest unless the expected return on the investment is greater than the expected (indeed, known) return on debts. He says explicitly that were this true then ‘there might not be much investment.’ Instead, he says that investment under uncertainty depends on ‘confidence’ (GT: 150). Therefore, the following looks compatible with his position.\nBayesians say that each gamble has a precise expected value. The expected return on a bet that pays $1 if some fair coin lands heads is 50 cents. On this theory, expected values are imprecise, because probabilities are imprecise. Formally, say \\(E_{Pr}(G) = \\alpha\\) means that the expected return on \\(G\\) according to probability function \\(Pr\\) is \\(\\alpha\\). Roughly, the expected value for an agent of a gamble \\(G\\) will be \\(\\{x: \\exists Pr \\in S: (E_{Pr}(G) = x)\\}\\), the set of expected values of the bet according to each probability function in the agent’s representor. Note that these are different from the possible outcomes of the bet. As we saw in the case of the coin, expected value can differ from any possible value of the bet. So let the expected value of inesting a certain sum be \\([\\alpha, \\beta]\\), and the expected value of buying a debt with that money be \\(\\chi\\). Then the agent will invest iff \\((1 - \\rho)\\alpha + \\rho \\beta \\geq \\chi\\), where \\(\\rho \\in [0, 1]\\) measures the ‘state of confidence.’4 Now when a crisis erupts, \\(\\rho\\) will go to 0, and investment will dry up. In such cases the decision theory is similar to the one advanced by Levi (1980), Strat (1990) and Jaffray (1994). Since we are interested in a theory of unemployment, we are primarily interested in the cases where \\(\\rho\\) is quite low, in which cases we can say uncertainty is reducing investment.\nThat last statement might seem dubious at face value. In part, what I mean by it is this. When \\(\\rho\\) is low the value of a set of bets will in general be more than the sum of the value of the bets taken separately. Because individual investors are fearful of exposure to uncertainty, which is presumably what \\(\\rho\\) being low means, sets of investments which if undertaken collectively would be profitable (and everyone agrees that they would) will not be undertaken individually. This suggests a reason that theorists have thought government intervention might be appropriate in times of crisis. Alternatively, if \\(\\rho\\) is low then the value of an investment, how much we will be prepared to pay for it, will probably be lower than our best estimate of its expected return, assuming the latter to be near \\((\\alpha + \\beta) /2\\).\nI shall focus more closely on the indirect effects of uncertainty in section 5. The central idea is that the rate of interest, being the price of money, is completely determined in the market for money. However, this market has some rather strange properties. After all, money is barren, and it can generally be traded for something that is not barren. So, as Keynes puts it, why would anyone ‘outside a lunatic asylum,’ want it? Why would the demand for money not drop to zero as soon as the rate of interest is positive?\n\nBecause, partly on reasonable and partly on instinctive grounds, our desire to hold money as a store of wealth is a barometer of the degree of our distrust of our own calculations and conventions concerning the future ... The possession of actual money lulls our disquietude; and the premium which we require to make us part with money is the measure of the degree of our disquietude (Keynes 1937b, 116).\n\nTherefore, more uncertainty means more demand for money means higher interest rates. The rest of the story is standard. Even the confident agent will be disinclined to invest once the rate of interest rises. Using the little decision theory outlined above, more uncertainty means the gap between \\(\\alpha\\) and \\(\\beta\\) grows, which if \\(\\rho\\) is low will tend to reduce \\((1-\\rho)\\alpha + \\rho \\beta\\), the ‘certainty equivalent’ of the expectation of the investment’s worth. On the other hand, uncertainty on the part of the community will tend, for similar reasons, to increase \\(\\chi\\). Either way, investment suffers, and hence so does employment.\nUncertainty and Money\nThere is something very odd about all that we have done so far. Agents react to uncertainty by making their returns measured in dollars more stable. However, in doing so they make their returns measured in any other good less stable. If you have no idea what the price of widgets will be in twelve months time, then holding only widgets increases the uncertainty about how many dollars you will be worth then. However, it makes you more certain about how many widgets you will be worth. Why this preference for money? We deserve an explanation as to why one kind of uncertainty is given such a central place and other kinds are completely ignored.\nKeynes has one explanation. He argues, or perhaps assumes, essentialism about money. Indeed the title of chapter 17 of The General Theory is ‘The Essential Properties of Interest and Money.’ These essential properties are entirely functional. As Hicks puts it, “Money is defined by its functions ... money is what money does” (Hicks 1967, 1). The explanation is that agents try to minimise uncertainty relative to whatever plays the functional role of money. Therefore, the explanation does not rely on any mystical powers of dollar bills. Rather, the work is done by the functional analysis of money.\nAs a first approximation, the functional role money plays is that it is a medium of exchange. Keynes does not think this is quite the essential property; rather he says that money is essentially ‘liquid,’ and perceived to be liquid. This means that if we hold money we are in a position to discharge obligations and make new purchases as they seem appropriate with greatest convenience and least cost. Even this is not what is given as the official essential property of money. To make the proof that demand for money is not demand for labour easier Keynes takes the essential properties of money to be its negligible elasticities of production and substitution. However, he makes clear that these are important because of their close connection to liquidity (GT: 241). Indeed, when he comes to define a non-monetary economy, he simply defines it as one where there is no good such that the benefits it confers via its liquidity, its ‘liquidity premium’ exceeds the carrying costs of the good. So the properties of having a negligible elasticity of production and substitution seem necessary but insufficient for something to be money.\nThe reason that money uncertainty is more problematic than widget uncertainty is just that money is liquid. At the end of the day, the point of holding investments, bonds or money is not to maximise the return in terms of such units; it is to be used somehow for consumption. Hence, we prefer, ceteris paribus, to store wealth in ways that can be easily exchanged for consumption goods as and when required. Further, we may be about to come across more information about productive uses for our wealth, and if we do, we would prefer to have the least inconvenience about changing how we use wealth. Money is going to be the best store of wealth for each of these purposes. The strength of these preferences determines the liquidity premium that attaches to money.\nSo Keynes’s story here is essentially a ‘missing markets’ story. If there were markets for every kind of transaction there would be no liquidity premium attaching to money, and hence no reason to be averse to uncertainty in terms of money returns as opposed to uncertainty in terms of X’s shares returns. There is a methodological difference here between decision theorists and economists. In decision theory it is common to specify what choices an agent does have. These will usually be finite, or at least simply specified. In economics it is more common to specify what choices an agent does not have, which markets are ‘missing.’ In a sense the difference is purely cosmetic, but it can change the way problems are looked at. Since Keynes requires here some markets to be missing, it might be worth investigating what happens here from the more restrictive framework ordinarily applied in decision theory.\nIn some decision-theoretic contexts, we can prefer liquidity even when we are completely certain about what our choices are and what their outcomes will be. Say we are in a game where the object is to maximise our money over 2 days. We start with $100. On day 1, we have a choice of buying for $100 a ticket that will pay $200 at the end of day 2, and is non-transferable, or doing nothing. On day 2, if we still have our $100, we can buy with it a voucher which pays $300 at the end of day 2, or do nothing. Obviously, the best strategy is to do nothing on day 1, and buy the voucher on day 2. The point is just that money here has enough of a liquidity premium on day 1 that we are prepared to hold it and earn no interest for that day rather than buy the ticket (or two day bond) which will earn interest. So uncertainty is not a necessary condition for liquidity premia to exist. On the other hand, perhaps it is necessary for liquidity premia to exist in a world something like ours, where agents neither have all the choices they would have in a perfect market, nor as few as in this simple game. If we added a market for tickets and vouchers to our simple game the prices would be fixed so that money would lose its liquidity premium. Keynes suggests something like this is true for the worlds he is considering: “uncertainty as to the future course of the rate of interest is the sole intelligible explanation of the type of liquidity preference \\[under consideration\\]” (GT: 201). However here he merely means lack of certainty; there is no proof that if every agent had precise credences liquidity preference ought to disappear. So it looks like uncertainty in the sense discussed here, vague reasonable beliefs, does no theoretical work. Perhaps this is a bit quick, as the little game I considered is so far from a real-life situation. So I will look more closely at the effects uncertainty is supposed to have. Since it has received the bulk of the theoretical attention, I start with the indirect effects of uncertainty.\nUncertainty and Liquidity Preference\nKeynes thinks the question of why money is demanded at all, why we do not all move from holding money into holding debts as soon as the rate of interest goes positive, needs answering. And he thinks the answer here will be particularly relevant to theories about the rate of interest. If the market in general is at equilibrium then the market in trades between any two goods must also be in equilibrium; in particular it cannot be that there are people holding money who would be prepared to buy debts at the current interest rate. So if the equilibrium interest rate is positive, there must be some people who would prefer to hold money than hold debts. This fact Keynes takes to be central to the correct theory of the rate of interest. Hence, to determine what the rate of interest will be, and what will cause it to change, I need to determine what causes a demand for money.\nKeynes distinguishes four motives for holding money (GT: Ch. 13; (Keynes 1937a, 215–23)). Two of these, the transactions motive and the finance motive, need not detain us. They just relate to the need to make payments in money and on time. The third, the speculative motive, is often linked to uncertainty, and indeed Keynes does so (GT: 201). But ‘uncertainty’ here is just used to mean absence of certainty, that is the existence of risk, which as noted above is not how I am using ‘uncertainty.’ As Runde (1994b) points out, an agent who is certain as to future movements in interest rates may still hold money for speculative reasons, as long as other agents who are not so certain have made mistaken judgements. The fourth motive will hold most of my attention. Keynes argues that we may hold money for purely precautionary reasons.\n\nTo provide for contingencies requiring sudden expenditure and for unforeseen opportunities of advantageous purchases, and also to hold an asset of which the value is fixed in terms of money to met a subsequent liability fixed in terms of money, are further motives for holding cash (GT: 196).\n\nDavidson (1988, 1991) justifies this as follows. Uncertainty arises whenever agents do not have sufficient knowledge to calculate the numerical probability of an event. This is given a rather frequentist gloss in Davidson, but that is not necessary. His idea is that we know what the probability of \\(p\\) is when we know the frequency of \\(p\\)-type events in the past and we know the future will resemble the past in this respect. The latter is cashed out as saying \\(p\\) is governed by an ‘ergodic process.’ We can replace all this by saying that \\(p\\) is subject to uncertainty whenever we do not know its objective chance, whether or not objective chance ought to be analysed by frequentist approaches. Davidson then argues that since for most \\(p\\) we do not have this knowledge, we have to adopt ‘sensible’ approaches like holding money.\nRunde (1994b) objects that Davidson’s story is incoherent. On Davidson’s theoretical story there are only two epistemic states relative to \\(p\\) that are possible. An agent can know the chance of \\(p\\), in which case their credence is set equal to it, or they are completely uncertain about it. In the latter case there can be no reason for taking some action rather than another. Now the reason that it is ‘sensible’ to hold money is that we expect money to be liquid. However, we do not know the chance of money remaining liquid; whether or not money remains liquid is not determined by an ergodic process. Hence, we have no reason for letting that partial belief be a guide to action.\nThis is a fair criticism, but it can be met by amending the theory rather than by giving it up. On my theory, if an agent knows the chance of \\(p\\) they will have a precise degree of belief in \\(p\\). When they do not their degree of belief will, in general, be vague but not totally vague. As with Keynes, I have uncertainty come in degrees. This amendment is enough to rescue Davidson’s theory. An agent might not know the chance that money will become illiquid in the next short period of time, but they might know enough for it to be reasonable to have a credence in that proposition which is vague over some small interval close to zero. It may still be sensible to hold some money even when the expected return on other investments really is vague. But is it sensible to prefer fixed to uncertain returns? In other words, is there a direct effect of uncertainty that makes people prefer bonds to investments?\nUncertainty and Indecision\nAs Keynes repeatedly stressed, investment is not like a game of chance where the expected results are known in advance. And this is part of the explanation for the extreme instability in investment levels compared to other economic variables.\n\nThe state of long-term expectation ... does not solely depend on the most probable forecast we can make. It also depends on the confidence with which we make this forecast (GT: 148).\nHuman decisions affecting the future, whether personal or political or economic, cannot depend on strict mathematical expectation, since the basis for making such calculations does not exist ... it is our innate urge to activity which makes the wheels go round, our rational selves choosing between the alternatives as best we are able, calculating where we can, but often falling back for our motive on whim or sentiment or chance (GT: 162-3).\n\nThe most charitable reading of Keynes here is to say he agreed, in principle, with what is sometimes referred to as a Horwitz-style decision rule. If the expected return of an investment is vague over \\([\\alpha, \\beta]\\) then its ‘value’ is given by \\((1-\\rho)\\alpha + \\rho \\beta\\), where \\(\\rho \\in [0, 1]\\) is a measure of confidence. By the 1937 article, he has become more interested in the special case where confidence has collapsed and \\(\\rho\\) is approaching 0. This interpretation would explain all his references to decision-making under uncertainty in The General Theory and subsequent discussion, provided we make the safe assumption that ‘cold calculation’ would only have us spend \\(x\\) on an investment with expected return \\([\\alpha, \\beta]\\) when \\(\\alpha \\geq x\\). In particular, any interpretation of the underlying decision theory here will have to give some role to ‘whim or sentiment or chance,’ and I give it a variable, ‘\\(\\rho\\).’ With this theory, I have the extensions needed to avoid Runde’s objection to Davidson. I have a continuum of degrees of uncertainty, rather than a raw dichotomy, and I have an explanation of why it is ‘sensible’ to prefer gambles with known expected returns, at least when \\(\\rho\\) is relatively low.\nThis theory is meant to serve two related purposes. It is meant to show why we might prefer money to debts, even though our best estimate of the expected return of the debts is positive, and again it is meant to show why we might prefer debts to investments even when our best estimate of the expected return of the investment is higher. And I think if the decision rule stipulated were plausible, it would show that uncertainty did have an economic effect. In particular, I think it would show both that in times of crises when \\(\\rho\\) heads down, the level of investment will decrease even with other things being equal, and that collective action can be justified even when individual action is not. That is, the government can make sets of investments that are expected to be profitable although none of the individual investments is expected to be profitable.\nThe decision theory does not, however, seem plausible. First, there are some technical problems for this theory. The problem is that if \\(\\rho < \\frac{1}{2}\\), then in cases where uncertainty is guaranteed to increase in the near future agents following this rule will make decisions they are sure to regret. For example, assume an agent with \\(\\rho = \\frac{1}{3}\\) now has credence \\(\\frac{1}{2}\\) in \\(p\\), but knows that some evidence will come in such that her credence in \\(p\\) will become vague over \\([0.3, 0.7]\\) whatever the result of the experiment. As we saw in the case of poker players, this is plausible in some situations. The agent will now pay 50 cents for a bet which pays $1 if \\(p\\) and nothing otherwise, but after the evidence comes in she’ll sell that bet for about 44 cents, incurring a sure loss. I leave it to the reader to judge the importance of these technical problems, given the rarity of cases where uncertainty is guaranteed to rise.\nThere is also a philosophical problem. What precisely is \\(\\rho\\) supposed to represent? If it is some kind of belief, its effects should have been incorporated into the credences. If it is some kind of desire its effects should have been incorporated into the evaluation of each of the states. This objection could be avoided, perhaps, if Keynes was trying to argue against the theory that investors just maximise dollar expected returns. It is not entirely clear whom Keynes thinks he is arguing against at some points. If this is his enemy, he is fighting a straw man, one who is vulnerable to much simpler objections. Whoever thought that all investment is profit driven, that no one ever went into business because they thought it would be fun to run a newspaper? Keynes’s only viable opponents here are saying that investors calculate the expected return, in utils, of each possible investment and choose the one whose returns are highest. Now perhaps for many dollar returns are the most important factor in determining util returns, but this is certainly not the only cause.\nIf \\(\\rho\\) represents something which is neither a belief nor a desire, then it is hard to see what effect it could have on action. Perhaps there are some exceptions to the rule that actions are caused only by beliefs and desires combining in the right way, such as actions caused by values, but these appear irrelevant to Keynes’s considerations, and he does not appeal to such exemptions. After all, he describes investment decisions made where the ‘cold calculations’ do not determine what should be done as being made by ‘whim or sentiment or chance.’ Now whims and sentiments are surely desires, although chance is in a different boat. If he had just said ‘chance’ here he may have committed himself to a different decision theory, one where the agent can under uncertainty make any decision which is not known to be sub-optimal. But this does not justify the conclusion that uncertainty decreases investment; under that theory it is random whether uncertainty increases or decreases investment. Hence Keynes appears to be implausibly committed to a mental state which is neither a belief nor a desire but affects action.\nIt might be objected here that I am relying on an overly individualistic theory of motivation; that what Keynes is committed to is nothing more than what anyone who has learned the difference between Robinson Crusoe economics and real-world economics would believe. There is an important truth behind this objection: the social causes of action cannot be overlooked. But this is not what I have done. The core assumption I made is that the only mental states relevant to action are beliefs and desires. Now the beliefs and desires that are relevant may not be (directly) concerned with the action at hand; they may be beliefs and desires about how society will view this action, or about similar actions which may or may not be performed by other members in society. And the beliefs and desires may not have as their immediate cause careful inference by the agent in question; they may be caused by the wave of panic or optimism in which the agent is caught up. In the real world, agents do not always change their beliefs and desires by reflection on new evidence, often emotion plays a larger role. So society has both evidential and non-evidential effects on action. But every time, the causal chain goes via the beliefs and desires of the agent. Society causes actions by causing changes in the beliefs and desires of individuals. It is wrong to think that action is never caused by beliefs and desires about society, it is wrong to think that society never directly causes beliefs and desires which lead to action, but none of this implies that there can be mental states other than belief and desire relevant to action.\nDisquietude\nThere are some comments from Keynes that suggest this reading is a little unfair. Rather than having a distinctive decision theory, he perhaps has a distinctive theory about what ought enter into the decision-theoretic calculations. The standard theory for why there is a demand for insurance is the falling marginal utility of money. Agents purchase insurance, and accept a lower expected dollar return because with insurance their expected util return, at the end of the duration of the insurance, is higher than if they had not purchased. This is the story given in, for example, Freidman and Savage (1952) where the existence of demand for insurance is taken as evidence for the declining marginal utility of money. But there is another reason agents might buy insurance. They might simply feel happier, over the duration of the insured period, knowing that they have insurance and are hence exposed to fewer risks or uncertainties than otherwise. If this is true then their expected ‘wealth’ in both dollars and utils at the end of a period might be lower if they insure than if otherwise, but it will be worthwhile because of the benefits during the period. Keynes suggests that this same desire for quietude can cause a demand for money. I presume, though it is not entirely clear, that this desire should be included within the precautionary motives for holding money.\n\nThere are not two separate factors affecting the rate of investment, namely, the schedule of the marginal efficiency of capital \\[the\nexpected return of investments\\] and the state of confidence. The state of confidence is relevant because it is one of the major factors determining the former (GT: 149).\nFor the fact that each individual investor flatters himself that his commitment is “liquid” ... calms his nerves and makes him much more willing to run a risk (GT: 160).\nThe possession of actual money lulls our disquietude; and the premium which we require to make us part with money is the measure of the degree of our disquietude (Keynes 1937b, 116).\nA liquidity premium ... is not even expected to be rewarded. It is a payment, not for the expectation of increased tangible income at the end of the period, but for an increase sense of comfort and confidence during the period (Keynes 1938, 293–94).\n\nThis explanation of the demand for certain returns is in some ways conservative and some ways radical. It is conservative because it does not immediately change the technical properties of preference. Many heterodox theories of preference drop such theoretical restrictions as transitivity of preferences. By contrast the theory Keynes appears to be advocating is it least in principle conservative on this front. Agents are still going round maximising expected utility, just now it is expected utility over a period, not at the end of the period.\nBut it is not all conservative. If we explain economic decisions in terms of the disquietude of the investor we discard the distinction between investment and consumption. It was always known that there were some goods that were not comfortably categorised, particularly cars, but this move makes every good in part a consumption good. If all this meant was that some helpful classifications have to be questioned, it would not be important. Rather, its importance flows from its implications for the norms for investment. It is always irrational to make an investment which will incur a sure loss. This principle is used to derive wide-ranging implications for decision-theory. But it is not irrational to make a consumption decision which will result in sure loss at the end of a period in exchange for goods during that period. It is not always irrational to pay ten dollars for a movie ticket, even though this will incur a sure loss in the sense the buyer will surely have less wealth at the end of the movie than if they had not bought the ticket.\nGiven this, the technical complaint I raised against the Horvitz-style decision rule misses the target. And the philosophical concern about what \\(\\rho\\) represents is irrelevant. If the expected returns only measure how much various gambles will be worth at the end of the period, then some desires have not yet been included in our calculations. That is, \\(\\rho\\) represents some desires but the theory is not guilty of double-counting. So far this all seems to work, and explain the role of uncertainty. Indeed, I think this is the best extension of Keynes’s views in this area.\nWhile there seem to be few theoretical objections which can be raised at this point, there is a rather telling empirical objection. The only role given to disquietude in this theory is in deciding between alternatives where the returns on at least one are uncertain. But it seems implausible that disquietude could have this effect, but have no effect when choices are being made between alternatives where at least one is risky. I doubt the feelings of disquiet would be any different were I to have a large fortune riding on a roulette wheel or a baseball game. Disquietude arises because we do not know what will happen; maybe for some people it is greater when we do not know the expected returns, but I doubt it. Again, perhaps there is an explanation for demand for money in the real world to be found here, but uncertainty plays no role in the story, or at best a small cameo.\nSummary\nKeynes argued that uncertainty has a major economic impact. By driving people to store their wealth in ways with more stable returns, it increases the demand for cash and decreases the demand for investments. Not only does it drive down investments in this direct way, the increased demand for cash leads to higher interest rates and hence people are driven out of investment into bonds. However, there are a few problems with the story. First, the motivation for demanding returns fixed with respect to a certain good can only be that the markets between that good and other goods are more complete. But if that is the case there is a reason to demand that good even when the world is completely certain. Secondly, the only decision-theoretic justification for this demand for fixed returns could be the disquiet generated by not knowing the return. This follows from the formalisation of uncertainty advocated in sections 1 and 2. But this disquiet could just as easily be generated by risk as by uncertainty. So Keynes has not shown that uncertainty has any particular economic impact. That’s the bad news. The good news is that many of the arguments seem to work without the reliance on uncertainty.\n\n\nBateman, Bradley. 1996. Keynes’s Uncertain Revolution. Ann Arbor: University of Michigan Press.\n\n\nCarnap, Rudolf. 1950. Logical Foundations of Probability. Chicago: University of Chicago Press.\n\n\nCoates, John. 1996. The Claims of Common Sense. Cambridge: Cambridge University Press.\n\n\nDavidson, Paul. 1988. “A Technical Definition of Uncertainty and the Long-Run Non-Neutrality of Money.” Cambridge Journal of Economics 12: 329–38. https://doi.org/10.1093/oxfordjournals.cje.a035063.\n\n\n———. 1991. “Is Probability Theory Relevant for Uncertainty? A Post Keynesian Perspective.” Journal of Economic Perspectives 5 (1): 129–44. https://doi.org/10.1257/jep.5.1.129.\n\n\nDavis, John. 1994. Keynes’s Philosophical Development. Cambridge: Cambridge University Press.\n\n\nDempster, Arthur. 1967. “Upper and Lower Probabilities Induced by a Multi-Valued Mapping.” Annals of Mathematical Statistics 38: 325–39. https://doi.org/10.1214/aoms/1177698950.\n\n\nFraassen, Bas Fraassenvan. 1990. “Figures in a Probability Landscape.” In Truth or Consequences, edited by J. M. Dunn and A. Gupta, 345–56. Amsterdam: Kluwer.\n\n\nFreidman, M., and L. Savage. 1952. “The Expected Utility Hypothesis and the Measurability of Utility.” Journal of Political Economy 60 (6): 463–74. https://doi.org/10.1086/257308.\n\n\nGärdenfors, Peter, and Nils-Eric Sahlin. 1982. “Unreliable Probabilities, Risk Taking and Decision Making.” Synthese 53 (3): 361–86. https://doi.org/10.1007/bf00486156.\n\n\nHart, A. G. 1942. “Risk, Uncertainty and the Unprofitability of Compounding Probabilities.” In Studies in Mathematical Economics and Econometrics, edited by F. McIntyre O. Lange and T. O. Yntema., 110–18. Chicago: University of Chicago Press.\n\n\nHicks, John. 1967. Critical Essays in Monetary Theory. Oxford: Clarendon Press.\n\n\nJaffray, J. Y. 1994. “Decision Making with Belief Functions.” In Advances in the Dempster- Shafer Theory of Evidence, edited by R. Yager, M. Fedrizzi, and J. Kacprzyk, 331–52. New York: John Wiley.\n\n\nJeffrey, Richard. 1983. “Bayesianism with a Human Face.” In Testing Scientific Theories, edited by J. Earman (ed.). Minneapolis: University of Minnesota Press.\n\n\nKeynes, John Maynard. 1921. Treatise on Probability. London: Macmillan.\n\n\n———. 1936. The General Theory of Employment, Interest and Money. London: Macmillan.\n\n\n———. 1937a. “The Ex Ante Theory of the Rate of Interest.” Economic Journal 47 (188): 663–68. https://doi.org/10.2307/2225323.\n\n\n———. 1937b. “The General Theory of Employment.” Quarterly Journal of Economics 51 (2): 209–23. https://doi.org/10.2307/1882087.\n\n\n———. 1938. “Letter to Hugh Townshend Dated 7 December.” In The Collected Writings of John Maynard Keynes, by John Maynard Keynes, 14:293–94. London: Macmillan.\n\n\nLevi, Isaac. 1980. The Enterprise of Knowledge. Cambridge, MA.: MIT Press.\n\n\n———. 1982. “Ignorance, Probability and Rational Choice.” Synthese 53 (3): 387–417. https://doi.org/10.1007/bf00486157.\n\n\nRamsey, Frank. 1926. “Truth and Probability.” In Philosophical Papers, edited by D. H. Mellor, 52–94. Cambridge: Cambridge University Press.\n\n\nRunde, Jochen. 1990. “Keynesian Uncertainty and the Weight of Arguments.” Economics and Philosophy 6 (2): 275–93. https://doi.org/10.1017/s0266267100001255.\n\n\n———. 1994a. “Keynes After Ramsey: In Defence of ‘a Treatise on Probability’.” Studies in the History and Philosophy of Science 25 (1): 97–124. https://doi.org/10.1016/0039-3681(94)90022-1.\n\n\n———. 1994b. “Keynesian Uncertainty and Liquidity Preference.” Cambridge Journal of Economics 18: 129–44. https://doi.org/10.1093/oxfordjournals.cje.a035266.\n\n\nShafer, Glenn. 1976. A Mathematical Theory of Evidence. Princeton: Princeton University Press.\n\n\nStrat, Thomas. 1990. “Decision Analysis Using Belief Functions.” International Journal of Approximative Reasoning 4 (5-6): 391–417. https://doi.org/10.1016/0888-613x(90)90014-s.\n\n\nTintner, Gerhard. 1941. “The Theory of Choice Under Subjective Risk and Uncertainty.” Econometrica 9 (3/4): 298–304. https://doi.org/10.2307/1907198.\n\n\nWalley, Peter. 1991. Statisical Reasoning with Imprecise Probabilities. London: Chapman & Hall.\n\n\nYager, R., M. Fedrizzi, and J. Kacprzyk, eds. 1994. Advances in the Dempster- Shafer Theory of Evidence. New York: John Wiley.\n\n\nFor this paper I follow Walley (1991) in describing those theorists who require that all agents have precise degrees of belief and these degrees form a probability function as Bayesians. There is some dispute as to the accuracy of this labelling, particularly as some paradigm case Bayesians, such as Jeffrey (1983) and Fraassen (1990), accept that degrees of belief can be vague. However, there is probably no other name as convenient or as recognisable.↩︎\nSee, for example, Gärdenfors and Sahlin (1982), Levi (1982).↩︎\nStandard, but I bring it up because the modern theorist whose decision theory is closest to the one Keynes seems to adopt, Levi, explicitly rejects it.↩︎\nIn case the reader fears I am being absurdly formal with an essentially informal idea, Keynes had such a variable, there described as measuring the ‘state of the news,’ in early drafts, but it did not survive to the final stage. So my proposal is not a million miles from what Keynes intended merely by virtue of being algebraic.\n\n↩︎\n",
    "preview": "posts/2021-01-07-keynes-uncertainty-and-interest-rates/cambridge.jpg",
    "last_modified": "2021-03-04T11:18:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-13-keynes-and-wittgenstein/",
    "title": "Keynes and Wittgenstein",
    "description": "Three recent books have argued that Keynes's philosophy, like Wittgenstein's, underwent a radical foundational shift. It is argued that Keynes, like Wittgenstein, moved from an atomic Cartesian individualism to a more conventionalist, intersubjective philosophy. It is sometimes argued this was caused by Wittgenstein's concurrent conversion. Further, it is argued that recognising this shift is important for understanding Keynes's later economics. In this paper I argue that the evidence adduced for these theses is insubstantial, and other available evidence contradicts their claims.",
    "author": [
      {
        "name": "Brian Weatherson",
        "url": "http://brian.weatherson.org"
      }
    ],
    "date": "2001-01-01",
    "categories": [
      "history of analytic",
      "economics",
      "unpublished"
    ],
    "contents": "\n\nContents\nIntroduction\nBateman’s Case for Change\nConventionalism\nKeynes and Vagueness\nKeynes on Rules and Private Language\n\nIntroduction\nThree recent books (Davis 1994; Bateman 1996; Coates 1996) have argued that the philosophy behind Keynes’s later economics (in particular the General Theory) is closer to Wittgenstein’s post Tractarian theorising than to his early philosophy as expressed in his Treatise on Probability.1 If Keynes did follow Wittgenstein in the ways suggested it would represent a substantial change from his early neoplatonist epistemology. In this paper I argue that the evidence for this thesis is insubstantial, and the best explanation of the evidence is that Keynes’s philosophical views remained substantially unchanged.\n\nPicture by Bernhard Ellefsen via Creative Commons.\nThere are three reasons for being interested in this question. The first is that it is worthwhile getting the views of a thinker as important as Keynes right. The second is that it would be mildly unfortunate for those of us attracted to Keynes’s epistemology to find out that it was eventually rejected by its creator2. Most importantly, all parties agree that Keynes thought his philosophical theories had substantial consequences for economic theory. It is a little unusual for philosophical theories to have practical consequences; if one is claimed to it is worthwhile identifying and evaluating the claim.\nSection 2 examines Bateman’s claim that Keynes abandoned the foundations of his early theory of probability. Bateman’s arguments turn, it seems, on an equivocation between different meanings of ‘Platonism.’ On some interpretations the arguments are sound but don’t show what Bateman wants, on all others they are unsound. Section 3 looks at the conventionalist, intersubjective theory of probability Bateman and Davis claim Keynes adopted after abandoning his early objective theory. As they express it the theory’s coherence is dubious; I show how it might be made more plausible. Nevertheless, there is little to show that Keynes adopted it. The only time he talks about conventions is in the context of speculative markets and in these contexts a conventionalist theory will give the same results as an objectivist theory.\nSection 4 looks at Coates’s quite different arguments for an influence from Wittgenstein to Keynes. Part of the problem with Coates’s argument is that the textual evidence he presents is capable of several readings; indeed competing interpretations of the pages he uses exist. A bigger problem is that even when he has shown a change in Keynes’s views occurred, he immediately infers the change was at the foundations of Keynes’s beliefs. Section 5 notes one rather important point of Wittgenstein’s of which Keynes seemed to take no notice, leading to an error in the General Theory. This should cast doubt on the claim that Keynes’s later philosophy, indeed later economics, was based on theories of Wittgenstein.\nBateman’s Case for Change\nA brief biographical sketch of Keynes is in order to frame the following discussions, though I expect most readers are familiar with the broad outlines3. Keynes arrived as an undergraduate at Cambridge in 1902 and was based there for the rest of his life. For the next six years he largely studied philosophy under the influence of Moore and Russell. In 1907 he (unsuccessfully) submitted his theory of probability as a fellowship dissertation; this was successfully resubmitted the following year. His plans to make a book of this were interrupted by work on Indian finance, the war and its aftermath. It appeared as Treatise on Probability (hereafter, TP) in (1921), after substantial work on it in 1920. Modern subjectivist theories of probability, generally known as Bayesian theories, first appeared in critical reviews of this book (e.g. Borel (1924), Ramsey (1926)). After leaving philosophy for many years, Wittgenstein returned to Cambridge in 1929, and subsequently had many discussions with Keynes. In Keynes’s General Theory (hereafter, GT) of (1936) and in some of the ensuing debate, Keynes referred to some distinctive elements of the TP, leading some interpreters to suspect that there was a theoretical link between his early philosophy and his later economics.\nThere are two distinctive elements of Keynes’s early theory of probability for our purposes. The first is its objectivism. Keynes held the probability of p given h is the degree of reasonable belief in p on evidence h, or, as Carnap (1950) put it, the degree of confirmation of p by h. These degrees are determined by logic; Keynes held that there was a partial entailment relation between p and h, of which the ordinary entailment relation (then thought to have been given its best exposition by Russell and Whitehead) was just a limiting case. And these relations are Platonic entities, we discover what they are by perceiving them through our powers of intuition. The second element is that the degrees may be non-numerical. So if the probability of p given h is \\({\\alpha}\\), we may be able to say \\({\\alpha}\\) > 0.3, and \\({\\alpha}\\) < 0.5, but not be able to give any finer numerical limits. As a corollary, there are now two dimensions of confirmatory support. Keynes claimed that as well as determining the probability of p given h, we could determine the ‘weight’ of this probability, where weight measures how much evidence we have. The more evidence is in h, the greater the weight. Keynes thought the distinction between saying that on evidence h, p has a low probability, and saying that the weight of that probability is low is important for understanding investment behaviour (GT: Ch. 12).\nBateman and Davis both claim that Keynes gave up this theory for an intersubjective theory in the GT. I’ll focus on Bateman’s book, largely because the structure of his argument is more straightforward4. Bateman sets himself to offer another solution to ‘das Maynard Keynes problem,’ which he describes as follows.\n\n“[Future theorists] will read Treatise on Probability’s account of the objective nature of probabilities and the way that rational people employ them, and they will wonder at how this person could have turned around 15 years later and written a book [the GT] in which irrational people who base their decisions on social conventions cause mass unemployment in the capitalist system” (7)\n\nI doubt this is the right thing to say about the GT, but that’s another story. For now we might simply note that there’s no obvious conflict here. For one thing, if the people in the TP are rational, and in the GT are irrational, as Bateman allows, it’s not too surprising they behave differently. More generally, it’s to be expected (sadly) that normative and descriptive theories are different, and by Bateman’s lights that should explain the difference between the outlook of the explicitly normative TP and the at least partially descriptive GT. If the agents in the GT are irrational, that book cannot but be a purely normative account of rationality. On the other hand, if the TP were taken to be descriptive and not just normative, if it claimed that people really conform to its epistemological exhortations, there could be a conflict. I can’t imagine, however, what the evidence or motivation for that reading could be.\nIf there were a conflict between the GT and TP, there ought be a greater one between the ‘rational people’ of the TP and the blatantly irrational leaders in Economic Consequences of the Peace (ECP). These books were published about 15 months apart, not 15 years. And the most memorable parts of ECP are the descriptions of the mental failings of President Wilson, who Lloyd George could ‘bamboozle’ into believing it was just to crush Germany completely, but not ‘de-bamboozle’ out of this view when it became necessary. Or maybe we should say there’s a conflict because the characters in David Hume’s histories do not meet his ethical or epistemological norms.\nIf we give up Bateman’s claim that the actors in the GT are irrational, and substitute the claim that the norms of rationality in the two books differ, then we have a real conflict. And the most charitable interpretation of Bateman is that this is the conflict he intends to discuss. At the bottom of page 12 he goes close to saying exactly this, but then proceeds to support his position with evidence that Keynes changed his position on how rational people actually are. Once we are claiming the change of view is with regard to norms, evidence of opinion changes about empirical questions becomes irrelevant. This does mean much of Bateman’s case goes, though not yet all of it.\nThe main problem with Bateman’s argument is that it rests on an equivocation over the use of the term ‘Platonism.’ In TP Keynes held that probability relations are objective, non-natural and part of logic. I’ll use ‘logical’ for the last property. When Bateman says Keynes believed probability relations were Platonic entities, he is alternately referring to each of these properties. He seems to explicitly use ‘Platonic’ to mean ‘objective’ on page 30, ‘non-natural’ on page 131, and ‘logical’ on page 123. But this isn’t the important equivocation.\nSay a theory about some entities is a ‘Strong Platonist’ theory if it concords with all Keynes’s early beliefs: those entities are objective, non-natural and logical. Bateman wants to conclude that by the time of the GT, Keynes no longer had an objectivist theory of probability. But showing he no longer held a Strong Platonist view won’t get that conclusion, because there are 3 interesting objectivist positions which are not Strong Platonist. The following names are my own, but they should be helpful.\nCarnapian\nProbability relations are objective, natural and logical. This is what Carnap held in his (1950).\n\nGödelism\nProbability relations are objective, non-natural and non-logical. Gödel held this view about numbers, hence the name. I’d normally call this position Platonism, but that name’s under dispute. Indeed I suspect this is what Keynes means by Platonism in My Early Beliefs. (Keynes 1938b)\n\nReductionism\nProbability relations are objective, natural and non-logical. Such positions don’t have to reduce probability to something else, but they usually will. Russell held such a position in his (1948).\n\nThese categories could apply to other entities, like numbers or moral properties or colours, but we will be focussing on probability relations here. Say a theory is ‘Weak Platonist’ if it is Strong Platonist or one of these three types. The most interesting equivocation in Bateman is using ‘Platonist’ to refer to either Strong or Weak Platonist positions. He argues that Keynes gave up his early Platonist position. These arguments are sound if he means Strong Platonist, unsound if he means Weak Platonist. But if he means Strong Platonist he can’t draw the extra conclusion that Keynes gave up objectivism about probability relations, which he does in fact draw. So I’ll examine his arguments under the assumption that he means to show Keynes gave up Weak Platonism.\nWhatever Bateman means by Keynes’s Platonism, he isn’t very sympathetic to it. It gets described as ‘obviously flawed’ (4) and ‘fatally flawed’ (17), and is given as the reason for his work being ignored by ‘early positivists and members of the Vienna Circle’ (61). Given that the TP is cited extensively, and often approvingly, by Carnap in his 1950, this last claim is clearly false. Most stunningly, he claims writers committed to the existence of Platonic entities cannot ‘be considered to be a part of the analytic tradition’ (39), though he does concede in a footnote that some ‘early analytical philosophers’ (he gives Frege as an example) were Platonist. Bateman’s paradigm of philosophy seems to be the logical positivism of Ayer’s Language, Truth and Logic: “nowhere would one less expect to find metaphysics than in modern analytical philosophy” (Ayer 1936, 39).\nThere is an implicit argument in this derision. Keynes must, so the argument goes, have given up (Weak) Platonism because no sensible person could believe it. If anything like this were sound it should apply to Weak Platonism about other entities. But the history of ‘modern analytical philosophy’ shows that Weak Platonism (though not under that name) is quite widespread in metaphysical circles. Modern philosophy includes believers in possible worlds both concrete and ersatz, in universals and in numbers. All these positions would fall under Weak Platonism. Even Quine’s ontologically sparse Word and Object was Weak Platonist about classes, though he probably wouldn’t like the label. So by analogy Weak Platonism about probability relations isn’t so absurd as to assume Keynes must have seen its flaws.\nBateman’s more important argument is direct quotation from Keynes. This argument is undermined largely because of Bateman’s somewhat selective quotation. There are two sources where Keynes appears to recant some of his early beliefs. Which early beliefs, and how early these beliefs were, is up for debate. The two are his 1938 memoir My Early Beliefs (hereafter, MEB), and his 1931 review of Ramsey’s posthumous Foundations of Mathematics. MEB wasn’t published until 1949, three years after Keynes’s death, but according to its introduction it is unchanged from the version Keynes gave as a talk in 1938. In it he largely discusses the influence of Moore, and particularly Principia Ethica, on his beliefs before the first world war.\nThere are several connections between Moore’s work and Keynes. The most pertinent here is that Keynes’s metaphysics of probability in TP is borrowed almost completely from Moore’s metaphysics of goodness. Not only are probability relations objective and non-natural, they are simple and unanalysable. These are all attributes Moore assigns to goodness. The only addition Keynes makes is that his probability relations are logical. So Moore’s position on goodness is, in our language, Gödelian.\nAs he says in MEB, Keynes became convinced of Moore’s metaethics, though he differed with Moore over the implications this had for ethics proper. In particular he disagreed with Moore’s claim that individuals are morally bound to conform to social norms. Bateman seems to assume that at any time Keynes’s metaphysics of goodness and probability will be roughly the same, and with the exception of questions about their logical status, this seems a safe enough assumption.\nBateman quotes Keynes saying that his, and his friends’, belief in Moore’s metaethics was ‘a religion, some sort of relation of neo-platonism’ (Keynes 1938b, 438). This is part of the evidence that Keynes meant what I’m calling Gödelism by ‘Platonism.’ Not only does he use it to describe Moore’s position, but comparing Platonism with religion would be quite apt if he intends it to involve a commitment to objective, non-natural entities. The important point to note is that he is using ‘religion’ to include his metaethics, a point Bateman also makes, though it probably also includes some broad ethical generalisations. Bateman then describes the following paragraph as removing ‘any doubt that [Keynes] had thrown over his youthful Platonism as untenable.’ (40)\n\nThus we were brought up – with Plato’s absorption in the good in itself, with a scholasticism which outdid St. Thomas, in calvinistic withdrawal from the pleasures and successes of Vanity Fair, and oppressed with all the sorrows of Werther. It did not pervert us from laughing most of the time and we enjoyed supreme self-confidence, superiority and contempt towards all the rest of the unconverted world. But it was hardly a state of mind which a grown-up person in his senses could sustain literally. (Keynes 1938a, 442).\n\nAs it stands, perhaps the last sentence signals a change in metaphysical beliefs, as opposed to say a change in the importance of pleasure-seeking. In any case the following paragraph (which Bateman neglects to quote) shows such an interpretation to be mistaken.\n\nIt seems to me looking back, that this religion of ours was a very good one to grow up under. It remains nearer the truth than any other I know, with less extraneous matter and nothing to be ashamed of ... It was a purer, sweeter air than Freud cum Marx. It is still my religion under the surface. (Keynes 1938a, 442).\n\nSo was Keynes confessing to ‘a state of mind which a grown-up person in his senses couldn’t sustain literally?’ No; his ‘religion’ which he held onto was a very broad, abstract doctrine. It needed supplementation with a even general ethical view, to wit an affirmative answer to one of Moore’s ‘open questions.’ And then it needed some bridging principles to convert those ethics into moral conduct in the world as we find it. His early position included all these, and it seems it was in effect his early ‘bridging principles’ he mocks in the above quote. These relied, the memoir makes clear throughout, on an excessively optimistic view of human nature, so he thought in effect that he could prevent wrong by simply proving to its perpetrators that they were wrong. Now giving up one’s bridging principles doesn’t entail abandonment of a general ethical view, let alone one’s metaethics. Indeed, let alone one’s metaphysics of probability! And as the last quote makes clear, Keynes was quite content with the most general, most abstract parts of his early belief. If this were all Bateman had to go on it wouldn’t even show Keynes had abandoned Strong Platonism5.\nThere is more to Bateman’s case. In Keynes’s review6 of Ramsey (1931), he recanted on some of his theory of probability. This is quite important to the debate, so I’ll quote the relevant section at some length.\n\nRamsey argues as against the view which I had put forward, that probability is concerned not with objective relations between propositions but (in some sense) with degrees of belief, and he succeeds in showing that the calculus of probabilities simply amounts to a set of rules for ensuring that the system of degrees of belief which we hold shall be a consistent system. Thus the calculus of probability belongs to formal logic. But the basis of our degrees of belief – or the a priori probabilities, as they used to be called – is part of our human outfit, perhaps given us merely by natural selection, analogous to our perceptions and our memories rather than to formal logic. So far I yield to Ramsey – I think he is right. But in attempting to distinguish ‘rational’ degrees of belief from belief in general he was not yet, I think, quite successful. It is not getting to the bottom of the principle of induction to merely say it is a useful mental habit. (Keynes 1931, 338–39).\n\nTellingly, Bateman neglects to quote the final two sentences. I think there is an ambiguity here, turning on the scope of the ‘so far’ in the fourth sentence. If it covers the whole section quoted, it does amount to a wholesale recantation of Keynes’s theory, and this is Bateman’s interpretation. But if we take the first sentence, or at least the first clause, as being outside its scope it does not. And there are two reasons for doing this. First, it seems inconsistent with Keynes’s later reliance on the TP in parts of the GT, as (O’Donnell 1989 Ch. 6) has stressed. Secondly, it is inconsistent with Keynes’s complaint that on Ramsey’s view induction is merely a ‘useful habit.’ If Keynes had become a full-scale subjectivist, he ought have realised that patterns of reasoning could only possibly be valid (if deductive) or useful (otherwise). Since he still thought there must be something more, he seems to believe an objectvist theory is correct, though by now he is probably quite unsure as to its precise form. So in effect what Keynes does in this paragraph is summarise Ramsey’s view, list the details he agrees with (that probability relations aren’t logical), notes his agreement with them, and then lists the details he disagrees with (that probability relations aren’t objective).\nThere is more evidence that all this quote represents is a recantation of the view that probability relations are logical. Earlier in that review he notes how little formal logic is now believed to achieve compared with its promise at the start of the century.\n\nThe first impression conveyed by the work of Russell was that the field of formal logic was enormously extended. The gradual perfection of the formal treatment at the hands of himself, of Wittgenstein and of Ramsey had been, however, gradually to empty it of content and to reduce it more and more to mere dry bones, until finally it seemed to exclude not only all experience, but most of the principles, usually reckoned logical, of reasonable thought. (Keynes 1931, 338).\n\nMore speculatively, I suggest Keynes’s change of mind here (for this shows he had surely given up the view that probability relations are logical) might be influenced by Gödel’s incompleteness theorem. In the TP Keynes had followed Russell is saying mathematics is part of logic (Keynes 1921, 293n). That view was often held to be threatened by Gödel’s proof that there are mathematical truths which can’t be proven, and that the consistency of mathematics can’t be proven. But no one suggested this meant mathematics is merely subjective, or that mathematical Platonism was therefore untenable. If this response to Gödel is right, it shows there are objective standards of reasoning (i.e. mathematical standards) that are not part of logic. This makes it less of a leap to say there are objective principles of reasonable thought that are not ‘logical’ in the narrow sense we’ve been using.\nSo would Keynes have known of Gödel’s theorem when he wrote this review? I think it’s possible, though some more research is needed. Keynes’s review was published in The New Statesman and Nation on October 3, 1931. This was a weekly political and literary magazine of which Keynes was chairman. So we can safely conclude the piece was drafted not long before publication. Gödel’s theorem was first announced at a conference in Vienna in September 1930 (Wang 1987), and was published in early 1931. While Keynes would certainly have not read Gödel’s paper, its content could easily have reached him through Cambridge in that 12 month ‘window.’ Since the explicit aim of Gödel’s paper was to show the incompleteness of Principia Mathematica, it would have immediately had some effect in Cambridge, both in philosophy and mathematics. Given this evidence, the probability Keynes knew of Gödel’s theorem when he wrote the review of Ramsey still mightn’t be greater than one-half, but it mightn’t be less than that either.\nIn sum, I conclude that Keynes had given up his earlier belief that all rules of reasonable belief are logical. This is what he yields to Ramsey. This concession would be supported by the ‘drying up’ of formal logic that Keynes notes, perhaps most dramatically expressed in Gödel’s theorem. But he hadn’t given up the belief that there are objective rules which are extra-logical, and given the identification of probability with degree of reasonable belief, he had no reason to reject Gödelism or Reductionism about probability. Hence Bateman’s argument that he rejected objectivist theories of probability fails.\nConventionalism\nBateman and Davis each argue that Keynes adopted a conventionalist, intersubjectivist theory of probability. In Davis this is explicity attributed to Wittgenstein’s influence, however in Bateman it is less clear what the source of this idea is. It isn’t obvious what they mean by an intersubjective theory. In particular, it isn’t clear whether they mean this to be an empirical or a normative theory; whether Keynes is claiming that we ought set our degrees of belief by convention or that we in general do. Since the empirical theory would be consistent with his objectivist norms, and they stress the change in his views, I conclude they are claiming this is a new normative view. According to this view being reasonable is analysed as conforming to conventions. This is not a very standard epistemological position, but something similar is often endorsed in ethics. Bateman marshals the evidence that Keynes moves from an objectivist to a conventionalist position in ethics as evidence for this epistemological shift, but this doesn’t seem of overwhelming significance7.\nHere’s the closest Bateman gets to a definition of what he means by an intersubjective theory of probability.\n\nWhen probabilities are formed according to group norms, they are referred to as intersubjective probabilities ... I take it to be the case that in a world of subjective probabilities some individuals will form their own estimates and others will form them on the basis of group norms (50n).\n\nThis makes it look very much like an empirical theory, as it refers to how people actually form beliefs, not how they ought. So his intersubjectivism looks perfectly consistent with Keynes’s objectivism. I am completely baffled by the ‘world of subjective probabilities.’ I wonder what such a world looks like, and how it compares to our world of tables, chairs and stock markets?\nFortunately there is a theory that does the work Bateman needs. Ayer (1936) rejects orthodox subjectivism about probability on the grounds that it doesn’t allow people to have mistaken probabilistic beliefs. But he can’t admit Keynesian probability relations into his sparse ontology. The solution he adopts is to define probability as degree of rational belief, but with this caveat.\n\nHere we may repeat that the rationality of a belief is defined, not by reference to any absolute standard, but by reference to part of our own actual practice (Ayer 1936, 101).\n\nThe ‘our’ is a bit ambiguous; interpreting it to refer to the community doesn’t do violence to the text, though it is just as plausible that it refers to a particular agent. The ‘part of our practice’ referred to is just our general rules for belief formation. These aren’t justified by an absolute standard; they are justified by the fact they are our rules, and presumably by their generality. Given Bateman’s views about metaphysics, it seems quite reasonable to suppose he’d follow Ayer on this point.\nThe evidence Keynes adopted such a position is usually taken to be some passages from the GT and the 1937 QJE paper in which he replied to some attacks on that book. Here’s the key points from the two quotes Bateman uses to support his view.\n\nIn practice we have agreed to fall back on what is, in truth, a convention. The essence of this convention – though it does not, of course, work out quite so simply – lies in assuming that the existing state of affairs will continue indefinitely, except in so far as we have specific reasons for expecting a change (GT: 152).\n\n\nHow do we manage in such circumstances to behave in a manner which saves out faces as rational, economic men? We have devised for the purposes a variety of techniques, of which much the most important are the three following: ...\n\\(3\\) Knowing that our own individual judgement is worthless, we endeavour to fall back on the judgement of the rest of the world which is perhaps better informed. That is, we endeavour to conform with the behaviour of the majority or the average. The psychology of a society of individuals each of whom is endeavouring to copy the others leads to what we may strictly term a conventional judgement (Keynes 1937, 115).\n\nThere are two problems with using this evidence the way Bateman does. The first is the old one that they seem expressly directed to empirical questions, though perhaps appearances are deceptive here. The more important one is that Keynes is attempting to answer a very specific question with these passages; in ignorance of the question we can easily misinterpret the answer.\nHow much ought one pay for a share in company X? Well, if one intends to hold the share come what may, all that matters is the expected prospective yield of X’s shares, appropriately discounted, as compared to the potential yield of that money in other uses. But as Keynes repeatedly stresses (GT: 149; (Keynes 1937, 114)) we have no basis for forming such expectations. Were this the only reason for investing then purely commercial investment may never happen.\nThere is another motivation for investment, one that avoids this problem. We might buy a share in X today on the hope that we will sell it next week (or next month or perhaps next year) for more than we paid. To judge whether such a purchase will be profitable, we need a theory about how the price next week will be determined. Presumably those buyers and sellers will be making much the same evaluations that we are. That is, they’ll be thinking about how much other people think X is worth.\n\nWe have reached the third degree where we devote our intelligences to anticipating what average opinion expects the average opinion to be. And there are some, I believe, who practice the fourth, fifth and higher degrees (GT: 156).\n\nThere is simply no solution to this except to fall back on convention. That is, we are forced into a conventionalist theory of value, at least of investment goods. But this doesn’t mean that we have a conventionalist epistemology. On the contrary, it means that our ordinary (objectivist) empiricism is unimpeded. For the question that Keynes has us solve by reference to convention is: What is the value of X? This is equivalent to, what will be value of X be, or again, to what are the conventional beliefs about X’s value? We need to answer a question about the state of conventions, and as good empiricists we answer it by observing conventions.\nAn analogy may help here. Here’s something that Hempel believed: to gain rational beliefs about the colour of ravens, one has to look at some birds. Did this mean he had an ornithological epistemology? No; he had an empiricist epistemology which when applied to a question about ravens issued the directive: Observe ravens! Similarly Keynes’s belief that to answer questions about value, i.e. about conventions, one has to look at conventions, does not imply a conventionalist epistemology. It just means he has an empiricist epistemology which when applied to a question about conventions issues the directive: Observe conventions!\nThere might be another motivation for using conventions, again consistent with Keynes’s objectivist empiricism. Sometimes we may have not made enough observations, or may not have the mental power to convert these to a theory. So we’ll piggyback on someone else’s observations or mental powers. (This seems to be what’s going on in the quote from Keynes (1937).) Or even better, we’ll piggyback on everyone’s work, the conventions. To see how this is consistent with an objectivist epistemology (if it isn’t already obvious) consider another analogy.\nWhat is the best way to work out the derivative of a certain function? Unless your memory of high-school calculus is clear, the simplest solution will be to consult an authority. Let’s assume for the sake of argument that the easiest authorities to consult are maths texts. It seems like the rational thing to do is to act as if the method advanced by the maths texts is the correct method. Does this mean that you have adopted some kind of authoritarian metaphysics of mathematics, where what it is for something to be correct is for it to be asserted by an authority? Not at all. It is assumed that what the textbook says is correct, but the authoritarian has to make the extra claim that the answer is correct because it is in the textbook. This is false; that answer is in the textbook because it is correct. In sum, the authoritarian gets the direction of fit wrong.\nSimilarly in the ‘piggyback’ cases the intersubjectivist gets the direction of fit wrong. We are accepting that p has emerged as ‘average opinion,’ then it is reasonable to believe p. But we aren’t saying with the intersubjectivist it is reasonable to believe p because p is average opinion; rather we are assuming p is average opinion because it is reasonable to believe p.\nThe evidence so far suggests Keynes’s statements are consistent with his denying intersubjectivism. We might be able to go further and show they are inconsistent with his adopting that theory. After the quote on GT page 152 he spends the next page or so defending the use of conventions here. The defence is, in part, that decisions made in accord with conventions are reversible in the near future, so they won’t lead to great loss. If he really were an intersubjectivist, the use of conventions would either not need defending, or could be defended by general philosophical principles. Secondly, there is this quote which in context seems inconsistent with adopting a conventionalist view.\n\nFor it is not sensible to pay 25 for an investment which you believe the prospective yield to justify a value of 30, if you also believe that the market will value it at 20 three months hence (GT: 155).\n\nThe context is that he is discussing why reasonable professional investors base their valuations on convention rather than on long-term expectation. Hence the ‘you’ in the quote is assumed to be reasonable. Hence it is reasonable, Keynes thinks, to believe that an investment’s prospective yield justifies a value of 30, and that conventional wisdom is that its prospective yield is much lower. But if all reasonable beliefs were formed by accordance with conventional wisdom, this would be inconsistent. Hence Keynes cannot have adopted a conventionalist epistemology.\nKeynes and Vagueness\nWhat a terrible state Keynes interpretation has got into! From the same few pages (the opening of GT Ch. 4) Coates (1996) reads into Keynes a preference for basing theory on vague predicates, Bradford and Harcourt (1997) read Keynes as denying that predicates which are unavoidably vague can be used in theory, and O’Donnell (1997) sees Keynes as holding a position in between these.\nCoates’s theory is that Keynes abandoned the narrowly analytic foundations of his early philosophy because of the problems of vagueness that were pointed out to him by Wittgenstein. He has Keynes in 1936 adopting a middle way between analytic and Continental philosophy, which gives up on analysis because of unavoidable vagueness, but which doesn’t follow Derrida in saying all that’s left after analysis is ‘poetry.’ He also wants to argue for the philosophical importance of this theory. In this essay I’ll focus on his exegetical theories, though there are concerns to be raised about his philosophy.\nAs in Bateman, analytic philosophy gets very narrowly defined in Coates8. Here it includes the claim that truth-value gaps are not allowed (xii). This excludes from the canon some of the most important papers in analytical philosophy of the last few decades (e.g. Dummett (1959), Fraassen (1966), Fine (1975), Kripke (1975)), and hence must be a mistake. To use one of Coates’s favourite terms, ‘analytic philosophy’ is a family resemblance concept, not to be so narrowly cast. In particular, as we’ll see, analytic philosophers don’t have to follow Frege in being nihilist about vagueness.\nEven more bizarrely, Coates defines empiricism so it includes both psychologism in logic and utilitarianism in ethics (72-3). Since Ayer (1936) opposes each of these doctrines, does that makes Ayer an anti-empiricist? If Ayer is a paradigm empiricist (as seems plausible) Keynes’s rejection of psychologism and utilitarianism can hardly count as proof of opposition to empiricism, as Coates wants it to do. Apart from the fact that Mill believed all three, there is no interesting connection between empiricism, psychologism and utilitarianism.\nCoates’s story is that in the GT Keynes allowed both his units and his definitions to be quantitatively vague so as to follow natural language. This constitutes a new ‘philosophy of social science’ (85) that is based on the ordinary language philosophy of the later Wittgenstein. There are several problems with this story. The first is that most of Coates’s evidence comes from obiter dicta in early drafts of the GT; by the time the book was finished most of these suggestions are expunged. The second is that it’s quite possible to accept vagueness within a highly analytic philosophical framework. The third is that the way Keynes uses vagueness is only consistent within such a framework.\nThe first part of the story focuses on how Keynes derided his predecessors for using concepts that were vague as if they were precise. Coates adduces evidence to show Keynes in this context used ‘vague’ as a synonym for ‘quantitatively inexact.’ The most important concept misused by Keynes’s predecessors in this way was the general price level. Of course this was hardly a new point in the GT; Keynes (1909) says similar things. Coates claims that Keynes’s reaction to this misuse was to ‘criticise formal methods’ (83), and to conclude that ‘economic analysis can do without the “mock precision” of formal methods’ (85). This is all hard to square with Keynes’s explicit comments.\n\nThe well-known, but unavoidable, element of vagueness which admittedly attends the concept of the general price-level makes this term very unsatisfactory for the purposes of a causal analysis, which ought to be exact (GT: 39).\n\nFurther, Keynes then defends his choice of units of quantity (quantity of money-value and quantities of employment) on the grounds that they are not quantitatively vague. Coates is surely right when he says that Keynes’s analysis of vagueness here is ‘not very controversial’; although it is perhaps misleading to say it is controversial at all.\nThe second, and central, part of the story focuses on how Keynes allowed his definitions to be vague, but defended this on the grounds of conformity to ordinary language. This ‘introduces what is distinctive about his later philosophy of the social sciences’ (85). The bulk of Coates’s evidence comes from Keynes’s commentary on his own definitions; usually this includes a claim that he has captured the ordinary usage of the term. Since he uses ‘common usage’ to explicitly mean ‘usage amongst economists’ (GT: 79) the support these dicta give to Coates’s theory might be minimal, but we’ll ignore that complication. The real problem is that this commentary extends to cases where he has changed his mind over the best definition. For example, Coates quotes Keynes writing in a draft of the GT about the definition of income.\n\nBut finally I have come to the conclusion that the use of language, which is most convenient on a balance of considerations and involves the least departure from current usage, is to call the actual sale proceeds income and the present value of the expected sale proceeds effective demand (Keynes 1934, 425).\n\nCoates comments:\n\nBy choosing definitions on the ground that they correspond with actual usage Keynes was formulating an ordinary language social science, one that bears a resemblance to those argued for by philosophers of hermeneutics (90).\n\nHe then goes on to note some comments from the GT apparently about this definition, and how it relates to common usage. The problem is that this isn’t the definition of income Keynes settles on in the GT. There he defines income of an agent as “the excess of the value of his finished output sold during the period over the prime cost” (GT: 54), and net income (which Coates fails to distinguish) as income less supplementary cost. Given that at every stage Keynes justified his current definitions by their (alleged) conformity with common usage, even when he changed definitions, it is hard to believe that these justifications are more than rhetorical flourishes. After all, who will deny that ceteris paribus technical definitions should follow ordinary usage?\nIf Keynes’s early choice of definitions showed an adherence to a ‘philosophy of hermeneutics,’ perhaps his abandonment of those definitions constitutes abandonment of that philosophy. One change doesn’t necessarily mean a change in foundations, so it is worth looking at those foundations.\nAs I mentioned, allowing that vagueness exists doesn’t mean abandoning the Russellian program of giving a precise analysis of language. There are two reasons for this. First, contra Wittgenstein it is possible to analyse vague terms. Secondly, there are semantic programs very much in the spirit of Russell which allow vagueness. I’ll deal with these in order.\nIn Philosophical Investigations, Wittgenstein (1953) argued that the existence of vagueness frustrated the program of analysis (ss. 60, 71). The argument presumably is that analyses are precise, and hence they cannot accurately capture vague terms. (See also his comments about the impossibility of drawing the boundaries of ‘game’ in s. 68.) This is a simple philosophical mistake. We can easily give an analysis of a vague term, we just have to make the analysans vague in exactly the same way as the analysandum.\nTo see this in action, consider that paradigm of modern philosophy, Lewis’s analysis of subjunctive conditionals or counterfactuals. Lewis (1973) says that the conditional ‘If p were the case, it would be that q’ is true iff q is true in the most similar possible world in which p. He considers the objection that ‘most similar’ is completely vague and imprecise.\n\nImprecise it may be; but that is all to the good. Counterfactuals are imprecise too. Two imprecise concepts may be rigidly fastened to one another, swaying together rather than separately, and we can hope to be precise about their connection Lewis (1973).\n\nWhatever the fate of Lewis’s theory, his methodology seems uncontestable. Wittgenstein’s claim that analysis must be abandoned because of vagueness is refuted by these observations of Lewis. Hence Coates’s claim that allowing vagueness (as Keynes does) means giving up on analytic philosophy is mistaken.\nThe second problem with Coates’s comments on vagueness is that he hasn’t allowed for what I’ll call ‘orthodox’ responses to vagueness. The aim of the early analytics drifted between giving a precise model for natural language, and replacing natural language with an artificial precise language. The latter, claims Coates, ought be abandoned because of the pragmatic virtues of a vague language. Let’s agree to that; can the spirit of the early aim of giving a precise analysis of language be preserved?\nTwo approaches which seem to meet this requirement are the supervaluational and epistemic theories of vagueness. The supervaluationist says language can’t be represented by a precise classical model, but it can be represented by a set of such models. The epistemic theorist says that there is a precise model of language, but we cannot know what it is9. Call a theorist who adopts one of these approaches ‘orthodox.’ The name is chosen because supporters and critics of orthodoxy agree that these positions represent attempts to minimise deviations from the classical, Russellian program.\nClearly Keynes did not explicitly adopt an orthodox theory of vagueness. Williamson (1994) attempts to trace the epistemic theory back to the Stoics, but general consensus is that these approaches were all but unknown until recently. What I want to argue is that Keynes’s intuitions are clearly with orthodoxy. Coates, on the other hand, wants to place Keynes in a tradition that is critical of classical analysis, and perhaps finds its best modern expression in the exponents of fuzzy logics. To see this is wrong, note that the following beliefs are all in the GT.\nAll goods are (definitely) investment goods or consumption goods.\nFor some goods it is vague whether they are an investment or consumption good. (GT: 61)\nThe yield of an investment, q, is vague.\nThe carrying cost of an investment, c, is vague.\nThe net yield of an investment, q - c, can be precisely determined. (GT: 226)\nSince Keynes believed (1) to (5) we can safely conclude he believed they were consistent. More importantly, since the GT has been analysed more thoroughly than any other economic text written this century, and no one has criticised the consistency of (1) to (5), it seems many people agree with him. Hence if conformity with pre-theoretic intuitions of consistency is a central desideratum of a theory of vagueness, we can discard any theory that does not say they are consistent. However, of those theories on the market, only orthodox theories meet this requirement. It might also be noted that (1) and (2) are repeated in just about every introductory macro textbook, again without to my knowledge any question of their consistency.\nWe can quickly see that these propositions are all consistent on either orthodox theory. The supervaluationist says there is a set of classical models for a language; a sentence is true iff it is true on all models, false iff it is false on all models, and truth-valueless otherwise. Vague terms have different meanings on different models. So for a particular good, say a car, about which it is vague whether it is an investment or consumption good, the supervaluationist says it is an investment good on some models and a consumption good on others. So (2) is satisfied; however on all models it, like everything else, is either a consumption or investment good, so (1) is satisfied. Similarly because it is vague whether some costs should be counted as deductions from the yield of an investment or increments to its carrying cost, the values of q and c will be different on different models. Hence (3) and (4) are true, but q - c is constant across models10, so (5) is true.\nThe epistemic theorist says that vagueness is just ignorance. As we can know that a car is an investment or consumption good without knowing which, (1) and (2) can be satisfied. Similarly, since we can know that a cost is incurred without knowing how to account for it in Keynes’s terms, we can know q - c precisely without knowing q or c precisely, and hence (3) to (5) can be satisfied.\nThe heterodox theorist has a harder time. The theorist who, following Russell (1923), says that vagueness is infectious, if a part is vague so is the whole, will deny that (1) and (2) can be true together. Unless it’s definitely true that a car is an investment or definitely true it’s a consumption good it can’t be definitely true that it’s one or the other. This also seems to be the position taken by Wittgenstein (1953).\nThe nihilist about vagueness, who follows Frege in saying vague terms can’t be used coherently, similarly can’t endorse both (1) and (2). On that view, if p and q are both vague, then their disjunction can’t be true. Arguably, on this position the disjunction of p with anything cant be true, as it is nonsense, but we don’t need anything that strong.11\nThe extra truth-values approach to vagueness (of which fuzzy logic is a variant) also can’t make (1) and (2) consistent. On any such approach (whether 3-valued, n-valued or continuum-valued) the degree of truth of a disjunction can’t be higher than the degree of truth of each of the disjuncts. So if neither ‘This is an investment’ nor ‘This is a consumption good’ is absolutely true (true to degree 1), ‘This is an investment or consumption good’ can’t be absolutely true. Yet this is just what Keynes asserted to be possible, and what several generations of readers have found perfectly consistent. I have only remarked about the problem the consistency of (1) and (2) poses for heterodox theories. These remarks apply, mutatis mutandis, to (3), (4) and (5), but as theorists rarely discuss quantitative vagueness (as opposed to truth-value vagueness) these cases involve a bit more speculation as to what heterodoxy says.\nHence Keynes did not belong to a heterodox tradition vis a vis vagueness, and heterodox theories fail to capture a crucial pre-theoretic intuition about vague terms. So Coates’s claims that Keynes followed Wittgenstein into heterodoxy here, and that he ought have, are both mistaken.\nEven if all of the above is mistaken, there remains serious doubt that Keynes had in mind anything like what Coates attributes to him. Coates makes the chapters on definitions in the GT into the foundations of a new philosophy, and constituting an important revolution in theory. This is crucial to Coates’s story about the influence of Wittgenstein on Keynes. But this attribution is totally at odds with Keynes’s comments on these chapters, comments that not only reveal his attitudes towards his definitions but also seem a fair commentary on them.\n\nI have felt that these chapters were a great drag on getting on to the real business, and would perplex the reader quite unnecessarily with a lot of points which really do not matter to my proper theme (Keynes to Roy Harrod, 9 August 1935, quoted in (Keynes 1971 XIII: 537)).\nBut the main point I would urge is that all this is not fundamental. Being clear is fundamental, but the choice of definitions of income and investment is not (Keynes to Dennis Robertson, 29 January 1935, quoted in (Keynes 1971 XIII, 495, italics in original)).\n\nKeynes on Rules and Private Language\nHad Keynes followed Wittgenstein in the ways suggested by either Bateman or Coates he would have been led into error. Fortunately he was not tempted. There was, however, one point on which Keynes clearly did not follow Wittgenstein, and sadly so for Wittgenstein was right. If Kripke (1982) is correct and this is the crucial point in the later Wittgenstein’s thinking, Keynes’s failure to observe it provides strong evidence that Wittgenstein’s influence on him was at best slight.\nKeynes, as we saw above, thought we dealt with uncertainty by assuming that the future would resemble the present. Call this Keynes’s maxim. But this, points out Wittgenstein, gets us nowhere. We know that the future will resemble the present; what we don’t know is how it will do so. Wittgenstein illustrates this with examples from mathematics and semantics, but we can apply it more broadly.\nSay that a particle in a one-dimensional Euclidean space is now at position d, travelling at velocity v under acceleration a. Assuming things stay the same, where will the particle be in 1 unit of time? This question simply can’t be answered, until we know what in what respect things will ‘stay the same.’ If it is in respect of position, the answer is d, in respect of velocity it is d + v, in respect of acceleration d + v + a/2. Perhaps our Newtonian intuitions make us prefer the second answer, perhaps not.\nThe same story applies in economics. When we assume things will stay the same, does that mean we are assuming the unemployment rate or the rate of change of the unemployment rate to be the same; real growth or nominal growth to be constant? At the level of the firm, we can ask whether Keynes’s maxim would have us assume real or nominal profits to be constant, or perhaps the growth rate of real or nominal profits, or perhaps sales figures (real or nominal, absolute or variation), or perhaps one of the variables which play a role like acceleration (rate of change of sales growth)? In some computing firms we might even take some of the logarithmic variables (growth of logarithm of sales) to be the constant. We can’t in consistency assume more than one or two of these variables to be unchanged, yet Keynes provides us with nothing to tell between them.\nMore importantly, it looks like Keynes hasn’t even seen the problem. The mechanical example above looks very similar to some of the paradoxes of indifference (TP: Ch. 4). For example, in von Kries’s cube factory example, we know that a factory makes cubes with side length between 0 and 2cm. If that’s all we know, what should we say is the probability that the next cube’s side length will be greater than 1cm? According to Laplace’s principle of indifference we should divide the probabilities equally between the possibilities, which seems to give an answer of 1/2. However we could have set out the problem by saying that the volume of cubes produced is between 0 and 8cm3 and we want to know the probability the volume of the next cube is greater than 1cm3. Now the answer (to the same problem) looks to be 7/8. And if we set out the problem in terms of surface area we seem to get the answer 3/4. The conclusion is that the principle of indifference could only be saved if we have a small designated set of predicates to which we can exclusively apply it. But now it seems Keynes’s maxim can only work if we have a small designated set of predicates to which we can exclusively apply it, and if we do that we can avoid the paradoxes of indifference. Keynes explicitly adopts his maxim to avoid the paradoxes of indifference (GT: 152). He would hardly have done this if he knew structurally similar problems beset the maxim as best the principle of indifference. As further evidence he just missed this point, note that while he was not averse to wielding philosophical tools in economic writing (like the paradoxes of indifference), Wittgenstein’s point is not mentioned; not in the GT, not in any of its drafts and not in any of the correspondence after it was published.\nFor Kripke, this point is central to Wittgenstein’s private language argument. All that we can know about the meaning of a word is how our community has used it in the past. We must assume they’ll use it the same way in the future. But what is to count as using it the same way? A priori it looks like any usage of a word could count; the only thing that could make usage of a word wrong is the user has a different way of using the word ‘the same way’ to everyone else. Hence if there is no community to set such standards there are no bars on how words can be used. And if there are no such bars, there is nothing that can properly be called a language. Hence there can’t be a private language.\nGiven the importance of that conclusion to Wittgenstein’s later philosophy, if Kripke is even close to right in his reconstruction then it is central to the later Wittgenstein that Keynes’s maxim is contentless. As Keynes clearly didn’t think this (witness the central role it plays in summaries of the GT like Keynes 1937) he hasn’t adopted a central tenet of the later Wittgenstein’s work. This puts a rather heavy burden on those who would say he became a Wittgensteinian. The arguments presented so far do nothing to lift that burden.\n\n\nAyer, Alfred. 1936. Language, Truth and Logic. London: Gollantz.\n\n\nBateman, Bradley. 1996. Keynes’s Uncertain Revolution. Ann Arbor: University of Michigan Press.\n\n\nBochvar, D. A. 1939. “On a Three Valued Calculus and Its Application to the Analysis of Contradictories.” Matematicheskii Sbornik 4 (2): 287–308. https://doi.org/10.2307/2269081.\n\n\nBorel, Emile. 1924. “A Propos d’un Traité de Probabilités.” Revue Philosophique 98: 321–36.\n\n\nBradford, Wylie, and Geoff Harcourt. 1997. “Definitions and Units.” In A ‘Second Edition’ of the General Theory, edited by G. C. Harcourt and P. A. Riach, 1:107–31. London: Routledge.\n\n\nCarnap, Rudolf. 1950. Logical Foundations of Probability. Chicago: University of Chicago Press.\n\n\nCoates, John. 1996. The Claims of Common Sense. Cambridge: Cambridge University Press.\n\n\n———. 1997. “Keynes, Vague Concepts and Fuzzy Logic.” In A ‘Second Edition’ of the General Theory, edited by G. C. Harcourt and P. A. Riach, 2:244–60. London: Routledge.\n\n\nDavis, John. 1994. Keynes’s Philosophical Development. Cambridge: Cambridge University Press.\n\n\n———. 1995. “Keynes’ Later Philosophy.” History of Political Economy 27 (2): 237–60. https://doi.org/10.1215/00182702-27-2-237.\n\n\nDummett, Michael. 1959. “Truth.” Proceedings of the Aristotelian Society 59 (1): 141–62. https://doi.org/10.1093/aristotelian/59.1.141.\n\n\nFine, Kit. 1975. “Vagueness, Truth and Logic.” Synthese 30 (3-4): 265–300. https://doi.org/10.1007/bf00485047.\n\n\nFraassen, Bas van. 1966. “Singular Terms, Truth–Value Gaps and Free Logic.” Journal of Philosophy 66 (17): 481–95. https://doi.org/10.2307/2024549.\n\n\nHaack, Susan. 1974. Deviant Logic. Chicago: University of Chicago Press.\n\n\nKeefe, Rosanna. 2000. Theories of Vagueness. Cambridge: Cambridge University Press.\n\n\nKeynes, John Maynard. 1909. “The Method of Index Numbers with Special Reference to the Measurement of General Exchange Value.” In The Collected Writings of John Maynard Keynes, XI:50–156. London: Macmillan.\n\n\n———. 1921. Treatise on Probability. London: Macmillan.\n\n\n———. 1931. “Review of Foundations of Mathematics by Frank Ramsey.” The New Statesman and Nation 2: 407.\n\n\n———. 1934. “Draft of the General Theory.” In The Collected Writings of John Maynard Keynes, by John Maynard Keynes, XIII:423–49. London: Macmillan.\n\n\n———. 1936. The General Theory of Employment, Interest and Money. London: Macmillan.\n\n\n———. 1937. “The General Theory of Employment.” Quarterly Journal of Economics 51 (2): 209–23. https://doi.org/10.2307/1882087.\n\n\n———. 1938a. “Letter to Hugh Townshend Dated 7 December.” In The Collected Writings of John Maynard Keynes, by John Maynard Keynes, 14:293–94. London: Macmillan.\n\n\n———. 1938b. “My Early Beliefs.” In The Collected Writings of John Maynard Keynes, X:433–51. London: Macmillan.\n\n\n———. 1971. The Collected Writings of John Maynard Keynes. London: Macmillan.\n\n\nKripke, Saul. 1975. “Outline of a Theory of Truth.” Journal of Philosophy 72 (19): 690–716. https://doi.org/10.2307/2024634.\n\n\n———. 1982. Wittgenstein on Rules and Private Language. Oxford: Basil Blackwell.\n\n\nLewis, David. 1973. Counterfactuals. Oxford: Blackwell Publishers.\n\n\nMoggridge, Donald. 1992. Maynard Keynes: An Economist’s Biography. London: Routledge.\n\n\nO’Donnell, Rod. 1989. Keynes: Philosophy, Economics and Politics. London: Macmillan.\n\n\n———. 1991. “Reply.” In Keynes as Philosopher-Economist, edited by Rod O’Donnell, 78–102. London: Macmillan.\n\n\n———. 1997. “Keynes and Formalism.” In A ‘Second Edition’ of the General Theory, edited by G. C. Harcourt and P. A. Riach, 2:131–65. London: Routledge.\n\n\nRamsey, Frank. 1929/1990. “Probability and Partial Belief.” In Philosophical Papers, edited by D. H. Mellor, 95–96. Cambridge University Press.\n\n\n———. 1926. “Truth and Probability.” In Philosophical Papers, edited by D. H. Mellor, 52–94. Cambridge: Cambridge University Press.\n\n\n———. 1931. The Foundations of Mathematics and Other Logical Essays. Edited by R. B. Braithwaite. London: Routledge.\n\n\nRussell, Bertrand. 1923. “Vagueness.” Australasian Journal of Philosophy and Psychology 1 (2): 84–92. https://doi.org/10.1080/00048402308540623.\n\n\n———. 1948. Human Knowledge: Its Scope and Limits. London: Allen; Unwin.\n\n\nSkidelsky, Robert. 1983. John Maynard Keynes. Vol. I: Hopes Betrayed, 1883-1920. London: Macmillan.\n\n\n———. 1992. John Maynard Keynes. Vol. II: The Economist as Saviour, 1920-1937. London: Macmillan.\n\n\nWang, Hao. 1987. Reflections on gôdel. Cambridge, MA: MIT Press.\n\n\nWilliamson, Timothy. 1994. Vagueness. Routledge.\n\n\nWittgenstein, Ludwig. 1953. Philosophical Investigations. London: Macmillan.\n\n\nDavis’s views are also set out in his (1995), and Coates’s to some extent in his (1997), but I will focus on the more detailed position in their respective books.↩︎\nIn the way that, for example, subjective Bayesianism was arguably invented by and eventually rejected by Ramsey. See his Ramsey (1926) and Ramsey (1929/1990).↩︎\nFor more details see Skidelsky (1983, 1992) or Moggridge (1992).↩︎\nAll page references in sections 2 and 3 (unless otherwise stated) to Bateman 1996. Space considerations preclude a detailed examination of Davis’s arguments, which are quite different to Bateman’s. However his conclusions are subject to the same criticisms I make of Bateman’s in section 3, and of Coates’s in section 5.↩︎\nThe above points are similar in all substantial respects to those made by O’Donnell (1991) in response to an earlier version of Bateman’s account.↩︎\nThis is often mistakenly referred to as an obituary in the literature, e.g. (Coates 1996, 139).↩︎\nIf Keynes had adopted a framework which implied a tight connection between epistemological and ethical norms, such as a form of utilitarianism that stressed maximisation of expected utility, this would be important, since he couldn’t change ethics and keep his epistemology. But such frameworks aren’t compulsory, and given the vehemence with which Keynes denounced utilitarianism (Keynes 1938b, 445) it seems he didn’t adopt one.↩︎\nAll page references in this section (unless otherwise stated) to Coates (1996).↩︎\nSee Williamson (1994) for the best epistemic account, Fine (1975) and Keefe (2000) for the best supervaluationist accounts.↩︎\nA particular cost will either remove an amount from q or add an equal amount to c, depending on how it is categorised.↩︎\nCompare the logic in Bochvar (1939), where p \\({\\vee}\\) q is truth-valueless if p is true and q truth-valueless. Summaries of this and many other many-valued logics are in Haack (1974).\n\n↩︎\n",
    "preview": {},
    "last_modified": "2021-03-13T10:43:18-05:00",
    "input_file": {}
  }
]
